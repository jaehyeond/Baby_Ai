No.,Year,Title,Title (KR),Abstract,Abstract (KR),URL,Research Area,Key Technology,Evaluation Method,Output Type
1,2025,"""Everyone Knows You're Watching P*rn"": Reflecting on First-Hand User Accounts on the Use and Public Perception of in-Transit Passenger XR","""당신이 P*rn을 보고 있다는 것을 모두가 알고 있습니다"": 환승 승객 XR의 사용 및 대중 인식에 대한 직접 사용자 계정 반영","AR/VR use in-transit is increasing due to the advantages it provides over more limited mobile and seatback displays for work and play. However, despite the now common support for “travel mode” in consumer XR headsets such as the Meta Quest 3 and Apple Vision Pro, little is known about how this emergent technology has been used in-practice by passengers - from why and how they use it, to what real-world challenges they face during usage in these very public, shared contexts. To address this gap, we contribute a novel large scale analysis of Reddit posts discussing first-hand use of VR/AR in-transit. We captured 11,150 posts, producing 437 user accounts which were synthesized into themes in relation to: in-the-wild use of immersive devices, users' positive/negative sentiment about usage, challenges they faced, and social interactions and impediments that occurred. Our snapshot of the current reality of passenger XR provides real-world insight evidencing the need for more passenger-centric designs of spatial interfaces, interaction techniques, and reality awareness.","업무 및 놀이를 위한 제한된 모바일 및 좌석 등받이 디스플레이에 비해 제공되는 이점으로 인해 이동 중 AR/VR 사용이 증가하고 있습니다. 그러나 Meta Quest 3 및 Apple Vision Pro와 같은 소비자 XR 헤드셋에서 ""여행 모드""에 대한 일반적인 지원에도 불구하고 승객이 실제로 이 신기술을 어떻게 사용했는지에 대해 알려진 바가 거의 없습니다. 이 기술을 사용하는 이유와 방법부터 매우 공개적이고 공유된 상황에서 사용하는 동안 직면하는 실제 문제에 이르기까지 말입니다. 이러한 격차를 해소하기 위해 우리는 이동 중 VR/AR의 직접적인 사용을 논의하는 Reddit 게시물에 대한 새로운 대규모 분석을 제공합니다. 우리는 11,150개의 게시물을 캡처하여 몰입형 장치의 실제 사용, 사용에 대한 사용자의 긍정적/부정적 감정, 직면한 문제, 사회적 상호 작용 및 발생한 장애와 관련된 주제로 종합된 437개의 사용자 계정을 생성했습니다. 승객 XR의 현재 현실에 대한 스냅샷은 공간 인터페이스, 상호 작용 기술 및 현실 인식에 대한 승객 중심 설계의 필요성을 입증하는 실제 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00066,Perception & Cognition; Collaboration & Social,Other,Other,Hardware / Device; Interaction Technique
2,2025,A BI-Directional Deep Learning Interface for Gaze-Controlled Wheelchair Navigation: Overcoming the Midas Touch Problem,시선 제어 휠체어 내비게이션을 위한 양방향 딥러닝 인터페이스: Midas Touch 문제 극복,"We present a gaze-based augmented reality control interface for electric wheelchairs, addressing the challenges faced by individuals with mobility impairments. The development transitions through three stages: model training with offline evaluation, Virtual Reality (VR) simulations, and physical deployment. First, we trained deep learning models, comparing Transformers and LSTMs, to predict locomotion intentions based on gaze data. While gaze predicts steering intentions well, it sometimes diverges from locomotion goals. To tackle this, we classify gaze movements as either indicative of locomotor intention or not. This novel approach addresses the Midas Touch Problem of gaze. Datasets were collected in controlled VR environments featuring different tasks. We find that data sets with tasks that encouraged diverse navigation and gaze behaviors enable strong generalization. The online VR simulation evaluation phase enabled safe and immersive testing, allowing the assessment of system performance and the integration of feedback for user guidance. Our approach provided smoother navigation control compared to traditional “Where-You-Look-Is-Where-You-Go” methods. Feedback improved user ratings of the system. In the final stage, the system was deployed on a physical wheelchair equipped with an augmented reality (AR) device to provide feedback about the predictions to the user, allowing real-world evaluation. Despite differences in user behavior between VR and physical environments, the system successfully translated gaze inputs into precise and safe navigation commands. Users were able to steer the wheelchair solely using their eyes while simultaneously being able to look at destinations at the side of the path.","우리는 이동 장애가 있는 개인이 직면한 문제를 해결하는 전동 휠체어용 시선 기반 증강 현실 제어 인터페이스를 제시합니다. 개발은 오프라인 평가를 통한 모델 훈련, 가상 현실(VR) 시뮬레이션, 물리적 배포의 세 단계를 통해 전환됩니다. 먼저, 시선 데이터를 기반으로 이동 의도를 예측하기 위해 Transformer와 LSTM을 비교하는 딥 러닝 모델을 훈련했습니다. 시선은 조종 의도를 잘 예측하지만 때로는 이동 목표와는 다릅니다. 이 문제를 해결하기 위해 우리는 시선 움직임을 운동 의도를 나타내는지 여부로 분류합니다. 이 새로운 접근 방식은 시선의 마이다스 터치 문제를 해결합니다. 데이터세트는 다양한 작업을 수행하는 통제된 VR 환경에서 수집되었습니다. 우리는 다양한 탐색 및 시선 행동을 장려하는 작업이 포함된 데이터 세트가 강력한 일반화를 가능하게 한다는 것을 발견했습니다. 온라인 VR 시뮬레이션 평가 단계를 통해 안전하고 몰입도 높은 테스트가 가능해졌으며, 시스템 성능을 평가하고 사용자 안내를 위한 피드백을 통합할 수 있었습니다. 우리의 접근 방식은 기존의 ""당신이 보는 곳이 어디인지"" 방법에 비해 더 부드러운 탐색 제어를 제공했습니다. 피드백은 시스템의 사용자 평가를 향상시켰습니다. 마지막 단계에서는 증강 현실(AR) 장치가 장착된 실제 휠체어에 시스템을 배포하여 사용자에게 예측에 대한 피드백을 제공함으로써 실제 평가가 가능하도록 했습니다. VR과 물리적 환경 간의 사용자 행동 차이에도 불구하고 시스템은 시선 입력을 정확하고 안전한 탐색 명령으로 성공적으로 변환했습니다. 사용자는 눈으로만 휠체어를 조종하는 동시에 길가에 있는 목적지를 볼 수 있었다.",https://doi.org/10.1109/ISMAR67309.2025.00095,Interaction & Input; Education & Training,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method
3,2025,A Comprehensive Analysis of the Influence of Cognitive Load on Physiological Signals in Virtual Reality,가상 현실에서 인지 부하가 ​​생리적 신호에 미치는 영향에 대한 종합 분석,"The study of cognitive load (CL) has been an active field of research across disciplines such as psychology, education, and computer graphics and visualization for decades. In the context of Virtual Reality (VR), understanding mental demand becomes particularly relevant, as immersive experiences increasingly integrate multisensory stimuli that require users to distribute their limited cognitive resources. In this work, we investigate the effects of cognitive load during a search task in VR, combining objective and subjective measurements, including physiological signals and validated questionnaires. We designed an experiment in which participants performed a visual search task under two cognitive load conditions (either alone or while responding to a concurrent auditory task) and across two visual search areas (90° and 360°). We collected a rich dataset comprising task performance, eye tracking, electrocardiogram (ECG), electrodermal activity (EDA), photoplethysmography (PPG), and inertial measurements, along with subjective assessments (NASA-TLX questionnaires). Our analysis shows that increased cognitive load hinders visual search performance and affects multiple physiological markers, offering a solid foundation for future research on cognitive load in multisensory virtual environments.","인지 부하(CL)에 대한 연구는 수십 년 동안 심리학, 교육, 컴퓨터 그래픽 및 시각화와 같은 분야에 걸쳐 활발한 연구 분야였습니다. 가상 현실(VR)의 맥락에서 몰입형 경험은 사용자가 제한된 인지 자원을 분배하도록 요구하는 다감각 자극을 점점 더 통합하기 때문에 정신적 요구를 이해하는 것이 특히 중요합니다. 이 연구에서는 생리학적 신호와 검증된 설문지를 포함하여 객관적이고 주관적인 측정을 결합하여 VR에서 검색 작업 중 인지 부하의 영향을 조사합니다. 우리는 참가자들이 두 가지 인지 부하 조건(단독으로 또는 동시 청각 작업에 응답하는 동안)과 두 가지 시각적 검색 영역(90° 및 360°)에서 시각적 검색 작업을 수행하는 실험을 설계했습니다. 우리는 주관적 평가(NASA-TLX 설문지)와 함께 작업 성능, 안구 추적, 심전도(ECG), 피부 전기 활동(EDA), 광용적맥파(PPG) 및 관성 측정으로 구성된 풍부한 데이터 세트를 수집했습니다. 우리의 분석에 따르면 인지 부하의 증가는 시각적 검색 성능을 방해하고 여러 생리학적 지표에 영향을 미쳐 다감각 가상 환경의 인지 부하에 대한 향후 연구를 위한 견고한 기반을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00054,Tracking & Localization; Interaction & Input,Sensor Fusion,Questionnaire / Survey,User Study / Empirical Findings
4,2025,A Multi-Sensor Approach for Cognitive Load Assessment in Mobile Augmented Reality,모바일 증강 현실의 인지 부하 평가를 위한 다중 센서 접근 방식,"Augmented reality displays are becoming more powerful and simultaneously more mobile. Although mobile AR is gaining popularity, it remains difficult to get an insight into users' cognitive load, despite its relevance for many mobile-based tasks. Usually, cognitive load is measured via subjective, task-disruptive self-reports such as NASA TLX. While biosensors such as galvanic skin response, heart rate variability, or pulse have been used to obtain more objective measures, these are highly susceptible to motion-induced noise. More robust techniques like EEG offer higher reliability but are impractical for mobile, real-world use. In this paper, we report on a non-contact multi-sensor approach to assess cognitive load in mobile AR. Our approach combines pupillometry, facial expression tracking, and thermal imaging for respiratory rate analysis. Within the frame of our study, we analysed the aptness of the methods, comparing load assessment for low and high cognitive load tasks under both stationary and mobile conditions. Using an XGBoost classifier, our model achieved 86.11% accuracy for binary cognitive load assessment (low vs. high cognitive load) and 84.24% accuracy for four-way classification (cognitive load $\times$ mobility). Feature importance analysis revealed that robust predictors included gaze dynamics (e.g., fixation, pursuit, and saccade durations), pupil diameter metrics (such as FFT band power and variability measures), and facial and respiratory features (including brow lowering and nostril temperature quantiles) for assessing cognitive load in mobile AR.","증강 현실 디스플레이는 더욱 강력해지고 있으며 동시에 더욱 모바일화되고 있습니다. 모바일 AR이 인기를 얻고 있지만, 많은 모바일 기반 작업과의 관련성에도 불구하고 사용자의 인지 부하에 대한 통찰력을 얻는 것은 여전히 ​​어렵습니다. 일반적으로 인지 부하는 NASA TLX와 같은 주관적이고 업무를 방해하는 자가 보고를 통해 측정됩니다. 갈바닉 피부 반응, 심박수 변화 또는 맥박과 같은 바이오 센서는 보다 객관적인 측정을 얻기 위해 사용되었지만 동작으로 인한 소음에 매우 취약합니다. EEG와 같은 보다 강력한 기술은 더 높은 신뢰성을 제공하지만 모바일 실제 사용에는 실용적이지 않습니다. 본 논문에서는 모바일 AR의 인지 부하를 평가하기 위한 비접촉식 다중 센서 접근 방식에 대해 보고합니다. 우리의 접근 방식은 호흡률 분석을 위해 동공 측정, 얼굴 표정 추적 및 열 화상을 결합합니다. 우리 연구의 틀 내에서 우리는 고정 및 이동 조건 모두에서 낮은 인지 부하 작업과 높은 인지 부하 작업에 대한 부하 평가를 비교하여 방법의 적합성을 분석했습니다. XGBoost 분류기를 사용하여 우리 모델은 이진 인지 부하 평가(낮은 인지 부하 대 높은 인지 부하)에서 86.11%의 정확도를 달성했고 4방향 분류(인지 부하 $\times$ 이동성)에서는 84.24%의 정확도를 달성했습니다. 기능 중요도 분석을 통해 모바일 AR의 인지 부하를 평가하기 위한 강력한 예측 변수에는 시선 역학(예: 시선 고정, 추적 및 단속적 지속 시간), 동공 직경 측정 항목(예: FFT 밴드 파워 및 변동성 측정), 얼굴 및 호흡 기능(눈썹 내리기 및 콧구멍 온도 분위수 포함)이 포함되어 있는 것으로 나타났습니다.",https://doi.org/10.1109/ISMAR67309.2025.00033,Interaction & Input,Eye / Gaze Tracking; Sensor Fusion,Questionnaire / Survey,Algorithm / Method
5,2025,A Study of Multimodal Pen + Gaze Interaction Techniques for Shape Point Translation in Extended Reality,확장 현실에서 형상점 변환을 위한 멀티모달 펜 + 시선 상호작용 기법에 관한 연구,"Eye-tracking offers new ways to augment our interaction possibilities in extended reality. This paper investigates how gaze can assist pen users in translating shape points within graphical models. By leveraging gaze, we can support the usual design activities with an option where objects can be selected and repositioned through eye movements, with the pen serving as a confirmation tool. This can reduce manual effort and enhance efficiency and ergonomics. To evaluate its effectiveness, we compare four interaction techniques: two pen-based baselines (direct and ray-based) and two gaze-supported methods (gaze for selection and/or object dragging), using a probability based selection scheme. In a user study, 16 participants carried out a shape point translation task and their performance, effort, and user experience were measured. The results highlight the performance trade-offs of each technique—while the gaze-based dragging method introduced marginally more errors, it significantly reduced task time. Our findings offer comparative insights into the strength and limitations of gaze-and pen-based interaction methods, supporting the design of future multimodal 3D design tools.","시선 추적은 확장된 현실에서 상호 작용 가능성을 높이는 새로운 방법을 제공합니다. 이 문서에서는 펜 사용자가 그래픽 모델 내에서 모양 점을 변환하는 데 시선이 어떻게 도움이 될 수 있는지 조사합니다. 시선을 활용하면 펜을 확인 도구로 사용하여 눈 움직임을 통해 개체를 선택하고 위치를 변경할 수 있는 옵션을 통해 일반적인 디자인 활동을 지원할 수 있습니다. 이를 통해 수동 작업을 줄이고 효율성과 인체공학성을 향상할 수 있습니다. 효율성을 평가하기 위해 확률 기반 선택 체계를 사용하여 두 가지 펜 기반 기준선(직접 및 광선 기반)과 두 가지 시선 지원 방법(선택 및/또는 개체 드래그를 위한 시선)의 네 가지 상호 작용 기술을 비교합니다. 사용자 연구에서는 16명의 참가자가 형상점 변환 작업을 수행하고 그들의 성과, 노력 및 사용자 경험을 측정했습니다. 결과는 각 기술의 성능 상충관계를 강조합니다. 시선 기반 드래그 방법은 약간 더 많은 오류를 발생시켰지만 작업 시간을 크게 줄였습니다. 우리의 연구 결과는 시선 및 펜 기반 상호 작용 방법의 강점과 한계에 대한 비교 통찰력을 제공하여 미래의 다중 모드 3D 디자인 도구의 설계를 지원합니다.",https://doi.org/10.1109/ISMAR67309.2025.00056,Interaction & Input,Other,User Study,Algorithm / Method; User Study / Empirical Findings
6,2025,AR Surgical Navigation with Surface Tracing: Comparing In-Situ Visualization with Tool-Tracking Guidance for Neurosurgical Applications,표면 추적을 통한 AR 수술 내비게이션: 신경외과 응용 분야에 대한 도구 추적 지침과 현장 시각화 비교,"Augmented Reality (AR) surgical navigation systems are emerging as the next generation of intraoperative surgical guidance, promising to overcome limitations of traditional navigation systems. However, known issues with AR depth perception due to vergenceaccommodation conflict and occlusion handling limitations of the currently commercially available display technology present acute challenges in surgical settings where precision is paramount. This study presents a novel methodology for utilizing AR guidance to register anatomical targets and provide real-time instrument navigation using placement of simulated external ventricular drain catheters on a phantom model as the clinical scenario. The system registers target positions to the patient through a novel surface tracing method and uses real-time infrared tool tracking to aid in catheter placement, relying only on the onboard sensors of the Microsoft HoloLens 2. A group of intended users performed the procedure of simulated insertions under two AR guidance conditions: static in-situ visualization, where planned trajectories are overlaid directly onto the patient anatomy, and real-time tool-tracking guidance, where live feedback of the catheter's pose is provided relative to the plan. Following the insertion tests, computed tomography scans of the phantom models were acquired, allowing for evaluation of insertion accuracy, target deviation, angular error, and depth precision. System Usability Scale surveys assessed user experience and cognitive workload. Tool-tracking guidance improved performance metrics across all accuracy measures and was preferred by users in subjective evaluations.","증강 현실(AR) 수술 내비게이션 시스템은 차세대 수술 중 수술 안내로 떠오르고 있으며 기존 내비게이션 시스템의 한계를 극복할 것으로 기대됩니다. 그러나 현재 상업적으로 이용 가능한 디스플레이 기술의 vergenceaccommodation 충돌 및 폐색 처리 제한으로 인한 AR 깊이 인식과 관련된 알려진 문제는 정밀도가 가장 중요한 수술 환경에서 심각한 문제를 제시합니다. 이 연구는 AR 안내를 활용하여 해부학적 목표를 등록하고 임상 시나리오로 팬텀 모델에 시뮬레이션된 외부 심실 배수 카테터 배치를 사용하여 실시간 기기 탐색을 제공하는 새로운 방법론을 제시합니다. 이 시스템은 새로운 표면 추적 방법을 통해 환자에게 목표 위치를 등록하고 실시간 적외선 도구 추적을 사용하여 Microsoft HoloLens 2의 온보드 센서에만 의존하여 카테터 배치를 지원합니다. 의도된 사용자 그룹은 두 가지 AR 안내 조건 하에서 시뮬레이션된 삽입 절차를 수행했습니다. 즉, 계획된 궤적이 환자 해부학에 직접 오버레이되는 정적 현장 시각화와 카테터 자세의 실시간 피드백이 이루어지는 실시간 도구 추적 안내입니다. 계획과 관련하여 제공됩니다. 삽입 테스트 후에 팬텀 모델의 컴퓨터 단층 촬영 스캔을 획득하여 삽입 정확도, 목표 편차, 각도 오류 및 깊이 정밀도를 평가할 수 있습니다. 시스템 사용성 규모 설문조사에서는 사용자 경험과 인지 작업 부하를 평가했습니다. 도구 추적 지침은 모든 정확도 측정에 걸쳐 성능 지표를 개선했으며 주관적 평가에서 사용자가 선호했습니다.",https://doi.org/10.1109/ISMAR67309.2025.00123,Medical & Healthcare; Display & Optics,Sensor Fusion,Quantitative Experiment,Algorithm / Method; Hardware / Device
7,2025,Adaptive Hand Visibility for Accurate 3D User Interactions in Virtual Environments,가상 환경에서 정확한 3D 사용자 상호 작용을 위한 적응형 손 가시성,"Hand visualization significantly impacts user performance in virtual reality (VR), particularly in tasks requiring precise finger movements. Common hand avatar visualizations, such as opaque or transparent, often occlude critical elements or distract users, potentially reducing accuracy. To address this issue, we investigated adaptive hand visibility techniques, which vary the hand avatar visibility based on the current movement sub-task aiming to enhance user performance. We evaluated these techniques on a VR-based Pedicle Screw Placement task with 15 participants. Each participant inserted virtual screws into a spine using five hand visualization conditions: opaque, transparent, invisible, speed-based visibility (hand visibility changes with hand speed), and position-based visibility (hand visibility adapts to the proximity of critical elements). Results showed that speed-based and position-based visualizations significantly improved accuracy, usability, and overall performance compared to traditional methods. The widely adopted opaque visualization yielded the lowest accuracy and usability. Our findings emphasize the benefits of adaptive hand visualization based on sub-tasks, recommending its implementation in VR applications to increase usability and user accuracy.","손 시각화는 가상 현실(VR), 특히 정확한 손가락 움직임이 필요한 작업에서 사용자 성능에 큰 영향을 미칩니다. 불투명 또는 투명과 같은 일반적인 손 아바타 시각화는 중요한 요소를 가리거나 사용자의 주의를 산만하게 하여 잠재적으로 정확도를 떨어뜨리는 경우가 많습니다. 이 문제를 해결하기 위해 우리는 사용자 성능 향상을 목표로 현재 움직임 하위 작업을 기반으로 손 아바타 가시성을 변화시키는 적응형 손 가시성 기술을 조사했습니다. 우리는 15명의 참가자가 참여한 VR 기반 척추경 나사 배치 작업에서 이러한 기술을 평가했습니다. 각 참가자는 불투명, 투명, 보이지 않음, 속도 기반 가시성(손 속도에 따라 손 가시성이 변경됨) 및 위치 기반 가시성(손 가시성이 중요한 요소의 근접성에 따라 조정됨)의 5가지 손 시각화 조건을 사용하여 척추에 가상 나사를 삽입했습니다. 결과에 따르면 속도 기반 및 위치 기반 시각화는 기존 방법에 비해 정확성, 유용성 및 전반적인 성능을 크게 향상시키는 것으로 나타났습니다. 널리 채택된 불투명 시각화는 정확도와 유용성이 가장 낮았습니다. 우리의 연구 결과는 하위 작업을 기반으로 한 적응형 손 시각화의 이점을 강조하며 VR 애플리케이션에서의 구현을 권장하여 유용성과 사용자 정확성을 높입니다.",https://doi.org/10.1109/ISMAR67309.2025.00015,Interaction & Input,Other,User Study,Algorithm / Method
8,2025,An Embodied AR Navigation Agent: Integrating BIM with Retrieval-Augmented Generation for Language Guidance,구현된 AR 내비게이션 에이전트: 언어 안내를 위한 검색 증강 생성과 BIM 통합,"Delivering intelligent and adaptive navigation assistance in augmented reality (AR) requires more than visual cues, as it demands systems capable of interpreting flexible user intent and reasoning over both spatial and semantic context. Prior AR navigation systems often rely on rigid input schemes or predefined commands, which limit the utility of rich building data and hinder natural interaction. In this work, we propose an embodied AR navigation system that integrates Building Information Modeling (BIM) with a multi-agent retrieval-augmented generation (RAG) framework to support flexible, language-driven goal retrieval and route planning. The system orchestrates three language agents, Triage, Search, and Response, built on large language models (LLMs), which enables robust interpretation of open-ended queries and spatial reasoning using BIM data. Navigation guidance is delivered through an embodied AR agent, equipped with voice interaction and locomotion, to enhance user experience. A real-world user study yields a System Usability Scale (SUS) score of 80.5, indicating excellent usability, and comparative evaluations show that the embodied interface can significantly improves users' perception of system intelligence. These results underscore the importance and potential of language-grounded reasoning and embodiment in the design of user-centered AR navigation systems. Video demonstrations are available at https://woven-visionai.github.io/ar-navigation-agent-project.","증강 현실(AR)에서 지능형 및 적응형 내비게이션 지원을 제공하려면 시각적 단서 이상의 것이 필요합니다. 유연한 사용자 의도를 해석하고 공간적, 의미적 맥락 모두에 대해 추론할 수 있는 시스템이 필요하기 때문입니다. 이전 AR 내비게이션 시스템은 종종 엄격한 입력 방식이나 사전 정의된 명령에 의존하여 풍부한 건물 데이터의 유용성을 제한하고 자연스러운 상호 작용을 방해합니다. 본 연구에서는 유연한 언어 기반 목표 검색 및 경로 계획을 지원하기 위해 BIM(Building Information Modeling)과 다중 에이전트 RAG(Retrieval-Augmented Generation) 프레임워크를 통합한 구현된 AR 내비게이션 시스템을 제안합니다. 이 시스템은 BIM 데이터를 사용하여 개방형 쿼리 및 공간 추론을 강력하게 해석할 수 있는 LLM(대형 언어 모델)을 기반으로 구축된 Triage, Search 및 Response의 세 가지 언어 에이전트를 조율합니다. 내비게이션 안내는 음성 상호작용과 이동 기능을 갖춘 내장된 AR 에이전트를 통해 전달되어 사용자 경험을 향상시킵니다. 실제 사용자 연구에서는 SUS(System Usability Scale) 점수가 80.5로 우수한 유용성을 나타냈으며, 비교 평가에서는 구현된 인터페이스가 시스템 인텔리전스에 대한 사용자의 인식을 크게 향상시킬 수 있음을 보여줍니다. 이러한 결과는 사용자 중심 AR 내비게이션 시스템 설계에 있어 언어 기반 추론과 구현의 중요성과 잠재력을 강조합니다. 비디오 데모는 https://짠-visionai.github.io/ar-navigation-agent-project에서 볼 수 있습니다.",https://doi.org/10.1109/ISMAR67309.2025.00150,Interaction & Input; Perception & Cognition,Deep Learning / Neural Networks; Natural Language Processing,User Study,System / Framework
9,2025,An Evaluation of Movement Data Analysis Techniques for Virtual Reality,가상현실을 위한 움직임 데이터 분석 기법의 평가,"Researchers frequently collect position and orientation data from tracking hardware during virtual reality research studies. These data capture important information about participants' movements during experiments, but often result in large and complex datasets that can be challenging to analyze and interpret. We explore the potential of standard statistical measures that could be used to interpret position and orientation tracking data, and discuss advantages and uses of each measure. We further present three new measurement techniques - 95% range, time spent at mode, and coefficient of determination. We then evaluate the effectiveness of each statistical measure to detect differences in position and orientation on two existing data sets. We find that all of the tested measures are effective in discerning known main effects in position and orientation with varying degrees of sensitivity. This work is intended to guide researchers on future analysis and interpretation of motion tracking data sets.","연구원들은 가상 현실 연구 중에 추적 하드웨어로부터 위치 및 방향 데이터를 자주 수집합니다. 이러한 데이터는 실험 중 참가자의 움직임에 대한 중요한 정보를 캡처하지만 종종 분석하고 해석하기 어려울 수 있는 크고 복잡한 데이터 세트가 생성됩니다. 우리는 위치 및 방향 추적 데이터를 해석하는 데 사용할 수 있는 표준 통계 측정의 잠재력을 탐색하고 각 측정의 장점과 용도에 대해 논의합니다. 우리는 또한 95% 범위, 모드에서 소요된 시간, 결정 계수라는 세 가지 새로운 측정 기술을 제시합니다. 그런 다음 두 개의 기존 데이터 세트에서 위치와 방향의 차이를 감지하기 위해 각 통계 측정의 효율성을 평가합니다. 우리는 테스트된 모든 측정값이 다양한 민감도 수준으로 위치와 방향에서 알려진 주요 효과를 식별하는 데 효과적이라는 것을 발견했습니다. 이 작업은 모션 추적 데이터 세트의 향후 분석 및 해석에 대해 연구자에게 안내하기 위한 것입니다.",https://doi.org/10.1109/ISMAR67309.2025.00160,Tracking & Localization,Other,User Study,Algorithm / Method
10,2025,"An Open Testbed for Mixed Reality Precise Rotation Guidance: Comparative Case Study of Arrow, Gestalt and Magnifier Cues","혼합 현실의 정확한 회전 안내를 위한 공개 테스트베드: 화살표, 게슈탈트 및 돋보기 단서의 비교 사례 연구","A wide range of tools, machines from diverse industries are operated by the precise rotation of physical devices (e.g., knobs, encoders). Literature proved that Mixed Reality can provide rotation guidance by superimposing digital cues onto the real environment. However, existing studies primarily focus on speed rather than precision and lack proven solutions with standard comparison methods. We presented a novel open MR testbed to evaluate MR rotation guidance cue designs. It comprises (i) a 3D-printed structure with a controller mount for MR cue tracking, (ii) an industrial encoder (resolution 1.8º), (iii) an Esp32 microcontroller, (iv) a firmware for USB connection, and (v) a USB foot pedal to confirm rotation, (vi) test software and documentation. The testbed uses available off-the-shelf components, and it is distributed with an open source license, Unity package with cues examples and scripts for running tests with randomized targets and collecting performance data (error count and speed). We evaluated the MR testbed by comparing the Arrow cue (baseline) and two novel designs: Magnifier with a zoomed region and Gestalt leveraging related perception principles. A within-subjects study ($\mathrm{N}=36,180^{\prime \prime}, 18$ ọ, $\mathrm{M}=27.58, \pm 3.55$) showed that the Magnifier is the most precise ($p<.001$), while the Gestalt is the fastest ($p<.001$). Moreover, males (higher motor and gaming skills) ($\mathrm{p}<.002, \mathrm{t}=3.89$) resulted in faster ($\mathrm{r}=-0.458, \mathrm{p}<$. 005). Participants who declared with higher gaming skills demonstrated less task load during the experiment ($\mathrm{r}=+0.548, \mathrm{p}<.005$). Overall, the participants slightly preferred the Gestalt (18) over the Magnifier (15), while the Arrow was selected by a few (3). Ultimately, the testbed has proven to be an effective research and evaluation platform for future mixed reality interfaces.","다양한 산업의 다양한 도구, 기계는 물리적 장치(예: 손잡이, 인코더)의 정확한 회전을 통해 작동됩니다. 문헌에서는 혼합 현실이 실제 환경에 디지털 신호를 겹쳐서 회전 안내를 제공할 수 있음을 입증했습니다. 그러나 기존 연구는 주로 정밀도보다는 속도에 중점을 두고 있으며 표준 비교 방법을 갖춘 입증된 솔루션이 부족합니다. 우리는 MR 회전 안내 큐 디자인을 평가하기 위해 새로운 개방형 MR 테스트베드를 제시했습니다. 이는 (i) MR 큐 추적을 위한 컨트롤러 마운트가 있는 3D 인쇄 구조, (ii) 산업용 인코더(해상도 1.8°), (iii) Esp32 마이크로컨트롤러, (iv) USB 연결용 펌웨어, (v) 회전 확인을 위한 USB 풋 페달, (vi) 테스트 소프트웨어 및 문서로 구성됩니다. 테스트베드는 기성 구성 요소를 사용하며 오픈 소스 라이선스, 큐 예제가 포함된 Unity 패키지, 무작위 대상으로 테스트를 실행하고 성능 데이터(오류 수 및 속도)를 수집하기 위한 스크립트와 함께 배포됩니다. 우리는 화살표 큐(기준선)와 두 가지 새로운 디자인(확대된 영역이 있는 돋보기 및 관련 인식 원리를 활용하는 게슈탈트)을 비교하여 MR 테스트베드를 평가했습니다. 피험자 내 연구($\mathrm{N}=36,180^{\prime \prime}, 18$ ọ, $\mathrm{M}=27.58, \pm 3.55$)에 따르면 돋보기가 가장 정확하고($p<.001$), 게슈탈트가 가장 빠릅니다($p<.001$). 더욱이, 남성(운동 및 게임 능력이 높음)($\mathrm{p}<.002, \mathrm{t}=3.89$)은 더 빠른 결과를 보였습니다($\mathrm{r}=-0.458, \mathrm{p}<$.005). 게임 실력이 더 높다고 선언한 참가자는 실험 동안 작업 부하가 더 적은 것으로 나타났습니다($\mathrm{r}=+0.548, \mathrm{p}<.005$). 전반적으로 참가자들은 돋보기(15)보다 게슈탈트(18)를 약간 선호한 반면, 소수(3)는 화살표를 선택했습니다. 궁극적으로 테스트베드는 미래 혼합 현실 인터페이스를 위한 효과적인 연구 및 평가 플랫폼임이 입증되었습니다.",https://doi.org/10.1109/ISMAR67309.2025.00037,Interaction & Input,Deep Learning / Neural Networks,User Study; Technical Evaluation,System / Framework
11,2025,AniDream: Generating Skeleton-Guided Anime Avatars from Text Prompts,AniDream: 텍스트 프롬프트에서 뼈대 안내 애니메이션 아바타 생성,"Generating high-quality anime avatars has become an increasingly important task in the fields of animation, gaming, and virtual reality. However, existing frameworks often face challenges in achieving anatomical consistency and mitigating visual artifacts. To address these limitations, we introduce AniDream, a novel framework designed for the generation of high-quality anime avatars. Unlike previous approaches that primarily relied on image-based inputs, AniDream incorporates text-guided generation, allowing users to create diverse anime avatars directly from text prompts. AniDream uses a skeleton-guided approach, ensuring anatomical consistency while focusing on refining attention around key regions. Our framework introduces a novel loss function that simulates a cel-shading effect and encourages the generated avatars to maintain sharp contour definitions and shadowing consistent with anime aesthetics. Experiments show that AniDream outperforms other frameworks by reducing artifacts and maintaining visual consistency across various poses and viewpoints. It also achieves an average CLIPScore of 33.07, demonstrating its effectiveness in closely aligning generated avatars with text prompts.","고품질 애니메이션 아바타를 생성하는 것은 애니메이션, 게임 및 가상 현실 분야에서 점점 더 중요한 작업이 되었습니다. 그러나 기존 프레임워크는 해부학적 일관성을 달성하고 시각적 인공물을 완화하는 데 종종 어려움을 겪습니다. 이러한 한계를 해결하기 위해 고품질 애니메이션 아바타 생성을 위해 설계된 새로운 프레임워크인 AniDream을 소개합니다. 주로 이미지 기반 입력에 의존했던 이전 접근 방식과 달리 AniDream은 텍스트 기반 생성을 통합하여 사용자가 텍스트 프롬프트에서 직접 다양한 애니메이션 아바타를 만들 수 있도록 합니다. AniDream은 골격 기반 접근 방식을 사용하여 해부학적 일관성을 보장하는 동시에 주요 부위에 대한 주의를 집중시키는 데 중점을 둡니다. 우리의 프레임워크는 셀 셰이딩 효과를 시뮬레이션하고 생성된 아바타가 애니메이션 미학과 일치하는 선명한 윤곽 정의와 섀도우를 유지하도록 장려하는 새로운 손실 함수를 도입합니다. 실험에 따르면 AniDream은 아티팩트를 줄이고 다양한 포즈와 관점에서 시각적 일관성을 유지함으로써 다른 프레임워크보다 성능이 뛰어난 것으로 나타났습니다. 또한 평균 CLIPS점수 33.07을 달성하여 생성된 아바타를 텍스트 프롬프트와 밀접하게 정렬하는 데 있어 효율성을 입증합니다.",https://doi.org/10.1109/ISMAR67309.2025.00111,Interaction & Input; Perception & Cognition,Sensor Fusion,Other,System / Framework
12,2025,Are You Empathizing with Me? Exploring External Expressions of Empathy in Interpersonal VR Communication,당신은 나와 공감하고 있습니까? 대인 VR 커뮤니케이션에서 공감의 외부 표현 탐색,"Empathy is central to social interaction, yet how it is externally expressed in virtual reality (VR) communication remains underexplored. In this study, we examined how directionality-aware cues of empathy, such as mimicry, eye contact, and body proximity, relate to cognitive and emotional empathy. We designed high- and low-empathy scenarios and recruited participants with acting experience to ensure clear emotional expressions. Our findings indicate that facial mimicry patterns differ by empathy type: cognitive empathy involves subtle, speech-related muscle movements, whereas emotional empathy is associated with more intense affective expressions. Interestingly, we also found that while facial expressions and lower-body mimicry tend to emerge unconsciously, upper-body mimicry occurs more consciously, suggesting distinct pathways of empathic embodiment. We also observed that vocal intensity mimicry and pitch variability serve as important indicators of empathy, and a consistent hand approach is closely linked to empathy. Additionally, emotional empathy fosters longer eye contact, whereas cognitive empathy stabilizes gaze and head movements. Finally, we constructed machine learning models to predict empathy from these external expressions. Our best classifier achieved an accuracy of 0.756 for cognitive empathy and 0.704 for emotional empathy, indicating the feasibility of objective assessment. These findings provide a deeper understanding of how empathy is manifested in VR communication and support the development of empathy-aware virtual agents and training systems.","공감은 사회적 상호 작용의 핵심이지만, 가상 현실(VR) 커뮤니케이션에서 공감이 외부적으로 어떻게 표현되는지는 여전히 연구가 부족합니다. 본 연구에서 우리는 흉내, 눈맞춤, 신체 근접성과 같은 방향 인식 공감 단서가 인지적, 감정적 공감과 어떻게 관련되는지 조사했습니다. 공감도가 높은 시나리오와 낮은 시나리오를 디자인하고, 연기 경험이 있는 참가자를 모집해 명확한 감정 표현이 가능하도록 했습니다. 우리의 연구 결과에 따르면 얼굴 모방 패턴은 공감 유형에 따라 다릅니다. 인지적 공감은 미묘한 언어 관련 근육 움직임을 포함하는 반면, 정서적 공감은 보다 강렬한 정서적 표현과 관련이 있습니다. 흥미롭게도 우리는 얼굴 표정과 하체 모방이 무의식적으로 나타나는 경향이 있는 반면, 상체 모방은 더 의식적으로 발생하여 공감 구체화의 뚜렷한 경로를 제안한다는 사실도 발견했습니다. 우리는 또한 목소리의 강도 모방과 음조 변화가 공감의 중요한 지표로 작용하고 일관된 손 접근 방식이 공감과 밀접하게 연관되어 있음을 관찰했습니다. 또한, 정서적 공감은 더 긴 눈맞춤을 촉진하는 반면, 인지적 공감은 시선과 머리 움직임을 안정시킵니다. 마지막으로 우리는 이러한 외부 표현으로부터 공감을 예측하기 위해 기계 학습 모델을 구축했습니다. 우리의 최고의 분류기는 인지적 공감의 경우 0.756, 정서적 공감의 경우 0.704의 정확도를 달성하여 객관적인 평가의 타당성을 나타냅니다. 이러한 연구 결과는 VR 커뮤니케이션에서 공감이 어떻게 나타나는지에 대한 더 깊은 이해를 제공하고 공감 인식 가상 에이전트 및 교육 시스템의 개발을 지원합니다.",https://doi.org/10.1109/ISMAR67309.2025.00143,Interaction & Input; Perception & Cognition,Other,User Study,User Study / Empirical Findings; Algorithm / Method
13,2025,Artspeak: An Interactive AR Application for Lifelike Speaking with Art Portraits,Artspeak: 예술 초상화를 통해 실제와 같은 말하기를 위한 대화형 AR 애플리케이션,"Museum visits often lack personalized and interactive experiences, limiting visitor engagement with art and historical artifacts. To address this, we present ArtSpeak, a standalone augmented reality (AR) application that transforms traditional art viewing into an interactive storytelling experience. When users point their mobile cameras at an artwork, the system responds to their questions with lifelike, talking-head video narratives generated from historical portraits. However, generating such talking-head videos at runtime is computationally expensive, often requiring over a minute per response. To address this challenge, ArtSpeak introduces two major contributions. First, it employs a collection of frequently asked questions (FAQ) to generate a set of lifelike video responses for various art portraits. Second, it introduces a novel retrieval-based approach that uses GPT-based embeddings and cosine similarity to select the most relevant response. As a result, the system dynamically presents the video reply that best aligns with the user's inquiry, reducing computational overhead and ensuring a real-time, low-latency experience. More precisely, ArtSpeak achieves over 30 x lower latency and reduces energy consumption by approximately 81 % compared to the real-time video generation method. User studies further validate the system's effectiveness, with 85 % of participants rating the retrieved responses as relevant to their queries and 90 % reporting smooth video playback. These results highlight the efficiency and user satisfaction enabled by our retrieval-based approach.","박물관 방문에는 개인화되고 상호 작용적인 경험이 부족한 경우가 많으며, 이는 방문객의 예술 및 역사적 유물에 대한 참여를 제한합니다. 이 문제를 해결하기 위해 우리는 전통적인 예술 감상을 대화형 스토리텔링 경험으로 전환하는 독립형 증강 현실(AR) 애플리케이션인 ArtSpeak를 선보입니다. 사용자가 모바일 카메라로 예술 작품을 가리키면 시스템은 역사적 초상화에서 생성된 실제와 같은 말하는 비디오 내러티브로 질문에 응답합니다. 그러나 런타임에 이러한 토킹 헤드 비디오를 생성하는 것은 계산 비용이 많이 들고 응답당 1분 이상이 소요되는 경우가 많습니다. 이 문제를 해결하기 위해 ArtSpeak는 두 가지 주요 기여를 소개합니다. 첫째, 자주 묻는 질문(FAQ) 모음을 사용하여 다양한 예술 초상화에 대한 생생한 비디오 응답 세트를 생성합니다. 둘째, GPT 기반 임베딩과 코사인 유사성을 사용하여 가장 관련성이 높은 응답을 선택하는 새로운 검색 기반 접근 방식을 도입합니다. 결과적으로 시스템은 사용자의 문의에 가장 적합한 비디오 응답을 동적으로 제시하여 계산 오버헤드를 줄이고 실시간, 짧은 지연 시간 경험을 보장합니다. 보다 정확하게 말하면 ArtSpeak는 실시간 비디오 생성 방법에 비해 30배 이상 낮은 대기 시간을 달성하고 에너지 소비를 약 81% 줄입니다. 사용자 연구는 참가자의 85%가 검색된 응답을 자신의 쿼리와 관련이 있다고 평가하고 90%가 원활한 비디오 재생을 보고하는 등 시스템의 효율성을 더욱 검증합니다. 이러한 결과는 검색 기반 접근 방식을 통해 구현된 효율성과 사용자 만족도를 강조합니다.",https://doi.org/10.1109/ISMAR67309.2025.00091,Interaction & Input,Deep Learning / Neural Networks,User Study,Algorithm / Method
14,2025,Audiovisual Disparities in VR: Impact on Spatial Perception,VR의 시청각 차이: 공간 인식에 미치는 영향,"Virtual reality (VR) experiences often leverage rich and spatialized multimodal environments to increase immersion and engagement. This demands a consistent spatial perception of audiovisual stimuli, since perceived discrepancies can disrupt the sense of presence. In this work, we investigate the consequences of two types of spatial audiovisual disparities: true disparity, where there is a measurable spatial offset between auditory and visual cues, and perceptual disparity, where users report misalignment despite cues being colocated. Unlike most previous studies that employed controlled but simplified experimental setups, our research focuses on complex, realistic VR environments, allowing us to assess the actual implications for VR content design. Our experiments indicate that users are highly sensitive to true audiovisual disparities in controlled environments, detecting even minor misalignments. However, when engaged in additional tasks within realistic settings, their ability to notice such discrepancies diminishes significantly. We also observed that previously found perceptual disparities persist in complex audiovisual environments. However, we identify self-initiated head rotations as a key factor; its absence prevents the effect entirely. We hope our findings offer practical insights for designing more immersive and perceptually coherent VR experiences.","가상 현실(VR) 경험은 몰입도와 참여도를 높이기 위해 풍부하고 공간화된 다중 모드 환경을 활용하는 경우가 많습니다. 인식된 불일치가 존재감을 방해할 수 있기 때문에 시청각 자극에 대한 일관된 공간 인식이 필요합니다. 이 작업에서 우리는 두 가지 유형의 공간적 시청각 불균형, 즉 청각 신호와 시각적 신호 사이에 측정 가능한 공간적 오프셋이 있는 실제 불일치와 신호가 같은 위치에 있음에도 불구하고 사용자가 오정렬을 보고하는 지각 불일치의 결과를 조사합니다. 제어되었지만 단순화된 실험 설정을 사용했던 대부분의 이전 연구와는 달리, 우리 연구는 복잡하고 사실적인 VR 환경에 중점을 두어 VR 콘텐츠 디자인에 대한 실제 영향을 평가할 수 있습니다. 우리의 실험에 따르면 사용자는 통제된 환경에서 실제 시청각 차이에 매우 민감하여 사소한 정렬 오류도 감지하는 것으로 나타났습니다. 그러나 현실적인 환경에서 추가 작업을 수행하면 이러한 불일치를 알아차리는 능력이 크게 저하됩니다. 우리는 또한 이전에 발견된 지각 차이가 복잡한 시청각 환경에서 지속된다는 것을 관찰했습니다. 그러나 우리는 자체 시작 머리 회전을 핵심 요소로 식별합니다. 그것의 부재는 효과를 완전히 방해합니다. 우리의 연구 결과가 보다 몰입감 있고 지각적으로 일관된 VR 경험을 디자인하는 데 실용적인 통찰력을 제공할 수 있기를 바랍니다.",https://doi.org/10.1109/ISMAR67309.2025.00053,Perception & Cognition; Audio & Sound,Sensor Fusion,Other,Design Guidelines
15,2025,Augmented Reality Visualization Techniques for Search and Rescue: Findings from a User Study with Subject Matter Experts,수색 및 구조를 위한 증강 현실 시각화 기술: 주제 전문가와 함께한 사용자 연구 결과,"Our research explored the use of Whiskers, Wedge, and Compass augmented reality (AR) visualization techniques (VTs) in an outdoor field-based user study conducted with Search and Rescue (SAR) subject matter experts (SMEs). The goal of this study was to understand what hazard-based out-of-view information SAR responders need and how SAR SMEs prefer information to be presented. We investigated why certain design elements and preattentive properties of these visualization techniques are most preferred for use during a SAR mission. A key aim was to gain insight on what AR design elements can help guide SAR responders away from dangerous out-of-view hazards, while still allowing them to efficiently search for victims. We derived a set of AR VT design guidelines, specifically applicable to dynamic, potentially dangerous environments, through semi-structured interviews with SMEs and thematic analysis of data. We additionally identified how SAR SMEs envision these AR VTs fitting in with the current SAR ecosystem.","우리의 연구에서는 SAR(수색 및 구조) 주제 전문가(SME)와 함께 실시한 실외 현장 기반 사용자 연구에서 Whiskers, Wedge 및 Compass 증강 현실(AR) 시각화 기술(VT)의 사용을 조사했습니다. 이 연구의 목표는 SAR 대응자에게 어떤 위험 기반 외부 정보가 필요한지, SAR SME가 정보 제공을 선호하는 방식을 이해하는 것이었습니다. 우리는 이러한 시각화 기술의 특정 디자인 요소와 사전 주의적 속성이 SAR 임무 중에 사용하는 데 가장 선호되는 이유를 조사했습니다. 주요 목표는 AR 설계 요소가 SAR 대응요원을 위험한 시야 밖 위험으로부터 멀리 안내하는 동시에 피해자를 효율적으로 검색하는 데 도움이 될 수 있는지에 대한 통찰력을 얻는 것이었습니다. 우리는 SME와의 반구조화된 인터뷰와 데이터의 주제별 분석을 통해 특히 역동적이고 잠재적으로 위험한 환경에 적용할 수 있는 일련의 AR VT 설계 지침을 도출했습니다. 우리는 또한 SAR SME가 현재 SAR 생태계에 적합한 AR VT를 어떻게 구상하는지 확인했습니다.",https://doi.org/10.1109/ISMAR67309.2025.00137,Rendering & Visualization,Other,Qualitative Analysis,Design Guidelines; User Study / Empirical Findings
16,2025,Automated Scalable Brightness and Black Offset Seamlessness for Multi-Projector Displays,멀티 프로젝터 디스플레이를 위한 자동화된 확장 가능한 밝기 및 블랙 오프셋 끊김 없음,"Multi-projector displays show significant variation of brightness and black offset. Prior work use single-camera based methods to create a seamless display. However, the limited field-of-view (FOV) of single-camera based solutions limit their applicability to large, non-flat display shapes such as surround displays or domes. Although using a single camera at multiple locations can provide some ad-hoc corrections, it lacks both automation and accuracy. Further, automated black offset seamlessness is arguably still one of the most challenging problems in multi-projector displays. In this paper, we present the first scalable brightness and black offset seamlessness method for multi-projector displays using multiple uncalibrated cameras that scale to a large number of projectors on developable display geometries, and can operate in the presence of camera vignetting effects and varying projector brightness. Using a display partitioning method, we relate the brightness response for every pixel of the display to a common reference via appropriate camera correction factors. Then, we adapt a prior single-camera-based brightness seamlessness method to achieve seamlessness across the entire display. Next, we show that the same technique can be used to provide an automated method to achieve black offset seamlessness across the display. We demonstrate the scalability of our method to displays with upto 32 projectors on complex developable projection surfaces with cameras that have different color properties. We also provide quantitative results to demonstrate the significantly positive impact of the brightness and black offset seamlessness methods on the quality of the display.","멀티 프로젝터 디스플레이는 밝기와 블랙 오프셋의 상당한 변화를 보여줍니다. 이전 작업에서는 단일 카메라 기반 방법을 사용하여 원활한 디스플레이를 만들었습니다. 그러나 단일 카메라 기반 솔루션의 제한된 시야(FOV)로 인해 서라운드 디스플레이나 돔과 같은 크고 평평하지 않은 디스플레이 형태에 대한 적용이 제한됩니다. 여러 위치에서 단일 카메라를 사용하면 일부 임시 수정이 가능하지만 자동화와 정확성이 모두 부족합니다. 또한 자동화된 블랙 오프셋 끊김 없음은 여전히 ​​멀티 프로젝터 디스플레이에서 가장 어려운 문제 중 하나입니다. 본 논문에서는 개발 가능한 디스플레이 기하학적 구조에서 다수의 프로젝터로 확장되고 카메라 비네팅 효과와 다양한 프로젝터 밝기가 있는 경우 작동할 수 있는 보정되지 않은 여러 대의 카메라를 사용하여 멀티 프로젝터 디스플레이를 위한 최초의 확장 가능한 밝기 및 블랙 오프셋 연속성 방법을 제시합니다. 디스플레이 분할 방법을 사용하여 디스플레이의 모든 픽셀에 대한 밝기 응답을 적절한 카메라 보정 요소를 통해 공통 기준과 연관시킵니다. 그런 다음 이전 단일 카메라 기반 밝기 연속성 방법을 적용하여 전체 디스플레이에 걸쳐 연속성을 달성합니다. 다음으로, 동일한 기술을 사용하여 디스플레이 전반에 걸쳐 블랙 오프셋의 끊김 없는성을 달성하는 자동화된 방법을 제공할 수 있음을 보여줍니다. 우리는 다양한 색상 속성을 가진 카메라를 사용하여 복잡하게 개발 가능한 프로젝션 표면에 최대 32개의 프로젝터로 표시하는 방법의 확장성을 보여줍니다. 또한 밝기 및 블랙 오프셋 연속성 방법이 디스플레이 품질에 미치는 긍정적인 영향을 입증하기 위한 정량적 결과도 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00119,Display & Optics,Optical / Display Technology,Quantitative Experiment,Algorithm / Method
17,2025,AuxiScope: Handheld Augmented Reality Tablet as an Auxiliary Display for Large-Scale Display Systems,AuxiScope: 대형 디스플레이 시스템용 보조 디스플레이로서의 휴대용 증강 현실 태블릿,"We present AuxiScope, a novel AR-based system designed to enhance personalized data exploration on large wall displays (LWDs) by integrating handheld tablets as auxiliary visualization interfaces. While LWDs offer expanded visual real estate and intuitive embodied interaction, they pose challenges related to effective interfaces for data exploration and analysis, specifically in multi-user settings. AuxiScope addresses these by overlaying supplementary visualizations onto corresponding LWD content, enabling individualized exploration without interference with the visual data displayed on the LWD. To achieve this, we have designed a geometric alignment pipeline that synchronizes the auxiliary visualizations atop the virtual scene. Specifically, by leveraging AR technology, AuxiScope resolves the tablet physical localization, viewpoint computation, and user interaction translation into the virtual space. Subsequently, based on a client-server architecture, it employs remote rendering and delegates computational tasks to the LWD compute nodes in order to minimize memory load on portable devices. We demonstrate the potential of AuxiScope through multiple AR-based interaction techniques across information and scientific visualization scenarios, for both 2D and 3D contexts.","우리는 휴대용 태블릿을 보조 시각화 인터페이스로 통합하여 대형 벽면 디스플레이(LWD)에서 개인화된 데이터 탐색을 향상하도록 설계된 새로운 AR 기반 시스템인 AuxiScope를 소개합니다. LWD는 확장된 시각적 공간과 직관적으로 구현된 상호 작용을 제공하지만, 특히 다중 사용자 설정에서 데이터 탐색 및 분석을 위한 효과적인 인터페이스와 관련된 문제를 제기합니다. AuxiScope는 해당 LWD 콘텐츠에 보충 시각화를 오버레이하여 LWD에 표시되는 시각적 데이터를 방해하지 않고 개별화된 탐색을 가능하게 함으로써 이러한 문제를 해결합니다. 이를 달성하기 위해 우리는 가상 장면 위에서 보조 시각화를 동기화하는 기하학적 정렬 파이프라인을 설계했습니다. 특히 AuxiScope는 AR 기술을 활용하여 태블릿의 물리적 위치 파악, 시점 계산, 사용자 상호 작용을 가상 공간으로 변환하는 문제를 해결합니다. 그 후, 클라이언트-서버 아키텍처를 기반으로 휴대용 장치의 메모리 부하를 최소화하기 위해 원격 렌더링을 사용하고 계산 작업을 LWD 컴퓨팅 노드에 위임합니다. 우리는 2D 및 3D 컨텍스트 모두에 대해 정보 및 과학적 시각화 시나리오 전반에 걸쳐 다양한 AR 기반 상호 작용 기술을 통해 AuxiScope의 잠재력을 보여줍니다.",https://doi.org/10.1109/ISMAR67309.2025.00169,Rendering & Visualization; Interaction & Input,Cloud / Edge Computing,Other,System / Framework; Interaction Technique
18,2025,Beyond Supervised Limits: Semi-Supervised Cybersickness Prediction from Physiological Signals with Minimal Labeled Data,감독 한계 너머: 최소한의 레이블이 지정된 데이터를 사용하여 생리적 신호로부터 반 감독 사이버 질병 예측,"Cybersickness, characterized by discomforts such as dizziness, nausea, and eye strain, remains a significant barrier to the widespread adoption of virtual reality (VR). Recent research have proposed supervised machine learning models to predict the onset of cybersickness; however, these approaches depend heavily on labeled datasets. Acquiring labeled datasets typically necessitates time-consuming and resource-intensive user studies, limiting the feasibility of these supervised methods for consumer-level VR applications where obtaining labeled user data during use is impractical. Moreover, due to individual differences, often these datasets are not generalizable in consumer VR use. To address these limitations, we propose a novel semi-supervised learning framework for predicting cybersickness (i.e., Fast Motion Sickness (FMS)) using eye tracking, heart rate, and galvanic skin response data. Our proposed semi-supervised approach uses pseudo-labeling techniques (i.e., Self-training, Label Propagation, and Label Spreading) fused with temporal deep learning models (i.e., DeepTCN, CNN-LSTM, Transformers, LSTM). We evaluated our approach on three public cybersickness datasets (i.e., Bumpy Ride, Simulation 21, Maze) and our proposed semi-supervised approach demonstrates strong cybersickness predictive performance using only $1-5 {\%}$ labeled data (i.e., $95-99 {\%}$ data remains unlabeled). Notably, the self-training approach with a DeepTCN model achieved an accuracy of $\mathbf{7 5. 8 6 \%}$ in FMS prediction, outperforming the other models and pseudolabeling approaches. Our findings establish the viability of semisupervised learning for cybersickness prediction with minimally labeled datasets, paving the way for more practical and potentially generalizable cybersickness prediction systems in consumer VR applications.","현기증, 메스꺼움, 눈의 피로 등의 불편함을 특징으로 하는 사이버 멀미는 가상현실(VR)의 광범위한 도입을 가로막는 중요한 장벽으로 남아 있습니다. 최근 연구에서는 사이버 멀미의 시작을 예측하기 위한 지도형 기계 학습 모델을 제안했습니다. 그러나 이러한 접근 방식은 레이블이 지정된 데이터 세트에 크게 의존합니다. 레이블이 지정된 데이터 세트를 획득하려면 일반적으로 시간이 많이 걸리고 리소스 집약적인 사용자 연구가 필요하므로 사용 중에 레이블이 지정된 사용자 데이터를 얻는 것이 비현실적인 소비자 수준 VR 애플리케이션에 대한 감독 방법의 타당성이 제한됩니다. Moreover, due to individual differences, often these datasets are not generalizable in consumer VR use. 이러한 한계를 해결하기 위해 우리는 시선 추적, 심박수 및 피부 전기 반응 데이터를 사용하여 사이버 멀미(예: FMS(Fast Motion Sickness))를 예측하기 위한 새로운 반지도 학습 프레임워크를 제안합니다. 우리가 제안한 준지도 접근 방식은 임시 딥 러닝 모델(예: DeepTCN, CNN-LSTM, Transformers, LSTM)과 융합된 의사 라벨링 기술(예: 자체 훈련, 레이블 전파 및 레이블 확산)을 사용합니다. 우리는 세 가지 공개 사이버 멀미 데이터 세트(예: Bumpy Ride, Simulation 21, Maze)에 대한 접근 방식을 평가했으며 제안된 반 감독 접근 방식은 $1-5 {\%}$ 레이블이 지정된 데이터(즉, $95-99 {\%}$ 데이터는 레이블이 지정되지 않은 상태로 유지됨)만을 사용하여 강력한 사이버 멀미 예측 성능을 보여줍니다. 특히, DeepTCN 모델을 사용한 자가 훈련 접근 방식은 FMS 예측에서 $\mathbf{7 5. 8 6 \%}$의 정확도를 달성하여 다른 모델 및 의사 라벨링 접근 방식보다 성능이 뛰어났습니다. 우리의 연구 결과는 최소한의 레이블이 지정된 데이터 세트를 사용하여 사이버 질병 예측을 위한 반지도 학습의 실행 가능성을 확립하여 소비자 VR 애플리케이션에서 보다 실용적이고 잠재적으로 일반화 가능한 사이버 질병 예측 시스템을 위한 길을 열었습니다.",https://doi.org/10.1109/ISMAR67309.2025.00113,Education & Training; Interaction & Input,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method; System / Framework
19,2025,Bichronous Collaboration: Using Spatiotemporal Cues to Collaborate Across Time and Space on Physical Tasks,이시성 협업: 시공간 단서를 사용하여 물리적 작업에서 시간과 공간을 초월하여 협업,"We introduce bichronous collaboration in an eXtended Reality (XR) remote collaboration workflow that seamlessly links asynchronous review with synchronous cooperation on physical tasks. The aim is to reduce the misunderstandings and errors that often arise during remote work. The system records a novice's actions and presents them with three spatiotemporal cues-2D, 3D Basic, and 3D Action. An expert can review and provide feedback asynchronously, and also guide the novice synchronously to fix the error. A user study showed that the 3D Action cue allowed experts to identify faults more quickly, enabled novices to resolve problems in less time with higher success rates than a conventional 2D timeline, without increasing cognitive workload or cybersickness. These findings highlight the value of coupling asynchronous insight with synchronous teamwork and position bichronous collaboration as a practical solution for remote troubleshooting, training, and inspection in XR environments.","비동기식 검토와 물리적 작업에 대한 동기식 협력을 원활하게 연결하는 XR(eXtended Reality) 원격 협업 워크플로에 쌍시적 협업을 도입합니다. 원격근무 중 자주 발생하는 오해와 오류를 줄이는 것이 목표다. 시스템은 초보자의 행동을 기록하고 2D, 3D 기본 및 3D 동작의 세 가지 시공간적 단서를 제공합니다. 전문가는 비동기적으로 피드백을 검토하고 제공할 수 있으며, 초보자에게 동기적으로 오류 수정을 안내할 수도 있습니다. 사용자 연구에 따르면 3D 액션 큐를 통해 전문가는 결함을 더 빠르게 식별할 수 있고, 초보자는 인지 작업량이나 사이버 멀미를 증가시키지 않고도 기존 2D 타임라인보다 더 높은 성공률로 더 짧은 시간에 문제를 해결할 수 있는 것으로 나타났습니다. 이러한 연구 결과는 동기식 팀워크와 비동기식 통찰력을 결합하는 것의 가치를 강조하고 XR 환경에서 원격 문제 해결, 교육 및 검사를 위한 실용적인 솔루션으로 이시성 협업을 포지셔닝합니다.",https://doi.org/10.1109/ISMAR67309.2025.00144,Perception & Cognition; Collaboration & Social,Optical / Display Technology,User Study,User Study / Empirical Findings
20,2025,"Big Frog Vibes: Jumping Superhuman Distances Forward, Vertically and Sideways in VR Using Comfortably High Translational Gain","Big Frog Vibes: 편안하게 높은 변환 이득을 사용하여 VR에서 초인적인 거리를 앞, 세로, 옆으로 점프","Jumping is an engaging physical activity bounded by individual physical capabilities. However in Virtual Reality (VR) we can apply perceptual manipulations like translational gain to real jumps, amplifying the user's apparent capabilities - jumping higher and further than is possible in reality. Prior work has focused on imperceptible gains, but this greatly limits the range of available experiences users can engage in, and designers can create. Research outside of jumping has shown that higher, clearly noticeable gains can still be comfortable, engaging and usable in VR. Therefore, in this paper, we evaluate ($\mathrm{N}=18$) the maximal comfortable gain applied to three jump types: forward, vertical, and sideways. We found, for the majority of participants, high gains ($>100 \mathrm{x}$) are not only comfortable but made users feel enhanced whilst retaining jump ownership, deriving guidelines on the extent to which we can amplify real jumps to feel superhuman in virtuality.","점프는 개인의 신체 능력에 제한을 받는 매력적인 신체 활동입니다. 그러나 가상 현실(VR)에서는 실제 점프에 병진 이득과 같은 지각 조작을 적용하여 사용자의 명백한 능력을 증폭시켜 현실에서 가능한 것보다 더 높이 더 멀리 점프할 수 있습니다. 이전 작업은 눈에 띄지 않는 이득에 중점을 두었지만 이로 인해 사용자가 참여할 수 있고 디자이너가 만들 수 있는 사용 가능한 경험의 범위가 크게 제한됩니다. 점프 이외의 연구에 따르면 더 높고 명확하게 눈에 띄는 이득은 VR에서 여전히 편안하고 매력적이며 사용할 수 있는 것으로 나타났습니다. 따라서 본 논문에서는 앞으로, 수직, 옆으로의 세 가지 점프 유형에 적용되는 최대 편안한 이득($\mathrm{N}=18$)을 평가합니다. 우리는 대다수 참가자의 경우 높은 이득($>100 \mathrm{x}$)이 편안할 뿐만 아니라 점프 소유권을 유지하면서 사용자가 향상된 느낌을 갖게 하여 가상에서 초인적인 느낌을 주기 위해 실제 점프를 증폭할 수 있는 정도에 대한 지침을 도출했습니다.",https://doi.org/10.1109/ISMAR67309.2025.00038,Interaction & Input,Other,User Study,Other
21,2025,Birds of a Feather Augment Together: Exploring Sonic Links Between Real and Virtual Worlds in Audio Augmented Reality,Birds of a Feather Augment Together: 오디오 증강 현실에서 현실 세계와 가상 세계 사이의 음향 링크 탐색,"Augmented reality (AR) applications present virtual elements that are connected to the real world. While these may be aware of a user's geographical or visual context, the real sounds in a user's environment are rarely used in the AR experience. We investigate audio augmented reality (AAR) with sound as the primary output. We present the first evaluation of sonic linking, where an AAR application uses real sounds (bird calls and car engines) in a user's surroundings to drive the interaction. We developed two AAR applications to investigate how to design and use such links: a game where entities are spawned based on real-world sounds and a music player with sound-reactive filtering and volume adjustment. Design variations are compared to cover different AAR scenarios, types of sonic link, and existing unlinked equivalents. The results show that sonic linking can create a more augmented, engaging AAR experience, and may alter a user's relationship with their real-world surroundings, enabling new types of augmented reality applications.","증강현실(AR) 애플리케이션은 현실 세계와 연결된 가상 요소를 제시합니다. 이들은 사용자의 지리적 또는 시각적 맥락을 인식할 수 있지만 사용자 환경의 실제 소리는 AR 경험에 거의 사용되지 않습니다. 우리는 소리를 기본 출력으로 하는 오디오 증강 현실(AAR)을 조사합니다. 우리는 AAR 애플리케이션이 사용자 주변의 실제 소리(새 울음소리 및 자동차 엔진)를 사용하여 상호 작용을 유도하는 음파 연결에 대한 첫 번째 평가를 제시합니다. 우리는 이러한 링크를 디자인하고 사용하는 방법을 조사하기 위해 두 가지 AAR 애플리케이션을 개발했습니다. 하나는 실제 사운드를 기반으로 개체가 생성되는 게임이고, 하나는 사운드 반응 필터링 및 볼륨 조정 기능이 있는 음악 플레이어입니다. 다양한 AAR 시나리오, 음향 링크 유형 및 연결되지 않은 기존 장치를 다루기 위해 설계 변형을 비교합니다. 결과는 음향 연결이 더욱 증강되고 매력적인 AAR 경험을 창출할 수 있고 사용자와 실제 주변 환경의 관계를 변경하여 새로운 유형의 증강 현실 애플리케이션을 가능하게 할 수 있음을 보여줍니다.",https://doi.org/10.1109/ISMAR67309.2025.00153,Interaction & Input; Audio & Sound,Other,Other,User Study / Empirical Findings
22,2025,Can AI Inspire Biophilic Design in Immersive Virtual Reality Workspaces to Enhance Well-Being?,AI가 몰입형 가상 현실 작업 공간에서 생체친화적 디자인에 영감을 주어 웰빙을 향상시킬 수 있습니까?,"Due to the growing adoption of Immersive Virtual Reality (IVR) in various contexts, understanding how virtual workspaces impact humans and how they should be designed is crucial. Exploring optimal design strategies that can enhance well-being within IVR environments becomes important. In real contexts, biophilic design offers a way to improve well-being by introducing natural elements. IVR can go beyond real-world constraints for integrating biophilic elements, but this aspect is scarcely addressed in literature. A valuable assistance to support the design of IVR workspaces could be offered by generative Artificial Intelligence (AI), which is becoming increasingly integrated into everyday life. The aim of this work is twofold: (i) assessing if the biophilic design of IVR workspaces can improve human well-being and (ii) investigating if AI can inspire the biophilic design properly. Thus, we conducted a user study comparing psycho-physiological and subjective metrics in three conditions. A workspace without biophilia was used as a baseline. The same was enriched with two biophilic designs: one suggested by expert designers and another inspired by AI-generated graphics, simulating a not-expert developer. The findings showed that both biophilic designs significantly improved well-being; however, AI suggestions led to a reduced sense of presence in the IVR environment.",다양한 맥락에서 몰입형 가상 현실(IVR)의 채택이 증가함에 따라 가상 작업 공간이 인간에게 미치는 영향과 이를 설계하는 방법을 이해하는 것이 중요합니다. IVR 환경 내에서 웰빙을 향상할 수 있는 최적의 설계 전략을 탐색하는 것이 중요해졌습니다. 실제 상황에서 생체친화적 디자인은 자연 요소를 도입하여 웰빙을 향상시키는 방법을 제공합니다. IVR은 생체친화적 요소를 통합하기 위한 실제 제약 조건을 뛰어넘을 수 있지만 이 측면은 문헌에서 거의 다루지 않습니다. 일상 생활에 점점 더 통합되고 있는 생성 인공 지능(AI)은 IVR 작업 공간 설계를 지원하는 귀중한 지원을 제공할 수 있습니다. 이 작업의 목적은 두 가지입니다. (i) IVR 작업 공간의 생체친화적 디자인이 인간의 웰빙을 향상시킬 수 있는지 평가하고 (ii) AI가 생체친화적 디자인에 적절하게 영감을 줄 수 있는지 조사합니다. 따라서 우리는 세 가지 조건에서 정신 생리적 지표와 주관적 지표를 비교하는 사용자 연구를 수행했습니다. 바이오애호증이 없는 작업 공간이 기준선으로 사용되었습니다. 두 가지 생체친화적 디자인으로 더욱 풍성해졌습니다. 하나는 전문 디자이너가 제안한 것이고 다른 하나는 AI 생성 그래픽에서 영감을 받아 비전문가 개발자를 시뮬레이션한 것입니다. 연구 결과에 따르면 두 가지 친생체 디자인 모두 웰빙을 크게 향상시키는 것으로 나타났습니다. 그러나 AI 제안으로 인해 IVR 환경에서 존재감이 감소했습니다.,https://doi.org/10.1109/ISMAR67309.2025.00161,Content Authoring,Generative AI,Technical Evaluation,User Study / Empirical Findings
23,2025,Can the Perceived Capability of Your Virtual Avatar Enhance Exercise Performance?,가상 아바타의 인지된 기능이 운동 성능을 향상시킬 수 있습니까?,"The rise of Virtual Reality (VR) sports has been driven by evolving work patterns, limited access to physical exercise spaces, and a growing focus on health and wellness. Beyond the enjoyment and convenience offered by VR exercise, we aim to enhance users' athletic performance through this medium. Prior research on the Proteus Effect in VR sports has demonstrated the potential of customized, stronger, or younger avatars to improve exercise outcomes. However, such effects are limited for users who already perceive themselves as strong and youthful. To address this, we propose a more general approach: representing perceived avatar capability through facial expressions and vocal cues to influence user performance. In this study, we examined how manipulating the perceived capability level of virtual avatars (high, neutral, low) during dumbbell lateral raises in a VR gym affected exercise performance. Results indicated that avatars with high-capability expressions significantly enhanced performance and motivation compared to low-capability avatars. These findings underscore the promise of using facial and auditory cues to represent perceived capability, offering new directions for designing emotionally intelligent fitness applications.","가상 현실(VR) 스포츠의 부상은 업무 패턴의 진화, 신체 운동 공간에 대한 제한된 접근성, 건강과 웰니스에 대한 관심 증가에 의해 주도되었습니다. VR 운동이 제공하는 즐거움과 편리함을 넘어, 이 매체를 통해 사용자의 운동 능력을 향상시키는 것을 목표로 합니다. VR 스포츠의 프로테우스 효과(Proteus Effect)에 대한 사전 연구는 운동 결과를 향상시키기 위한 맞춤형, 더 강하거나 더 젊은 아바타의 잠재력을 보여주었습니다. 그러나 이미 자신을 강하고 젊다고 인식하는 사용자에게는 이러한 효과가 제한됩니다. 이 문제를 해결하기 위해 우리는 보다 일반적인 접근 방식을 제안합니다. 즉, 얼굴 표정과 음성 신호를 통해 인지된 아바타 능력을 표현하여 사용자 성능에 영향을 줍니다. 본 연구에서는 VR 체육관에서 덤벨 측면 들어올리기 중 가상 아바타의 인지된 능력 수준(높음, 중립, 낮음)을 조작하는 것이 운동 수행에 어떤 영향을 미치는지 조사했습니다. 연구 결과, 능력이 뛰어난 아바타는 능력이 낮은 아바타에 비해 성과와 동기가 크게 향상되는 것으로 나타났습니다. 이러한 발견은 얼굴 및 청각 신호를 사용하여 인지된 능력을 표현하고 감성 지능 피트니스 애플리케이션을 설계하기 위한 새로운 방향을 제시한다는 가능성을 강조합니다.",https://doi.org/10.1109/ISMAR67309.2025.00078,Perception & Cognition,Other,Other,Algorithm / Method
24,2025,"Clean Training, Clear Skies: Virtual Reality Training for Expert Smoke Opacity Certification","깨끗한 훈련, 맑은 하늘: 전문 연기 불투명 인증을 위한 가상 현실 훈련","Current smoke opacity certification methods are environmentally harmful and logistically inefficient, requiring real emissions, travel, and time off work. In this study, we present a Virtual Reality (VR) training and assessment system that simulates black and white smoke emissions in an immersive, controlled environment, designed to mirror real-world training and testing standards. Participants in the VR group completed two training sessions and a standardized test adapted from an official certification exam. Their performance was compared to a control group that received traditional lecture-based training and paper-based testing. Certification required a test score below 37 for both smoke colors. The VR group achieved mean scores of 33.44 (black smoke) and 57.75 (white smoke), outperforming the control group, which scored 46.00 and 72.00 respectively. The VR group significantly outperformed the control on black smoke $(p<0.05)$ achieving passing scores on nearly all tested opacities ($<3$ points (15%) deviation per reading). Subjectively, VR participants reported feeling more confident, better prepared, and found the application enjoyable. These results suggest VR is an effective, scalable alternative to traditional training, enhancing performance, increasing user satisfaction, and reducing environmental impact.","현재의 연기 불투명 인증 방법은 환경에 해롭고 물류적으로 비효율적이므로 실제 배출, 여행 및 휴가가 필요합니다. 본 연구에서는 실제 교육 및 테스트 표준을 반영하도록 설계된 몰입형 제어 환경에서 흑백 연기 방출을 시뮬레이션하는 가상 현실(VR) 교육 및 평가 시스템을 제시합니다. VR 그룹의 참가자들은 두 번의 교육 세션과 공식 인증 시험에서 채택한 표준화된 테스트를 완료했습니다. 그들의 성과는 전통적인 강의 기반 교육과 종이 기반 테스트를 받은 통제 그룹과 비교되었습니다. 인증을 받으려면 두 가지 연기 색상 모두에 대해 37점 미만의 시험 점수가 필요합니다. VR 그룹은 평균 33.44점(검은 연기)과 57.75점(흰 연기)을 달성하여 각각 46.00점과 72.00점을 얻은 대조군을 능가했습니다. VR 그룹은 검은 연기에 대한 대조군보다 훨씬 뛰어난 성능을 발휘하여 $(p<0.05)$ 거의 모든 테스트 불투명도에서 합격 점수를 획득했습니다(판독당 $<3$ 포인트(15%) 편차). 주관적으로 VR 참가자들은 더 자신감이 생기고, 더 잘 준비되었으며, 애플리케이션이 즐겁다고 느꼈습니다. 이러한 결과는 VR이 기존 교육에 대한 효과적이고 확장 가능한 대안으로 성능을 향상하고 사용자 만족도를 높이며 환경에 미치는 영향을 줄이는 것임을 시사합니다.",https://doi.org/10.1109/ISMAR67309.2025.00163,Interaction & Input,Sensor Fusion,User Study,System / Framework
25,2025,Comparing Hand and Controller Avatars with Hand Tracking and Controller-Based Interaction,손 추적 및 컨트롤러 기반 상호 작용을 통해 손 및 컨트롤러 아바타 비교,"Previous research suggests that the congruency between common VR input devices - such as controllers or hand tracking - and their visual representations (e.g., hand or controller avatars) influences user experience and performance. However, the specific effects of input-avatar combinations remain underexplored. We study the effects of common input devices (hand tracking and controllers) and visual representations (hand and controller avatars) on performance and perceived success in target acquisition tasks. We included both grasping and pinching gestures across 16 combinations of input, avatar, and target size. Results indicate that hand tracking benefits from any form of visual representation-even when mismatched-achieving up to 5.8% greater accuracy compared to having no avatar, likely due to its reliance on visual feedback in the absence of a physical prop. Controllers were generally preferred and offered faster task completion. However, mismatched avatars had a stronger negative effect with controllers, particularly when the virtual gesture did not align with the physical action, leading to a 5.6% drop in accuracy compared to the matched condition-suggesting that inaccurate feedback can be more disruptive than having no avatar feedback at all.","이전 연구에서는 컨트롤러 또는 손 추적과 같은 일반적인 VR 입력 장치와 시각적 표현(예: 손 또는 컨트롤러 아바타) 간의 일치성이 사용자 경험과 성능에 영향을 미치는 것으로 나타났습니다. 그러나 입력-아바타 조합의 구체적인 효과는 아직 충분히 연구되지 않은 상태입니다. 우리는 일반적인 입력 장치(손 추적 및 컨트롤러)와 시각적 표현(손 및 컨트롤러 아바타)이 목표 획득 작업의 성능 및 인지된 성공에 미치는 영향을 연구합니다. 우리는 입력, 아바타 및 대상 크기의 16가지 조합에 걸쳐 잡기 및 꼬집기 제스처를 모두 포함했습니다. 결과에 따르면 핸드 트래킹은 물리적인 소품이 없을 때 시각적 피드백에 의존하기 때문에 아바타가 없는 경우에 비해 최대 5.8% 더 높은 정확도를 달성하는 경우에도 모든 형태의 시각적 표현에서 이점을 얻을 수 있는 것으로 나타났습니다. 컨트롤러는 일반적으로 선호되었으며 더 빠른 작업 완료를 제공했습니다. 그러나 일치하지 않는 아바타는 컨트롤러에 더 큰 부정적인 영향을 미쳤으며, 특히 가상 제스처가 물리적 동작과 일치하지 않을 때 일치된 조건에 비해 정확도가 5.6% 감소했습니다. 이는 부정확한 피드백이 아바타 피드백이 전혀 없는 것보다 더 파괴적일 수 있음을 시사합니다.",https://doi.org/10.1109/ISMAR67309.2025.00029,Interaction & Input,Hand / Gesture Recognition,Quantitative Experiment,User Study / Empirical Findings
26,2025,Crafting Dynamic Virtual Activities with Advanced Multimodal Models,고급 다중 모드 모델을 사용하여 동적 가상 활동 제작,"In this paper, we investigate the use of multimodal large language models (MLLMs) for generating virtual activities, leveraging the integration of vision-language modalities to enable the interpretation of virtual environments. Our approach recognizes and abstracts key scene elements including scene layouts, semantic contexts, and object identities with MLLMs' multimodal reasoning capabilities. By correlating these abstractions with massive knowledge about human activities, MLLMs are capable of generating adaptive and contextually relevant virtual activities. We propose a structured framework to articulate abstract activity descriptions, emphasizing detailed multi-character interactions within virtual spaces. Utilizing the derived high-level contexts, our approach accurately positions virtual characters and ensures that their interactions and behaviors are realistically and contextually appropriate through strategic optimization. Experiment results demonstrate the effectiveness of our approach, providing a novel direction for enhancing the realism and context-awareness in simulated virtual environments.","In this paper, we investigate the use of multimodal large language models (MLLMs) for generating virtual activities, leveraging the integration of vision-language modalities to enable the interpretation of virtual environments. 우리의 접근 방식은 MLLM의 다중 모드 추론 기능을 통해 장면 레이아웃, 의미적 맥락, 객체 ID를 포함한 주요 장면 요소를 인식하고 추상화합니다. MLLM은 이러한 추상화를 인간 활동에 대한 방대한 지식과 연관시킴으로써 상황에 맞게 적응할 수 있는 가상 활동을 생성할 수 있습니다. 우리는 추상적인 활동 설명을 명확하게 표현하고 가상 공간 내에서 상세한 다중 문자 상호 작용을 강조하는 구조화된 프레임워크를 제안합니다. 파생된 상위 수준 컨텍스트를 활용하는 우리의 접근 방식은 가상 캐릭터의 위치를 ​​정확하게 지정하고 전략적 최적화를 통해 캐릭터의 상호 작용과 행동이 현실적이고 상황에 맞게 적절하도록 보장합니다. 실험 결과는 시뮬레이션된 가상 환경에서 현실감과 상황 인식을 향상시키기 위한 새로운 방향을 제공하여 우리 접근 방식의 효율성을 보여줍니다.",https://doi.org/10.1109/ISMAR67309.2025.00025,Perception & Cognition; Content Authoring,Deep Learning / Neural Networks,Simulation,Algorithm / Method
27,2025,Cross-Media Color Appearance Reproduction in Optical See-Through Augmented Reality,광학 투명 증강 현실을 통한 매체 간 색상 표현 재현,"In optical see-through (OST) augmented reality (AR), displayed colors blend with the real-world scene, affecting perceived color. Studies showed that AR's color appearance depends not only on the additive chromaticity of the display and the real scene but also on ambient illumination. However, these studies often overlook changes in observer's adaptation state under varying illumination. To address this, a series of color-matching experiments between AR and display devices was conducted in an immersive lighting environment. The first experiment under a D65 illuminant found a slightly higher correlated color temperature (CCT) level of the internal white point in OST AR than in the display. Considering media's white point differences, this study proposes a three-step chromatic adaptation transform (CAT) framework to improve color appearance reproduction accuracy in AR. The second experiment used three Planckian radiators varying in CCT and two offPlanckian colorful ones to validate the proposed CAT under varying illumination, indicating that AR reached a more complete adaptation state than the display, especially under high luminance. A third validation experiment had observers rate color differences between reference and reproduced stimuli in AR. Results showed the effectiveness of our three-step CAT for cross-media color reproduction of OST AR under diverse illumination conditions.","광학 투명(OST) 증강 현실(AR)에서는 표시된 색상이 실제 장면과 혼합되어 인지된 색상에 영향을 줍니다. 연구에 따르면 AR의 색상 표현은 디스플레이와 실제 장면의 추가 색도뿐만 아니라 주변 조명에도 영향을 받는 것으로 나타났습니다. 그러나 이러한 연구에서는 다양한 조명 하에서 관찰자의 적응 상태 변화를 간과하는 경우가 많습니다. 이를 해결하기 위해 몰입형 조명 환경에서 AR과 디스플레이 장치 간의 일련의 색상 매칭 실험을 수행했습니다. D65 광원을 사용한 첫 번째 실험에서는 디스플레이보다 OST AR의 내부 백색점의 상관 색온도(CCT) 수준이 약간 더 높은 것으로 나타났습니다. 본 연구에서는 미디어의 백색점 차이를 고려하여 AR의 색상 재현 정확도를 향상시키기 위한 3단계 CAT(Chromatic Adaptation Transform) 프레임워크를 제안합니다. 두 번째 실험에서는 다양한 조명 하에서 제안된 CAT를 검증하기 위해 CCT가 다양한 3개의 Planckian 라디에이터와 2개의 offPlanckian 컬러 라디에이터를 사용했는데, 이는 AR이 특히 높은 휘도에서 디스플레이보다 더 완전한 적응 상태에 도달했음을 나타냅니다. 세 번째 검증 실험에서는 관찰자가 AR에서 참조 자극과 재현 자극 간의 색상 차이를 평가하도록 했습니다. 결과는 다양한 조명 조건에서 OST AR의 매체 간 색상 재현을 위한 3단계 CAT의 효율성을 보여주었습니다.",https://doi.org/10.1109/ISMAR67309.2025.00135,Rendering & Visualization; Display & Optics,Sensor Fusion,Quantitative Experiment,Hardware / Device
28,2025,Cross-Reality for Autonomous Mobility in Autistic Individuals: Evaluating at-Home VR Training and in-Situ AR Support in a Field Study,자폐인의 자율적 이동성을 위한 교차 현실: 현장 연구에서 재택 VR 교육 및 현장 AR 지원 평가,"Autonomy is a central concern for many caregivers of autistic individuals, particularly in domains such as mobility, where environmental complexity and variability pose significant challenges. In this work, we explore a cross-reality approach to support the development of autonomous public transportation skills—specifically intercity train travel—through a combination of Virtual Reality (VR), in-situ Augmented Reality (AR), and a conversational agent. Our system provides at-home VR training to simulate travel scenarios, AR-based support during real-world travel, and interactive guidance through a conversational interface, creating a cohesive and adaptive learning experience. We conducted a field study with ten participants over three months, which showed autonomy improvements comparable to state-of-the-art methods. Our findings contribute updated insights and design considerations for the application of cross-reality systems in accessibility and mobility training for neurodiverse users.","자율성은 많은 자폐인 보호자의 핵심 관심사이며, 특히 환경의 복잡성과 가변성이 심각한 문제를 야기하는 이동성과 같은 영역에서는 더욱 그렇습니다. 이 작업에서 우리는 가상 현실(VR), 현장 증강 현실(AR) 및 대화형 에이전트의 조합을 통해 자율 대중 교통 기술, 특히 도시 간 열차 여행의 개발을 지원하는 교차 현실 접근 방식을 탐구합니다. 우리 시스템은 여행 시나리오를 시뮬레이션하기 위한 재택 VR 교육, 실제 여행 중 AR 기반 지원, 대화형 인터페이스를 통한 대화형 안내를 제공하여 응집력 있고 적응력 있는 학습 경험을 제공합니다. 우리는 3개월에 걸쳐 10명의 참가자를 대상으로 현장 연구를 진행했는데, 이는 최첨단 방법에 필적하는 자율성 향상을 보여주었습니다. 우리의 연구 결과는 신경다양성 사용자를 위한 접근성 및 이동성 교육에 교차 현실 시스템을 적용하기 위한 업데이트된 통찰력과 설계 고려 사항에 기여합니다.",https://doi.org/10.1109/ISMAR67309.2025.00044,Education & Training,Sensor Fusion,Technical Evaluation,Algorithm / Method
29,2025,Cross-Subject Cognitive Load Recognition in VR Using Multimodal Fusion with EEG and Eye-Tracking,EEG 및 안구 추적과 다중 모드 융합을 사용한 VR의 교차 피사체 인지 부하 인식,"In virtual reality (VR) environments, excessive information density can lead to cognitive overload, while overly simplistic experiences may result in user disengagement. Therefore, effective cognitive load design is crucial in VR, however, cross-subject cognitive load recognition using multimodal physiological signals remains a significant challenge. In this paper, we proposed a new method for accurate cognitive load recognition in a VR driving environment using electroencephalogram (EEG) and eye-tracking data. Fifteen participants performed tasks designed to induce three levels of cognitive load while their EEG signals and eye-tracking data were recorded. We utilized a two-stage deep learning framework comprising pretraining and fine-tuning. During pretraining, a crossattention mechanism was employed to effectively fuse multimodal features, leveraging complementary information between EEG and eye-tracking data. Additionally, a domain-adversarial adaptation network and a shared encoder-decoder structure were introduced to extract subject-independent representations, enhancing the model's generalization to unseen subjects. In the fine-tuning stage, a classifier was added to refine cognitive load classification. Experimental results demonstrated that our method achieved the highest average accuracy on our dataset and a public dataset, outperforming existing approaches in cross-subject cognitive load recognition. These findings highlighted the potential of our method for cognitive load assessment in VR applications, providing new insights into cognitive load monitoring and applications according to the cognitive load.","가상 현실(VR) 환경에서 과도한 정보 밀도는 인지 과부하로 이어질 수 있고, 지나치게 단순한 경험은 사용자 이탈을 초래할 수 있습니다. 따라서 VR에서는 효과적인 인지 부하 설계가 중요하지만, 다중 모드 생리학적 신호를 사용한 주체 간 인지 부하 인식은 여전히 ​​중요한 과제로 남아 있습니다. 본 논문에서는 뇌전도(EEG)와 시선 추적 데이터를 활용하여 VR 운전 환경에서 정확한 인지 부하 인식을 위한 새로운 방법을 제안했습니다. 15명의 참가자는 EEG 신호와 시선 추적 데이터가 기록되는 동안 세 가지 수준의 인지 부하를 유도하도록 설계된 작업을 수행했습니다. 우리는 사전 훈련과 미세 조정으로 구성된 2단계 딥 러닝 프레임워크를 활용했습니다. 사전 훈련 중에는 EEG와 안구 추적 데이터 간의 보완 정보를 활용하여 다중 모달 기능을 효과적으로 융합하기 위해 교차 주의 메커니즘이 사용되었습니다. 또한, 주제 독립적 표현을 추출하기 위해 도메인 적대적 적응 네트워크와 공유 인코더-디코더 구조가 도입되어 보이지 않는 주제에 대한 모델의 일반화가 향상되었습니다. 미세 조정 단계에서는 인지 부하 분류를 세분화하기 위해 분류기를 추가했습니다. 실험 결과는 우리의 방법이 데이터 세트와 공개 데이터 세트에서 가장 높은 평균 정확도를 달성하여 주제 간 인지 부하 인식에서 기존 접근 방식을 능가한다는 것을 보여주었습니다. 이러한 발견은 VR 애플리케이션에서 인지 부하 평가를 위한 우리 방법의 잠재력을 강조하여 인지 부하 모니터링 및 인지 부하에 따른 애플리케이션에 대한 새로운 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00090,Perception & Cognition; Education & Training,Deep Learning / Neural Networks,User Study; Technical Evaluation,Algorithm / Method
30,2025,DGDiff: Immersive 3D Indoor Scene Synthesis via Dialog-Graph Conditioned Diffusion,DGDiff: 대화 그래프 조건 확산을 통한 몰입형 3D 실내 장면 합성,"Immersive 3D indoor scene synthesis is essential for applications such as AR/VR and 3D content creation. However, existing approaches fail to meet the immersive AR/VR requirements for fidelity, user-system interaction, and production speed simultaneously. Traditional scene synthesis methods are overly rigid, limiting user interactivity, whereas large language model (LLM)-based approaches suffer from slow response times and imprecise spatial structuring. To address these issues, we propose DGDiff, a novel dialog-graph conditioned diffusion framework for immersive, controllable, continuous synthesis and editing of 3D indoor scenes. This framework combines a conversational module powered by LLMs with a multimodal diffusion model. The conversational module translates user dialogue into structured semantic graphs, while the diffusion model integrates textual and graph-based conditions to synthesize realistic, editable indoor scenes. Experimental results demonstrate that DGDiff outperforms single-modality baselines, achieving an improvement of over 10 % in FID and a reduction of approximately 30 % in response time for dynamic scene interactive editing, offering an immersive and user-friendly synthesis experience. Project page: https://gitee.com/VR_NAVE/dgdiff.git","몰입형 3D 실내 장면 합성은 AR/VR 및 3D 콘텐츠 제작과 같은 애플리케이션에 필수적입니다. 그러나 기존 접근 방식은 충실도, 사용자 시스템 상호 작용 및 생산 속도에 대한 몰입형 AR/VR 요구 사항을 동시에 충족하지 못합니다. 기존의 장면 합성 방법은 지나치게 엄격하여 사용자 상호 작용을 제한하는 반면, LLM(대형 언어 모델) 기반 접근 방식은 응답 시간이 느리고 공간 구조가 부정확하다는 문제가 있습니다. 이러한 문제를 해결하기 위해 우리는 3D 실내 장면의 몰입적이고 제어 가능하며 지속적인 합성 및 편집을 위한 새로운 대화 그래프 조건 확산 프레임워크인 DGDiff를 제안합니다. 이 프레임워크는 LLM으로 구동되는 대화 모듈과 다중 모드 확산 모델을 결합합니다. 대화 모듈은 사용자 대화를 구조화된 의미 그래프로 변환하는 반면, 확산 모델은 텍스트 및 그래프 기반 조건을 통합하여 사실적이고 편집 가능한 실내 장면을 합성합니다. 실험 결과에 따르면 DGDiff는 FID에서 10% 이상의 개선을 달성하고 동적 장면 대화형 편집의 응답 시간을 약 30% 단축하여 몰입감 있고 사용자 친화적인 합성 경험을 제공함으로써 단일 모드 기준선보다 성능이 뛰어납니다. 프로젝트 페이지: https://gitee.com/VR_NAVE/dgdiff.git",https://doi.org/10.1109/ISMAR67309.2025.00055,Interaction & Input; Content Authoring,Deep Learning / Neural Networks; Natural Language Processing,Technical Evaluation,Algorithm / Method; System / Framework
31,2025,DOF-Separation for 3D Manipulation in XR: Understanding Finger-Wrist Separation to Simultaneously Translate and Rotate Objects,XR의 3D 조작을 위한 DOF 분리: 개체를 동시에 변환하고 회전하기 위한 손가락-손목 분리 이해,"Hand-tracking based 3D object manipulation in Extended Reality (XR) typically employs a pinch gesture for acquisition and manipulation through a direct mapping from 6-degrees-of-freedom (DOF) hand movement to that of the object. In this work, we investigate the effect of separating this mapping to concurrent 3DOF controls (DOF-Separation) of translation and rotation of the virtual object using the position and orientation of the hand independently. We aim to understand how DOF-Separation could ease manipulation for different techniques with varying requirements for hand position and orientation during acquisition, including Virtual Hand, Hand Ray, and Gaze&Pinch. Through a user study that features a docking task in VR, we found that DOF-Separation significantly improves the manipulation performance of Hand Ray, while improving that of Virtual Hand only in difficult tasks of combined translation and rotation. We suggest future XR systems to adopt DOF-Separation for input in manipulation-heavy applications, such as 3D design.","확장 현실(XR)의 손 추적 기반 3D 객체 조작은 일반적으로 6자유도(DOF) 손 움직임에서 객체 손 움직임으로의 직접 매핑을 통해 획득 및 조작을 위해 핀치 제스처를 사용합니다. 본 연구에서는 손의 위치와 방향을 독립적으로 사용하여 가상 객체의 변환 및 회전을 동시 3DOF 제어(DOF-분리)로 분리하는 효과를 조사합니다. 우리는 DOF 분리가 Virtual Hand, Hand Ray 및 Gaze&Pinch를 포함하여 획득 중 손 위치 및 방향에 대한 다양한 요구 사항을 사용하여 다양한 기술에 대한 조작을 어떻게 쉽게 할 수 있는지 이해하는 것을 목표로 합니다. VR의 도킹 작업을 특징으로 하는 사용자 연구를 통해 DOF-Separation은 Hand Ray의 조작 성능을 크게 향상시키는 반면, 변환 및 회전이 결합된 어려운 작업에서만 Virtual Hand의 조작 성능을 향상시키는 것으로 나타났습니다. 우리는 미래의 XR 시스템이 3D 디자인과 같이 조작이 많은 애플리케이션의 입력을 위해 DOF 분리를 채택할 것을 제안합니다.",https://doi.org/10.1109/ISMAR67309.2025.00130,Interaction & Input,Sensor Fusion,User Study,User Study / Empirical Findings
32,2025,DPIK: User Embodiment of Dual-Point Tracked Avatars Using Hand IK and Face Tracking for Smartphone AR Users,DPIK: 스마트폰 AR 사용자를 위한 Hand IK 및 얼굴 추적을 사용한 듀얼 포인트 추적 아바타의 사용자 구현,"Smartphone Augmented Reality (AR) has already introduced numerous social applications. However, extending smartphone AR to support social experiences akin to VRChat or AltspaceVR requires robust user tracking to animate avatars accurately. While Virtual Reality (VR) benefits from devices like hand-tracking systems, controllers, and Head-mounted displays (HMDs) for precise avatar tracking, smartphone AR is limited to back and front cameras alongside inertial measurement units (IMUs). Previous work has explored animating smartphone users' avatars using singlepoint Inverse Kinematics (IK) derived from smartphone tracking data. In this paper, we propose an enhanced approach that combines face tracking with single-point IK to improve avatar animation. Our system leverages ARKit to track hand and face positions and orientations to estimate user poses through IK. We evaluate our system's performance against a commercial motion tracking system and traditional single-point IK in terms of tracking accuracy, user embodiment, and user experience. Our findings suggests that our dual-point IK (DPIK) method demonstrated significantly lower mean tracking errors and more consistent performance across joints compared to traditional single-point IK. Although statistical differences in embodiment measures were not significant, users reported a stronger sense of control and agency, particularly in head-tracked movements. Participants also favored DPIK in terms of both Pragmatic and Hedonic qualities, highlighting its potential for improving avatar representation in social AR experiences.","스마트폰 증강현실(AR)은 이미 수많은 소셜 애플리케이션을 선보였습니다. 그러나 스마트폰 AR을 확장하여 VRChat 또는 AltspaceVR과 유사한 소셜 경험을 지원하려면 아바타에 정확하게 애니메이션을 적용하기 위한 강력한 사용자 추적이 필요합니다. 가상 현실(VR)은 정확한 아바타 추적을 위해 핸드 트래킹 시스템, 컨트롤러, 머리 장착형 디스플레이(HMD)와 같은 장치의 이점을 활용하는 반면, 스마트폰 AR은 관성 측정 장치(IMU)와 함께 후면 및 전면 카메라로 제한됩니다. 이전 연구에서는 스마트폰 추적 데이터에서 파생된 단일점 역운동학(IK)을 사용하여 스마트폰 사용자의 아바타 애니메이션을 탐색했습니다. 본 논문에서는 아바타 애니메이션을 개선하기 위해 얼굴 추적과 단일 지점 IK를 결합한 향상된 접근 방식을 제안합니다. 우리 시스템은 ARKit을 활용하여 손과 얼굴 위치 및 방향을 추적하여 IK를 통해 사용자 포즈를 추정합니다. 추적 정확도, 사용자 구현 및 사용자 경험 측면에서 상용 동작 추적 시스템 및 기존 단일 지점 IK에 대해 시스템 성능을 평가합니다. 연구 결과에 따르면 DPIK(이중점 IK) 방법은 기존 단일점 IK에 비해 평균 추적 오류가 훨씬 낮고 관절 전체에서 보다 일관된 성능을 보여주었습니다. 구현 측정의 통계적 차이는 중요하지 않았지만 사용자는 특히 머리 추적 움직임에서 더 강한 통제력과 주체성을 보고했습니다. 참가자들은 또한 실용적이고 쾌락적인 특성 측면에서 DPIK를 선호하여 소셜 AR 경험에서 아바타 표현을 향상시킬 수 있는 잠재력을 강조했습니다.",https://doi.org/10.1109/ISMAR67309.2025.00100,Display & Optics; Interaction & Input,Sensor Fusion,Quantitative Experiment,Algorithm / Method
33,2025,Defying Gravity: Towards Gravitoinertial Retargeting of Acceleration for Virtual Vertical Motion in In-Car VR,중력을 무시하다: 차량 내 VR의 가상 수직 모션을 위한 가속도의 중력 관성 재타겟팅을 향하여,"In-car VR applications typically synchronize virtual motion with real vehicle movement to minimize visual-vestibular mismatch. However, this approach limits virtual movement to directions in which the vehicle can physically move, typically restricting the experience to horizontal motion. This study introduces a method to expand the range of virtual motion by simulating vertical movement, leveraging vehicle acceleration to induce a vertical pitch illusion via manipulation of gravitoinertial perception. We conducted a two-phase study evaluating (1) optimal vertical gain values for maximizing perceptual realism in a controlled environment and (2) user experience factors such as motion sickness and presence in an on-road VR flight simulation under realistic driving conditions. Our findings show that users tend to prefer vertical gains that exceed theoretically valid mappings, and highlight the importance of aligning virtual motion with perceived inertial cues to enhance the realism and coherence of vertical motion in in-car VR applications.","차량 내 VR 애플리케이션은 일반적으로 가상 모션을 실제 차량 움직임과 동기화하여 시각적 전정 불일치를 최소화합니다. 그러나 이 접근 방식은 가상 이동을 차량이 물리적으로 이동할 수 있는 방향으로 제한하여 일반적으로 경험을 수평 이동으로 제한합니다. 본 연구에서는 중력관성 인식 조작을 통해 수직 피치 환상을 유도하기 위해 차량 가속도를 활용하여 수직 이동을 시뮬레이션함으로써 가상 모션 범위를 확장하는 방법을 소개합니다. 우리는 (1) 통제된 환경에서 인지적 사실성을 극대화하기 위한 최적의 수직 이득 값과 (2) 현실적인 운전 조건에서 온로드 VR 비행 시뮬레이션에서 멀미 및 존재감과 같은 사용자 경험 요인을 평가하는 2단계 연구를 수행했습니다. 우리의 연구 결과에 따르면 사용자는 이론적으로 유효한 매핑을 초과하는 수직 이득을 선호하는 경향이 있으며, 차량 내 VR 애플리케이션에서 수직 모션의 사실성과 일관성을 향상시키기 위해 가상 모션을 인지된 관성 신호와 정렬하는 것이 중요하다는 점을 강조합니다.",https://doi.org/10.1109/ISMAR67309.2025.00019,Perception & Cognition,Sensor Fusion,Simulation,Algorithm / Method
34,2025,Design and Evaluation of Pseudo-Haptic Techniques for Simulating Surface Stickiness in VR,VR에서 표면 끈적거림을 시뮬레이션하기 위한 의사 햅틱 기술의 설계 및 평가,"Pseudo-haptics alters vision to evoke haptic perception without dedicated hardware. We introduce a method that reproduces sticky-surface interactions during attaching-detaching 3D direct manipulation in VR by blending three cues—motion gain, surfacedeformation, and vibration—in various combinations. Accurately sensing how strongly a surface “clings” is vital for realistic grasping, adhesion training and material evaluation, yet it has been largely overlooked in pseudo-haptic research. Three studies evaluated these cues. The first experiment compared individual and combined cues on perceived stickiness, confirming that additional cues reliably strengthened perceived stickiness. The second experiment tested how cue number affects tolerance for visual-physical mismatch, indicating that they lowered the minimum detectable threshold though they did not widen the overall tolerated mismatch. The third experiment measured whether and how much added cues sharpen perceptual resolution, showing multiple cues improved perceptual resolution by reducing just noticeable differences by 44 % ($1.8 \times$ finer), doubling discriminable levels from roughly eight with a single cue to sixteen with all cues.","의사 햅틱은 전용 하드웨어 없이도 시각을 변경하여 촉각 인식을 불러일으킵니다. 모션 게인, 표면 변형, 진동의 세 가지 큐를 다양한 조합으로 혼합하여 VR에서 3D 직접 조작을 부착 및 분리하는 동안 끈적한 표면 상호 작용을 재현하는 방법을 소개합니다. 표면이 얼마나 강하게 달라붙는지 정확하게 감지하는 것은 사실적인 파지, 접착력 훈련 및 재료 평가에 필수적이지만 의사 촉각 연구에서는 크게 간과되어 왔습니다. 세 가지 연구에서 이러한 단서를 평가했습니다. 첫 번째 실험에서는 인지된 끈적임에 대한 개별 단서와 결합된 단서를 비교하여 추가 단서가 인지된 끈적임을 확실하게 강화한다는 것을 확인했습니다. 두 번째 실험에서는 큐 번호가 시각적-물리적 불일치에 대한 허용 오차에 어떻게 영향을 미치는지 테스트했습니다. 이는 전체 허용 불일치를 넓히지는 않았지만 감지 가능한 최소 임계값을 낮추었음을 나타냅니다. 세 번째 실험에서는 추가된 단서가 지각 해상도를 얼마나 날카롭게 하는지 여부와 그 정도를 측정했습니다. 즉, 눈에 띄는 차이를 44%($1.8 \times$ 더 미세하게) 줄여 여러 단서가 지각 해상도를 향상시켰음을 보여 주며, 식별 가능한 수준이 단일 단서의 약 ​​8개에서 모든 단서의 16개로 두 배로 늘어났습니다.",https://doi.org/10.1109/ISMAR67309.2025.00108,Interaction & Input,Haptic / Tactile Feedback,Technical Evaluation,Algorithm / Method
35,2025,Design and Evaluation of a Virtual Agent for Interpersonal Emotion Regulation in VR,VR의 대인 감정 조절을 위한 가상 에이전트의 설계 및 평가,"Managing negative emotions through emotion regulation (ER) is key to mental well-being. While virtual reality (VR) shows promise for supporting ER, prior work has primarily focused on selfregulation. This paper introduces a virtual agent that helps users manage emotions through conversation-based ER strategies. We compared three conditions: no agent, an agent with non-supportive responses, and an agent with ER-supportive responses. Results showed that the ER-supportive agent significantly improved users' emotional states and overall experience. Building on this, we conducted a second experiment to examine how the agent's appearance (realistic vs. cartoon) and voice tone (emotional vs. neutral) affect ER. Results indicated that an emotional voice tone improved users' ability to regulate emotions. Although a realistic appearance did not directly improve ER, it increased users' trust and sense of social presence. This paper contributes to VR and human-agent interaction by demonstrating the potential of virtual agents to support ER and offering design implications for future ER-supportive agents.","감정 조절(ER)을 통해 부정적인 감정을 관리하는 것이 정신 건강의 핵심입니다. 가상 현실(VR)은 ER 지원에 대한 가능성을 보여 주지만 이전 연구는 주로 자기 규제에 중점을 두었습니다. 본 논문에서는 대화 기반의 ER 전략을 통해 사용자의 감정 관리를 돕는 가상 에이전트를 소개합니다. 우리는 에이전트 없음, 비지원 응답 에이전트, ER 지원 응답 에이전트의 세 가지 조건을 비교했습니다. 결과는 ER 지원 에이전트가 사용자의 감정 상태와 전반적인 경험을 크게 향상시키는 것으로 나타났습니다. 이를 바탕으로 에이전트의 외모(현실적 대 만화적)와 음성 톤(감정적 대 중립적)이 ER에 어떤 영향을 미치는지 조사하기 위해 두 번째 실험을 수행했습니다. 결과는 감정적인 목소리 톤이 사용자의 감정 조절 능력을 향상시키는 것으로 나타났습니다. 사실적인 외관이 ER을 직접적으로 향상시키지는 못했지만 사용자의 신뢰와 사회적 존재감을 높였습니다. 이 문서는 ER을 지원하는 가상 에이전트의 잠재력을 입증하고 미래의 ER 지원 에이전트에 대한 설계 의미를 제공함으로써 VR 및 인간-에이전트 상호 작용에 기여합니다.",https://doi.org/10.1109/ISMAR67309.2025.00159,Interaction & Input; Perception & Cognition,Natural Language Processing,Other,Design Guidelines
36,2025,Do Vibrotactile-Rendered Virtual Walking Sensations Reduce VR Cybersickness?,진동촉각으로 렌더링된 가상 보행 감각이 VR 사이버 멀미를 줄입니까?,"Cybersickness is a major obstacle to immersive VR experiences, and various strategies have been proposed to overcome it. In particular, providing artificial sensory stimuli is considered a prominent approach, which improves presence and real-virtual congruence, thereby helping the situation. In this study, we evaluated the effectiveness of artificial sensory stimuli in inducing virtual walking sensations in mitigating cybersickness. We conducted two experiments involving a total of 80 participants: (1) a static user experiment ($\mathrm{N}=40$), in which participants used controller-based steering without physical movement, and (2) a dynamic user experiment ($\mathrm{N}=40$), in which participants walked on an omnidirectional treadmill, enabling virtual movement similar to real walking. In both experiments, participants were divided into two groups based on the presence or absence of vibrotactile feedback. We collected heart rate and subjective discomfort data during exposure to a VR environment and then evaluated cybersickness and presence. Across both experiments, the results indicated that virtual walking sensations induced through vibrotactile feedback significantly reduced cybersickness without negatively affecting the perceived task load or presence. Additionally, in the dynamic user locomotion experiment, a greater reduction in cybersickness was observed when vibrotactile feedback was present. These findings suggest that even if the real-virtual sensations are synchronized, vibrotactile feedback may induce cognitive distraction that further mitigates cybersickness.","사이버 멀미는 몰입형 VR 경험에 있어 주요 장애물이며, 이를 극복하기 위한 다양한 전략이 제안되었습니다. 특히, 인공 감각 자극을 제공하는 것은 실재감과 실제 가상 일치성을 향상시켜 상황에 도움을 주는 탁월한 접근 방식으로 간주됩니다. 본 연구에서는 사이버 멀미 완화에 있어 가상 보행 감각을 유도하는 인공 감각 자극의 효과를 평가했습니다. 우리는 총 80명의 참가자가 참여한 두 가지 실험을 수행했습니다. (1) 참가자가 물리적 움직임 없이 컨트롤러 기반 스티어링을 사용하는 정적 사용자 실험($\mathrm{N}=40$)과 (2) 참가자가 전방향 런닝머신 위를 걸어 실제 걷기와 유사한 가상 움직임이 가능한 동적 사용자 실험($\mathrm{N}=40$). 두 실험 모두에서 참가자는 진동촉각 피드백의 유무에 따라 두 그룹으로 나뉘었습니다. VR 환경에 노출되는 동안 심박수와 주관적 불편 데이터를 수집한 후 사이버 멀미와 존재감을 평가했습니다. 두 실험 모두에서 결과는 진동 촉각 피드백을 통해 유도된 가상 보행 감각이 인지된 작업 부하나 존재감에 부정적인 영향을 주지 않으면서 사이버 멀미를 크게 감소시키는 것으로 나타났습니다. 또한 동적 사용자 운동 실험에서는 진동 촉각 피드백이 있을 때 사이버 멀미가 더 크게 감소하는 것으로 관찰되었습니다. 이러한 연구 결과는 실제 가상 감각이 동기화되더라도 진동 촉각 피드백이 인지 방해를 유발하여 사이버 멀미를 더욱 완화할 수 있음을 시사합니다.",https://doi.org/10.1109/ISMAR67309.2025.00097,Perception & Cognition; Interaction & Input,Haptic / Tactile Feedback; Redirected Walking / Locomotion,User Study,Algorithm / Method
37,2025,"Do You Even Need a City Guide Anymore?"" Expert Adoption of XR Technology for Guided City Tours","시티 가이드가 더 이상 필요합니까?"" 시티 가이드 투어를 위한 전문가의 XR 기술 채택","In the tourism industry, Extended Reality (XR) technologies are evolving beyond simple marketing tools into sophisticated platforms that deliver immersive experiences to visitors. While these technologies offer significant potential for enhancing tourism experiences, current research primarily examines the tourist perspective, with limited attention to the professional guides who facilitate these interactions. This study investigates how expert tourist guides adopt and integrate XR technology into their professional practice through a novel process for Expert Adoption of XR technology that theoretically grounds the participatory development process in TAM's acceptance mechanisms. Through a co-design process involving experienced guides, we developed an XR application aimed at complementing, rather than replacing, the guide-tourist relationship. Utilizing the Technology Acceptance Model (TAM) for qualitative research, we identified specific concerns and considerations of tourist guides regarding XR adoption and outlined strategies for addressing these challenges during both development and implementation phases. Our findings reveal task-related, individual, and organizational factors for successful XR integration, as well as misconceptions that must be addressed. We suggest that initial scepticism toward XR can be mitigated by ensuring a positive first encounter with technology. Additionally, recognizing stakeholder preferences for specific XR features and prioritizing the most accepted options can enhance adoption. All relevant components of the task, in our case guided tour, must be considered during development, as neglecting any component can impact the overall acceptance. Furthermore, city guides often feel personally responsible for technological failures, highlighting the need for reliable and stable XR applications. These insights provide a methodology applicable to other domains where expert adoption is essential.","관광 산업에서 확장 현실(XR) 기술은 단순한 마케팅 도구를 넘어 방문객에게 몰입형 경험을 제공하는 정교한 플랫폼으로 진화하고 있습니다. 이러한 기술은 관광 경험을 향상시키는 데 상당한 잠재력을 제공하지만 현재 연구에서는 이러한 상호 작용을 촉진하는 전문 가이드에 대한 관심이 제한되어 주로 관광 관점을 조사합니다. 이 연구는 TAM 수용 메커니즘의 참여 개발 프로세스를 이론적으로 기반으로 하는 XR 기술의 전문가 채택을 위한 새로운 프로세스를 통해 전문 관광 가이드가 XR 기술을 전문 업무에 채택하고 통합하는 방법을 조사합니다. 숙련된 가이드가 참여하는 공동 설계 프로세스를 통해 가이드-관광객 관계를 대체하기보다는 보완하는 것을 목표로 하는 XR 애플리케이션을 개발했습니다. 질적 연구를 위해 TAM(Technology Acceptance Model)을 활용하여 XR 채택에 관한 관광 가이드의 구체적인 우려 사항과 고려 사항을 파악하고 개발 및 구현 단계에서 이러한 문제를 해결하기 위한 전략을 간략하게 설명했습니다. 우리의 연구 결과는 성공적인 XR 통합을 위한 작업 관련, 개인 및 조직적 요인과 해결해야 할 오해를 보여줍니다. 우리는 기술에 대한 긍정적인 첫 만남을 보장함으로써 XR에 대한 초기 회의론을 완화할 수 있다고 제안합니다. 또한, 특정 XR 기능에 대한 이해관계자의 선호도를 인식하고 가장 수용 가능한 옵션의 우선순위를 지정하면 채택률이 높아질 수 있습니다. 작업의 모든 관련 구성 요소(우리의 경우 안내 투어)는 개발 중에 고려해야 합니다. 구성 요소를 무시하면 전체 승인에 영향을 미칠 수 있기 때문입니다. 또한 도시 가이드는 기술적인 실패에 대해 개인적으로 책임을 느끼는 경우가 많으며, 이는 신뢰할 수 있고 안정적인 XR 애플리케이션의 필요성을 강조합니다. 이러한 통찰력은 전문가 채택이 필수적인 다른 영역에 적용할 수 있는 방법론을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00126,Interaction & Input,Deep Learning / Neural Networks,Qualitative Analysis,Algorithm / Method
38,2025,"DocVision: A Seamless, Cross-Device Immersive Active Reading Framework for Digital Academic Literature",DocVision: 디지털 학술 문헌을 위한 원활한 교차 장치 몰입형 능동 읽기 프레임워크,"Active reading is crucial for the acquisition of information and the comprehension of literature. However, existing digital media still encounter several challenges in facilitating active reading, such as the limited contextual space caused by constrained screen size, which hinders effective information integration. Furthermore, the rationality of the layout of various types of literature resources also significantly influences the coherence of the reading experience. To address these challenges, we propose DocVision, a seamless, cross-device immersive active reading framework. To reduce the difficulty of information integration, DocVision incorporates a lightweight resource acquisition framework, which seamlessly integrates resources such as diagrams and literature references into the mixed reality (MR) environment. Furthermore, we propose a dynamic layout method for MR to improve reading continuity in multi-resource contexts. Experimental results demonstrate that DocVision substantially enhances the coherence of readers' reading experience and significantly alleviates cognitive load, offering valuable insights for developing future active reading approaches based on digital media.",적극적인 독서는 정보 습득과 문헌 이해에 매우 중요합니다. 그러나 기존 디지털 미디어는 제한된 화면 크기로 인해 발생하는 제한된 상황 공간과 같은 능동적인 읽기를 촉진하는 데 여전히 몇 가지 문제에 직면해 있으며 이는 효과적인 정보 통합을 방해합니다. 또한 다양한 유형의 문헌 자원 배치의 합리성도 독서 경험의 일관성에 큰 영향을 미칩니다. 이러한 문제를 해결하기 위해 우리는 원활한 교차 장치 몰입형 능동 읽기 프레임워크인 DocVision을 제안합니다. 정보 통합의 어려움을 줄이기 위해 DocVision은 다이어그램 및 문헌 참조와 같은 리소스를 혼합 현실(MR) 환경에 원활하게 통합하는 경량 리소스 획득 프레임워크를 통합합니다. 또한 다중 자원 상황에서 읽기 연속성을 향상시키기 위해 MR의 동적 레이아웃 방법을 제안합니다. 실험 결과는 DocVision이 독자의 독서 경험의 일관성을 크게 향상시키고 인지 부하를 크게 완화하여 디지털 미디어를 기반으로 한 미래의 능동적 독서 접근 방식을 개발하는 데 귀중한 통찰력을 제공한다는 것을 보여줍니다.,https://doi.org/10.1109/ISMAR67309.2025.00093,Interaction & Input,Other,Technical Evaluation,Algorithm / Method
39,2025,Don't Look at Me Like That - How AR Face Recognition Changes Our Social Behaviour,나를 그렇게 보지 마세요 - AR 얼굴 인식이 우리의 사회적 행동을 어떻게 변화시키는가,"Pervasive Augmented Reality, as a context-aware and ubiquitous technology, captures and scans a user's environment to create and tailor augmentations for the user. We assume that Pervasive AR will not only be commonplace in recognising the general environment of a user but also include the identification of people in its context by way of face recognition. While there is a body of research addressing privacy issues with Pervasive AR technology and face recognition individually, less is known about the implications of the asymmetry of information availability and agency on users' perceptions, acceptability, and ethical concerns. In addition, little is known about potential social behaviour changes due to those aspects. We exposed 50 participants to a purpose-developed Pervasive AR technology probe and explored the ethical and social implications of the experience. Our findings show that Pervasive AR, in combination with face recognition and asymmetric information delivery, can lead to skewed social interactions with consequences that affect users' sense of control, agency, and identity. Furthermore, this exploration raises questions about technology acceptability in general when bringing together emerging technologies, like AR and face recognition, as queried here.","상황 인식 및 유비쿼터스 기술인 퍼베이시브 증강 현실(Pervasive Augmented Reality)은 사용자 환경을 캡처하고 스캔하여 사용자를 위한 증강을 생성하고 맞춤화합니다. 우리는 Pervasive AR이 사용자의 일반적인 환경을 인식하는 데 일반적일 뿐만 아니라 얼굴 인식을 통해 상황에 따라 사람을 식별하는 것도 포함할 것이라고 가정합니다. 퍼베이시브 AR 기술 및 얼굴 인식과 관련된 개인 정보 보호 문제를 개별적으로 다루는 연구가 있지만, 정보 가용성의 비대칭성과 주체가 사용자의 인식, 수용성 및 윤리적 우려에 미치는 영향에 대해서는 알려진 바가 거의 없습니다. 또한 이러한 측면으로 인한 잠재적인 사회적 행동 변화에 대해서는 알려진 바가 거의 없습니다. 우리는 목적에 맞게 개발된 퍼베이시브 AR 기술 조사에 50명의 참가자를 노출시키고 해당 경험의 윤리적, 사회적 영향을 탐구했습니다. 우리 연구 결과에 따르면 Pervasive AR은 얼굴 인식 및 비대칭 정보 전달과 결합되어 사용자의 통제력, 주체성 및 정체성에 영향을 미치는 결과를 초래하는 왜곡된 사회적 상호 작용을 초래할 수 있습니다. 또한 이 탐구는 여기에서 쿼리된 AR 및 얼굴 인식과 같은 새로운 기술을 통합할 때 일반적으로 기술 수용 가능성에 대한 질문을 제기합니다.",https://doi.org/10.1109/ISMAR67309.2025.00057,Interaction & Input; Collaboration & Social,Other,User Study,Algorithm / Method
40,2025,Don't Miss Notifications: Exploring Gaze Notifications for Virtual Reality Cooking Environment,알림을 놓치지 마세요: 가상 현실 요리 환경에 대한 시선 알림 탐색,"In 3D environments, designing efficient notifications is crucial for capturing user attention. While visual notifications -such as objectattached and fixed-position ones- are commonly used in virtual environments, they often require users to shift their gaze away from task-relevant areas, which can interrupt workflow and delay responses. To address these limitations, we designed two gaze-based notification techniques to provide responsive and intuitive notification in a virtual reality (VR) cooking task. We evaluated four different notification types: two world-fixed notifications (onObject and onDock) and two gaze-based methods (GazeCue and Gaze+Dock) with 16 participants. Our results show that participants performed better in using gaze-based notifications compared to world-fixed ones. Questionnaire results indicated higher usability, greater perceived presence, and lower cognitive load for gaze-based notifications. These results highlight the effectiveness of gaze-based notification techniques in VR by reducing the need for visual search and minimizing cognitive load. Our findings provide insights for developers or engineers to design more intuitive and responsive user-interfaces for 3D environments.","3D 환경에서는 사용자의 관심을 끌기 위해 효율적인 알림을 디자인하는 것이 중요합니다. 객체 부착 및 고정 위치 알림과 같은 시각적 알림은 일반적으로 가상 환경에서 사용되지만 사용자가 작업 관련 영역에서 시선을 이동해야 하는 경우가 많아 작업 흐름을 방해하고 응답이 지연될 수 있습니다. 이러한 제한 사항을 해결하기 위해 우리는 가상 현실(VR) 요리 작업에서 반응적이고 직관적인 알림을 제공하는 두 가지 시선 기반 알림 기술을 설계했습니다. 우리는 4가지 알림 유형, 즉 16명의 참가자를 대상으로 2개의 고정된 알림(onObject 및 onDock)과 2개의 시선 기반 방법(GazeCue 및 Gaze+Dock)을 평가했습니다. 우리의 결과는 참가자들이 세계 고정 알림에 비해 시선 기반 알림을 사용하는 경우 더 나은 성과를 냈다는 것을 보여줍니다. 설문지 결과에 따르면 시선 기반 알림의 사용성이 더 높고, 인지된 존재감이 더 높으며, 인지 부하가 ​​더 낮은 것으로 나타났습니다. 이러한 결과는 시각적 검색의 필요성을 줄이고 인지 부하를 최소화함으로써 VR에서 시선 기반 알림 기술의 효율성을 강조합니다. 우리의 연구 결과는 개발자나 엔지니어가 3D 환경을 위한 보다 직관적이고 반응성이 뛰어난 사용자 인터페이스를 설계할 수 있는 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00062,Perception & Cognition; Interaction & Input,Other,User Study,Algorithm / Method; User Study / Empirical Findings
41,2025,Dual Focus Multiscale Attention for Object Detection in Mixed Reality: Leveraging Customizable Synthetic Datasets,혼합 현실에서 객체 감지를 위한 이중 초점 다중 규모 주의: 사용자 정의 가능한 합성 데이터 세트 활용,"We propose a novel object detection framework tailored for mixed reality (MR), combining a customizable synthetic dataset with a lightweight attention-enhanced detection model. Our dataset generation pipeline synthesizes planetary and telescope foregrounds with hybrid real-synthetic backgrounds, enabling robust learning across variable lighting and occlusion scenarios—challenges common in educational MR environments. At the core of our architecture is the Dual Focus Multiscale Attention (DFMA) module, which simultaneously refines spatial and channel-wise features at multiple scales. Integrated into a YOLO-based (You Only Look Once) backbone and FPN, DFMA significantly improves feature discrimination while preserving real-time efficiency. On MS COCO our model improves mean Average Precision (mAP) across Intersection over Union (IoU) thresholds from 0.5 to 0.95 (mAP@0.5:0.95) over state-of-the-art nano detectors from 39.3% to 41.3% (± 2%) at only +6% params and +3% GFLOPs, with a notable reduction in false positives on visually similar, low-textured objects. We further demonstrate real-time deployment in a Unity-based MR application, highlighting the system's effectiveness in immersive astronomy-focused educational scenarios. Our results underscore the potential of synthetic data and multiscale attention to bridge accuracy, speed, and realism in next generation MR systems.","우리는 사용자 정의 가능한 합성 데이터 세트와 가벼운 주의 강화 감지 모델을 결합하여 혼합 현실(MR)에 맞춰진 새로운 객체 감지 프레임워크를 제안합니다. Our dataset generation pipeline synthesizes planetary and telescope foregrounds with hybrid real-synthetic backgrounds, enabling robust learning across variable lighting and occlusion scenarios—challenges common in educational MR environments. 우리 아키텍처의 핵심에는 공간 및 채널별 기능을 여러 규모로 동시에 개선하는 DFMA(Dual Focus Multiscale Attention) 모듈이 있습니다. YOLO(You Only Look Once) 기반 백본 및 FPN에 통합된 DFMA는 실시간 효율성을 유지하면서 기능 차별성을 크게 향상시킵니다. MS COCO에서 우리 모델은 +6% 매개변수 및 +3% GFLOP에서 최첨단 나노 검출기에 비해 IoU(Intersection over Union) 임계값에 대한 평균 평균 정밀도(mAP)를 0.5에서 0.95(mAP@0.5:0.95)로 향상시켰으며, 시각적으로 유사하고 텍스처가 낮은 개체에 대한 오탐률이 눈에 띄게 감소했습니다. 또한 Unity 기반 MR 애플리케이션의 실시간 배포를 시연하여 몰입형 천문학 중심 교육 시나리오에서 시스템의 효율성을 강조합니다. 우리의 결과는 합성 데이터의 잠재력과 차세대 MR 시스템의 교량 정확성, 속도 및 현실성에 대한 다중 규모 관심을 강조합니다.",https://doi.org/10.1109/ISMAR67309.2025.00118,Perception & Cognition; Rendering & Visualization,Deep Learning / Neural Networks,Quantitative Experiment; Technical Evaluation,System / Framework; Algorithm / Method
42,2025,EEG2Gaussian: Decoding and Visualizing Visual-Evoked EEG for VR Scenes Using 3D Gaussian Splatting,EEG2Gaussian: 3D 가우스 스플래팅을 사용하여 VR 장면에 대한 시각적 유발 EEG 디코딩 및 시각화,"Decoding and visualizing brain activity evoked by visual stimuli is critical for both understanding neural mechanisms and advancing brain-computer interfaces (BCIs). However, non-invasive signals such as Electroencephalogram (EEG) present significant challenges due to their inherently low signal-to-noise ratios. Although recent deep learning methods have resolved this task, most approaches are confined to 2D visualizations that fail to capture the complexities of real-world 3D perception. In this research, we investigate the relationship between EEG signals and 3D visual stimuli presented in virtual reality (VR) scenes, aiming to extract taskrelevant semantics from the EEG responses elicited by these stimuli. We introduce EEG2Gaussian, a novel framework for decoding and visualizing visual-evoked EEG signals by reconstructing immersive VR scenes using 3D Gaussian Splatting. The framework consists of three stages. The preprocessing stage removes noise and artifacts from raw EEG signals to provide cleaner input for subsequent processing. In the encoding stage, we propose a Neural Temporal-Frequency Encoder (NTF-Encoder) to extract temporal and frequency features using fused channel and band attention mechanisms, and disentangles them into high-level and low-level semantic representations. In the decoding stage, a 3D EEG Decoder takes these multi-level features through separate pathways as conditional inputs to guide the reconstruction of semantically consistent VR scenes. Furthermore, we construct a VR-EEG dataset that pairs real-time EEG recordings with VR scenes, and analyze how different types of scenes affect EEG responses across frequency bands. Our experimental results show that EEG2Gaussian can reconstruct VR scenes that are semantically aligned with the visual stimuli. Ablation studies verify the effectiveness of channel and band attention in EEG feature encoding, and demonstrate that combining high-level and low-level semantic features enhances the consistency and interpretability of the reconstructed scenes.",시각적 자극에 의해 유발되는 뇌 활동을 디코딩하고 시각화하는 것은 신경 메커니즘을 이해하고 뇌-컴퓨터 인터페이스(BCI)를 발전시키는 데 중요합니다. 그러나 뇌전도(EEG)와 같은 비침습적 신호는 본질적으로 낮은 신호 대 잡음비로 인해 심각한 문제를 안겨줍니다. 최근 딥 러닝 방법으로 이 문제가 해결되었지만 대부분의 접근 방식은 실제 3D 인식의 복잡성을 포착하지 못하는 2D 시각화에만 국한되어 있습니다. 본 연구에서는 EEG 신호와 가상 현실(VR) 장면에 제시된 3D 시각적 자극 사이의 관계를 조사하여 이러한 자극에 의해 유발된 EEG 반응에서 작업 관련 의미를 추출하는 것을 목표로 합니다. 3D Gaussian Splatting을 사용하여 몰입형 VR 장면을 재구성하여 시각적으로 유발되는 EEG 신호를 디코딩하고 시각화하기 위한 새로운 프레임워크인 EEG2Gaussian을 소개합니다. 프레임워크는 세 단계로 구성됩니다. 전처리 단계에서는 원시 EEG 신호에서 잡음과 아티팩트를 제거하여 후속 처리를 위한 보다 깨끗한 입력을 제공합니다. 인코딩 단계에서 우리는 융합된 채널 및 밴드 어텐션 메커니즘을 사용하여 시간 및 주파수 특징을 추출하고 이를 상위 수준 및 하위 수준 의미 표현으로 분리하는 NTF-인코더(Neural Temporal-Frequency Encoder)를 제안합니다. 디코딩 단계에서 3D EEG 디코더는 의미상 일관된 VR 장면의 재구성을 안내하기 위해 별도의 경로를 통해 이러한 다중 레벨 기능을 조건부 입력으로 사용합니다. 또한 실시간 EEG 기록과 VR 장면을 결합하는 VR-EEG 데이터 세트를 구성하고 다양한 유형의 장면이 주파수 대역 전반에 걸쳐 EEG 반응에 어떻게 영향을 미치는지 분석합니다. 우리의 실험 결과는 EEG2Gaussian이 시각적 자극과 의미론적으로 정렬된 VR 장면을 재구성할 수 있음을 보여줍니다. 절제 연구는 EEG 특징 인코딩에서 채널 및 밴드 주의의 효율성을 검증하고 높은 수준과 낮은 수준의 의미론적 특징을 결합하면 재구성된 장면의 일관성과 해석 가능성이 향상된다는 것을 보여줍니다.,https://doi.org/10.1109/ISMAR67309.2025.00105,Rendering & Visualization; Interaction & Input,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method
43,2025,Effects of Organizational and Behavioral Reactions of Virtual Crowds on Users' Visual Attention in a Stressful Virtual Reality Simulation,스트레스가 많은 가상 현실 시뮬레이션에서 가상 군중의 조직적 및 행동적 반응이 사용자의 시각적 주의에 미치는 영향,"Understanding visual attention in users during stressful situations with emotional crowds is crucial for the improvement of emergency response, safety, and efficiency. To explore this, we conducted a study examining how the organizational structure and reaction behavior of a virtual emotional crowd affected subjects' visual attention. Participants completed a task in a virtual reality simulation featuring a fire threat. The experiment employed two between-subjects factors (Crowd Type and Reaction Type) and one within-subjects factor (Simulation Phase). The simulation consisted of a normal phase, where participants interacted with virtual human (VHs) pedestrians and vendors in a virtual marketplace, followed by a stressful phase featuring a fire threat within the same setting. The Crowd Type varied between individuals walking alone (Individual Condition) and small groups of 3-4 members (Group Condition). The Reaction Type determined whether the virtual crowd ignored the threat or moved faster to escape. We analyzed subjects' fixation gaze across conditions, revealing that visual attention behavior was significantly influenced by Crowd Type, Reaction Type, and Simulation Phase.","감정적인 군중이 있는 스트레스가 많은 상황에서 사용자의 시각적 주의를 이해하는 것은 비상 대응, 안전 및 효율성을 향상시키는 데 중요합니다. 이를 탐구하기 위해 우리는 가상 감정 군중의 조직 구조와 반응 행동이 피험자의 시각적 주의에 어떤 영향을 미치는지 조사하는 연구를 수행했습니다. 참가자들은 화재 위협이 포함된 가상 현실 시뮬레이션에서 작업을 완료했습니다. 실험에서는 두 가지 개체 간 요인(군중 유형 및 반응 유형)과 한 가지 개체 내 요인(시뮬레이션 단계)을 사용했습니다. 시뮬레이션은 참가자가 가상 ​​시장에서 가상 인간(VH) 보행자 및 판매자와 상호 작용하는 일반 단계와 동일한 환경 내에서 화재 위협이 있는 스트레스가 많은 단계로 구성되었습니다. 군중 유형은 혼자 걷는 개인(개인 조건)과 3~4명의 소규모 그룹(그룹 조건)으로 다양했습니다. 반응 유형은 가상 군중이 위협을 무시했는지 또는 탈출하기 위해 더 빠르게 움직이는지 여부를 결정합니다. 우리는 조건에 따라 피험자의 시선을 분석하여 시각적 주의 행동이 군중 유형, 반응 유형 및 시뮬레이션 단계에 의해 크게 영향을 받는 것으로 나타났습니다.",https://doi.org/10.1109/ISMAR67309.2025.00072,Interaction & Input,Deep Learning / Neural Networks,User Study,User Study / Empirical Findings
44,2025,Effects of Peripheral Optic Flow Location and Speed on Unintended Positional Drift During Walk-in-Place in VR,VR에서 워크인 플레이(Walk-in-Place) 중 의도하지 않은 위치 드리프트에 대한 주변 광학 흐름 위치 및 속도의 영향,"Vection can elicit a compelling illusion of self-motion, even when the user remains physically stationary. When combined with user actions, this illusion can enhance the sense of presence in virtual environments. However, vection may unintentionally influence physical locomotion, causing unintended positional drift (UPD). UPD is especially problematic for Walk-In-Place (WIP) navigation in virtual reality (VR), where users simulate locomotion by mimicking walking motions without actual displacement. In this study, we investigated how vection affects UPD during WIP navigation in immersive VR. We specifically manipulated the location of peripheral optic flow (left, right, bilateral) and speed (low, high) using dynamic textures on the lateral walls of the virtual environment to induce various vection conditions. Twenty-seven participants performed WIP under six vection conditions (three locations × two speeds) and one baseline condition without optic flow. Our results revealed that bilateral vection caused the greatest UPD, particularly along the lateral axis, while unilateral vection tended to suppress lateral drift. However, peripheral optic flow speed did not significantly affect UPD. We discuss these findings and their implications for designing immersive virtual environments that mitigate UPD while maintaining the sense of presence.","Vection은 사용자가 물리적으로 정지해 있는 경우에도 자체 모션의 강력한 환상을 이끌어낼 수 있습니다. 사용자 작업과 결합하면 이러한 환상이 가상 환경에서 존재감을 향상시킬 수 있습니다. 그러나 벡터는 의도하지 않게 물리적 이동에 영향을 주어 의도하지 않은 위치 드리프트(UPD)를 일으킬 수 있습니다. UPD는 사용자가 실제 변위 없이 걷는 동작을 모방하여 운동을 시뮬레이션하는 가상 현실(VR)의 WIP(Walk-In-Place) 탐색에서 특히 문제가 됩니다. 본 연구에서는 몰입형 VR에서 WIP 탐색 중 벡터가 UPD에 어떤 영향을 미치는지 조사했습니다. 우리는 다양한 벡터 조건을 유도하기 위해 가상 환경 측면 벽의 동적 텍스처를 사용하여 주변 광 흐름(왼쪽, 오른쪽, 양측) 및 속도(낮음, 높음)의 위치를 ​​구체적으로 조작했습니다. 27명의 참가자는 6가지 벡터 조건(3개 위치 × 2개 속도)과 광학 흐름이 없는 하나의 기본 조건에서 WIP를 수행했습니다. 우리의 결과는 양측 벡터가 특히 측면 축을 따라 가장 큰 UPD를 유발하는 반면 일방적인 벡터는 측면 드리프트를 억제하는 경향이 있음을 보여주었습니다. 그러나 주변 광학 흐름 속도는 UPD에 큰 영향을 미치지 않았습니다. 우리는 존재감을 유지하면서 UPD를 완화하는 몰입형 가상 환경 설계에 대한 이러한 연구 결과와 그 의미에 대해 논의합니다.",https://doi.org/10.1109/ISMAR67309.2025.00154,Interaction & Input,Sensor Fusion,User Study,User Study / Empirical Findings
45,2025,EgoBlur: Blurry Egocentric XR Dataset for Robust Fast Hand Pose Estimation,EgoBlur: 강력하고 빠른 손 자세 추정을 위한 Blurry Egocentric XR 데이터 세트,"Hand tracking in XR serves as a fundamental interaction mechanism, as it allows users to directly interact with virtual content. Accurate 3D hand pose estimation is essential in scenarios involving dynamic hand motions, such as gaming, sports, and virtual musical instruments. These dynamic hand motions often result in motion blur when the hand moves faster than the frame rate of the cameras, making pose estimation challenging. The state of the art methods for 3D hand pose estimation uses deep learning that requires large amounts of data with 3D hand pose ground truth. However, most of the existing publicly available hand pose datasets are captured from static or slowly moving hands that do not contain any explicit motion blur. While techniques such as using short exposure times with higher frame rates have been employed to reduce motion blur, they still pose limitations for developing accurate hand pose estimation algorithms in the presence of fast motion. To address these challenges, firstly, we introduce a new dataset, EgoBlur, consisting of egocentric hand videos with real blur captured from a prototype Head-mounted headset. Our dataset contains $\sim 100 \mathrm{k}$ images along with accurate and temporally consistent 3D hand pose ground truth. Secondly, we propose EgoBlurNet, a deep learning model capable of estimating 3D hand keypoints from blurry egocentric images by employing a teacher-student paradigm. Experimental results demonstrate that our method provides reliable and accurate 3D hand pose for blurred hand images compared to existing methods, especially in realistic dynamic XR scenarios.","XR의 핸드 트래킹은 사용자가 가상 ​​콘텐츠와 직접 상호 작용할 수 있도록 해주기 때문에 기본적인 상호 작용 메커니즘 역할을 합니다. 게임, 스포츠, 가상 악기 등 동적 손 동작이 포함된 시나리오에서는 정확한 3D 손 자세 추정이 필수적입니다. 이러한 동적 손 동작으로 인해 손이 카메라의 프레임 속도보다 빠르게 움직일 때 모션 블러가 발생하여 포즈 추정이 어려워지는 경우가 많습니다. 3D 손 포즈 추정을 위한 최첨단 방법은 3D 손 포즈 실측 데이터가 포함된 대량의 데이터가 필요한 딥 러닝을 사용합니다. 그러나 기존에 공개적으로 사용 가능한 손 포즈 데이터 세트의 대부분은 명시적인 모션 블러가 포함되지 않은 정적 손 또는 천천히 움직이는 손에서 캡처됩니다. While techniques such as using short exposure times with higher frame rates have been employed to reduce motion blur, they still pose limitations for developing accurate hand pose estimation algorithms in the presence of fast motion. 이러한 문제를 해결하기 위해 먼저 프로토타입 머리 장착형 헤드셋에서 캡처한 실제 블러가 포함된 자기 중심적 손 비디오로 구성된 새로운 데이터 세트인 EgoBlur를 소개합니다. 우리의 데이터 세트에는 정확하고 시간적으로 일관된 3D 손 포즈 실제값과 함께 $\sim 100 \mathrm{k}$ 이미지가 포함되어 있습니다. 둘째, 교사-학생 패러다임을 적용하여 흐릿한 자기중심적 이미지로부터 3D 손 키포인트를 추정할 수 있는 딥러닝 모델인 EgoBlurNet을 제안합니다. 실험 결과는 우리의 방법이 특히 현실적인 동적 XR 시나리오에서 기존 방법에 비해 흐릿한 손 이미지에 대해 안정적이고 정확한 3D 손 포즈를 제공한다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR67309.2025.00106,Interaction & Input; Tracking & Localization,Hand / Gesture Recognition,Technical Evaluation,Algorithm / Method; Dataset / Benchmark
46,2025,Emotional Art: Exploring a Novel Paradigm of Artistic Recreation Based on Emotion Capture in VR,감성 예술: VR 감성 포착을 기반으로 한 예술적 재현의 새로운 패러다임 탐구,"Virtual reality (VR), as an immersive interactive technology that offers visual and auditory experiences, creates a fertile ground for both art appreciation and creation. However, current VR art face challenges such as limited experiences, restricted interaction and inadequate engagement. Accordingly, this study explores the design of the emotional feedback mechanism in museum-based VR environments. We proposed a novel paradigm for art interaction, embedding emotional feedback into the artistic re-creation to enrich and personalize the experience of artwork. Through an empirical study $(\mathrm{N}=48)$ with a between-subjects design, we demonstrates that this artistic recreation enhances user engagement and emotional states by emphasizing audiovisual quality, interactive flexibility, and narrative coherence with the physical museum. We also shed light on the future design of interaction between people and art that aim to facilitate deeper, more engaged experience.","가상 현실(VR)은 시각적, 청각적 경험을 제공하는 몰입형 인터랙티브 기술로서 예술 감상과 창작 모두를 위한 비옥한 기반을 조성합니다. 그러나 현재의 VR 예술은 제한된 경험, 제한된 상호 작용, 부적절한 참여와 같은 문제에 직면해 있습니다. 따라서 본 연구에서는 박물관 기반 VR 환경에서 감성 피드백 메커니즘의 설계를 탐구합니다. 우리는 예술 작품의 경험을 풍부하게 하고 개인화하기 위해 예술적 재현에 감정적 피드백을 포함시키는 예술 상호 작용을 위한 새로운 패러다임을 제안했습니다. 주제 간 디자인을 사용한 실증적 연구 $(\mathrm{N}=48)$를 통해 우리는 이러한 예술적 재현이 시청각 품질, 대화형 유연성 및 실제 박물관과의 서술적 일관성을 강조함으로써 사용자 참여와 감정 상태를 향상시킨다는 것을 보여줍니다. 우리는 또한 더 깊고 참여도가 높은 경험을 촉진하는 것을 목표로 하는 사람과 예술 간의 상호 작용의 미래 디자인에 대해 조명합니다.",https://doi.org/10.1109/ISMAR67309.2025.00073,Audio & Sound,Other,User Study,User Study / Empirical Findings
47,2025,Enhanced Adaptive Subdivision for Dynamic Mesh Compression to Improve Visual Quality in VR/AR Rendering,VR/AR 렌더링의 시각적 품질을 향상시키기 위해 동적 메시 압축을 위한 향상된 적응형 세분화,"Volumetric display technologies continue to advance, accompanied by increasing complexity in volumetric media services, particularly for XR (eXtended Reality) mobile devices. This trend underscores the need for efficient compression and transmission of volumetric video. In response, ISO/IEC JTC1/SC29 WG7 is developing a standard for mesh-based volumetric media, known as Video-based Dynamic Mesh Coding (V-DMC). V-DMC reduces geometry bitstream size through lossy mesh decimation and compensates for over-simplified geometry using mesh subdivision and displacement generation, which includes an adaptive face-and-edge update process. However, the existing subdivision framework often struggles to recover fine surface details due to rigid subdivision criteria and unintended mesh separations in geometrically irregular regions. This leads to visible artifacts-especially under dynamic lighting and viewpoint changes common in XR environments. To address these issues, this paper analyzes key performance bottlenecks and introduces several implementation strategies to improve both compression efficiency and visual fidelity through an enhanced adaptive subdivision framework. The proposed method introduces Level-of-Detail (LoD)-aware subdivision to improve the flexibility and effectiveness of the adaptive face-and-edge update. It also incorporates regional feature-aware adaptivity to mitigate geometric distortions, particularly in narrow or topologically complex regions. Experimental results using MPEG-I test content demonstrate a $10-20 \%$ reduction in displacement bitstream size and BD-rate gains of $2-5 \%$ compared to the V-DMC Test Model (TMM v12.0). These results confirm that the proposed method shows promise in enhancing both visual fidelity and compression efficiency, potentially contributing to the fulfillment of the stringent requirements of modern 3D rendering.",체적 디스플레이 기술은 특히 XR(eXtended Reality) 모바일 장치의 체적 미디어 서비스의 복잡성 증가와 함께 계속해서 발전하고 있습니다. 이러한 추세는 볼륨 비디오의 효율적인 압축 및 전송에 대한 필요성을 강조합니다. 이에 대응하여 ISO/IEC JTC1/SC29 WG7은 V-DMC(비디오 기반 동적 메시 코딩)로 알려진 메시 기반 체적 미디어에 대한 표준을 개발하고 있습니다. V-DMC는 손실이 있는 메시 축소를 통해 형상 비트스트림 크기를 줄이고 적응형 면 및 가장자리 업데이트 프로세스가 포함된 메시 세분화 및 변위 생성을 사용하여 과도하게 단순화된 형상을 보상합니다. 그러나 기존 세분화 프레임워크는 엄격한 세분화 기준과 기하학적으로 불규칙한 영역의 의도하지 않은 메쉬 분리로 인해 미세한 표면 세부 사항을 복구하는 데 종종 어려움을 겪습니다. 이로 인해 눈에 보이는 아티팩트가 발생합니다. 특히 동적 조명 및 XR 환경에서 흔히 발생하는 시점 변경이 발생합니다. 이러한 문제를 해결하기 위해 이 문서에서는 주요 성능 병목 현상을 분석하고 향상된 적응형 세분화 프레임워크를 통해 압축 효율성과 시각적 충실도를 모두 향상시키는 여러 구현 전략을 소개합니다. 제안된 방법은 적응형 면 및 가장자리 업데이트의 유연성과 효율성을 향상시키기 위해 세부 수준(LoD) 인식 세분화를 도입합니다. 또한 특히 좁거나 위상적으로 복잡한 지역에서 기하학적 왜곡을 완화하기 위해 지역적 특징 인식 적응성을 통합합니다. MPEG-I 테스트 콘텐츠를 사용한 실험 결과는 V-DMC 테스트 모델(TMM v12.0)과 비교하여 변위 비트스트림 크기가 $10-20 \%$ 감소하고 BD 속도가 $2-5\%$ 향상되었음을 보여줍니다. 이러한 결과는 제안된 방법이 시각적 충실도와 압축 효율성을 모두 향상시켜 현대 3D 렌더링의 엄격한 요구 사항을 충족하는 데 잠재적으로 기여할 수 있음을 확인합니다.,https://doi.org/10.1109/ISMAR67309.2025.00149,Rendering & Visualization,Deep Learning / Neural Networks,Other,Algorithm / Method
48,2025,Enhanced Velocity Field Modeling for Gaussian Video Reconstruction,가우스 비디오 재구성을 위한 향상된 속도장 모델링,"High-fidelity 3D video reconstruction is essential for enabling real-time rendering of dynamic scenes with realistic motion in VR/AR. The deformation field paradigm of 3D Gaussian splatting has achieved near-photorealistic results in video reconstruction due to the great representation capability of deep deformation networks. However, in videos with complex motion and significant scale variations, deformation networks often overfit to irregular Gaussian trajectories, leading to suboptimal visual quality. Moreover, the gradient-based densification strategy designed for static scene reconstruction proves inadequate to address the absence of dynamic content. In light of these challenges, we propose a flow-empowered velocity field modeling scheme tailored for Gaussian video reconstruction, dubbed FlowGaussian-VR. It consists of two core components: a velocity field rendering (VFR) pipeline which enables optical flow-based optimization, and a flow-assisted adaptive densification (FAD) strategy that adjusts the number and size of Gaussians in dynamic regions. We also explore a temporal velocity refinement (TVR) post-processing algorithm to further estimate and correct noise in Gaussian trajectories via extended Kalman filtering. We validate our model's effectiveness on multi-view dynamic reconstruction and novel view synthesis with real-world datasets containing challenging motion scenarios, demonstrating not only notable visual improvements (over 2.5 dB gain in PSNR) and less blurry artifacts in dynamic textures, but also regularized and trackable per-Gaussian trajectories.","VR/AR에서 사실적인 모션으로 역동적인 장면을 실시간 렌더링하려면 고화질 3D 비디오 재구성이 필수적입니다. The deformation field paradigm of 3D Gaussian splatting has achieved near-photorealistic results in video reconstruction due to the great representation capability of deep deformation networks. 그러나 동작이 복잡하고 규모 변화가 큰 비디오에서는 변형 네트워크가 불규칙한 가우스 궤적에 과도하게 맞춰져 시각적 품질이 최적화되지 않는 경우가 많습니다. 더욱이, 정적 장면 재구성을 위해 설계된 그라디언트 기반 치밀화 전략은 동적 콘텐츠의 부재를 해결하기에는 부적절하다는 것이 입증되었습니다. 이러한 과제를 해결하기 위해 우리는 FlowGaussian-VR이라고 불리는 가우시안 비디오 재구성에 맞춰진 흐름 기반 속도 필드 모델링 방식을 제안합니다. 이는 광학 흐름 기반 최적화를 가능하게 하는 속도 필드 렌더링(VFR) 파이프라인과 동적 영역에서 가우시안의 수와 크기를 조정하는 흐름 보조 적응형 밀도화(FAD) 전략이라는 두 가지 핵심 구성 요소로 구성됩니다. 또한 확장된 칼만 필터링을 통해 가우스 궤적의 노이즈를 추가로 추정하고 수정하기 위해 TVR(시간적 속도 개선) 후처리 알고리즘을 탐색합니다. 우리는 까다로운 모션 시나리오가 포함된 실제 데이터 세트를 사용하여 다중 뷰 동적 재구성 및 새로운 뷰 합성에 대한 모델의 효율성을 검증하여 눈에 띄는 시각적 개선(PSNR에서 2.5dB 이상 이득)과 동적 텍스처의 덜 흐릿한 아티팩트뿐만 아니라 정규화되고 추적 가능한 가우스별 궤적도 보여줍니다.",https://doi.org/10.1109/ISMAR67309.2025.00042,Rendering & Visualization,3D Reconstruction,Technical Evaluation,Algorithm / Method
49,2025,Enhancing Dazzling Sensation Through Pseudo-Blink and Afterimage Presentation in Virtual Reality,가상현실에서 가상 깜박임과 잔상 표현을 통해 눈부신 감각 강화,"Humans sometimes encounter dazzling light in everyday life, such as sunlight or high-beam headlights, which often evoke instinctive biological responses like blinking and afterimages. Accurately reproducing such phenomena is essential for enhancing realism in virtual reality (VR), especially in scenarios like training simulations or entertainment experiences. While conventional methods either rely on high-luminance displays or simulate isolated perceptual effects such as glare, few approaches have explored the combination of multiple physiological responses to intensify brightness perception without using physically intense light sources. This study addresses the gap by investigating how pseudo-blinking and afterimage presentation, in addition to glare effects, can enhance the dazzling sensation in VR. We developed a novel system that synchronizes screen darkening with skin-deformation-like vibration stimuli around the eyes to simulate a blink reflex, followed by gaze-dependent afterimage presentation that dynamically transitions to complementary colors in response to user blinking. Experimental results with 19 participants revealed that both pseudo-blinking and afterimage effects significantly enhanced the perceived dazzling sensation and realism. These findings suggest that simulating involuntary physiological reactions to light in VR can evoke strong perceptual impressions, enabling safe and cost-effective glare expression.","인간은 일상생활에서 햇빛이나 상향등 헤드라이트와 같은 눈부신 빛을 접할 때가 있는데, 이는 눈 깜박임이나 잔상과 같은 본능적인 생물학적 반응을 불러일으키는 경우가 많습니다. 특히 훈련 시뮬레이션이나 엔터테인먼트 경험과 같은 시나리오에서 가상 현실(VR)의 현실감을 높이려면 이러한 현상을 정확하게 재현하는 것이 필수적입니다. 기존 방법은 고휘도 디스플레이에 의존하거나 눈부심과 같은 고립된 지각 효과를 시뮬레이션하는 반면, 물리적으로 강렬한 광원을 사용하지 않고 밝기 지각을 강화하기 위해 여러 생리학적 반응의 조합을 탐구한 접근법은 거의 없습니다. 본 연구에서는 눈부심 효과 외에도 의사 깜박임과 잔상 표현이 VR에서 눈부신 감각을 향상시킬 수 있는 방법을 조사하여 이러한 격차를 해결합니다. 우리는 눈 주위의 피부 변형과 같은 진동 자극과 화면 어두워짐을 동기화하여 눈 깜박임 반사를 시뮬레이션한 다음, 사용자 깜박임에 반응하여 보색으로 동적으로 전환되는 시선 의존 잔상 표현을 동기화하는 새로운 시스템을 개발했습니다. 19명의 참가자를 대상으로 한 실험 결과, 의사 깜박임과 잔상 효과 모두 눈부신 감각과 현실감을 크게 향상시키는 것으로 나타났습니다. 이러한 연구 결과는 VR에서 빛에 대한 비자발적인 생리적 반응을 시뮬레이션하면 강한 지각적 인상을 불러일으켜 안전하고 비용 효율적인 눈부심 표현이 가능함을 시사합니다.",https://doi.org/10.1109/ISMAR67309.2025.00074,Perception & Cognition; Education & Training,Sensor Fusion,User Study,Algorithm / Method; User Study / Empirical Findings
50,2025,Enhancing Pseudo-Haptics on the Hand by Viewpoint Shifts in VR,VR의 관점 전환을 통해 손의 의사 햅틱 향상,"We present a pseudo-haptic technique that modulates perceived weight in full-body virtual reality by jointly scaling virtual-hand motion and user viewpoint. Unlike prior study that manipulates the hand alone, our method leverages visual self-motion cues, extending pseudo-haptics beyond seated interactions. A threshold study isolated perceptually salient gains, and a within-subjects lifting experiment ($N=23$) crossed the two factors. Both manipulations significantly modulated perceived weight ratings ($p<.001$), and their effects combined additively. However, presence ratings declined specifically under conditions designed to produce strong lightness illusions-namely, when the virtual hand moved more than its real-world counterpart, while the virtual viewpoint moved less ($p_{\text {Holm }}<.05$). In contrast, when the virtual hand moved more slowly or matched the real hand's motion, manipulating the virtual viewpoint had relatively little impact on presence, which remained stable. Hand scaling thus serves as a low-cost primary cue and viewpoint manipulation as a complementary channel for modulating the perceived weight without sacrificing presence. The technique provides actionable guidance for VR training, rehabilitation, and exergames that demand convincing sensations of physical effort.","우리는 가상 손 동작과 사용자 시점을 공동으로 조정하여 전신 가상 현실에서 인지된 무게를 조절하는 의사 촉각 기술을 제시합니다. 손만 조작하는 이전 연구와는 달리, 우리의 방법은 시각적인 자체 모션 신호를 활용하여 앉아 있는 상호 작용을 넘어 의사 햅틱을 확장합니다. 지각적으로 두드러진 이득을 분리한 임계값 연구와 피험자 내 리프팅 실험($N=23$)이 두 요소를 교차했습니다. 두 조작 모두 인지된 가중치 등급($p<.001$)을 크게 조정했으며 그 효과는 추가로 결합되었습니다. 그러나 강한 밝기 환상을 생성하도록 설계된 조건, 즉 가상 손이 실제 손보다 더 많이 움직이는 반면 가상 시점은 덜 움직이는 경우($p_{\text {Holm }}<.05$)에서는 존재감 등급이 특별히 감소했습니다. 대조적으로, 가상 손이 더 느리게 움직이거나 실제 손의 움직임과 일치할 때 가상 시점을 조작하는 것은 현재 상태에 상대적으로 거의 영향을 미치지 않아 안정적으로 유지됩니다. 따라서 핸드 스케일링은 존재감을 희생하지 않고 인지된 무게를 조절하기 위한 보완 채널로서 저비용 기본 단서 및 관점 조작 역할을 합니다. 이 기술은 실제 노력에 대한 설득력 있는 감각을 요구하는 VR 훈련, 재활 및 엑서게임에 대한 실행 가능한 지침을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00058,Interaction & Input,Haptic / Tactile Feedback,User Study,Algorithm / Method
51,2025,Ever: Edge-Assisted Auto-Verification for Mobile MR-Aided Operation,Ever: 모바일 MR 지원 작업을 위한 Edge 지원 자동 검증,"Mixed Reality (MR)-aided operation overlays digital objects on the physical world to provide a more immersive and intuitive operation process. A primary challenge is the precise and fast auto-verification of whether the user follows MR guidance by comparing frames before and after each operation. The pre-operation frame includes virtual guiding objects, while the post-operation frame contains physical counterparts. Existing approaches fall short of accounting for the discrepancies between physical and virtual objects due to imperfect 3D modeling or lighting estimation. In this paper, we propose EVER: an edge-assisted auto-verification system for mobile MR-aided operations. Unlike traditional frame-based similarity comparisons, EVER leverages the segmentation model and rendering pipeline adapted to the unique attributes of frames with physical pieces and those with their virtual counterparts; it adopts a threshold-based strategy using Intersection over Union (IoU) metrics for accurate auto-verification. To ensure fast auto-verification and low energy consumption, EVER offloads compute-intensive tasks to an edge server. Through comprehensive evaluations of public datasets and custom datasets with practical implementation, EVER achieves over 90% verification accuracy within 100 milliseconds (significantly faster than average human reaction time of approximately 273 milliseconds), while consuming only minimal additional computational resources and energy compared to a system without auto-verification.","혼합 현실(MR) 지원 작업은 실제 세계에 디지털 개체를 오버레이하여 보다 몰입감 있고 직관적인 작업 프로세스를 제공합니다. 주요 과제는 각 작업 전후의 프레임을 비교하여 사용자가 MR 지침을 따르는지 여부를 정확하고 빠르게 자동 검증하는 것입니다. 수술 전 프레임에는 가상 안내 개체가 포함되는 반면, 수술 후 프레임에는 물리적 대응 개체가 포함됩니다. 기존 접근 방식은 불완전한 3D 모델링 또는 조명 추정으로 인한 실제 객체와 가상 객체 간의 불일치를 설명하는 데 부족합니다. 본 논문에서는 모바일 MR 지원 작업을 위한 에지 지원 자동 검증 시스템인 EVER를 제안합니다. 기존의 프레임 기반 유사성 비교와 달리 EVER는 물리적 조각이 있는 프레임과 가상 부분이 있는 프레임의 고유한 특성에 맞게 분할 모델과 렌더링 파이프라인을 활용합니다. 정확한 자동 검증을 위해 IoU(Intersection over Union) 측정항목을 사용하는 임계값 기반 전략을 채택합니다. 빠른 자동 검증과 낮은 에너지 소비를 보장하기 위해 EVER는 컴퓨팅 집약적인 작업을 엣지 서버로 오프로드합니다. 실제 구현이 포함된 공개 데이터 세트 및 사용자 정의 데이터 세트에 대한 포괄적인 평가를 통해 EVER는 100밀리초 이내에 90% 이상의 검증 정확도(약 273밀리초의 평균 인간 반응 시간보다 훨씬 빠른 속도)를 달성하는 동시에 자동 검증이 없는 시스템에 비해 최소한의 추가 계산 리소스와 에너지만 소비합니다.",https://doi.org/10.1109/ISMAR67309.2025.00148,Rendering & Visualization,Computer Vision,Technical Evaluation,Algorithm / Method; System / Framework
52,2025,Examining Eye Vergence During Perceived Depth Changes with Eye Tracking System in Optical See-Through Augmented Reality,광학 시스루 증강 현실에서 시선 추적 시스템을 사용하여 인지된 깊이 변화 중 눈의 시선 검사,"A primary objective of optical see-through (OST) Augmented Reality (AR) systems is to enable users to perceive the depth of AR objects with the same accuracy as they perceive the depth of realworld objects. Previous studies have shown that individuals often make depth judgments that either underestimate or overestimate the depth of AR objects compared to real-world objects. Recently, OST AR devices have incorporated eye-tracking technology, offering the opportunity to objectively measure and investigate the depth-dependent components of the human visual system (e.g., eye vergence angle) while perceiving the depth of real and AR objects. This paper measures and examines the eye vergence angle (EVA) for both real and AR objects at four different depths (0.35 m, 0.75 m, 1.5 m, and 4.0 m) using integrated eye-tracking systems of the two commercial devices: Microsoft HoloLens 2 and Magic Leap 2. The experiment considered a four-alternative forced-choice visual discrimination task with a repeated-measures design, involving 24 participants. Our findings showed that subjective (verbal estimation) and objective (EVA) measures of depth perception were consistent with the depth of both real and AR objects in both OST AR devices. The results demonstrated individual differences in EVA for each device across various depths. No difference in EVA was identified between real and AR objects. Additionally, the EVA range differs between the OST AR devices for each depth. These findings indicate that objective EVA measurements can not be generalized across different OST AR devices and individuals.","광학 투명(OST) 증강 현실(AR) 시스템의 주요 목표는 사용자가 실제 물체의 깊이를 인식하는 것과 동일한 정확도로 AR 물체의 깊이를 인식할 수 있도록 하는 것입니다. 이전 연구에 따르면 개인은 실제 객체에 비해 AR 객체의 깊이를 과소평가하거나 과대평가하는 깊이 판단을 내리는 경우가 많습니다. 최근 OST AR 장치에는 시선 추적 기술이 통합되어 실제 객체와 AR 객체의 깊이를 인식하면서 인간 시각 시스템의 깊이 의존 구성 요소(예: 눈 방향 각도)를 객관적으로 측정하고 조사할 수 있는 기회를 제공합니다. 본 논문에서는 Microsoft HoloLens 2와 Magic Leap 2라는 두 상업용 장치의 통합 시선 추적 시스템을 사용하여 4가지 서로 다른 깊이(0.35m, 0.75m, 1.5m, 4.0m)에서 실제 객체와 AR 객체 모두에 대한 눈의 시선 각도(EVA)를 측정하고 검사합니다. 이 실험에서는 24명의 참가자가 참여하는 반복 측정 설계를 사용하여 4가지 대안 강제 선택 시각적 식별 작업을 고려했습니다. 우리의 연구 결과에 따르면 깊이 인식에 대한 주관적(언어적 추정) 및 객관적(EVA) 측정값은 두 OST AR 장치 모두에서 실제 개체와 AR 개체의 깊이와 일치하는 것으로 나타났습니다. The results demonstrated individual differences in EVA for each device across various depths. 실제 객체와 AR 객체 간에 EVA의 차이는 확인되지 않았습니다. 또한 EVA 범위는 각 깊이에 대한 OST AR 장치마다 다릅니다. 이러한 결과는 객관적인 EVA 측정이 다양한 OST AR 장치 및 개인에 걸쳐 일반화될 수 없음을 나타냅니다.",https://doi.org/10.1109/ISMAR67309.2025.00142,Display & Optics; Perception & Cognition,Eye / Gaze Tracking,Quantitative Experiment,Hardware / Device
53,2025,Experience Level Influences User's Criteria for Avatar Animation Realism,경험 수준은 아바타 애니메이션 사실성에 대한 사용자 기준에 영향을 미칩니다.,"The sense of realism in avatar animation is a widely pursued goal in social VR applications. A common approach to enhancing realism is improving the match between avatar motion and real-world human movement. However, experience with existing VR platforms may reshape users' expectations, suggesting that matching reality is not the only path to enhancing the sense of realism. This study examines how different levels of experience with a social VR platform influence users' criteria for evaluating the realism of avatar animation. Participants were shown a set of animations varying in the degree they reflected real-world motion and motion seen on the social VR platform VRChat. Results showed that users with no VRChat experience found animations recorded on VRChat unnatural and unrealistic, but experienced users in fact rated these animations as more likely to come from a real person than the motion-capture animations. Additionally, highly experienced users recognized the intent to imitate VRChat's style and noted the differences from genuine in-platform animations. All these results suggest users' expectations of and criteria for realistic animation were shaped by their experience level. The findings support the idea that realism in avatar animation does not solely depend on mimicking real-world movement. Experience with VR platforms can shape how users expect, perceive, and evaluate animation realism. This insight can inform the design of more immersive VR environments and virtual humans in the future.","아바타 애니메이션의 현실감은 소셜 VR 애플리케이션에서 널리 추구되는 목표입니다. 현실감을 향상시키는 일반적인 접근 방식은 아바타 모션과 실제 인간 움직임 간의 일치를 개선하는 것입니다. 그러나 기존 VR 플랫폼에 대한 경험은 사용자의 기대를 바꿀 수 있으며, 현실과 일치하는 것이 현실감을 높이는 유일한 방법은 아니라는 점을 시사합니다. 본 연구는 소셜 VR 플랫폼의 다양한 경험 수준이 사용자의 아바타 애니메이션 평가 기준에 어떤 영향을 미치는지 조사합니다. 참가자들에게는 소셜 VR 플랫폼인 VRChat에서 볼 수 있는 실제 동작과 동작을 반영하는 정도가 다양한 애니메이션 세트가 표시되었습니다. 결과에 따르면 VRChat 경험이 없는 사용자는 VRChat에 녹화된 애니메이션이 부자연스럽고 비현실적이라고 생각했지만 경험이 있는 사용자는 실제로 이러한 애니메이션이 모션 캡처 애니메이션보다 실제 사람에게서 나올 가능성이 더 높다고 평가했습니다. 또한 경험이 풍부한 사용자는 VRChat의 스타일을 모방하려는 의도를 인식하고 실제 플랫폼 내 애니메이션과의 차이점을 지적했습니다. 이러한 모든 결과는 사실적인 애니메이션에 대한 사용자의 기대와 기준이 사용자의 경험 수준에 따라 형성되었음을 시사합니다. 연구 결과는 아바타 애니메이션의 현실감이 실제 움직임을 모방하는 데에만 의존하지 않는다는 생각을 뒷받침합니다. VR 플랫폼 경험은 사용자가 애니메이션 현실감을 기대하고 인식하고 평가하는 방식을 형성할 수 있습니다. 이러한 통찰력은 미래에 더욱 몰입도 높은 VR 환경과 가상 인간을 디자인하는 데 도움이 될 수 있습니다.",https://doi.org/10.1109/ISMAR67309.2025.00084,Perception & Cognition,Other,User Study,System / Framework
54,2025,Exploring Body-Anchored Augmented Reality Interfaces Across Different Mobility and Social Contexts,다양한 이동성과 사회적 맥락에서 신체에 고정된 증강 현실 인터페이스 탐색,"Researchers explore various on-body locations for anchoring user interfaces in Augmented Reality (AR), primarily to leverage onbody haptic feedback and enhance usability. However, most studies are conducted in controlled laboratory settings, limiting their applicability to real-world use. Consequently, there remains a lack of research on identifying suitable on-body AR UI placements across diverse social and mobility contexts. To address this gap, we conduct a user study investigating user preferences for anchoring UIs on six on-body locations-palm, back of the palm, inner and outer forearm, and right and left lap-across different mobility conditions (standing, walking, sitting) and social contexts (private, semiprivate, and public settings). Results show that users prefer anchoring AR interfaces on the outer forearm and palm across all conditions. However, participants noted the limited interaction space on hand surfaces. To address this, a follow-up study evaluated six UI layouts anchored to the palm and forearm, comparing body-aligned vs. vertically oriented placements and small vs. enlarged interfaces. Results show that vertically oriented, enlarged layouts yield superior performance, offering context-sensitive guidance for designing AR interfaces that balance comfort, preference, and usability.","연구원들은 주로 신체 햅틱 피드백을 활용하고 사용성을 향상시키기 위해 증강 현실(AR)에서 사용자 인터페이스를 고정하기 위한 다양한 신체 위치를 탐색합니다. 그러나 대부분의 연구는 통제된 실험실 환경에서 수행되므로 실제 사용에 대한 적용 가능성이 제한됩니다. 결과적으로, 다양한 사회적 및 이동성 맥락에서 적합한 신체 내 AR UI 배치를 식별하는 연구는 여전히 부족합니다. 이러한 격차를 해결하기 위해 우리는 다양한 이동 조건(서기, 걷기, 앉기)과 사회적 맥락(개인, 반개인 및 공공 환경)에 걸쳐 손바닥, 손바닥 뒤쪽, 팔뚝 안쪽과 바깥쪽, 오른쪽과 왼쪽 무릎 등 신체의 6개 위치에 UI를 고정하기 위한 사용자 선호도를 조사하는 사용자 연구를 수행합니다. 결과는 사용자가 모든 조건에서 팔뚝 바깥쪽과 손바닥에 AR 인터페이스를 고정하는 것을 선호한다는 것을 보여줍니다. However, participants noted the limited interaction space on hand surfaces. 이 문제를 해결하기 위해 후속 연구에서는 손바닥과 팔뚝에 고정된 6가지 UI 레이아웃을 평가하여 신체 정렬과 수직 방향 배치, 작은 인터페이스와 확대된 인터페이스를 비교했습니다. 결과는 수직 방향의 확대된 레이아웃이 뛰어난 성능을 제공하고 편안함, 선호도 및 유용성의 균형을 맞추는 AR 인터페이스를 디자인하기 위한 상황에 맞는 지침을 제공한다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR67309.2025.00147,Interaction & Input,Haptic / Tactile Feedback,User Study,User Study / Empirical Findings
55,2025,"Exploring Gaze Dynamics in Vr Film Education: Gender, Avatar, and the Shift Between Male and Female Perspectives","VR 영화 교육의 시선 역학 탐구: 성별, 아바타, 남성과 여성의 관점 전환","In virtual reality (VR) education, especially in creative fields like film production, avatar design and narrative style extend beyond appearance and aesthetics. This study explores how the interaction between avatar gender, the dominant narrative actor's gender, and the learner's gender influences film production learning in VR, focusing on gaze dynamics and gender perspectives. Using a $2 \times 2 \times 2$ experimental design, 48 participants operated avatars of different genders and interacted with male or female-dominant narratives. The results show that the consistency between the avatar and gender affects presence, and learners' control over the avatar is also influenced by gender matching. Learners using avatars of the opposite gender reported stronger control, suggesting gender incongruity prompted more focus on the avatar. Additionally, female participants with female avatars were more likely to adopt a “female gaze,” favoring soft lighting and emotional shots, while male participants with male avatars were more likely to adopt a “male gaze,” choosing dynamic shots and high contrast. When male participants used female avatars, they favored “female gaze,” while female participants with male avatars focused on “male gaze”. These findings advance our understanding of how avatar design and narrative style in VRbased education influence creativity and the cultivation of gender perspectives, and they offer insights for developing more inclusive and diverse VR teaching tools going forward.","가상 현실(VR) 교육, 특히 영화 제작과 같은 창의적 분야에서 아바타 디자인과 내러티브 스타일은 외모와 미학을 넘어 확장됩니다. 본 연구는 시선 역학과 성별 관점에 초점을 맞춰 아바타 성별, 지배적인 내러티브 배우 성별, 학습자 성별 간의 상호 작용이 VR 영화 제작 학습에 어떻게 영향을 미치는지 탐구합니다. $2 \times 2 \times 2$ 실험 설계를 사용하여 48명의 참가자가 다양한 성별의 아바타를 운영하고 남성 또는 여성이 지배하는 내러티브와 상호작용했습니다. 연구 결과, 아바타와 성별의 일관성은 존재감에 영향을 미치며, 아바타에 대한 학습자의 통제력 역시 성별 매칭에 영향을 받는 것으로 나타났다. 이성의 아바타를 사용하는 학습자는 더 강한 통제력을 보고했는데, 이는 성별 불일치로 인해 아바타에 더 많은 집중이 촉발되었음을 시사합니다. 또한 여성 아바타를 착용한 여성 참가자는 부드러운 조명과 감성적인 사진을 선호하는 '여성 시선'을 채택할 가능성이 더 높았고, 남성 아바타를 착용한 남성 참가자는 역동적인 사진과 고대비를 선택하는 '남성 시선'을 채택할 가능성이 더 높았습니다. 남성 참여자들은 여성 아바타를 사용했을 때 '여성 시선'을 선호한 반면, 남성 아바타를 사용한 여성 참여자들은 '남성 시선'에 집중했습니다. 이러한 연구 결과는 VR 기반 교육의 아바타 디자인과 내러티브 스타일이 창의성과 성별 관점 함양에 어떻게 영향을 미치는지에 대한 이해를 높이고 앞으로 더욱 포괄적이고 다양한 VR 교육 도구를 개발하기 위한 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00027,Interaction & Input; Education & Training,Other,User Study,User Study / Empirical Findings
56,2025,Exploring Organizational Strategies in Immersive Computational Notebooks,몰입형 컴퓨팅 노트북에서 조직 전략 탐색,"Computational notebooks, which integrate code, documentation, tags, and visualizations into a single document, have become increasingly popular for data analysis tasks. With the advent of immersive technologies, these notebooks have evolved into a new paradigm, enabling more interactive and intuitive ways to perform data analysis. An immersive computational notebook, which integrates computational notebooks within an immersive environment, significantly enhances navigation performance with embodied interactions. However, despite recognizing the significance of organizational strategies in the immersive data science process, the organizational strategies for using immersive notebooks remain largely unexplored. In response, our research aims to deepen our understanding of organizations, especially focusing on spatial structures for computational notebooks, and to examine how various execution orders can be visualized in an immersive context. Through an exploratory user study, we found participants preferred organizing notebooks in half-cylindrical structures and engaged significantly more in non-linear analysis. Notably, as the scale of the notebooks increased (i.e., more code cells), users increasingly adopted multiple, concurrent non-linear analytical approaches.","코드, 문서, 태그 및 시각화를 단일 문서에 통합하는 계산 노트북은 데이터 분석 작업에 점점 더 많이 사용되고 있습니다. 몰입형 기술의 출현으로 이러한 노트북은 데이터 분석을 수행하는 보다 대화적이고 직관적인 방법을 가능하게 하는 새로운 패러다임으로 진화했습니다. 몰입형 환경 내에 컴퓨팅 노트북을 통합하는 몰입형 컴퓨팅 노트북은 구현된 상호 작용을 통해 탐색 성능을 크게 향상시킵니다. 그러나 몰입형 데이터 과학 프로세스에서 조직 전략의 중요성을 인식함에도 불구하고 몰입형 노트북을 사용하기 위한 조직 전략은 아직까지 탐구되지 않은 상태로 남아 있습니다. 이에 우리 연구는 특히 전산 노트북의 공간 구조에 초점을 맞춰 조직에 대한 이해를 심화하고, 몰입형 맥락에서 다양한 실행 명령을 어떻게 시각화할 수 있는지 살펴보는 것을 목표로 합니다. 탐색적 사용자 연구를 통해 참가자들은 반원통형 구조로 노트북을 구성하는 것을 선호하고 비선형 분석에 훨씬 더 많이 참여한다는 사실을 발견했습니다. 특히, 노트북의 규모가 증가함에 따라(예: 더 많은 코드 셀) 사용자는 점점 더 여러 개의 동시 비선형 분석 접근 방식을 채택했습니다.",https://doi.org/10.1109/ISMAR67309.2025.00022,Interaction & Input,Deep Learning / Neural Networks,User Study,User Study / Empirical Findings
57,2025,Exploring Plausible Preference of Body-Centric Locomotion with Reinforcement Learning in Virtual Reality,가상 현실에서 강화 학습을 통한 신체 중심 이동의 그럴듯한 선호 탐색,"Investigating users' plausible preferences for body-centric locomotion can help researchers better understand the impact of different factors on this locomotion method and optimize the locomotion configuration for providing a plausible locomotion experience. In this paper, we propose to evaluate users' plausible preferences for body-centric locomotion using a reinforcement learning method. This method can intelligently infer users' plausible preferences for different factor levels involved in the virtual locomotion by proposing possible modifications to the factor levels and asking users to accept or reject the modifications after experiencing the locomotion. We conducted a within-subject experiment to examine the impact of different factors (i.e., body parts used for virtual locomotion, the point of view, auditory feedback, the transfer function, and the coefficients of the transfer function) on users' plausible preferences for body-centric locomotion in sitting and standing postures. The results mainly indicated that (1) users preferred using arm swinging in standing posture, whereas they preferred using head tilting in sitting posture; (2) The point of view was identified as the most important factor in standing posture, while it was less important in sitting posture; (3) Participants showed consistent plausible preferences for auditory feedback, transfer function, and coefficient of the transfer function in both standing and sitting postures. Our research findings can guide the design of VR applications to enhance a plausible walking experience in different postures.","신체 중심 운동에 대한 사용자의 그럴듯한 선호도를 조사하면 연구자가 이 운동 방법에 대한 다양한 요인의 영향을 더 잘 이해하고 그럴듯한 운동 경험을 제공하기 위해 운동 구성을 최적화하는 데 도움이 될 수 있습니다. 본 논문에서는 강화 학습 방법을 사용하여 신체 중심 이동에 대한 사용자의 그럴듯한 선호도를 평가할 것을 제안합니다. 이 방법은 요인 수준에 대한 가능한 수정 사항을 제안하고 사용자에게 운동을 경험한 후 수정 사항을 수락하거나 거부하도록 요청함으로써 가상 이동과 관련된 다양한 요인 수준에 대한 사용자의 그럴듯한 선호도를 지능적으로 추론할 수 있습니다. 우리는 앉은 자세와 서 있는 자세에서 신체 중심 이동에 대한 사용자의 그럴듯한 선호도에 대한 다양한 요인(예: 가상 이동에 사용되는 신체 부위, 관점, 청각 피드백, 전달 함수 및 전달 함수 계수)이 미치는 영향을 조사하기 위해 피험자 내 실험을 수행했습니다. 결과는 주로 (1) 사용자가 서있는 자세에서 팔 흔들기 사용을 선호하는 반면 앉은 자세에서는 머리 기울임 사용을 선호한다는 것을 나타냅니다. (2) 관점은 선 자세에서는 가장 중요한 요인으로 확인되었으며, 앉은 자세에서는 덜 중요한 요인으로 나타났다. (3) 참가자들은 서 있는 자세와 앉은 자세 모두에서 청각 피드백, 전달 함수 및 전달 함수 계수에 대해 일관되고 타당한 선호를 보였습니다. 우리의 연구 결과는 다양한 자세에서 그럴듯한 걷기 경험을 향상시키기 위해 VR 애플리케이션의 설계를 안내할 수 있습니다.",https://doi.org/10.1109/ISMAR67309.2025.00063,Interaction & Input,Redirected Walking / Locomotion,User Study,User Study / Empirical Findings
58,2025,Exploring Pointing and Confirmation Techniques for Teleportation Across Varying Elevations in Virtual Reality,가상 현실에서 다양한 고도에 걸친 순간 이동을 위한 포인팅 및 확인 기술 탐색,"Teleportation in Virtual Reality (VR) is a locomotion technique that allows users to navigate between locations within a virtual environment instantly. Traditionally, VR teleportation is performed using physical controllers, where users control a teleportation pointer — represented by a straight line or parabola — and activate the teleportation to the target destination by pressing a button. Recent advances in hand and eye-tracking capabilities in Head-Mounted Displays (HMDs) enable designers to leverage hand and eye-based interactions to enhance the immersion and naturalness of controller-free VR usage. However, there has been limited research on comparing different controller-free methods for VR teleportation across various elevations. To address this gap, we conducted a user study exploring three controller-free pointing techniques (gaze, hand, and head), four confirmation modalities (finger pinch, eye-blink, dwell, and voice), and two types of teleportation pointers (linear and parabolic) for VR teleportation across various elevations. Our results show that head-based pointing was faster and more accurate than other techniques, with head and gaze achieving higher throughput than hand-based methods. For confirmation, finger pinch yielded the best performance in terms of task completion time and throughput, followed by dwell, voice, and eye-blink; dwell was the most accurate. The linear pointer outperformed the parabolic pointer in some contexts. Based on these findings, we propose design guidelines to enhance controller-free VR teleportation using various input modalities.","가상현실(VR)의 순간이동(Teleportation in Virtual Reality)은 사용자가 가상 ​​환경 내에서 위치 간을 즉시 이동할 수 있는 이동 기술입니다. 전통적으로 VR 순간이동은 사용자가 직선이나 포물선으로 표시되는 순간이동 포인터를 제어하고 버튼을 눌러 대상 목적지로 순간이동을 활성화하는 물리적 컨트롤러를 사용하여 수행됩니다. 머리 장착형 디스플레이(HMD)의 손 및 시선 추적 기능이 최근 발전함에 따라 디자이너는 손과 눈 기반 상호 작용을 활용하여 컨트롤러가 필요 없는 VR 사용의 몰입도와 자연성을 향상할 수 있습니다. 그러나 다양한 고도에서 VR 순간이동을 위한 다양한 컨트롤러 없는 방법을 비교하는 연구는 제한적이었습니다. 이러한 격차를 해결하기 위해 우리는 다양한 높이에서 VR 순간 이동을 위한 세 가지 컨트롤러 없는 포인팅 기술(시선, 손, 머리), 네 가지 확인 방식(손가락 모으기, 눈 깜박임, 유지 및 음성), 두 가지 유형의 순간 이동 포인터(선형 및 포물선)를 탐색하는 사용자 연구를 수행했습니다. 우리의 결과는 머리 기반 포인팅이 다른 기술보다 빠르고 정확하며 머리와 시선이 손 기반 방법보다 더 높은 처리량을 달성한다는 것을 보여줍니다. 확인을 위해 손가락 핀치는 작업 완료 시간 및 처리량 측면에서 가장 좋은 성능을 보였으며, 체류, 음성 및 눈 깜박임이 그 뒤를 이었습니다. 드웰이 가장 정확했어요. 일부 상황에서는 선형 포인터가 포물선 포인터보다 성능이 뛰어났습니다. 이러한 결과를 바탕으로 우리는 다양한 입력 양식을 사용하여 컨트롤러 없는 VR 순간이동을 향상시키기 위한 설계 지침을 제안합니다.",https://doi.org/10.1109/ISMAR67309.2025.00170,Interaction & Input; Display & Optics,Redirected Walking / Locomotion,Quantitative Experiment,User Study / Empirical Findings; Algorithm / Method
59,2025,Exploring the Effects and Neurophysiological Characteristics of VR Emotion Regulation Strategies,VR 감정 조절 전략의 효과 및 신경생리학적 특성 탐색,"Emotion regulation (ER) is a central topic in perception and cognition. With the development of virtual reality (VR) technology, its high levels of immersion, interactivity, and controllability have made it a promising tool for ER. Recent studies have demonstrated that VR-based interventions can enhance ER. However, the effectiveness of ER largely depends on the strategies employed. This critical factor has often been underexplored in VR-based ER research. To address this gap, this study investigates the effectiveness and neurophysiological characteristics of four ER strategies: distraction, emotional expression, cognitive reappraisal, and expressive suppression in VR, using both subjective ratings and objective neurophysiological data. A non-VR self-regulation strategy serves as the baseline. The results reveal that VR-based ER strategies significantly enhance individual emotional valence. For low-arousal negative emotions, distraction, emotional expression, and cognitive reappraisal show significant regulatory effects in VR, with emotional expression exhibiting the strongest impact. For high-arousal negative emotions, only cognitive reappraisal is effective. In contrast, expressive suppression does not produce significant effects for either type of negative emotion. These findings further support the role of VR in facilitating ER and provide empirical evidence for the informed selection of VR-based ER strategies. This research also contributes to the theoretical foundation of affective computing.","감정 조절(ER)은 인식과 인지의 핵심 주제입니다. 가상 현실(VR) 기술의 개발로 인해 높은 수준의 몰입감, 상호 작용성 및 제어 가능성이 ER을 위한 유망한 도구가 되었습니다. 최근 연구에 따르면 VR 기반 개입이 ER을 향상시킬 수 있음이 입증되었습니다. 그러나 ER의 효과는 주로 사용된 전략에 따라 달라집니다. 이 중요한 요소는 VR 기반 ER 연구에서 종종 과소 탐구되었습니다. 이러한 격차를 해결하기 위해 이 연구에서는 주관적 평가와 객관적인 신경생리학적 데이터를 모두 사용하여 주의 산만, 감정 표현, 인지 재평가 및 VR에서의 표현 억제라는 네 가지 ER 전략의 효과와 신경 생리학적 특성을 조사합니다. VR이 아닌 자체 규제 전략이 기준이 됩니다. 결과는 VR 기반 ER 전략이 개인의 정서적 가치를 크게 향상시키는 것으로 나타났습니다. 각성이 낮은 부정적인 감정의 경우 산만함, 감정 표현 및 인지 재평가는 VR에서 상당한 규제 효과를 나타내며 감정 표현이 가장 큰 영향을 미칩니다. 각성이 높은 부정적 감정의 경우 인지적 재평가만이 효과적입니다. 대조적으로, 표현적 억제는 두 유형의 부정적인 감정에 대해 중요한 효과를 생성하지 않습니다. 이러한 발견은 ER을 촉진하는 데 있어 VR의 역할을 더욱 뒷받침하고 VR 기반 ER 전략의 정보에 입각한 선택에 대한 경험적 증거를 제공합니다. 본 연구는 감성 컴퓨팅의 이론적 토대에도 기여합니다.",https://doi.org/10.1109/ISMAR67309.2025.00155,Perception & Cognition,Other,Technical Evaluation,User Study / Empirical Findings
60,2025,"Exploring the Influence of Crowd Size Across Different Tasks on User Performance, Experience and Social Presence in Shared Virtual Environments","공유 가상 환경에서 다양한 작업에 걸쳐 군중 규모가 사용자 성과, 경험 및 사회적 존재에 미치는 영향 탐색","Shared virtual environments are becoming essential platforms for collaborative interaction and immersive entertainment, enabling users to be co-located and engage in activities together. The presence of surrounding virtual humans forms an environmental crowd, serving as a component of ambient stimuli in these environments. However, it remains unclear how crowd size affects users under different cognitive and motor demands. This study investigates the influence of crowd size on user performance, experience and social presence across three fundamental VR tasks: Spatial Locomotion, Memory Search, and Motor Coordination. We conducted a controlled within-subjects experiment, manipulating each task's crowd size at Small, Medium, and Large levels. Our results show that crowd size significantly impacts user performance, experience, and social presence, but these effects are task-dependent. While Medium size can enhance performance, Large size in cognitively demanding tasks may induce attentional blindness and diminish sensitivity to social cues. Task functionality further shapes how users perceive and respond to virtual crowds. Additionally, users' preferences for crowd size varied across different tasks, and most participants expressed a desire for control over the number of visible avatars. These findings provide novel insights into human crowd perception mechanisms, revealing cross-task perceptual variations that pave the way for further exploring crowd perception in shared virtual environments.","공유 가상 환경은 협업 상호 작용과 몰입형 엔터테인먼트를 위한 필수 플랫폼이 되어가고 있으며, 사용자가 같은 위치에 있고 함께 활동에 참여할 수 있도록 해줍니다. 주변 가상 인간의 존재는 환경 군중을 형성하여 이러한 환경에서 주변 자극의 구성 요소 역할을 합니다. 그러나 군중 크기가 다양한 인지 및 운동 요구에 따라 사용자에게 어떤 영향을 미치는지는 불분명합니다. 이 연구에서는 공간 이동, 기억 검색, 운동 조정이라는 세 가지 기본 VR 작업 전반에 걸쳐 군중 규모가 사용자 성능, 경험 및 사회적 존재감에 미치는 영향을 조사합니다. 우리는 소규모, 중간, 대규모 수준에서 각 작업의 군중 크기를 조작하여 통제된 피험자 내 실험을 수행했습니다. 우리의 결과에 따르면 군중 규모는 사용자 성능, 경험 및 사회적 존재감에 큰 영향을 미치지만 이러한 효과는 작업에 따라 다릅니다. 중간 크기는 성능을 향상시킬 수 있지만, 인지적으로 까다로운 작업의 큰 크기는 주의력 상실을 유발하고 사회적 단서에 대한 민감도를 감소시킬 수 있습니다. 작업 기능은 사용자가 가상 ​​군중을 인식하고 반응하는 방식을 더욱 구체화합니다. 또한 군중 크기에 대한 사용자의 선호도는 다양한 작업에 따라 다양했으며 대부분의 참가자는 표시되는 아바타 수를 제어하려는 욕구를 표현했습니다. 이러한 발견은 인간의 군중 인식 메커니즘에 대한 새로운 통찰력을 제공하여 공유 가상 환경에서 군중 인식을 더욱 탐구할 수 있는 길을 열어주는 작업 간 인식 변화를 드러냅니다.",https://doi.org/10.1109/ISMAR67309.2025.00133,Perception & Cognition; Collaboration & Social,Sensor Fusion,User Study,User Study / Empirical Findings
61,2025,Fast SP-GS: Reconstructing Dynamic Scenes in Minutes,Fast SP-GS: 몇 분 만에 동적 장면 재구성,"Despite recent advances in Gaussian Splatting techniques-such as Superpoint Gaussian Splatting (SP-GS), which enables real-time, high-fidelity rendering-3D reconstruction of dynamic scenes remains a significant challenge in computer vision. However, SP-GS requires nearly an hour for dynamic scene optimization, severely limiting its practical applications in AR and VR. To address this limitation, we propose Fast SP-GS, an efficient approach that reduces training time to mere minutes. Building upon acceleration methods for static scenes (e.g., Mini-Splatting, Taming 3DGS, FlashGS), our novel 2D-GS-based framework enhances speed and quality via three key innovations: First, an aggressive 2D-GS densification strategy reduces required training iterations, while a Gaussian simplification strategy minimizes redundant parameters. Second, a novel 2D-GS optical flow loss provides explicit motion supervision, accelerating convergence. Third, an optimized CUDA implementation maximizes rendering efficiency. Extensive experiments on synthetic and real-world datasets confirm that Fast SP-GS reconstructs dynamic scenes in minutes, surpassing SP-GS in both rendering quality and computational efficiency. The source code is available at https://github.com/dnvtmf/Fast-SP-GS.","동적 장면의 실시간 고충실도 렌더링 3D 재구성을 가능하게 하는 SP-GS(Superpoint Gaussian Splatting)와 같은 Gaussian Splatting 기술의 최근 발전에도 불구하고 컴퓨터 비전에서는 여전히 중요한 과제로 남아 있습니다. 그러나 SP-GS는 동적 장면 최적화에 거의 1시간이 필요하므로 AR 및 VR에서의 실제 적용이 심각하게 제한됩니다. 이러한 한계를 해결하기 위해 우리는 훈련 시간을 단 몇 분으로 줄이는 효율적인 접근 방식인 Fast SP-GS를 제안합니다. 정적 장면(예: Mini-Splatting, Taming 3DGS, FlashGS)에 대한 가속 방법을 기반으로 구축된 새로운 2D-GS 기반 프레임워크는 세 가지 주요 혁신을 통해 속도와 품질을 향상시킵니다. 첫째, 공격적인 2D-GS 치밀화 전략은 필요한 교육 반복을 줄이는 동시에 가우스 단순화 전략은 중복 매개변수를 최소화합니다. 둘째, 새로운 2D-GS 광학 흐름 손실은 명시적인 모션 감독을 제공하여 수렴을 가속화합니다. 셋째, 최적화된 CUDA 구현은 렌더링 효율성을 극대화합니다. 합성 및 실제 데이터 세트에 대한 광범위한 실험을 통해 Fast SP-GS가 렌더링 품질과 계산 효율성 모두에서 SP-GS를 능가하여 몇 분 만에 동적 장면을 재구성한다는 것이 확인되었습니다. 소스 코드는 https://github.com/dnvtmf/Fast-SP-GS에서 확인할 수 있습니다.",https://doi.org/10.1109/ISMAR67309.2025.00146,Rendering & Visualization,3D Reconstruction; Computer Vision,Technical Evaluation,Algorithm / Method
62,2025,Field Dependence as a Predictor of Cybersickness Dropout,사이버 멀미 중퇴의 예측변수로서의 현장 의존성,"One of the primary barriers to the widespread adoption of virtual reality (VR) is cybersickness, a form of visually-induced motion sickness caused by the mismatch between visual and vestibular cues. Our study investigates the role of field-dependence on cybersickness susceptibility. We used a virtual reality roller coaster simulation to induce sickness and measured the time for participant dropout. Field dependence was measured using the Group Embedded Figure test and two variations of the Rod and Frame test. A survival analysis indicated that field dependence can be an effective measure of individual differences in cybersickness. Field-dependent individuals have a 37 % increased risk of dropping out and females showed an increased risk of 126 %. We did not find a significant effect from age or sleep quality. The study also confirms that the dropout rates paradigm can effectively identify susceptibility to cybersickness and may be preferable to questionnaires. Psychological tests that measure these variables may eventually blend seamlessly in virtual environments and allow for greater prediction of cybersickness susceptibility.",가상 현실(VR)의 광범위한 채택을 가로막는 주요 장벽 중 하나는 시각 신호와 전정 신호 간의 불일치로 인해 발생하는 시각적으로 유발되는 멀미의 한 형태인 사이버 멀미입니다. 우리의 연구는 사이버 멀미 민감성에 대한 현장 의존성의 역할을 조사합니다. 가상현실 롤러코스터 시뮬레이션을 이용하여 멀미를 유발하고 참가자 탈락 시간을 측정했습니다. 필드 의존성은 Group Embedded Figure 테스트와 Rod 및 Frame 테스트의 두 가지 변형을 사용하여 측정되었습니다. 생존 분석에 따르면 현장 의존성은 사이버 멀미의 개인차를 효과적으로 측정할 수 있는 것으로 나타났습니다. 현장 의존형 개인은 중퇴 위험이 37% 증가했고 여성은 126% 증가된 위험을 보였습니다. 연령이나 수면의 질에 따른 유의미한 효과는 발견되지 않았습니다. 이 연구는 또한 중퇴율 패러다임이 사이버 멀미에 대한 취약성을 효과적으로 식별할 수 있으며 설문지보다 더 나을 수 있음을 확인합니다. 이러한 변수를 측정하는 심리 테스트는 결국 가상 환경에서 원활하게 혼합되어 사이버 멀미 취약성을 더 잘 예측할 수 있습니다.,https://doi.org/10.1109/ISMAR67309.2025.00140,Perception & Cognition,Sensor Fusion,Questionnaire / Survey; Simulation,Algorithm / Method
63,2025,First-Person Vocal Auralisation in XR and its Influence on Perceived Presence and Audio-Visual Quality,XR의 1인칭 음성 청각화와 인지된 존재감 및 시청각 품질에 미치는 영향,"Sound auralisation in Immersive Virtual Environments (IVEs) has been shown to promote perceptual effects such as presence and accounting for the technical limitations of immersive technologies. Moreover, it has been found to influence perceived audio-visual quality in multisensory contexts. However, this has been examined with sounds produced by an external source within the environment, rather than by the user, such as their own voice. In this study, we examined how real-time first-person vocal auralisation influences the perceived sense of presence and audio-visual quality across different rendering resolutions and visual representations, including Virtual Reality (VR) and Mixed Reality (MR). Experiment 1 compared three levels of sensory feedback. Visual feedback considered full resolution ($\sim 4.6$ megapixels per eye), low resolution ($\sim 1.0$ megapixels per eye), and no visuals. Audio feedback included two auralisation conditions using measured and synthesised impulse responses and a condition with no auralisation. Experiment 2 explored different types of visual representation: MR, using the passthrough feature of the Head-Mounted Display (HMD) to render the physical environment, and VR, using a 3D modelled reconstruction of the space. Audio feedback conditions consisted of auralisation with measured impulse responses and no auralisation. Our findings consistently revealed higher ratings of presence and perceived audio-visual quality in conditions where real-time vocal auralisation was present, highlighting the importance of rendering the acoustic features of spaces for first-person vocal interaction in immersive experiences.","몰입형 가상 환경(IVE)의 소리 청각화는 몰입형 기술의 기술적 한계에 대한 설명 및 존재감과 같은 지각 효과를 촉진하는 것으로 나타났습니다. 더욱이, 이는 다감각적 맥락에서 인지된 시청각 품질에 영향을 미치는 것으로 밝혀졌습니다. 그러나 이는 사용자가 아닌 자신의 목소리 등 환경 내 외부 소스에서 생성되는 소리를 통해 검토되었습니다. 본 연구에서는 실시간 1인칭 음성 청각화가 가상 현실(VR) 및 혼합 현실(MR)을 포함한 다양한 렌더링 해상도 및 시각적 표현 전반에 걸쳐 인지된 존재감 및 시청각 품질에 어떻게 영향을 미치는지 조사했습니다. 실험 1에서는 세 가지 수준의 감각 피드백을 비교했습니다. 시각적 피드백은 전체 해상도(눈당 $\sim 4.6$ 메가픽셀), 저해상도(눈당 $\sim 1.0$ 메가픽셀)로 간주되며 시각적인 피드백은 없습니다. 오디오 피드백에는 측정 및 합성된 임펄스 응답을 사용하는 두 가지 청각화 조건과 청각화가 없는 조건이 포함되었습니다. 실험 2에서는 다양한 유형의 시각적 표현을 탐구했습니다. MR은 헤드 마운트 디스플레이(HMD)의 패스스루 기능을 사용하여 물리적 환경을 렌더링하고 VR은 공간의 3D 모델링 재구성을 사용했습니다. 오디오 피드백 조건은 측정된 임펄스 응답이 있는 청각화와 청각화가 없는 것으로 구성되었습니다. 우리의 연구 결과는 실시간 음성 청각화가 존재하는 조건에서 더 높은 존재감 등급과 인지된 시청각 품질을 지속적으로 나타냈으며, 몰입형 경험에서 1인칭 음성 상호 작용을 위한 공간의 음향 특징을 렌더링하는 것의 중요성을 강조했습니다.",https://doi.org/10.1109/ISMAR67309.2025.00162,Display & Optics; Audio & Sound,Natural Language Processing,Simulation,Algorithm / Method
64,2025,Focus-Aware Task Guidance: Adaptive AR Instruction Playback Via Gaze and Location Tracking,초점 인식 작업 안내: 시선 및 위치 추적을 통한 적응형 AR 지침 재생,"Augmented Reality (AR) has demonstrated significant potential in task guidance by offering hands-free, context-sensitive instructions. However, effective AR-based task guidance requires maintaining user focus, as lapses in attention can lead to inefficiencies and errors. This paper describes a novel approach for integrating real-time gaze and location tracking to dynamically adapt AR task guidance, adjusting the playback speed accordingly. A user study with 20 participants assessed the effectiveness of gaze-based and locationbased focus detection across four conditions: baseline, gaze-only, location-only, and combined interaction. The results indicate that both gaze-based and location-based guidance significantly reduce error rates and improve task efficiency, however, the simultaneous use of both methods leads to cognitive overload, increased errors, and interaction complexity. Participants found that the adaptive playback speed adjustments helped manage attention. These findings highlight the importance of balancing adaptive cues in AR task guidance to optimize user performance without overwhelming cognitive resources.","증강 현실(AR)은 핸즈프리 상황에 맞는 지침을 제공함으로써 작업 안내에 상당한 잠재력을 보여주었습니다. 그러나 효과적인 AR 기반 작업 안내를 위해서는 사용자의 집중력을 유지해야 합니다. 주의가 부족하면 비효율성과 오류가 발생할 수 있기 때문입니다. 본 논문에서는 실시간 시선과 위치 추적을 통합하여 AR 작업 안내를 동적으로 적용하고 이에 따라 재생 속도를 조정하는 새로운 접근 방식을 설명합니다. 20명의 참가자를 대상으로 한 사용자 연구에서는 기준선, 응시 전용, 위치 전용 및 결합된 상호 작용의 네 가지 조건에 걸쳐 시선 기반 및 위치 기반 초점 감지의 효율성을 평가했습니다. The results indicate that both gaze-based and location-based guidance significantly reduce error rates and improve task efficiency, however, the simultaneous use of both methods leads to cognitive overload, increased errors, and interaction complexity. 참가자들은 적응형 재생 속도 조정이 주의력 관리에 도움이 된다는 것을 발견했습니다. 이러한 연구 결과는 인지 자원에 부담을 주지 않으면서 사용자 성능을 최적화하기 위해 AR 작업 안내에서 적응 단서의 균형을 맞추는 것이 중요하다는 점을 강조합니다.",https://doi.org/10.1109/ISMAR67309.2025.00103,Interaction & Input; Perception & Cognition,Sensor Fusion,User Study,Algorithm / Method; User Study / Empirical Findings
65,2025,FootPorting: Exploring Foot-Based Teleportation Techniques for Seated Users in Confined Spaces,FootPorting: 제한된 공간에 앉아 있는 사용자를 위한 발 기반 순간 이동 기술 탐색,"Teleportation is widely used in Virtual Reality (VR) applications, especially for enabling efficient navigation in large-scale virtual environments. Leveraging foot movements for teleportation frees the hands for parallel tasks and helps maintain spatial orientation, making it particularly suitable for seated and multitasking VR scenarios. This work explores feasible foot actions for teleportation in confined seating conditions, such as traveling on an airplane or train, where users' leg motion is feasible, but its range is constrained. Such scenarios are common but are underexplored. We propose seven metaphor-inspired teleportation techniques based on foot interactions and evaluate their effectiveness through a user study. The findings provide valuable insights for designing effective teleportation techniques in VR scenarios where users are seated in constrained spaces.","순간이동은 가상 현실(VR) 애플리케이션, 특히 대규모 가상 환경에서 효율적인 탐색을 가능하게 하는 데 널리 사용됩니다. 순간 이동을 위해 발 움직임을 활용하면 병렬 작업을 위한 손이 자유로워지고 공간 방향을 유지하는 데 도움이 되므로 앉아서 작업하거나 멀티태스킹하는 VR 시나리오에 특히 적합합니다. 이 작업은 사용자의 다리 움직임이 가능하지만 범위가 제한되어 있는 비행기나 기차 여행과 같이 제한된 좌석 조건에서 순간 이동을 위한 가능한 발 동작을 탐구합니다. 이러한 시나리오는 일반적이지만 아직 연구가 부족합니다. 발 상호작용을 기반으로 한 7가지 은유적 순간이동 기술을 제안하고 사용자 연구를 통해 그 효과를 평가합니다. 이번 연구 결과는 사용자가 제한된 공간에 앉아 있는 VR 시나리오에서 효과적인 순간이동 기술을 설계하기 위한 귀중한 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00129,Interaction & Input,Redirected Walking / Locomotion,User Study,User Study / Empirical Findings
66,2025,From Notation to Gesture: Virtual Conductor Gesture Generation in VR Via Structured Score Semantics,표기법에서 제스처까지: 구조화된 점수 의미론을 통한 VR의 가상 지휘자 제스처 생성,"Conductor avatar plays a dual role in immersive Virtual Reality (VR) interactive systems by interpreting musical scores and guiding orchestral performance. Rule-based score-driven methods ensure precise synchronization with predefined conducting templates or videos, but are constrained by pre-authored data. Audio-driven frameworks offer greater adaptability through real-time gesture generation but often fail to capture the symbolic semantics of musical scores. To overcome these limitations, we propose a novel score-driven gesture generation framework that translates symbolic musical representations into plausible conducting gestures. Our approach adopts a two-stage architecture, combining a comparative learning stage for pre-training a score encoder with a generative learning stage for gesture synthesis. The score encoder explicitly models musical features such as tempo, chord, intensity, and cycle semantics, directly informing gesture generation. To support this research, we introduce Multimodal Symphonic Conducting Dataset (MSCD), the first synchronized dataset comprising conducting gestures, performance audio, and editable symbolic scores, effectively bridging the gap between musical semantics and gesture synthesis. Qualitative and quantitative analyses are provided to demonstrate the effectiveness of our approach, while a user study is designed to identify the strengths and limitations of the current work.","지휘자 아바타는 몰입형 가상 현실(VR) 대화형 시스템에서 악보를 해석하고 오케스트라 연주를 안내하는 이중 역할을 수행합니다. 규칙 기반 점수 기반 방법은 사전 정의된 지휘 템플릿 또는 비디오와의 정확한 동기화를 보장하지만 사전 작성된 데이터의 제약을 받습니다. 오디오 기반 프레임워크는 실시간 제스처 생성을 통해 뛰어난 적응성을 제공하지만 종종 악보의 상징적 의미를 포착하지 못합니다. 이러한 한계를 극복하기 위해 우리는 상징적인 음악 표현을 그럴듯한 지휘 제스처로 변환하는 새로운 악보 기반 제스처 생성 프레임워크를 제안합니다. 우리의 접근 방식은 점수 인코더 사전 훈련을 위한 비교 학습 단계와 제스처 합성을 위한 생성 학습 단계를 결합하는 2단계 아키텍처를 채택합니다. 악보 인코더는 템포, 코드, 강도, 주기 의미론과 같은 음악적 특징을 명시적으로 모델링하여 제스처 생성에 직접적으로 영향을 미칩니다. 이 연구를 지원하기 위해 지휘 제스처, 연주 오디오 및 편집 가능한 기호 악보로 구성된 최초의 동기화된 데이터 세트인 MSCD(Multimodal Symphonic Conducting Dataset)를 도입하여 음악 의미론과 제스처 합성 간의 격차를 효과적으로 메웁니다. 우리 접근 방식의 효과를 입증하기 위해 정성적 및 정량적 분석이 제공되는 반면, 사용자 연구는 현재 작업의 강점과 한계를 식별하도록 설계되었습니다.",https://doi.org/10.1109/ISMAR67309.2025.00070,Interaction & Input; Education & Training,Deep Learning / Neural Networks,Technical Evaluation,System / Framework; Algorithm / Method
67,2025,GSHOI Denoiser: Denoising Gaussian Hand-Object Interaction for Photorealistic Rendering,GSHOI Denoiser: 사실적 렌더링을 위한 가우스 손-객체 상호 작용의 노이즈 제거,"Many VR/AR applications require the photorealistic rendering of hand-object interactions. Virtual hands are driven by users' hand poses captured via motion tracking to interact with virtual objects. The driven pose can be very noisy due to the constraints of tracking hardware and computation accuracy. This noise may lead to distorted hand poses and penetration artifacts during rendering. In this paper, we introduce the Gaussian Hand-Object Interaction Denoiser, the Gaussian splatting-based hand-object interaction denoising method, which effectively denoises the input twisted and penetrated hand poses to produce photorealistic results. We first propose the innovative joint-to-Gaussian surface representation, which accurately models the spatial relationships between hand skeleton joints and object Gaussians while highlighting hand-object penetrations and generalizing well to new hand poses and objects. Then, we propose a geometry-aware de-penetration algorithm that eliminates penetrations by detecting intersections between skeleton bones and object Gaussians and reposing any penetrated fingers onto the estimated underlying surface of the object. Experiments demonstrate that our method not only effectively reduces hand-object penetration depth but also produces more realistic rendering quality compared to the state-of-the-art methods MANUS+GEARS, MANUS+GeneOH, and $2 \text{DGS}+\text{Gene} \text{OH}$. The user study results show that our method significantly improves the users' visual perceptual experience regarding penetration and stability metrics. Project page: https://github.com/ZhaoLizz/GSHOIDenoiser","많은 VR/AR 애플리케이션에는 손 개체 상호 작용의 사실적인 렌더링이 필요합니다. 가상 손은 모션 추적을 통해 캡처된 사용자의 손 자세에 의해 구동되어 가상 개체와 상호 작용합니다. 구동되는 포즈는 추적 하드웨어 및 계산 정확도의 제약으로 인해 매우 시끄러울 수 있습니다. 이 노이즈로 인해 렌더링 중에 왜곡된 손 자세와 침투 아티팩트가 발생할 수 있습니다. 본 논문에서는 가우스 스플래팅 기반의 손-객체 상호 작용 노이즈 제거 방법인 가우스 손-객체 상호 작용 디노이저를 소개합니다. 이 방법은 입력된 비틀리고 관통된 손 포즈를 효과적으로 노이즈 제거하여 사실적인 결과를 생성합니다. 우리는 먼저 혁신적인 관절-가우스 표면 표현을 제안합니다. 이 표현은 손 뼈대 관절과 객체 가우스 사이의 공간 관계를 정확하게 모델링하는 동시에 손 객체 침투를 강조하고 새로운 손 포즈와 객체에 잘 일반화합니다. 그런 다음 골격 뼈와 객체 가우시안 사이의 교차점을 감지하고 관통된 손가락을 객체의 추정된 기본 표면에 재배치하여 침투를 제거하는 형상 인식 비침투 알고리즘을 제안합니다. 실험을 통해 우리의 방법은 손 물체 침투 깊이를 효과적으로 줄일 뿐만 아니라 최첨단 방법인 MANUS+GEARS, MANUS+GeneOH 및 $2 \text{DGS}+\text{Gene} \text{OH}$에 비해 더욱 사실적인 렌더링 품질을 생성한다는 것을 보여줍니다. 사용자 연구 결과는 우리의 방법이 침투 및 안정성 지표와 관련된 사용자의 시각적 지각 경험을 크게 향상시키는 것으로 나타났습니다. 프로젝트 페이지: https://github.com/ZhaoLizz/GSHOIDenoiser",https://doi.org/10.1109/ISMAR67309.2025.00071,Rendering & Visualization; Interaction & Input,Hand / Gesture Recognition,Technical Evaluation,Algorithm / Method
68,2025,"Gender Congruence and Social Context in Xr: Effects on Partner Preference, Warmth, Competence, and Uncanniness","Xr의 성별 일치 및 사회적 맥락: 파트너 선호도, 따뜻함, 역량 및 기괴함에 미치는 영향","As immersive virtual environments become more prevalent, avatars serve as critical social interfaces. This study explores how combinations of visual appearance, vocal characteristics, and informed identity influence users' initial impressions and partner preferences in four distinct XR scenarios: physical, intellectual, social, and romantic. A within-subject experiment with 40 participants assessed perceived warmth, competence, uncanniness, and selection preferences across diverse avatar configurations. Results indicate that vocal cues had a particularly strong impact on social perception, often shaping feelings of approachability and clarity in communication. While some cue alignments enhanced perceived social comfort and engagement, inconsistencies across gender-related cues occasionally led to increased perceptions of uncanniness, especially in emotionally sensitive contexts. These findings highlight the importance of designing avatars that thoughtfully adapt to different interaction contexts, supporting inclusive and responsive user experiences in social XR platforms.","몰입형 가상 환경이 더욱 널리 보급됨에 따라 아바타는 중요한 소셜 인터페이스 역할을 합니다. 본 연구에서는 신체적, 지적, 사회적, 낭만적인 네 가지 XR 시나리오에서 시각적 외모, 음성 특성 및 정보에 근거한 신원의 조합이 사용자의 초기 인상과 파트너 선호도에 어떻게 영향을 미치는지 탐구합니다. 40명의 참가자를 대상으로 한 피험자 내 실험에서는 다양한 아바타 구성에 걸쳐 인지된 따뜻함, 능력, 기괴함 및 선택 선호도를 평가했습니다. 결과는 음성 단서가 사회적 인식에 특히 강한 영향을 미쳤으며 종종 의사소통의 접근성과 명확성에 대한 느낌을 형성한다는 것을 나타냅니다. 일부 단서 정렬은 인지된 사회적 편안함과 참여를 향상시키는 반면, 성별 관련 단서 간의 불일치는 특히 감정적으로 민감한 맥락에서 기괴함에 대한 인식을 증가시키는 경우가 있었습니다. 이러한 연구 결과는 소셜 XR 플랫폼에서 포괄적이고 반응성이 뛰어난 사용자 경험을 지원하면서 다양한 상호 작용 상황에 세심하게 적응하는 아바타를 디자인하는 것의 중요성을 강조합니다.",https://doi.org/10.1109/ISMAR67309.2025.00089,Interaction & Input; Perception & Cognition,Optical / Display Technology,User Study,System / Framework
69,2025,HaptiCeiling: An Encountered-Type Overhead Haptic Interface in Virtual Reality,HaptiCeiling: 가상 현실에서 마주치는 오버헤드 햅틱 인터페이스,"Environmental haptic interfaces have been extensively researched for their ability to enhance spatial realism, diversity, and immersion in Virtual Reality (VR). Among these, the Overhead Haptic Interface (OHI) has shown unique interaction opportunities in VR, such as space design, training, and entertainment. However, direct touch interaction with encountered-type OHI and users' perceptual abilities in these contexts remain underexplored. In this paper, we introduce HaptiCeiling, an OHI utilizing a parallel mechanical system driven by three linear actuators. The system allows translational motion along the z-axis from 0 to $450 m m$ and angular motion from −30 degrees to 30 degrees around both x and y-axes, enabling the rendering of various OHIs in VR. Through three user studies, we explored (1) just-noticeable difference (JND) in perceiving changes of HaptiCeiling's height and tilt angle, (2) the perceptual thresholds for virtual overhead interface height and tilt angle compared to HaptiCeiling without causing any semantic violations, and (3) the perceived realism and immersion of our system across three VR applications.","환경 촉각 인터페이스는 가상 현실(VR)에서 공간 현실감, 다양성 및 몰입감을 향상시키는 능력에 대해 광범위하게 연구되어 왔습니다. 이 중 OHI(Overhead Haptic Interface)는 공간 디자인, 교육, 엔터테인먼트 등 VR에서 독특한 상호 작용 기회를 보여주었습니다. 그러나 마주친 유형의 OHI와의 직접적인 터치 상호 작용과 이러한 맥락에서 사용자의 지각 능력은 여전히 ​​과소 탐구되어 있습니다. 본 논문에서는 3개의 선형 액추에이터로 구동되는 병렬 기계 시스템을 활용하는 OHI인 HaptiCeiling을 소개합니다. 이 시스템은 0에서 $450 m m$까지 z축을 따른 병진 운동과 x축과 y축 모두에서 -30도에서 30도까지의 각도 운동을 허용하여 VR에서 다양한 OHI를 렌더링할 수 있습니다. 세 가지 사용자 연구를 통해 우리는 (1) HaptiCeiling의 높이 및 기울기 각도 변화를 인지하는 데 있어서 JND(Just-Noticeable Difference), (2) 의미 위반을 일으키지 않고 HaptiCeiling과 비교하여 가상 오버헤드 인터페이스 높이 및 기울기 각도에 대한 인지 임계값, (3) 세 가지 VR 애플리케이션에 걸쳐 우리 시스템의 인지된 현실감 및 몰입도를 조사했습니다.",https://doi.org/10.1109/ISMAR67309.2025.00083,Interaction & Input; Perception & Cognition,Haptic / Tactile Feedback,Technical Evaluation,System / Framework
70,2025,Have a Seat: An Enhanced Reactive Alignment of a Single Target's Position and Angle from the User's Perspective in VR,자리 잡기: VR에서 사용자 관점에서 단일 대상의 위치 및 각도에 대한 향상된 반응 정렬,"Redirected Walking (RDW) techniques allow users to explore virtually infinite environments within constrained physical spaces. However, achieving precise alignment between physical and virtual targets remains a significant challenge. In particular, when both position and orientation of the targets need to align in order to get a proper haptic feedback like siting on a virtual chair. This paper introduces a revised version of the Reactive Alignment (REA) controller that simultaneously minimizes the Angular and Positional Distance Errors between a physical and a virtual target. The proposed method enhances spatial alignment and optimizes user navigation using a novel rotation gain control algorithm that takes angular misalignment into account. In addition, a new metric, $\Delta p$, is proposed to quantify the angular alignment, complementing the redefined Physical Distance Error (PDE) for positional accuracy. We implemented the algorithm on Oculus Quest head-mounted display and utilized the HMD's physical space tracking to locate the physical prop's location without the need of any external tracking. We also incorporated saccadic redirection by utilizing the HMD's eyetracking functionality to complement the revised REA approach. A user study demonstrates that the revised REA controller outperforms the original REA by reducing Physical Distance Error, angular error, and reset counts. It also enhanced user interaction with physical props by enabling users to successfully sit on a physical chair 60% of the time compared to 0% with the original REA when $\Delta p$ is zero.","Redirected Walking (RDW) techniques allow users to explore virtually infinite environments within constrained physical spaces. 그러나 물리적 대상과 가상 대상 간의 정확한 정렬을 달성하는 것은 여전히 ​​중요한 과제로 남아 있습니다. 특히, 가상 의자에 앉는 것과 같이 적절한 촉각 피드백을 얻기 위해 대상의 위치와 방향을 모두 정렬해야 하는 경우입니다. 이 문서에서는 물리적 대상과 가상 대상 간의 각도 및 위치 거리 오류를 동시에 최소화하는 REA(Reactive Alignment) 컨트롤러의 개정된 버전을 소개합니다. 제안된 방법은 각도 오정렬을 고려한 새로운 회전 이득 제어 알고리즘을 사용하여 공간 정렬을 향상시키고 사용자 탐색을 최적화합니다. 또한 위치 정확도를 위해 재정의된 PDE(물리적 거리 오류)를 보완하여 각도 정렬을 정량화하기 위해 새로운 측정 기준인 $\Delta p$가 제안되었습니다. 우리는 Oculus Quest 헤드 마운트 디스플레이에 알고리즘을 구현하고 HMD의 물리적 공간 추적을 활용하여 외부 추적 없이도 물리적 소품의 위치를 ​​찾았습니다. 또한 수정된 REA 접근 방식을 보완하기 위해 HMD의 시선 추적 기능을 활용하여 단속적 리디렉션을 통합했습니다. 사용자 연구에 따르면 개정된 REA 컨트롤러는 물리적 거리 오류, 각도 오류 및 재설정 횟수를 줄임으로써 원래 REA 컨트롤러보다 성능이 뛰어난 것으로 나타났습니다. 또한 $\Delta p$가 0일 때 사용자가 원래 REA의 0%에 비해 60%의 시간 동안 물리적 의자에 성공적으로 앉을 수 있도록 하여 물리적 소품과의 사용자 상호 작용을 향상시켰습니다.",https://doi.org/10.1109/ISMAR67309.2025.00069,Interaction & Input; Display & Optics,Redirected Walking / Locomotion,User Study,Algorithm / Method
71,2025,HeadDepth: Gaze Raycasting with Head Pitch for Depth Control,HeadDepth: 깊이 제어를 위한 헤드 피치를 사용한 시선 레이캐스팅,"Gaze is fast and intuitive for raycasting, but lacks a way to control depth for input in 3D. We propose HeadDepth to augment gaze with vertical head rotation (pitch) to control depth along the line of sight, and investigate three pitch-to-depth mappings: Relative maps pitch velocity to changes in depth; EdgeGain adds dynamic gain dependent at eccentric gaze angles; and PingPong provides an absolute back-and-forth mapping that repeats at different pitch angles. HeadDepth is not as fast as controller-based RayCursor but has the advantage of being hands-free. In a user study, all three variants proved effective for 3D positioning; PingPong required least effort and EdgeGain was least affected by differences in tasks. Eye-head coordination and task had a significant effect, as head movements in support of gaze can affect depth control synergistically or antagonistically. Our results have significance beyond HeadDepth as they generalize to any interaction where head rotational input concurs with gaze fixation on visual feedback. Interaction designs may benefit from dynamic adjustment to eye-in-head angle to prevent discomfort and maintain objects within the user's field of view.",Gaze는 레이캐스팅을 위해 빠르고 직관적이지만 3D 입력에 대한 깊이를 제어하는 ​​방법이 부족합니다. 우리는 수직 머리 회전(피치)으로 시선을 강화하여 시선을 따라 깊이를 제어하고 세 가지 피치-깊이 매핑을 조사하는 HeadDepth를 제안합니다. 상대 매핑은 피치 속도를 깊이 변화에 매핑합니다. EdgeGain은 편심 시선 각도에 따른 동적 게인을 추가합니다. PingPong은 다양한 피치 각도에서 반복되는 완벽한 앞뒤 매핑을 제공합니다. HeadDepth는 컨트롤러 기반 RayCursor만큼 빠르지는 않지만 핸즈프리라는 장점이 있습니다. 사용자 연구에서 세 가지 변형 모두 3D 포지셔닝에 효과적인 것으로 입증되었습니다. PingPong은 최소한의 노력이 필요했고 EdgeGain은 작업 차이의 영향을 가장 적게 받았습니다. 눈-머리 조정 및 작업은 시선을 지원하는 머리 움직임이 깊이 제어에 상승적 또는 적대적으로 영향을 미칠 수 있으므로 중요한 영향을 미쳤습니다. 우리의 결과는 머리 회전 입력이 시각적 피드백에 대한 시선 고정과 일치하는 모든 상호 작용을 일반화하므로 HeadDepth 이상의 의미를 갖습니다. 상호 작용 디자인은 불편함을 방지하고 사용자의 시야 내에 개체를 유지하기 위해 눈과 머리 각도를 동적으로 조정함으로써 이점을 얻을 수 있습니다.,https://doi.org/10.1109/ISMAR67309.2025.00048,Interaction & Input,Eye / Gaze Tracking,User Study,User Study / Empirical Findings
72,2025,Headzoom: Hands-Free Zooming and Panning for 2D Image Navigation Using Head Motion,Headzoom: 헤드 모션을 사용한 2D 이미지 탐색을 위한 핸즈프리 줌 및 패닝,"We introduce HeadZoom, a hands-free interaction technique for navigating two-dimensional visual content using head movements. HeadZoom enables fluid zooming and panning using only real-time head tracking. It supports natural control in applications such as map exploration, radiograph inspection, and image browsing, where physical interaction is limited. We evaluated HeadZoom in a withinsubjects study comparing three interaction techniques-Static, Tilt Zoom, and Parallel Zoom-across spatial, error, and subjective metrics. Parallel Zoom significantly reduced total head movement compared to Static and Tilt modes. Users reported significantly lower perceived exertion for Parallel Zoom, confirming its suitability for prolonged or precision-based tasks. By minimizing movement demands while maintaining task effectiveness, HeadZoom advances the design of head-based 2D interaction in VR and creates new opportunities for accessible hands-free systems for image exploration.","머리 움직임을 사용해 2차원 시각적 콘텐츠를 탐색하기 위한 핸즈프리 상호작용 기술인 HeadZoom을 소개합니다. HeadZoom은 실시간 머리 추적만을 사용하여 유연한 확대/축소 및 패닝을 가능하게 합니다. 물리적 상호작용이 제한된 지도 탐색, 방사선 검사, 이미지 탐색 등의 애플리케이션에서 자연스러운 제어를 지원합니다. 우리는 공간, 오류 및 주관적 지표에 걸쳐 정적, 틸트 줌, 병렬 줌의 세 가지 상호 작용 기술을 비교하는 주제 내 연구에서 HeadZoom을 평가했습니다. 평행 줌은 정적 및 틸트 모드에 비해 전체 머리 움직임을 크게 줄였습니다. 사용자들은 Parallel Zoom에 대해 인지된 운동량이 현저히 낮다고 보고했으며, 이는 장시간 또는 정밀 기반 작업에 적합하다는 것을 확인시켜 줍니다. HeadZoom은 작업 효율성을 유지하면서 움직임 요구를 최소화함으로써 VR에서 머리 기반 2D 상호 작용 설계를 발전시키고 이미지 탐색을 위한 접근 가능한 핸즈프리 시스템에 대한 새로운 기회를 창출합니다.",https://doi.org/10.1109/ISMAR67309.2025.00134,Interaction & Input,Other,User Study,System / Framework
73,2025,How Accurate is the Hololens 2? a Robotic Ground-Truth and Sensor Occlusion Evaluation,Hololens 2는 얼마나 정확합니까? 로봇 기반 실측 정보 및 센서 폐색 평가,"Spatial tracking in Mixed Reality (MR) relies on the device's sensor fusion and localization accuracy, directly impacting virtual object placement and user interaction. In this paper, we measure the minimum achievable projection error of Microsoft's HL2 with an industrial-grade robotic manipulator as ground truth. Specifically, we use a reproducible methodology to measure the device's highest achievable spatial accuracy and precision under controlled motion trajectories and tracking scenarios, including static positioning, sinusoidal 1D dynamic motions, and circular 2D planar movements at varying tangential velocities. Additionally, we investigated the contribution of different device sensors to overall positioning error by manually blocking specific sensor inputs. The analysis revealed an average localization error of approximately 5 mm with a standard deviation of 5.5 mm, representing the device's accuracy and precision, respectively. The goal is to provide realistic expectations of MR readiness levels with respect to its highest achievable accuracy and precision, shedding light on its capabilities and limitations for different applications.","혼합 현실(MR)의 공간 추적은 장치의 센서 융합 및 위치 파악 정확도에 의존하며 가상 개체 배치 및 사용자 상호 작용에 직접적인 영향을 미칩니다. 본 논문에서는 산업용 로봇 매니퓰레이터를 사용하여 Microsoft의 HL2에서 달성 가능한 최소 투영 오류를 Ground Truth로 측정합니다. 특히, 재현 가능한 방법론을 사용하여 다양한 접선 속도에서의 정적 포지셔닝, 정현파 1D 동적 모션 및 원형 2D 평면 이동을 포함하여 제어된 모션 궤적 및 추적 시나리오에서 장치의 달성 가능한 최고 공간 정확도 및 정밀도를 측정합니다. 또한 특정 센서 입력을 수동으로 차단하여 전체 위치 오류에 대한 다양한 장치 센서의 기여도를 조사했습니다. 분석 결과, 평균 위치 파악 오류는 약 5mm, 표준 편차는 5.5mm로 나타났습니다. 이는 각각 장치의 정확도와 정밀도를 나타냅니다. 목표는 달성 가능한 최고 정확도 및 정밀도와 관련하여 MR 준비 수준에 대한 현실적인 기대치를 제공하고 다양한 애플리케이션에 대한 기능 및 제한 사항을 밝히는 것입니다.",https://doi.org/10.1109/ISMAR67309.2025.00164,Tracking & Localization; Interaction & Input,Sensor Fusion,Quantitative Experiment,Hardware / Device
74,2025,How Spatial Ability Affects Response to Gaze-Adaptive Cueing in Mixed Reality Spatial Navigation,혼합 현실 공간 내비게이션에서 공간 능력이 시선 적응 신호에 대한 응답에 미치는 영향,"Gaze-adaptive interfaces are increasingly employed in mixed reality (MR) to support attention-aware interaction. However, it remains unclear how the usability and effectiveness of such interfaces are influenced by individual differences in spatial ability, cognition, or prior experience. This paper addresses this gap by focusing on user spatial navigation ability and its role in modulating user interaction with gaze-adaptive cues during spatial navigation. We compare gaze-adaptive cues to proximity-triggered cues in an MR navigation task, and leverage eye tracking to examine how users' gaze patterns relate to their spatial ability and spatial knowledge acquisition under each cue type. This enables us to explore contentindependent gaze measures as a potential subjective approach to assess spatial ability and learning outcomes in MR. Our findings show that gaze-adaptive cueing impacts users differently based on their spatial abilities. In particular, when adding gaze-adaptive cues, individuals with higher spatial ability seem to engaged with cues more and exhibited longer fixations. Furthermore, higher spatial ability and better learning outcomes were linked to more effective gaze patterns defined by longer fixation durations, larger saccadic amplitudes, and reduced gaze transition entropy, with effects varying by cue type. Our findings highlight the potential of gaze transition entropy for assessing spatial ability disparities in MR spatial tasks.","시선 적응형 인터페이스는 주의 인식 상호 작용을 지원하기 위해 혼합 현실(MR)에 점점 더 많이 사용되고 있습니다. However, it remains unclear how the usability and effectiveness of such interfaces are influenced by individual differences in spatial ability, cognition, or prior experience. 본 논문에서는 사용자 공간 탐색 능력과 공간 탐색 중 시선 적응 단서와 사용자 상호 작용을 조절하는 역할에 초점을 맞춰 이러한 격차를 해결합니다. 우리는 MR 탐색 작업에서 시선 적응 단서를 근접 트리거 단서와 비교하고 시선 추적을 활용하여 사용자의 시선 패턴이 각 단서 유형에서 공간 능력 및 공간 지식 획득과 어떻게 관련되는지 조사합니다. 이를 통해 우리는 MR의 공간 능력과 학습 결과를 평가하기 위한 잠재적인 주관적 접근 방식으로 콘텐츠 독립적인 시선 측정을 탐색할 수 있습니다. 우리의 연구 결과에 따르면 시선 적응 신호는 사용자의 공간 능력에 따라 다르게 영향을 미칩니다. 특히, 시선 적응 단서를 추가할 때 공간 능력이 높은 개인은 단서에 더 많이 관여하고 더 긴 고정을 보이는 것으로 보입니다. 또한, 더 높은 공간 능력과 더 나은 학습 결과는 큐 유형에 따라 효과가 달라지는 더 긴 고정 기간, 더 큰 단속성 진폭 및 감소된 시선 전환 엔트로피로 정의되는 더 효과적인 시선 패턴과 연결되었습니다. 우리의 연구 결과는 MR 공간 작업에서 공간 능력 차이를 평가하기 위한 시선 전환 엔트로피의 잠재력을 강조합니다.",https://doi.org/10.1109/ISMAR67309.2025.00167,Interaction & Input; Perception & Cognition,Eye / Gaze Tracking,Other,Algorithm / Method
75,2025,Icebreaking: Building Trust and Empathy with Virtual Museum Embodied Conversational Agents via Personalized Initial Interactions,Icebreaking: 개인화된 초기 상호 작용을 통해 가상 박물관에 구현된 대화 에이전트를 통해 신뢰와 공감 구축,"Embodied Conversational Agents (ECAs) show significant potential for enhancing user experience in virtual museums. However, effectively establishing user trust and empathy during the critical initial moments of encounter remains a key challenge. Existing research has paid limited attention to systematically fostering these qualities through personalized Icebreaking strategies, often focusing instead on agent appearance or guidance mechanisms. This paper investigates the impact of a multimodal personalized initial interaction strategy—integrating personalized openings based on user interests, guided self-disclosure, and adaptive embodied social rituals—on users' trust in and empathy towards an ECA guide. We implemented this strategy within a VR virtual museum themed around the Beijing Central Axis, featuring interactions led by an ECA guide named “Xiao Liang.” In a user study ($N=60$), we compared three conditions: (1) No initial interaction (control), (2) Standardized interaction, and (3) Personalized interaction. User trust and empathy were measured immediately post-interaction using established psychological scales. Results clearly demonstrated that the personalized initial interaction significantly enhanced both user trust ($p<0.05$) and empathy ($p<0.05$) compared to both the control and standardized interaction groups. These findings confirm that personalizing the integration of dialogue and embodied behaviors early in the interaction is an effective pathway for building positive user-agent relationships. Furthermore, this work provides valuable insights for designing more trustworthy and empathic initial interactions for ECAs, offering a significant contribution to existing ECA interaction paradigms.","ECA(Embodied Conversational Agent)는 가상 박물관에서 사용자 경험을 향상시키는 데 상당한 잠재력을 보여줍니다. 그러나 중요한 초기 순간에 사용자 신뢰와 공감을 효과적으로 구축하는 것은 여전히 ​​중요한 과제로 남아 있습니다. 기존 연구에서는 개인화된 아이스브레이킹 전략을 통해 이러한 자질을 체계적으로 육성하는 데 제한된 관심을 기울였으며, 대신 에이전트 출현이나 안내 메커니즘에 초점을 맞추는 경우가 많았습니다. 이 문서에서는 사용자 관심 사항, 안내된 자기 공개 및 적응형으로 구현된 사회적 의식을 기반으로 개인화된 오프닝을 통합하는 다중 모드 개인화된 초기 상호 작용 전략이 ECA 가이드에 대한 사용자의 신뢰와 공감에 미치는 영향을 조사합니다. 우리는 ""Xiao Liang""이라는 ECA 가이드가 주도하는 상호 작용을 특징으로 하는 베이징 중앙 축을 주제로 한 VR 가상 박물관 내에서 이 전략을 구현했습니다. 사용자 연구($N=60$)에서 우리는 (1) 초기 상호 작용 없음(대조), (2) 표준화된 상호 작용, (3) 개인화된 상호 작용의 세 가지 조건을 비교했습니다. 사용자의 신뢰와 공감은 확립된 심리적 척도를 사용하여 상호작용 직후 측정되었습니다. 결과는 개인화된 초기 상호 작용이 통제 그룹과 표준화된 상호 작용 그룹 모두에 비해 사용자 신뢰($p<0.05$)와 공감($p<0.05$) 모두 크게 향상되었음을 분명히 보여주었습니다. 이러한 연구 결과는 상호 작용 초기에 대화와 구체화된 행동의 통합을 개인화하는 것이 긍정적인 사용자-에이전트 관계를 구축하는 효과적인 경로임을 확인합니다. 또한 이 작업은 ECA에 대한 보다 신뢰할 수 있고 공감적인 초기 상호 작용을 설계하기 위한 귀중한 통찰력을 제공하여 기존 ECA 상호 작용 패러다임에 상당한 기여를 합니다.",https://doi.org/10.1109/ISMAR67309.2025.00122,Interaction & Input,Natural Language Processing,User Study,User Study / Empirical Findings
76,2025,ImmerJM: A 3D Design Tool for Creating User Journey Maps Based on Immersive Virtual Environments,ImmerJM: 몰입형 가상 환경을 기반으로 사용자 여정 맵을 생성하기 위한 3D 디자인 도구,"Immersive Virtual Environment (IVE) have demonstrated substantial potential in user research. However, the integration of IVE into Journey Map (JM) creation remains unexplored, as traditional JM creation still predominantly relies on 2D data. To bridge this research gap, we integrated IVE into the JM creation process, aiming to explore the potential of applying IVE in the JM creation process. Through a formative study with design experts ($\mathrm{N}=14$), six design goals were identified, reflecting designers' expectations for incorporating IVE into JM creation. Based on these goals, we developed ImmerJM, a tool that enables designers to analyze and document user journeys directly within immersive 3D environments. We conducted a comparative evaluation with User Experience (UX) participants ($\mathrm{N}=20$) to assess the JM creation process of ImmerJM against a traditional screen-based method. The quantitative and qualitative analyses revealed key improvements (e.g., phase-based JM mapping) in journey process and outputs. Our findings suggest that ImmerJM advances JM methodologies by leveraging spatial immersion, offering crucial implications for future JM creation and broader UX design practices involving IVE technologies.",몰입형 가상 환경(IVE)은 사용자 연구에서 상당한 잠재력을 보여주었습니다. 그러나 전통적인 JM 생성이 여전히 주로 2D 데이터에 의존하기 때문에 IVE를 JM(Journey Map) 생성에 통합하는 방법은 아직 연구되지 않은 상태입니다. 이러한 연구 격차를 해소하기 위해 우리는 IVE를 JM 생성 프로세스에 통합하여 JM 생성 프로세스에 IVE를 적용할 수 있는 가능성을 탐구하는 것을 목표로 했습니다. 디자인 전문가($\mathrm{N}=14$)와의 형성 연구를 통해 IVE를 JM 제작에 통합하려는 디자이너의 기대를 반영하여 6가지 디자인 목표가 확인되었습니다. 이러한 목표를 바탕으로 우리는 디자이너가 몰입형 3D 환경 내에서 직접 사용자 여정을 분석하고 문서화할 수 있는 도구인 ImmerJM을 개발했습니다. ImmerJM의 JM 생성 프로세스를 전통적인 화면 기반 방법과 비교하여 평가하기 위해 사용자 경험(UX) 참가자($\mathrm{N}=20$)와 비교 평가를 수행했습니다. 정량적 및 정성적 분석을 통해 여정 프로세스 및 결과의 주요 개선 사항(예: 단계 기반 JM 매핑)이 밝혀졌습니다. 우리의 연구 결과에 따르면 ImmerJM은 공간적 몰입을 활용하여 JM 방법론을 발전시키고 향후 JM 생성 및 IVE 기술과 관련된 광범위한 UX 디자인 사례에 중요한 의미를 제공합니다.,https://doi.org/10.1109/ISMAR67309.2025.00128,Perception & Cognition,Optical / Display Technology,User Study,Algorithm / Method
77,2025,Improving Pointing Accuracy for 3D Target Selection in Virtual Reality Through Depth Perception Biases Correction,깊이 인식 편향 보정을 통해 가상 현실에서 3D 대상 선택을 위한 포인팅 정확도 향상,"Accurate 3D target selection in virtual reality (VR) is fundamentally impeded by pointing uncertainty along the depth axis, a challenge that existing 2D pointing models fail to address due to the complexities of depth perception. Near-eye interactions in VR are influenced by binocular depth cues and vergence-accommodation conflicts (VAC), which introduce significant depth perception biases that impair predictive performance. To address this issue, we first investigate these factors and derive a Gaussian distribution to model near-field depth biases within a 2.5m range. Second, to analyze pointing performance across this extended depth range, we classify 3D target motions into three distinct types: motion-indepth, motion-in-plane, and combined motion. Our analysis identifies that interaction depth and motion amplitude are the two most critical factors influencing pointing accuracy. Accordingly, by incorporating these factors alongside our perceptual bias Gaussian into the Ternary-Gaussian framework, we demonstrate significantly improved predictive performance across diverse 3D motion scenarios. These findings enhance the understanding of user perception in virtual environments and support the development of precise, context-aware interaction cues. Future research can extend these models to design real-time adaptive interfaces, thereby elevating user experiences in VR.","가상 현실(VR)에서 정확한 3D 대상 선택은 깊이 인식의 복잡성으로 인해 기존 2D 포인팅 모델이 해결하지 못하는 문제인 깊이 축을 따라 불확실성을 가리키므로 근본적으로 방해를 받습니다. VR의 눈 근처 상호 작용은 양안 깊이 단서 및 VAC(수렴-조절 충돌)의 영향을 받으며, 이는 예측 성능을 저해하는 심각한 깊이 인식 편향을 유발합니다. 이 문제를 해결하기 위해 먼저 이러한 요인을 조사하고 2.5m 범위 내 근거리 깊이 편향을 모델링하기 위한 가우스 분포를 도출합니다. 둘째, 이 확장된 깊이 범위에서 포인팅 성능을 분석하기 위해 3D 대상 모션을 심층 모션, 평면 내 모션 및 결합 모션의 세 가지 유형으로 분류합니다. 우리의 분석에 따르면 상호 작용 깊이와 동작 진폭은 포인팅 정확도에 영향을 미치는 가장 중요한 두 가지 요소입니다. 따라서 지각 편향 가우시안과 함께 이러한 요소를 Ternary-Gaussian 프레임워크에 통합함으로써 다양한 3D 모션 시나리오에서 예측 성능이 크게 향상되었음을 보여줍니다. 이러한 발견은 가상 환경에서 사용자 인식에 대한 이해를 향상시키고 정확한 상황 인식 상호 작용 단서의 개발을 지원합니다. 향후 연구에서는 이러한 모델을 확장하여 실시간 적응형 인터페이스를 설계함으로써 VR에서의 사용자 경험을 향상시킬 수 있습니다.",https://doi.org/10.1109/ISMAR67309.2025.00020,Interaction & Input; Display & Optics,Other,Quantitative Experiment,User Study / Empirical Findings
78,2025,IntelliCap: Intelligent Guidance for Consistent View Sampling,IntelliCap: 일관된 뷰 샘플링을 위한 지능형 지침,"Novel view synthesis from images, for example, with 3D Gaussian splatting, has made great progress. Rendering fidelity and speed are now ready even for demanding virtual reality applications. However, the problem of assisting humans in collecting the input images for these rendering algorithms has received much less attention. High-quality view synthesis requires uniform and dense view sampling. Unfortunately, these requirements are not easily addressed by human camera operators, who are in a hurry, impatient, or lack understanding of the scene structure and the photographic process. Existing approaches to guide humans during image acquisition concentrate on single objects or neglect view-dependent material characteristics. We propose a novel situated visualization technique for scanning at multiple scales. During the scanning of a scene, our method identifies important objects that need extended image coverage to properly represent view-dependent appearance. To this end, we leverage semantic segmentation and category identification, ranked by a vision-language model. Spherical proxies are generated around highly ranked objects to guide the user during scanning. Our results show superior performance in real scenes compared to conventional view sampling strategies.",예를 들어 3D 가우스 스플래팅을 사용한 이미지의 새로운 뷰 합성은 큰 진전을 이루었습니다. 이제 까다로운 가상 현실 애플리케이션에서도 렌더링 충실도와 속도가 준비되었습니다. 그러나 이러한 렌더링 알고리즘에 대한 입력 이미지를 수집하는 데 있어 인간을 지원하는 문제는 훨씬 덜 주목을 받았습니다. 고품질 뷰 합성에는 균일하고 조밀한 뷰 샘플링이 필요합니다. 불행하게도 이러한 요구 사항은 서두르거나 참을성이 없거나 장면 구조와 사진 프로세스에 대한 이해가 부족한 카메라 작업자가 쉽게 해결할 수 없습니다. 이미지를 획득하는 동안 인간을 안내하는 기존 접근 방식은 단일 개체에 집중하거나 뷰에 따른 재료 특성을 무시합니다. 우리는 다양한 규모로 스캔하기 위한 새로운 위치 시각화 기술을 제안합니다. 장면을 스캔하는 동안 우리의 방법은 뷰 종속 모양을 적절하게 표현하기 위해 확장된 이미지 적용 범위가 필요한 중요한 개체를 식별합니다. 이를 위해 우리는 비전 언어 모델에 따라 순위가 매겨진 의미론적 분할 및 카테고리 식별을 활용합니다. 검색하는 동안 사용자를 안내하기 위해 순위가 높은 개체 주위에 구형 프록시가 생성됩니다. 우리의 결과는 기존 뷰 샘플링 전략에 비해 실제 장면에서 우수한 성능을 보여줍니다.,https://doi.org/10.1109/ISMAR67309.2025.00085,Rendering & Visualization; Interaction & Input,Computer Vision,Other,Algorithm / Method
79,2025,Investigating Dynamics of Subjective Anxiety and Behavior Due to Personal Space Violations and COVID-19-Related Stressors in a Social VR Simulation,소셜 VR 시뮬레이션에서 개인 공간 침해 및 코로나19 관련 스트레스 요인으로 인한 주관적 불안 및 행동의 역학 조사,"Personal space is the physical distance individuals prefer to maintain from others, and its violation induces stress, anxiety, and behavioral change—a phenomenon intensified during the COVID-19 pandemic. While prior research has largely relied on postexperiment questionnaires in passive, one-to-one scenarios, few studies have examined dynamic, crowd-based settings, and to our knowledge, none have collected time-resolved subjective anxiety aligned with moment-to-moment environmental stimuli. We introduce a novel social VR simulation that enables such dynamic analysis by combining continuous self-reports of anxiety with highresolution behavioral tracking during a two-stage, goal-oriented task. In Stage 1, participants navigated immersive VR environments (indoor/outdoor) while encountering virtual agents—some invading personal space, coughing, or (not) wearing masks. In Stage 2, they retrospectively annotated their experiences using first-person video playback. We also administered personality and COVID-related questionnaires. Across $N=109$ participants, we collected over 40,000 seconds of annotated data ($\approx 374.56$ s per person). Key findings include: 1) Personal space invasions significantly increased anxiety and movement, 2) Coughing agents elevated stress responses, while masks had no significant effect, and 3) Goal achievement consistently reduced anxiety, overriding other stressors. These results provide fine-grained insight into the temporal dynamics of emotional and behavioral responses to social stressors, and offer new design implications for social VR environments.","개인 공간은 개인이 다른 사람과 유지하기를 선호하는 물리적 거리이며, 이를 위반하면 스트레스, 불안, 행동 변화가 유발됩니다. 이러한 현상은 코로나19 팬데믹 기간 동안 더욱 심화되었습니다. 이전 연구는 수동적 일대일 시나리오의 실험 후 설문지에 크게 의존했지만 동적 군중 기반 설정을 조사한 연구는 거의 없었으며 우리가 아는 한 순간 순간의 환경 자극과 일치하는 시간 해결 주관적 불안을 수집한 연구는 없습니다. 우리는 2단계 목표 지향 작업 동안 불안에 대한 지속적인 자기 보고와 고해상도 행동 추적을 결합하여 이러한 동적 분석을 가능하게 하는 새로운 소셜 VR 시뮬레이션을 소개합니다. 1단계에서 참가자들은 가상 에이전트를 만나면서 몰입형 VR 환경(실내/실외)을 탐색했습니다. 일부는 개인 공간에 침입하거나 기침을 하거나 마스크를 착용하지 않았습니다. 2단계에서는 1인칭 비디오 재생을 사용하여 자신의 경험을 회고적으로 주석을 달았습니다. 우리는 또한 성격 및 코로나 관련 설문지를 관리했습니다. $N=109$ 참가자 전체에서 40,000초가 넘는 주석이 달린 데이터(1인당 $\about 374.56$s)를 수집했습니다. 주요 연구 결과는 다음과 같습니다: 1) 개인 공간 침입은 불안과 움직임을 크게 증가시켰고, 2) 기침제는 스트레스 반응을 증가시켰으나 마스크는 큰 효과가 없었으며, 3) 목표 달성은 지속적으로 불안을 감소시켜 다른 스트레스 요인을 압도했습니다. 이러한 결과는 사회적 스트레스 요인에 대한 정서적, 행동적 반응의 시간적 역학에 대한 세밀한 통찰력을 제공하고 소셜 VR 환경에 대한 새로운 디자인 시사점을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00124,Tracking & Localization,Sensor Fusion,Questionnaire / Survey,Design Guidelines; User Study / Empirical Findings
80,2025,Investigating Encoding and Perspective for Augmented Reality Motion Guidance,증강 현실 모션 안내에 대한 인코딩 및 관점 조사,"Augmented reality (AR) offers promising opportunities to support movement-based activities, such as personal training or physical therapy, with real-time, spatially-situated visual cues. While many approaches leverage AR to guide motion, existing design guidelines focus on simple, upper-body movements within the user's field of view. We lack evidence-based design recommendations for guiding more diverse scenarios involving movements with varying levels of visibility and direction. We conducted an experiment to investigate how different visual encodings and perspectives affect motion guidance performance and usability, using three exercises that varied in visibility and planes of motion. Our findings reveal significant differences in preference and performance across designs. Notably, the best perspective varied depending on motion visibility and showing more information about the overall motion did not necessarily improve motion execution. We provide empirically-grounded guidelines for designing immersive, interactive visualizations for motion guidance to support more effective AR systems.","증강 현실(AR)은 실시간, 공간적 위치의 시각적 신호를 통해 개인 훈련이나 물리 치료와 같은 움직임 기반 활동을 지원할 수 있는 유망한 기회를 제공합니다. 많은 접근 방식이 AR을 활용하여 모션을 안내하는 반면, 기존 디자인 지침은 사용자 시야 내에서 간단한 상체 움직임에 중점을 둡니다. 다양한 수준의 가시성과 방향을 가진 움직임과 관련된 보다 다양한 시나리오를 안내하기 위한 증거 기반 설계 권장 사항이 부족합니다. 우리는 가시성과 모션 평면이 다양한 세 가지 연습을 사용하여 다양한 시각적 인코딩과 관점이 모션 안내 성능과 유용성에 어떤 영향을 미치는지 조사하기 위한 실험을 수행했습니다. 우리의 연구 결과는 디자인 전반에 걸쳐 선호도와 성능에 상당한 차이가 있음을 보여줍니다. 특히 모션 가시성에 따라 최상의 관점이 달라지며 전체 모션에 대한 더 많은 정보를 표시한다고 해서 반드시 모션 실행이 향상되는 것은 아닙니다. 우리는 보다 효과적인 AR 시스템을 지원하기 위해 모션 안내를 위한 몰입형 대화형 시각화를 설계하기 위한 경험적 기반 지침을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00165,Rendering & Visualization,Other,Technical Evaluation,User Study / Empirical Findings; Design Guidelines
81,2025,Investigating the Effect of Visual Cue Density on Situational Awareness During Immersive Navigation,몰입형 탐색 중 상황 인식에 대한 시각적 신호 밀도의 영향 조사,"Navigation is a fundamental task supporting guided exploration and wayfinding as standalone or as part of other immersive applications. Previous research showed that navigational cues do not only impact wayfinding performance but can also affect perceptual and cognitive processes, e.g. divide attention and impair spatial memory. This study investigates whether varying the density of visual navigation cues can influence situational awareness. Additionally, we examine how cue density affects navigation usability and task performance. We compare three visual cue designs, ranging from high to low density: (1) PathLine, (2) ArrowTrail and (3) TurnMarker. We designed a user study augmenting a 3D scanned digital twin of a building with virtual machinery to simulate a factory floor maintenance task, where the cues guided participants to the next point of interest. A secondary task, the reporting of anomalies, was installed to assess situational awareness. Results showed that performance metrics remained unaffected by cue type, situational awareness and user experience results showed significant differences. Notably, the ArrowTrail cue, the medium-density design, was preferred by most participants and yielded the best overall results, e.g. in terms of anomaly detection and reaction time. These findings suggest that moderate cue density may offer an optimal balance between effective guidance and maintaining environmental awareness.","내비게이션은 독립형 또는 기타 몰입형 애플리케이션의 일부로 안내 탐색 및 길 찾기를 지원하는 기본 작업입니다. 이전 연구에서는 내비게이션 단서가 길 찾기 성능에 영향을 미칠 뿐만 아니라 지각 및 인지 과정에도 영향을 미칠 수 있음을 보여주었습니다. 주의를 분산시키고 공간 기억을 손상시킵니다. 이 연구는 시각적 내비게이션 단서의 밀도를 변화시키는 것이 상황 인식에 영향을 미칠 수 있는지 여부를 조사합니다. 또한 큐 밀도가 탐색 유용성과 작업 성능에 어떤 영향을 미치는지 조사합니다. 밀도가 높은 것부터 낮은 것까지 세 가지 시각적 큐 디자인을 비교합니다: (1) PathLine, (2) ArrowTrail 및 (3) TurnMarker. 우리는 가상 기계로 건물의 3D 스캔 디지털 트윈을 강화하여 공장 바닥 유지 관리 작업을 시뮬레이션하는 사용자 연구를 설계했습니다. 여기서 신호는 참가자를 다음 관심 지점으로 안내했습니다. 상황 인식을 평가하기 위해 두 번째 작업인 이상 현상 보고가 설치되었습니다. 결과에 따르면 성능 지표는 큐 유형, 상황 인식 및 사용자 경험 결과에 영향을 받지 않고 상당한 차이를 보였습니다. 특히, 중간 밀도 디자인인 ArrowTrail 큐는 대부분의 참가자가 선호했으며 전반적으로 가장 좋은 결과를 얻었습니다. 이상 징후 탐지 및 반응 시간 측면에서. 이러한 발견은 적당한 큐 밀도가 효과적인 안내와 환경 인식 유지 사이에 최적의 균형을 제공할 수 있음을 시사합니다.",https://doi.org/10.1109/ISMAR67309.2025.00138,Perception & Cognition; Interaction & Input,Sensor Fusion,User Study,User Study / Empirical Findings
82,2025,Isometric and Elastic Balance Boards for Virtual Reality Locomotion,가상 현실 이동을 위한 아이소메트릭 및 탄성 밸런스 보드,"Locomotion in virtual reality (VR) significantly impacts user immersion, spatial orientation, and overall experience. While traditional methods such as joystick-based movement and teleportation remain common, they present challenges like spatial disorientation, reduced presence, and motion sickness. In contrast, more embodied techniques, such as walking-in-place (WIP) and balance boards, aim to provide a more naturalistic form of locomotion. However, balance boards have proven to enhance the sense of control and presence in VR environments. Thus, we decided to further explore the benefits of isometric and elastic balance boards by examining their effects on VR locomotion movement behaviors, user experience, and motion sickness symptoms during a path-following task. We followed a 2 (feedback: isometric vs. elastic) $\times 3$ (orientation: virtual vs. physical vs. conditional virtual) within-group experimental design involving 28 participants. The collected data included movement metrics (i.e., trajectory length, speed, time, root mean square error, sway velocity), user experience metrics (i.e., usability, spatial presence, task load, intrinsic motivation), and motion sickness metrics. Our results indicated that orientation significantly affected trajectory length, speed, and sway velocity, while feedback strongly influenced movement accuracy. Additionally, the user experience was influenced by the combination of feedback and orientation, with participants favoring elastic feedback, particularly when paired with virtual orientation. Finally, we found motion sickness to be less pronounced with elastic feedback, especially in virtual and conditional virtual orientations, highlighting the importance of feedback in reducing discomfort during VR locomotion.","가상 현실(VR)의 이동은 사용자 몰입도, 공간 방향 및 전반적인 경험에 큰 영향을 미칩니다. 조이스틱 기반 이동 및 순간 이동과 같은 전통적인 방법은 여전히 ​​일반적이지만 공간 방향 감각 상실, 존재감 감소 및 멀미와 같은 문제를 야기합니다. 대조적으로, WIP(Walk-In-Place) 및 밸런스 보드와 같은 보다 구체화된 기술은 보다 자연스러운 형태의 이동을 제공하는 것을 목표로 합니다. 그러나 밸런스 보드는 VR 환경에서 제어감과 존재감을 향상시키는 것으로 입증되었습니다. 따라서 우리는 경로 따르기 작업 중 VR 운동 동작, 사용자 경험 및 멀미 증상에 미치는 영향을 조사하여 아이소메트릭 및 탄성 밸런스 보드의 이점을 더 조사하기로 결정했습니다. 우리는 28명의 참가자가 참여하는 2(피드백: 등척성 대 탄성) $\times 3$(방향: 가상 대 물리적 대 조건부 가상) 그룹 내 실험 설계를 따랐습니다. 수집된 데이터에는 이동 지표(예: 궤적 길이, 속도, 시간, 제곱 평균 제곱 오차, 흔들림 속도), 사용자 경험 지표(예: 유용성, 공간 존재, 작업 부하, 내재적 동기) 및 멀미 지표가 포함되었습니다. 우리의 결과는 방향이 궤적 길이, 속도 및 흔들림 속도에 큰 영향을 미치는 반면 피드백은 움직임 정확도에 큰 영향을 미치는 것으로 나타났습니다. 또한 사용자 경험은 피드백과 방향의 조합에 의해 영향을 받았으며 참가자는 특히 가상 방향과 결합된 경우 탄력적인 피드백을 선호했습니다. 마지막으로, 특히 가상 및 조건부 가상 방향에서 탄성 피드백을 사용하면 멀미가 덜 두드러지는 것을 발견하여 VR 이동 중에 불편함을 줄이는 피드백의 중요성을 강조했습니다.",https://doi.org/10.1109/ISMAR67309.2025.00018,Perception & Cognition,Redirected Walking / Locomotion,User Study,Algorithm / Method
83,2025,"Laughing Together: A Pilot Study on the Role of Virtual Agents in Emotional Contagion, Conformity, and Opinion Shaping in a Virtual Stand-Up Comedy Club","함께 웃기: 가상 스탠드업 코미디 클럽의 감정 전염, 순응 및 의견 형성에서 가상 에이전트의 역할에 대한 예비 연구","Interaction between humans and virtual agents is becoming increasingly prevalent. This research investigates the influence of virtual agents on users' opinions within a virtual stand-up comedy club setting, focusing on the socio-psychological mechanisms of emotional contagion and group conformity. We aimed to combine these two areas in a novel study approach. To this end, we designed and conducted an exploratory pilot study in which participants ($\mathrm{N}=20$) rated the funniness of jokes in VR, under varying conditions of virtual agent laughter (absence, medium intensity, strong intensity). Our analysis shows a significant positive linear effect on the log odds of higher funniness ratings when virtual agents exhibit laughter. Moreover, our study shows that this effect is mainly due to emotional contagion processes, individual differences in susceptibility to group conformity, and the interaction of both concepts, showing that emotional contagion happens only when the individual susceptibility to group conformity is high. The results offer valuable insights into the social impact of VR environments and the potential of virtual agents to shape user perception and opinion.","인간과 가상 에이전트 간의 상호 작용이 점점 더 보편화되고 있습니다. 본 연구는 가상 스탠드업 코미디 클럽 환경 내에서 가상 에이전트가 사용자 의견에 미치는 영향을 조사하고, 감정 전염과 집단 순응의 사회심리학적 메커니즘에 초점을 맞췄습니다. 우리는 새로운 연구 접근 방식으로 이 두 영역을 결합하는 것을 목표로 했습니다. 이를 위해 우리는 가상 에이전트 웃음의 다양한 조건(부재, 중간 강도, 강한 강도)에서 참가자($\mathrm{N}=20$)가 VR에서 농담의 재미를 평가하는 탐색적 파일럿 연구를 설계하고 수행했습니다. 우리의 분석은 가상 에이전트가 웃음을 보일 때 더 높은 재미 등급의 로그 확률에 유의미한 긍정적인 선형 효과를 보여줍니다. 더욱이, 우리의 연구는 이러한 효과가 주로 정서적 전염 과정, 집단 순응에 대한 민감성의 개인차 및 두 개념의 상호 작용에 기인한다는 것을 보여 주며, 정서적 전염은 집단 순응에 대한 개인의 민감성이 높을 때만 발생한다는 것을 보여줍니다. 결과는 VR 환경의 사회적 영향과 사용자 인식 및 의견을 형성하는 가상 에이전트의 잠재력에 대한 귀중한 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00152,Interaction & Input,Other,User Study,User Study / Empirical Findings
84,2025,Long-Term Experiences from Working with Extended Reality in the Wild,야생에서의 확장 현실 작업을 통한 장기적인 경험,"Extended Reality (XR) is increasingly used as a productivity tool and recent commercial XR devices have even been specifically designed as productivity tools, or, at least, are heavily advertised for such purposes, such as the Apple Vision Pro (AVP), which has now been available for more than one year. In spite of what marketing suggests, research still lacks an understanding of the long-term usage of such devices in ecologically valid everyday settings, as most studies are conducted in very controlled environments. Therefore, we conducted interviews with ten AVP users to better understand how experienced users engage with the device, and which limitations persist. Our participants report that XR can increase productivity and that they got used to the device after some time. Yet, a range of limitations persist that might hinder the widespread use of XR as a productivity tool, such as a lack of native applications, difficulties when integrating XR into current workflows, and limited possibilities to adapt and customize the XR experience.","확장 현실(XR)은 생산성 도구로 점점 더 많이 사용되고 있으며, 최근 상업용 XR 장치는 생산성 도구로 특별히 설계되었거나 적어도 1년 넘게 출시된 Apple Vision Pro(AVP)와 같이 그러한 목적으로 많이 광고되고 있습니다. 마케팅이 제안하는 것에도 불구하고 대부분의 연구는 매우 통제된 환경에서 수행되기 때문에 생태학적으로 유효한 일상 환경에서 이러한 장치의 장기적인 사용에 대한 연구는 여전히 부족합니다. 따라서 숙련된 사용자가 장치에 참여하는 방식과 지속되는 제한 사항을 더 잘 이해하기 위해 10명의 AVP 사용자와 인터뷰를 수행했습니다. 참가자들은 XR이 생산성을 높이고 일정 시간이 지나면 장치에 익숙해졌다고 보고했습니다. 그러나 네이티브 애플리케이션의 부족, XR을 현재 워크플로에 통합할 때의 어려움, XR 경험을 조정하고 사용자 정의할 수 있는 가능성의 제한 등 생산성 도구로서 XR의 광범위한 사용을 방해할 수 있는 다양한 제한 사항이 지속됩니다.",https://doi.org/10.1109/ISMAR67309.2025.00076,Other,Optical / Display Technology,User Study,User Study / Empirical Findings
85,2025,MAGIC: A Method for Analyzing the Grammar of Incomplete Cues,MAGIC: 불완전한 단서의 문법을 분석하는 방법,"Augmented reality (AR) and virtual reality (VR) applications commonly employ interaction cues that denote to the user what interaction to take. In this paper, we present a Method for Analyzing the Grammar of Incomplete Cues (MAGIC), which provides an approach for evaluating the design of interaction cues based on the completeness or incompleteness of the functional grammar that they convey through perceptual stimuli. To demonstrate the importance of complete cues, we also present a user study investigating the effects of complete and incomplete cues on which interactions participants choose. The results indicate that incomplete cues do not afford sufficient information, so users make assumptions about the intended interactions. Furthermore, the results indicate that users are more likely to choose intended interactions when the cues are complete. Hence, we present MAGIC as a potentially useful tool for helping interaction designers avoid usability issues with incomplete interaction cues.",증강 현실(AR) 및 가상 현실(VR) 애플리케이션은 일반적으로 어떤 상호 작용을 수행해야 하는지 사용자에게 알려주는 상호 작용 단서를 사용합니다. 이 논문에서는 지각 자극을 통해 전달하는 기능적 문법의 완전성 또는 불완전성을 기반으로 상호 작용 단서의 디자인을 평가하는 접근 방식을 제공하는 불완전한 단서의 문법 분석 방법(MAGIC)을 제시합니다. 완전한 단서의 중요성을 입증하기 위해 우리는 상호 작용 참가자가 선택하는 완전 및 불완전 단서의 효과를 조사하는 사용자 연구도 제시합니다. 결과는 불완전한 단서는 충분한 정보를 제공하지 못하므로 사용자는 의도된 상호 작용에 대해 가정을 한다는 것을 나타냅니다. 또한 결과는 단서가 완료되면 사용자가 의도한 상호 작용을 선택할 가능성이 더 높다는 것을 나타냅니다. 따라서 우리는 상호 작용 디자이너가 불완전한 상호 작용 단서로 인한 사용성 문제를 방지하는 데 도움이 되는 잠재적으로 유용한 도구로 MAGIC을 제시합니다.,https://doi.org/10.1109/ISMAR67309.2025.00082,Interaction & Input,Sensor Fusion,User Study,User Study / Empirical Findings; Algorithm / Method
86,2025,MIRAGE: Multimodal Intention Recognition and Admittance-Guided Enhancement in VR-Based Multi-Object Teleoperation,MIRAGE: VR 기반 다중 객체 원격 조작의 다중 모드 의도 인식 및 어드미턴스 기반 강화,"Effective human-robot interaction (HRI) in multi-object teleoperation tasks faces significant challenges due to perceptual ambiguities in virtual reality (VR) environments and the limitations of single-modality intention recognition. This paper proposes a shared control framework that combines a virtual admittance (VA) model with a Multimodal-CNN-based Human Intention Perception Network (MMIPN) to enhance teleoperation performance and user experience. The VA model employs artificial potential fields to guide operators toward target objects by adjusting admittance force and optimizing motion trajectories. MMIPN processes multimodal inputs-gaze movement, robot motions, and environmental context-to estimate human grasping intentions, helping overcome depth perception challenges in VR. Our user study evaluated four conditions across two factors, and the results showed that MMIPN significantly improved grasp success rates, while the VA model enhanced movement efficiency by reducing path lengths. Gaze data emerged as the most crucial input modality. These findings demonstrate the effectiveness of combining multimodal cues with implicit guidance in VR-based teleoperation, providing a robust solution for multi-object grasping tasks and enabling more natural interactions across various applications in the future.","다중 객체 원격 조작 작업에서 효과적인 인간-로봇 상호 작용(HRI)은 가상 현실(VR) 환경의 지각적 모호성과 단일 양식 의도 인식의 한계로 인해 심각한 과제에 직면해 있습니다. 본 논문에서는 원격 운용 성능과 사용자 경험을 향상시키기 위해 가상 어드미턴스(VA) 모델과 Multimodal-CNN 기반 인간 의도 인식 네트워크(MMIPN)를 결합한 공유 제어 프레임워크를 제안합니다. VA 모델은 인공 전위장을 사용하여 어드미턴스 힘을 조정하고 모션 궤적을 최적화함으로써 작업자를 대상 물체로 안내합니다. MMIPN은 시선 움직임, 로봇 동작, 환경적 맥락 등 다중 모드 입력을 처리하여 인간의 파악 의도를 추정하여 VR의 깊이 인식 문제를 극복하는 데 도움을 줍니다. 우리의 사용자 연구는 두 가지 요소에 걸쳐 네 가지 조건을 평가한 결과 MMIPN이 파악 성공률을 크게 향상시키는 반면 VA 모델은 경로 길이를 줄여 이동 효율성을 향상시키는 것으로 나타났습니다. 시선 데이터는 가장 중요한 입력 양식으로 나타났습니다. 이러한 연구 결과는 VR 기반 원격 조작에서 다중 모드 단서와 암시적 안내를 결합하여 다중 물체 파악 작업을 위한 강력한 솔루션을 제공하고 향후 다양한 응용 프로그램에서 보다 자연스러운 상호 작용을 가능하게 하는 효과를 보여줍니다.",https://doi.org/10.1109/ISMAR67309.2025.00060,Interaction & Input; Perception & Cognition,Deep Learning / Neural Networks,User Study,Algorithm / Method; User Study / Empirical Findings
87,2025,MMG: Manipulation-Aware Holistic Human Motion Generation from Sparse Tracking Signals,MMG: 희소 추적 신호에서 조작 인식 전체적 인간 동작 생성,"Generating realistic avatar motion via sparse tracking signals through VR devices is essential for enhancing the immersive user experience. Human-object manipulation behaviors not only affect hand motion but also significantly impact body motion. However, existing motion generation methods for human-object interactions overlook the coordinated coupling between body and hand motions during manipulations. Due to the diversity and complexity of holistic motion (body and hand motions simultaneously) in the latent motion space, generating physically plausible and temporally consistent holistic motion in real time, via the joint constraints imposed by sparse tracking signals and manipulation content, is a major challenge in the human motion generation task. We propose the manipulation-aware holistic human motion generation method (MMG) to help resolve this issue. In MMG, first, we construct a manipulation-aware holistic human motion generation framework that serially compresses the latent motion space distribution of the body and hand to generate realistic holistic human motion with object manipulation enabled. Second, to enhance the impact of object manipulation on holistic motion generation, MMG designs a novel object manipulation representation to extract effective manipulation features. Third, MMG is trained by an elaborate progressive manipulation-guided training algorithm to improve motion generation robustness and inference performance. Compared to state-of-the-art methods, MMG achieves up to a 39% improvement in the generated holistic motion quality with a 3.55 × speedup in generation performance. In manipulation-enabled scenes, MMG generates holistic motion in real time ($\geq 24 f p s$). Compared to the state-of-the-art methods, its perceived quality is significantly improved, and the task performance of holistic motion-required VR manipulation is high-significantly improved. This paper's code is at https://github.com/XRZ-BUAA/MMG.","VR 장치를 통해 희박한 추적 신호를 통해 사실적인 아바타 모션을 생성하는 것은 몰입형 사용자 경험을 향상시키는 데 필수적입니다. 인간 개체 조작 동작은 손 동작에 영향을 미칠 뿐만 아니라 신체 동작에도 큰 영향을 미칩니다. 그러나 인간-객체 상호 작용을 위한 기존 모션 생성 방법은 조작 중에 신체와 손 모션 간의 조정된 결합을 간과합니다. 잠재 모션 공간에서 전체적인 모션(신체와 손의 동시 모션)의 다양성과 복잡성으로 인해 희소 추적 신호 및 조작 콘텐츠에 의해 부과된 관절 제약을 통해 물리적으로 그럴듯하고 시간적으로 일관된 전체 모션을 실시간으로 생성하는 것은 인간 모션 생성 작업의 주요 과제입니다. 우리는 이 문제를 해결하는 데 도움이 되는 조작 인식 전체론적 인간 동작 생성 방법(MMG)을 제안합니다. MMG에서는 먼저 신체와 손의 잠재 동작 공간 분포를 연속적으로 압축하여 객체 조작이 가능한 현실적인 전체적 인간 동작을 생성하는 조작 인식 전체적 인간 동작 생성 프레임워크를 구축합니다. 둘째, 전체적인 모션 생성에 대한 객체 조작의 영향을 강화하기 위해 MMG는 효과적인 조작 특징을 추출하기 위한 새로운 객체 조작 표현을 설계합니다. 셋째, MMG는 모션 생성 견고성과 추론 성능을 향상시키기 위해 정교한 점진적 조작 유도 학습 알고리즘으로 학습됩니다. 최첨단 방법과 비교하여 MMG는 생성 성능의 3.55배 속도 향상으로 생성된 전체적인 모션 품질을 최대 39% 향상시킵니다. 조작이 가능한 장면에서 MMG는 실시간으로 전체적인 모션을 생성합니다($\geq 24 f p s$). 최첨단 방법에 비해 인지 품질이 크게 향상되었으며, 전체적인 모션이 필요한 VR 조작의 작업 성능이 크게 향상되었습니다. 이 문서의 코드는 https://github.com/XRZ-BUAA/MMG에 있습니다.",https://doi.org/10.1109/ISMAR67309.2025.00032,Interaction & Input,Sensor Fusion,Technical Evaluation,Algorithm / Method
88,2025,MRpilot: A Mixed-Reality System for Responsive Navigation of General Procedural Tasks,MRpilot: 일반 절차 작업의 반응형 탐색을 위한 혼합 현실 시스템,"People often need guidance to complete tasks with specific requirements or sophisticated steps, such as preparing a meal or assembling furniture. Traditional guidance often relies on unstructured paper instructions that require people to switch between reading instructions and performing actions, resulting in an unsmooth user experience. Recent Mixed Reality (MR) systems alleviate this problem by giving spatialized navigation but demand an authoring step and, therefore, cannot be easily adapted to general tasks. We propose MRPilot, an MR system empowered by Large Language Models (LLMs) and Computer Vision techniques, offering responsive navigation for general tasks without pre-authoring. MRPilot consists of three modules: a Navigation Builder Module using LLMs to generate structured instructions, an Object Anchor Module exploiting Computer Vision techniques to anchor physical objects with virtual proxies, and an Action Recommendation Module giving responsive navigation according to users’ interactions with physical objects. MRPilot bridges the gap between virtual instructions and physical interactions for general tasks, providing contextual and responsive navigation. We conducted a user study to compare MRPilot with a baseline MR system that also exploited LLMs. The results confirmed the effectiveness of MRPilot.","사람들은 식사 준비나 가구 조립과 같이 특정 요구 사항이나 복잡한 단계가 필요한 작업을 완료하기 위해 지침이 필요한 경우가 많습니다. 기존 지침은 사람들이 지침을 읽는 것과 작업을 수행하는 것 사이를 전환해야 하는 구조화되지 않은 종이 지침에 의존하는 경우가 많아 사용자 경험이 원활하지 않습니다. 최근 혼합 현실(MR) 시스템은 공간화된 탐색을 제공하여 이 문제를 완화하지만 작성 단계가 필요하므로 일반 작업에 쉽게 적용할 수 없습니다. 우리는 사전 작성 없이 일반 작업에 대한 반응형 탐색을 제공하는 LLM(대형 언어 모델) 및 컴퓨터 비전 기술을 기반으로 하는 MR 시스템인 MRPilot을 제안합니다. MRPilot은 LLM을 사용하여 구조화된 지침을 생성하는 Navigation Builder 모듈, 컴퓨터 비전 기술을 활용하여 가상 프록시로 물리적 객체를 고정하는 객체 앵커 모듈, 사용자와 물리적 객체의 상호 작용에 따라 반응형 탐색을 제공하는 Action Recommendation 모듈의 세 가지 모듈로 구성됩니다. MRPilot은 일반 작업에 대한 가상 지침과 물리적 상호 작용 간의 격차를 해소하여 상황에 맞는 반응형 탐색을 제공합니다. 우리는 MRPilot을 LLM도 활용하는 기본 MR 시스템과 비교하기 위해 사용자 연구를 수행했습니다. 결과를 통해 MRPilot의 효율성이 확인되었습니다.",https://doi.org/10.1109/ISMAR67309.2025.00031,Interaction & Input; Content Authoring,Deep Learning / Neural Networks,User Study,Algorithm / Method
89,2025,Measuring Rotational Inertia in HMDs: Calculation of Torque as an Unobtrusive Indicator of Expended Effort in Virtual Environments,HMD의 회전 관성 측정: 가상 환경에서 소비된 노력을 눈에 띄지 않는 지표로 토크 계산,"In this paper, we detail a novel method for calculating torque levels generated by users wearing a head-mounted display (HMD). This method considers not only the movement from a given user but also the weight characteristics of the specific configured headset. In this way, we can calculate a user's expended effort during a specific time frame to better understand the effort expended, as well as the risk of fatigue or injury, while using augmented and virtual reality HMDs. To illustrate the applicability of the method, we apply it in an initial user study analyzing torque and force during a basic target selection task and determined that (1) both force and torque decrease significantly over time as participants tire, and (2) the lighter HMD incurred lower levels of force and torque throughout the various trials. This research substantially furthers the practice of human factors and usability in augmented and virtual reality and fills a need within the community by adding to the extant literature an easy to use and well-documented method that can determine the effect of the total HMD system (HMD hardware, HMD interface, and task elements) on end users over both long- and short-duration tasks.","본 논문에서는 헤드 마운트 디스플레이(HMD)를 착용한 사용자가 생성하는 토크 수준을 계산하는 새로운 방법을 자세히 설명합니다. 이 방법은 특정 사용자의 움직임뿐만 아니라 구성된 특정 헤드셋의 무게 특성도 고려합니다. 이러한 방식으로 우리는 증강 현실 및 가상 현실 HMD를 사용하는 동안 소비된 노력은 물론 피로나 부상의 위험을 더 잘 이해하기 위해 특정 기간 동안 사용자가 소비한 노력을 계산할 수 있습니다. 이 방법의 적용 가능성을 설명하기 위해 우리는 기본 대상 선택 작업 중 토크와 힘을 분석하는 초기 사용자 연구에 이 방법을 적용했으며 (1) 참가자가 지치면 시간이 지남에 따라 힘과 토크가 모두 크게 감소하고 (2) 가벼운 HMD가 다양한 시험을 통해 더 낮은 수준의 힘과 토크를 발생시키는 것으로 확인했습니다. 이 연구는 증강 현실과 가상 현실에서 인적 요소와 유용성의 실행을 실질적으로 향상시키고 장기 및 단기 작업 모두에서 전체 HMD 시스템(HMD 하드웨어, HMD 인터페이스 및 작업 요소)이 최종 사용자에게 미치는 영향을 결정할 수 있는 사용하기 쉽고 잘 문서화된 방법을 기존 문헌에 추가함으로써 커뮤니티 내 요구를 충족시킵니다.",https://doi.org/10.1109/ISMAR67309.2025.00116,Display & Optics,Other,User Study,User Study / Empirical Findings; Hardware / Device
90,2025,"Merging Bodies, Dividing Conflict: Body-Swapping in Mixed Reality Increases Closeness Yet Weakens the Joint Simon Effect","신체 병합, 갈등 분할: 혼합 현실에서 신체 교환은 친밀감을 높이지만 공동 사이먼 효과를 약화시킵니다.","Mixed Reality (MR) presents novel opportunities to investigate how individuals perceive themselves and others during shared, augmented experiences within a common physical environment. Previous research has demonstrated that users can embody avatars in MR, temporarily extending their sense of self. However, there has been limited exploration of body-swapping, a condition in which two individuals simultaneously inhabit each other's avatars, and its potential effects on social interaction in immersive environments. To address this gap, we adapted the Joint Simon Task (JST), a wellestablished implicit paradigm, to examine how body-swapping influences the cognitive and perceptual boundaries between self and other. Our results indicate that body-swapping led participants to experience themselves and their partner as functioning like a single, unified system, as in two bodies operating as one agent. This suggests possible cognitive and perceptual changes that go beyond simple collaboration. Our findings have significant implications for the design of MR systems intended to support collaboration, empathy, social learning, and therapeutic interventions through shared embodiment.","혼합 현실(MR)은 공통의 물리적 환경 내에서 공유되고 증강된 경험을 통해 개인이 자신과 다른 사람을 어떻게 인식하는지 조사할 수 있는 새로운 기회를 제공합니다. 이전 연구에서는 사용자가 MR에서 아바타를 구현하여 일시적으로 자기 감각을 확장할 수 있음을 보여주었습니다. 그러나 두 개인이 동시에 서로의 아바타에 거주하는 조건인 신체 교환과 몰입형 환경에서 사회적 상호 작용에 대한 잠재적인 영향에 대한 탐구는 제한적이었습니다. 이러한 격차를 해결하기 위해 우리는 신체 교환이 자신과 타인 사이의 인지 및 지각 경계에 어떻게 영향을 미치는지 조사하기 위해 잘 확립된 암시적 패러다임인 JST(Joint Simon Task)를 적용했습니다. 우리의 결과는 신체 교환을 통해 참가자들이 하나의 에이전트로 작동하는 두 신체에서처럼 단일 통합 시스템처럼 기능하는 것처럼 자신과 파트너를 경험하도록 유도했음을 나타냅니다. 이는 단순한 협업을 넘어서는 인지적, 지각적 변화가 가능함을 시사합니다. 우리의 연구 결과는 공유 구현을 통해 협업, 공감, 사회적 학습 및 치료 중재를 지원하기 위한 MR 시스템 설계에 중요한 의미를 갖습니다.",https://doi.org/10.1109/ISMAR67309.2025.00156,Perception & Cognition; Collaboration & Social,Sensor Fusion,User Study,System / Framework
91,2025,MirrorPose: Enabling Full-Body Gestures Interaction for Head-Mounted Devices with a Full-Length Mirror,MirrorPose: 전신 거울을 사용하여 머리 장착형 장치에 대한 전신 제스처 상호 작용 활성화,"Human-computer interaction based on full-body gestures has been successfully adopted in various applications, such as motionsensing games. Typically, full-body gestures are captured using vision-based pose estimation or multiple inertial measurement units (IMUs) attached to the limbs. Gesture-based interactions in virtual and augmented reality environments allow for seamless and intuitive engagement across virtual and real domains. However, due to the design of head-mounted devices, only partial body tracking—such as hand tracking—is typically available for interactions. Capturing full-body pose using head-mounted sensors is inherently challenging due to device placement constraints. Furthermore, the limited computational resources of AR devices (e.g., constrained processing power and memory bandwidth) present significant challenges for the real-time deployment of sophisticated 3D human pose estimation architectures. To address these challenges, we propose MirrorPose, a lightweight framework that integrates a 3D pose estimation network (PoseARNet), optimized for the resource constraints of AR headsets and the dynamic viewpoint changes inherent in mirror-mediated spatial perception. This design enables practical, full-body gesture interaction on AR devices. To demonstrate its practicality, we developed a 3D virtual teaching application on Microsoft HoloLens 2, to enhance students' understanding of human poses. Extensive experiments and evaluations confirm that our system provides users with accurate and timely feedback. The codes and dataset are available at https://github.com/zhchlong/mirror_pose.",전신 제스처를 기반으로 한 인간-컴퓨터 상호 작용은 모션 감지 게임과 같은 다양한 애플리케이션에 성공적으로 채택되었습니다. 일반적으로 전신 제스처는 비전 기반 자세 추정이나 팔다리에 부착된 다중 관성 측정 장치(IMU)를 사용하여 캡처됩니다. 가상 및 증강 현실 환경의 제스처 기반 상호 작용을 통해 가상 및 실제 영역 전반에 걸쳐 원활하고 직관적인 참여가 가능합니다. 그러나 머리 장착형 장치의 설계로 인해 일반적으로 상호 작용에는 손 추적과 같은 부분적인 신체 추적만 사용할 수 있습니다. 머리 장착 센서를 사용하여 전신 포즈를 캡처하는 것은 장치 배치 제약으로 인해 본질적으로 어렵습니다. 또한 AR 장치의 제한된 계산 리소스(예: 제한된 처리 능력 및 메모리 대역폭)는 정교한 3D 인간 자세 추정 아키텍처의 실시간 배포에 중요한 과제를 제시합니다. 이러한 문제를 해결하기 위해 우리는 AR 헤드셋의 리소스 제약과 거울 매개 공간 인식에 내재된 동적 시점 변경에 최적화된 3D 포즈 추정 네트워크(PoseARNet)를 통합하는 경량 프레임워크인 MirrorPose를 제안합니다. 이 디자인은 AR 장치에서 실용적인 전신 제스처 상호 작용을 가능하게 합니다. 실용성을 입증하기 위해 Microsoft HoloLens 2에서 3D 가상 교육 애플리케이션을 개발하여 인간 자세에 대한 학생들의 이해를 높였습니다. 광범위한 실험과 평가를 통해 우리 시스템이 사용자에게 정확하고 시기적절한 피드백을 제공한다는 사실이 확인되었습니다. 코드와 데이터 세트는 https://github.com/zhchlong/mirror_pose에서 확인할 수 있습니다.,https://doi.org/10.1109/ISMAR67309.2025.00035,Interaction & Input; Tracking & Localization,Sensor Fusion,Technical Evaluation,Hardware / Device; System / Framework
92,2025,Move Like an Ammonite: Personalizing Force Feedback for Avatar Embodiment in Virtual Reality,암모나이트처럼 이동: 가상 현실에서 아바타 구현을 위한 포스 피드백 개인화,"In virtual environments, users can embody diverse avatars beyond physical constraints. The avatar-induced cognitive transformation (i.e. Proteus effect) can contribute to engineer self-perception, promote empathy and augment human capabilities. However, when the physicality of the avatar differs greatly from that of the user, the discrepancy between bodily sensation and prediction will inhibit embodiment. Therefore, it's challenging to evoke a high sense of embodiment in avatars that have different body structures or textures based on visual feedback alone. We propose a method to modulate movement impedance (inertia, viscosity, stiffness) of our own body in response to embodied avatars using a wearable haptic device, enhancing the sense of body ownership and the plausibility of interaction using that new avatar. Using Bayesian optimization, we identified individually optimized haptic parameters that maximize subjective plausibility for each user. Our results revealed that Bayesian optimization significantly enhanced users' perceived plausibility of the avatar via haptic feedback. The optimal parameters exhibited substantial inter-individual variation, highlighting the importance of characterizing user-specific motor sensations. On the other hand, regression analysis found no clear correlation between avatar impression ratings and optimal haptic parameters, suggesting subjective plausibility formation involves personal interpretations beyond measurable impressions.","가상 환경에서 사용자는 물리적 제약을 넘어 다양한 아바타를 구현할 수 있습니다. 아바타에 의해 유도된 인지 변환(즉, 프로테우스 효과)은 자기 인식을 개선하고 공감을 촉진하며 인간 능력을 강화하는 데 기여할 수 있습니다. 그러나 아바타의 물리적 특성이 사용자의 물리적 특성과 크게 다를 경우 신체 감각과 예측 간의 불일치로 인해 구현이 불가능해집니다. 따라서 시각적 피드백만으로는 신체 구조나 질감이 다른 아바타에서 높은 체화감을 불러일으키는 것이 어렵습니다. 우리는 웨어러블 햅틱 장치를 사용하여 구현된 아바타에 반응하여 신체의 움직임 임피던스(관성, 점성, 강성)를 조절하는 방법을 제안하고, 새로운 아바타를 사용하여 신체 주인의식과 상호 작용의 타당성을 향상시킵니다. 베이지안 최적화를 사용하여 각 사용자의 주관적 타당성을 최대화하는 개별적으로 최적화된 햅틱 매개변수를 식별했습니다. 우리의 결과는 베이지안 최적화가 햅틱 피드백을 통해 사용자가 인식하는 아바타의 타당성을 크게 향상시키는 것으로 나타났습니다. 최적의 매개변수는 개인간 상당한 차이를 나타내어 사용자별 운동 감각을 특성화하는 것의 중요성을 강조합니다. 반면, 회귀 분석에서는 아바타 인상 등급과 최적의 촉각 매개변수 사이에 명확한 상관관계가 발견되지 않았으며, 이는 주관적 타당성 형성에는 측정 가능한 인상 이상의 개인적인 해석이 포함되어 있음을 시사합니다.",https://doi.org/10.1109/ISMAR67309.2025.00098,Perception & Cognition; Rendering & Visualization,Haptic / Tactile Feedback,Technical Evaluation,Algorithm / Method; Hardware / Device
93,2025,Multimodal Classification of Student Distractions in Educational VR Environments: A Foundation for Real-Time Adaptation,교육용 VR 환경에서 학생의 방해 요소에 대한 다중 모드 분류: 실시간 적응을 위한 기초,"Virtual reality offers immersive educational environments but presents challenges in accurately detecting user distraction. Prior studies often define distraction based solely on the presence of external interventions, overlooking spontaneous cognitive lapses and assuming that all interventions are effective distractors. In this study, we investigate the feasibility of distraction detection in VR lectures using machine learning models trained on physiological sensor data, including electroencephalography, eye tracking, and heart rate. We evaluate a binary classification model intended for future real-time deployment, incorporating interventioninduced and self-reported cognitive distractions. Our best performing binary model achieved an F1 score of 87.3%. We also explore a multiclass classification approach across five distraction types, which yielded a lower F1 score of 32.8%. Our preprocessing pipeline and model architecture are optimized with real-time compatibility in mind, supporting future applications in adaptive educational VR systems. This work contributes to developing distractionaware VR learning environments by demonstrating the potential for scalable, real-time distraction classification using multimodal physiological data.","가상 현실은 몰입형 교육 환경을 제공하지만 사용자의 방해 요소를 정확하게 감지하는 데 어려움을 겪습니다. 이전 연구에서는 자발적인 인지 장애를 간과하고 모든 개입이 효과적인 주의 분산 요인이라고 가정하면서 오로지 외부 개입의 존재에만 기초하여 산만함을 정의하는 경우가 많습니다. 본 연구에서는 뇌파검사, 안구 추적, 심박수 등 생리학적 센서 데이터에 대해 훈련된 기계 학습 모델을 사용하여 VR 강의에서 방해 요소 감지의 타당성을 조사합니다. 우리는 개입 유발 및 자체 보고된 인지 방해를 통합하여 향후 실시간 배포를 위한 이진 분류 모델을 평가합니다. 우리의 최고 성능 이진 모델은 87.3%의 F1 점수를 달성했습니다. 또한 5가지 방해 유형에 대한 다중 클래스 분류 접근 방식을 탐색하여 F1 점수가 32.8%로 낮았습니다. 우리의 전처리 파이프라인과 모델 아키텍처는 실시간 호환성을 염두에 두고 최적화되어 적응형 교육용 VR 시스템의 미래 애플리케이션을 지원합니다. 이 작업은 다중 모드 생리학적 데이터를 사용하여 확장 가능한 실시간 주의 산만 분류의 가능성을 보여줌으로써 주의 산만 인식 VR 학습 환경을 개발하는 데 기여합니다.",https://doi.org/10.1109/ISMAR67309.2025.00104,Education & Training; Perception & Cognition,Eye / Gaze Tracking,Questionnaire / Survey,System / Framework; Algorithm / Method
94,2025,MyGO: Virtual Reality Locomotion Prediction Using Multitask Learning,MyGO: 멀티태스크 학습을 사용한 가상 현실 이동 예측,"Locomotion is a fundamental interaction in Virtual Reality (VR). Current locomotion methods, such as redirected walking, walking-in-place, and teleportation, make use of limited physical space and interaction mapping. However, there remains significant potential for improvement, particularly in reducing equipment burden and enhancing immersion. To locate these limitations, we rethink the procedure of VR walking interaction through the Human Information Processing paradigm. Finding that the peripherals' requirements and potential conflict in artificially designed interaction mappings are the bottlenecks in bridging intention and action, we developed MyGO, an AI-assisted locomotion prediction method. MyGO predicts users' future trajectories from their subtle movements, collected only by a VR headset, using a multitask learning (MTL) model. The proposed model demonstrates competitive results in both dataset validation and real-time studies. The code is available at https://github.com/ZichengLiu-seu/basic-MyGO.","이동은 가상현실(VR)의 기본적인 상호작용입니다. 방향 변경 걷기, 제자리 걷기, 순간 이동과 같은 현재의 이동 방법은 제한된 물리적 공간과 상호 작용 매핑을 활용합니다. 그러나 특히 장비 부담을 줄이고 몰입도를 높이는 측면에서 개선의 여지가 상당히 남아 있습니다. 이러한 한계를 찾기 위해 우리는 인간 정보 처리 패러다임을 통해 VR 걷기 상호 작용의 절차를 다시 생각합니다. 인위적으로 설계된 상호 작용 매핑에서 주변 장치의 요구 사항과 잠재적인 충돌이 의도와 행동을 연결하는 데 병목 현상이 발생한다는 사실을 발견하고 AI 지원 운동 예측 방법인 MyGO를 개발했습니다. MyGO는 멀티태스킹 학습(MTL) 모델을 사용해 VR 헤드셋을 통해서만 수집한 미묘한 움직임을 통해 사용자의 미래 궤적을 예측합니다. 제안된 모델은 데이터 세트 검증과 실시간 연구 모두에서 경쟁력 있는 결과를 보여줍니다. 코드는 https://github.com/ZichengLiu-seu/basic-MyGO에서 확인할 수 있습니다.",https://doi.org/10.1109/ISMAR67309.2025.00041,Interaction & Input,Redirected Walking / Locomotion,Technical Evaluation,Algorithm / Method
95,2025,Navigation Pixie: Implementation and Empirical Study Toward on-Demand Navigation Agents in Commercial Metaverse,Navigation Pixie: 상업용 메타버스의 주문형 네비게이션 에이전트에 대한 구현 및 실증적 연구,"While commercial metaverse platforms offer diverse usergenerated content, they lack effective navigation assistance that can dynamically adapt to users' interests and intentions. Although previous research has investigated on-demand agents in controlled environments, implementation in commercial settings with diverse world configurations and platform constraints remains challenging. We present Navigation Pixie, an on-demand navigation agent employing a loosely coupled architecture that integrates structured spatial metadata with LLM-based natural language processing while minimizing platform dependencies, which enables experiments on the extensive user base of commercial metaverse platforms. Our cross-platform experiments on commercial metaverse platform Cluster with 99 PC client and 94 VR-HMD participants demonstrated that Navigation Pixie significantly increased dwell time and free exploration compared to fixed-route and noagent conditions across both platforms. Subjective evaluations revealed consistent on-demand preferences in PC environments versus context-dependent social perception advantages in VRHMD. This research contributes to advancing VR interaction design through conversational spatial navigation agents, establishes cross-platform evaluation methodologies revealing environment-dependent effectiveness, and demonstrates empirical experimentation frameworks for commercial metaverse platforms.","상업용 메타버스 플랫폼은 다양한 사용자 생성 콘텐츠를 제공하지만 사용자의 관심과 의도에 동적으로 적응할 수 있는 효과적인 탐색 지원이 부족합니다. 이전 연구에서는 통제된 환경에서 주문형 에이전트를 조사했지만 다양한 세계 구성과 플랫폼 제약이 있는 상업적 환경에서의 구현은 여전히 ​​어려운 과제로 남아 있습니다. 구조화된 공간 메타데이터를 LLM 기반 자연어 처리와 통합하는 동시에 플랫폼 종속성을 최소화하는 느슨하게 결합된 아키텍처를 사용하는 주문형 탐색 에이전트인 Navigation Pixie를 소개합니다. 이를 통해 상용 메타버스 플랫폼의 광범위한 사용자 기반에 대한 실험을 가능하게 합니다. 99명의 PC 클라이언트와 94명의 VR-HMD 참가자가 포함된 상업용 메타버스 플랫폼 클러스터에 대한 크로스 플랫폼 실험에서는 Navigation Pixie가 두 플랫폼 모두에서 고정 경로 및 에이전트 없음 조건에 비해 체류 시간과 자유 탐색이 크게 증가한 것으로 나타났습니다. 주관적인 평가에서는 VRHMD의 상황에 따른 사회적 인식 이점과 비교하여 PC 환경의 일관된 주문형 선호도가 나타났습니다. 본 연구는 대화형 공간 내비게이션 에이전트를 통해 VR 상호 작용 디자인을 발전시키는 데 기여하고, 환경 의존적 효율성을 나타내는 크로스 플랫폼 평가 방법론을 확립하며, 상용 메타버스 플랫폼에 대한 실증적 실험 프레임워크를 보여줍니다.",https://doi.org/10.1109/ISMAR67309.2025.00120,Interaction & Input; Perception & Cognition,Deep Learning / Neural Networks,User Study,System / Framework
96,2025,OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications,OpenFLAME: 대규모 증강 현실 애플리케이션을 지원하는 통합 시각적 포지셔닝 시스템,"World-scale augmented reality (AR) applications need a ubiquitous 6DoF localization backend to anchor content to the real world consistently across devices. Large organizations such as Google and Niantic are 3D scanning outdoor public spaces in order to build their own Visual Positioning Systems (VPS). These centralized VPS solutions fail to meet the needs of many future AR applications-they do not cover private indoor spaces because of privacy concerns, regulations, and the labor bottleneck of updating and maintaining 3D scans. In this paper, we present OpenFLAME, a federated VPS backend that allows independent organizations to 3D scan and maintain a separate VPS service for their own spaces. This enables access control of indoor 3D scans, distributed maintenance of the VPS backend, and encourages larger coverage. Sharding of VPS services introduces several unique challenges-coherency of localization results across spaces, quality control of VPS services, selection of the right VPS service for a location, and many others. We introduce the concept of federated image-based localization and provide reference solutions for managing and merging data across maps without sharing private data.","세계 규모의 증강 현실(AR) 애플리케이션에는 여러 장치에서 일관되게 콘텐츠를 현실 세계에 고정하기 위한 유비쿼터스 6DoF 현지화 백엔드가 필요합니다. Google 및 Niantic과 같은 대규모 조직에서는 자체 VPS(시각적 위치 확인 시스템)를 구축하기 위해 야외 공공 장소를 3D 스캐닝하고 있습니다. 이러한 중앙 집중식 VPS 솔루션은 미래의 많은 AR 애플리케이션의 요구 사항을 충족하지 못합니다. 개인 정보 보호 문제, 규정 및 3D 스캔 업데이트 및 유지 관리에 대한 노동 병목 현상으로 인해 개인 실내 공간을 다루지 않습니다. 본 백서에서는 독립 조직이 자체 공간에 대해 별도의 VPS 서비스를 3D 스캔하고 유지 관리할 수 있도록 하는 통합 VPS 백엔드인 OpenFLAME을 소개합니다. 이를 통해 실내 3D 스캔의 액세스 제어, VPS 백엔드의 분산 유지 관리 및 더 넓은 적용 범위를 장려합니다. VPS 서비스 샤딩에는 공간 전반에 걸친 현지화 결과의 일관성, VPS 서비스의 품질 관리, 위치에 적합한 VPS 서비스 선택 등 여러 가지 고유한 문제가 발생합니다. 연합된 이미지 기반 위치 파악 개념을 도입하고 개인 데이터를 공유하지 않고 지도 전반에 걸쳐 데이터를 관리하고 병합할 수 있는 참조 솔루션을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00080,Tracking & Localization; Interaction & Input,Deep Learning / Neural Networks,Other,System / Framework
97,2025,P-MARS: Design of a VR-Based Ergotherapy System for Children with Autism and its Longitudinal Tracking Evaluation,P-MARS: 자폐 아동을 위한 VR 기반 인체공학 시스템 설계 및 종방향 추적 평가,"Autism Spectrum Disorder (ASD) impacts social interaction, communication, and cognitive functioning. In recent years, virtual reality (VR) technology, with its advantages such as strong immersive capabilities, has been increasingly applied to ASD ergotherapy. However, existing systems often overlook human factors considerations, including users' perceptual preferences and attentional load during system interactions. This study proposes a Personalized Multisensory Adaptive Roaming System (P-MARS), which integrates large-scale models to analyze users' sensory preferences and generates personalized VR scenarios combined with visual, auditory, and olfactory stimuli, aiming to enhance ergotherapy outcomes for children with ASD. A longitudinal study employing functional near-infrared spectroscopy (fNIRS) was conducted to assess participants' neural states. The results demonstrate a significant reduction in central neural activation levels among children during the later stages of training. These findings suggest that periodic VR-based training can substantially alleviate cognitive load and anxiety in children with ASD, thereby accelerating ergotherapy progress. The study further validates the critical importance and efficacy of incorporating human factors into ergotherapy system design.","자폐 스펙트럼 장애(ASD)는 사회적 상호작용, 의사소통 및 인지 기능에 영향을 미칩니다. 최근에는 강력한 몰입 능력과 같은 장점을 지닌 가상 현실(VR) 기술이 ASD 에르고테라피에 점점 더 많이 적용되고 있습니다. 그러나 기존 시스템은 시스템 상호 작용 중 사용자의 지각 선호도 및 주의력 부하를 비롯한 인적 요소 고려 사항을 간과하는 경우가 많습니다. 본 연구에서는 대규모 모델을 통합하여 사용자의 감각 선호도를 분석하고 시각, 청각, 후각 자극과 결합된 개인화된 VR 시나리오를 생성하는 개인화된 다감각 적응형 로밍 시스템(P-MARS)을 제안하여 ASD 아동의 작용요법 결과를 향상시키는 것을 목표로 합니다. 참가자의 신경 상태를 평가하기 위해 기능적 근적외선 분광법(fNIRS)을 사용하는 종단적 연구가 수행되었습니다. 결과는 훈련 후반 단계에서 어린이의 중추 신경 활성화 수준이 크게 감소했음을 보여줍니다. 이러한 결과는 주기적인 VR 기반 훈련이 ASD 아동의 인지 부하와 불안을 실질적으로 완화하여 에르고테라피 진행을 가속화할 수 있음을 시사합니다. 이 연구는 인간적 요소를 에르고테라피 시스템 설계에 통합하는 것의 중요성과 효능을 더욱 입증합니다.",https://doi.org/10.1109/ISMAR67309.2025.00157,Perception & Cognition; Collaboration & Social,Sensor Fusion,User Study,System / Framework
98,2025,PanoFloor: Reconstruction and Immersive Exploration of Large Multi-Room Scenes from a Minimal Set of Registered Panoramic Images Using Denoised Density Maps,PanoFloor: 노이즈 제거된 밀도 맵을 사용하여 등록된 최소 세트의 파노라마 이미지에서 대규모 멀티룸 장면의 재구성 및 몰입형 탐색,"We introduce a deep learning approach to automatically generate 3D floor plans and immersive multi-room virtual visit experiences from a small set of co-registered 360° panoramas - down to just one per room. We integrate novel neural networks that leverage panoramic image broad context and large annotated room datasets to build a geometric and visual graph. Nodes represent stereo-viewable multiple-center-of-projection (MCOP) 360° images at the capture locations, while arcs connect them with paths through doors, avoiding clutter and minimizing disocclusions to maximize visual quality. The process starts with depth prediction and floor-plan projection to create a comprehensive but noisy global density map, which is refined via a latent diffusion model. A segmentation network then extracts room layouts, openings, and clutter. This structured representation is lifted to a visual one by creating a 360° stereo-explorable MCOP representation at each node, produced using a view-synthesis network from the original image and its predicted depth map. Arc paths are then computed using an optimization process that considers structural constraints, including openings and obstacles, while minimizing visual discontinuities, occlusions, and disocclusions. Finally, 360° video transitions are synthesized using a specialized view-synthesis network to obtain a fully precomputed WebXR-ready explorable representation that can be efficiently experienced on Head-Mounted-Displays with limited graphics capabilities. The extracted floor plan not only aids in documenting the captured building but can also enhance immersive experiences by serving as a live map of the building. Our experiments show that the method achieves state-of-the-art reconstruction from sparse inputs and supports compelling immersive visits.","공동 등록된 360° 파노라마의 작은 세트에서 방당 하나만으로 3D 평면도와 몰입형 멀티룸 가상 방문 경험을 자동으로 생성하는 딥 러닝 접근 방식을 소개합니다. 우리는 파노라마 이미지의 광범위한 컨텍스트와 주석이 달린 대규모 공간 데이터세트를 활용하여 기하학적이고 시각적인 그래프를 구축하는 새로운 신경망을 통합합니다. 노드는 캡처 위치에서 스테레오로 볼 수 있는 MCOP(Multiple-Center-of Projection) 360° 이미지를 나타내는 반면, 호는 이를 문을 통과하는 경로와 연결하여 혼잡함을 피하고 노출을 최소화하여 시각적 품질을 최대화합니다. 이 프로세스는 깊이 예측과 평면도 투영으로 시작하여 포괄적이지만 잡음이 많은 전역 밀도 맵을 생성하며, 이는 잠재 확산 모델을 통해 개선됩니다. 그런 다음 분할 네트워크는 방 레이아웃, 개구부 및 혼란을 추출합니다. 이 구조화된 표현은 각 노드에서 원본 이미지와 예측된 깊이 맵의 뷰 합성 네트워크를 사용하여 생성된 360° 스테레오 탐색 가능한 MCOP 표현을 생성하여 시각적 표현으로 향상됩니다. 그런 다음 개구부와 장애물을 포함한 구조적 제약 조건을 고려하는 동시에 시각적 불연속성, 폐색, 폐색을 최소화하는 최적화 프로세스를 사용하여 호 경로를 계산합니다. 마지막으로, 360° 비디오 전환은 제한된 그래픽 기능을 갖춘 헤드 마운트 디스플레이에서 효율적으로 경험할 수 있는 완전히 사전 계산된 WebXR 지원 탐색 가능한 표현을 얻기 위해 특수 뷰 합성 네트워크를 사용하여 합성됩니다. 추출된 평면도는 캡처된 건물을 문서화하는 데 도움이 될 뿐만 아니라 건물의 실시간 지도 역할을 하여 몰입감 있는 경험을 향상시킬 수도 있습니다. 우리의 실험은 이 방법이 희박한 입력으로부터 최첨단 재구성을 달성하고 강력한 몰입형 방문을 지원한다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR67309.2025.00052,Interaction & Input; Display & Optics,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method
99,2025,Parameter-Free Neural Lens Blur Rendering for High-Fidelity Composites,충실도가 높은 합성물을 위한 매개변수 없는 신경 렌즈 블러 렌더링,"Consistent and natural camera lens blur is important for seamlessly blending 3D virtual objects into photographed real-scenes. Since lens blur typically varies with scene depth, the placement of virtual objects and their corresponding blur levels significantly affect the visual fidelity of mixed reality compositions. Existing pipelines often rely on camera parameters (e.g., focal length, focus distance, aperture size) and scene depth to compute the circle of confusion (CoC) for realistic lens blur rendering. However, such information is often unavailable to ordinary users, limiting the accessibility and generalizability of these methods. In this work, we propose a novel compositing approach that directly estimates the CoC map from RGB images, bypassing the need for scene depth or camera metadata. The CoC values for virtual objects are inferred through a linear relationship between its signed CoC map and depth, and realistic lens blur is rendered using a neural reblurring network. Our method provides flexible and practical solution for real-world applications. Experimental results demonstrate that our method achieves high-fidelity compositing with realistic defocus effects, outperforming state-of-the-art techniques in both qualitative and quantitative evaluations.","일관되고 자연스러운 카메라 렌즈 블러는 3D 가상 객체를 사진에 담긴 실제 장면에 원활하게 혼합하는 데 중요합니다. 렌즈 흐림은 일반적으로 장면 깊이에 따라 달라지므로 가상 개체의 배치와 해당 흐림 수준은 혼합 현실 구성의 시각적 충실도에 큰 영향을 미칩니다. 기존 파이프라인은 사실적인 렌즈 블러 렌더링을 위해 CoC(혼란원)를 계산하기 위해 카메라 매개변수(예: 초점 거리, 초점 거리, 조리개 크기)와 장면 깊이에 의존하는 경우가 많습니다. 그러나 이러한 정보는 일반 사용자가 사용할 수 없는 경우가 많아 이러한 방법의 접근성과 일반화 가능성이 제한됩니다. 이 연구에서는 장면 깊이나 카메라 메타데이터의 필요성을 우회하여 RGB 이미지에서 CoC 맵을 직접 추정하는 새로운 합성 접근 방식을 제안합니다. 가상 객체의 CoC 값은 서명된 CoC 맵과 깊이 간의 선형 관계를 통해 추론되며, 신경 재블러링 네트워크를 사용하여 사실적인 렌즈 블러가 렌더링됩니다. 우리의 방법은 실제 응용 분야에 유연하고 실용적인 솔루션을 제공합니다. 실험 결과는 우리의 방법이 사실적인 디포커스 효과로 충실도가 높은 합성을 달성하고 정성적 및 정량적 평가 모두에서 최첨단 기술을 능가한다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR67309.2025.00145,Display & Optics,Other,Quantitative Experiment,Algorithm / Method
100,2025,Passive Haptics Role in VR-Based Ergonomic Workplace Assessment,VR 기반 인체공학적 작업장 평가에서 수동적 햅틱 역할,"Ergonomic Workplace Assessment aims to assess potential risks and measure the ergonomic characteristics of various workplaces. Virtual reality might offer great benefits as it enables cost-effective workplace evaluations in early planning phases. This study aims to investigate the validity of motion-capture-based ergonomic evaluations in virtual environments (VE) compared to real environments (RE), focusing on the role of passive haptics. In a 3\4 design, 22 subjects performed four work tasks in three environmental conditions: RE, VE, and VE with passive haptic feedback (PH). Whole-body motion data were recorded and analyzed using repeated measures ANOVA. Eighteen joint angle percentiles were analyzed across all tasks and environments. Significant differences in trunk and shoulder movements were found between RE and VE, especially during physically demanding tasks such as repetitive lifting. Passive haptics reduced differences in nine task-joint constellations but introduced new ones in seventeen others, particularly shoulder joints. Overall, the results highlight the context-specific effects of passive haptics: while it can improve motion realism in VE, it can also induce task-specific artifacts.","인체공학적 작업장 평가는 잠재적 위험을 평가하고 다양한 작업장의 인체공학적 특성을 측정하는 것을 목표로 합니다. 가상 현실은 초기 계획 단계에서 비용 효율적인 작업장 평가를 가능하게 하므로 큰 이점을 제공할 수 있습니다. 본 연구는 수동적 햅틱의 역할에 초점을 맞춰 실제 환경(RE)과 비교하여 가상 환경(VE)에서 모션 캡처 기반 인체공학적 평가의 타당성을 조사하는 것을 목표로 합니다. 3\4 디자인에서 22명의 피험자는 수동적 햅틱 피드백(PH)을 사용하여 RE, VE 및 VE의 세 가지 환경 조건에서 4가지 작업을 수행했습니다. 전신 동작 데이터를 기록하고 반복 측정 ANOVA를 사용하여 분석했습니다. 모든 작업과 환경에 걸쳐 18개의 관절 각도 백분위수가 분석되었습니다. RE와 VE 사이에서 몸통과 어깨 움직임의 상당한 차이가 발견되었으며, 특히 반복적인 리프팅과 같은 육체적으로 힘든 작업 중에 발견되었습니다. 수동적 햅틱은 9개의 작업-관절 별자리의 차이를 줄였지만, 특히 어깨 관절 등 17개의 다른 별자리에서는 새로운 차이가 발생했습니다. 전반적으로 결과는 수동적 햅틱의 상황별 효과를 강조합니다. VE에서 모션 현실성을 향상할 수 있지만 작업별 아티팩트를 유도할 수도 있습니다.",https://doi.org/10.1109/ISMAR67309.2025.00050,Interaction & Input,Haptic / Tactile Feedback,User Study,User Study / Empirical Findings
101,2025,PersoNo: Personalised Notification Urgency Classifier in Mixed Reality,PersoNo: 혼합 현실의 개인화된 알림 긴급 분류자,"Mixed Reality (MR) is increasingly integrated into daily life, providing enhanced capabilities across various domains. However, users face growing notification streams that disrupt their immersive experience. We present PersoNo, a personalised notification urgency classifier for MR that intelligently classifies notifications based on individual user preferences. Through a user study ($\mathrm{N}=18$), we created the first MR notification dataset containing both selflabelled and interaction-based data across activities with varying cognitive demands. Our thematic analysis revealed that, unlike in mobiles, the activity context is equally important as the content and the sender in determining notification urgency in MR. Leveraging these insights, we developed PersoNo using large language models that analyse users' replying behaviour patterns. Our multi-agent approach achieved 81.5% accuracy and significantly reduced false negative rates (0.381) compared to baseline models. PersoNo has the potential not only to reduce unnecessary interruptions but also to offer users understanding and control of the system, adhering to Human-Centered Artificial Intelligence design principles.",혼합 현실(MR)은 점점 더 일상 생활에 통합되어 다양한 영역에 걸쳐 향상된 기능을 제공합니다. 그러나 사용자는 몰입도 높은 경험을 방해하는 알림 스트림이 점점 늘어나는 상황에 직면해 있습니다. 개별 사용자 선호도에 따라 알림을 지능적으로 분류하는 MR용 개인화 알림 긴급성 분류기 PersoNo를 소개합니다. 사용자 연구($\mathrm{N}=18$)를 통해 우리는 다양한 인지적 요구가 있는 활동 전반에 걸쳐 자체 레이블이 지정된 데이터와 상호 작용 기반 데이터를 모두 포함하는 최초의 MR 알림 데이터 세트를 만들었습니다. 우리의 주제별 분석에 따르면 모바일과 달리 활동 컨텍스트는 MR에서 알림의 긴급성을 결정하는 데 있어 콘텐츠 및 보낸 사람만큼 중요합니다. 이러한 통찰력을 활용하여 우리는 사용자의 응답 행동 패턴을 분석하는 대규모 언어 모델을 사용하여 PersoNo를 개발했습니다. 우리의 다중 에이전트 접근 방식은 기본 모델에 비해 81.5%의 정확도를 달성하고 위음성 비율(0.381)을 크게 줄였습니다. PersoNo는 불필요한 중단을 줄일 뿐만 아니라 인간 중심 인공 지능 설계 원칙을 준수하여 사용자에게 시스템에 대한 이해와 제어를 제공할 수 있는 잠재력을 가지고 있습니다.,https://doi.org/10.1109/ISMAR67309.2025.00112,Interaction & Input,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method; Design Guidelines
102,2025,PresenceLens: Interpreting Dynamic Presence in Virtual Reality,PresenceLens: 가상 현실의 동적 존재 해석,"Presence, the felt experience of “being there” in virtual environments, is central to immersive VR, yet its dynamic structure is underexplored. Most prior work treats presence as static or analyzes isolated modalities, limiting both theory and application. We introduce PresenceLens, a computational framework that models presence as a temporally evolving, multimodal phenomenon. Using synchronized visual, auditory, gaze, and interaction data from 120 participants across 20 VR applications, PresenceLens identifies eight recurring patterns linked to high presence. These patterns evolve over time in distinct trajectories and form the basis of Pattern Orchestration Theory, which conceptualizes presence as the temporal coordination of perceptual, cognitive, and interactive processes. Our model achieves high predictive accuracy ($R^{2}=0.64$) and provides interpretable mappings between real-time behavior and subjective presence. This work links theory and temporal modeling, enabling VR systems to dynamically adapt to evolving user states.","가상 환경에서 '거기 있다는 느낌'을 느끼는 존재감(Presence)은 몰입형 VR의 핵심이지만, 그 역동적인 구조는 충분히 탐구되지 않았습니다. 대부분의 이전 연구는 존재감을 정적인 것으로 취급하거나 고립된 양식을 분석하여 이론과 적용을 모두 제한했습니다. 존재감을 시간적으로 진화하는 다중 모드 현상으로 모델링하는 계산 프레임워크인 PresenceLens를 소개합니다. PresenceLens는 20개의 VR 애플리케이션에 걸쳐 120명의 참가자로부터 동기화된 시각, 청각, 시선 및 상호 작용 데이터를 사용하여 높은 존재감과 관련된 8가지 반복 패턴을 식별합니다. 이러한 패턴은 시간이 지남에 따라 뚜렷한 궤적으로 진화하며 존재감을 지각, 인지 및 상호 작용 프로세스의 시간적 조정으로 개념화하는 패턴 조정 이론의 기초를 형성합니다. 우리 모델은 높은 예측 정확도($R^{2}=0.64$)를 달성하고 실시간 행동과 주관적 존재 사이에 해석 가능한 매핑을 제공합니다. 이 작업은 이론과 시간적 모델링을 연결하여 VR 시스템이 진화하는 사용자 상태에 동적으로 적응할 수 있도록 합니다.",https://doi.org/10.1109/ISMAR67309.2025.00075,Interaction & Input; Perception & Cognition,Other,User Study,System / Framework
103,2025,PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy,PrivateXR: 설명 가능한 AI 기반 차등 개인 정보 보호를 통해 확장된 현실에서 개인 정보 공격 방어,"The convergence of artificial intelligence (AI) and extended reality (XR) technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eyetracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43 % and 39 %, respectively, for cybersickness tasks while preserving model utility with up to 97 % accuracy using Transformer models. Furthermore, it improves inference time by up to $\approx 2 \times$ compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay. Finally, we validate our approach through a user study, which confirms that participants found the PrivateXR UI effective, with satisfactory utility and user experience.","인공 지능(AI)과 확장 현실(XR) 기술(AI XR)의 융합은 다양한 영역에 걸쳐 혁신적인 애플리케이션을 약속합니다. 그러나 이러한 시스템에 사용되는 데이터(예: 시선 추적)의 민감한 특성은 공격자가 이러한 데이터와 모델을 악용하여 높은 성공률로 MIA(회원 추론 공격) 및 RDA(재식별)를 통해 개인 정보를 추론하고 유출할 수 있기 때문에 상당한 개인 정보 보호 문제를 야기합니다. 연구원들은 차등 개인 정보 보호(DP)를 포함하여 이러한 개인 정보 공격을 완화하기 위한 다양한 기술을 제안했습니다. 그러나 AI XR 데이터 세트에는 수많은 기능이 포함되는 경우가 많으며 DP를 균일하게 적용하면 관련성이 낮은 기능에 불필요한 노이즈가 발생하고 모델 정확도가 저하되며 추론 시간이 늘어나 실시간 XR 배포가 제한될 수 있습니다. 이에 동기를 부여하여 우리는 개인정보 공격을 방어하기 위해 설명 가능한 AI(XAI)와 DP 지원 개인정보 보호 메커니즘을 결합한 새로운 프레임워크를 제안합니다. 특히 사후 설명을 활용하여 AI XR 모델에서 가장 영향력 있는 기능을 식별하고 추론 중에 해당 기능에 DP를 선택적으로 적용합니다. 우리는 세 가지 최첨단 AI XR 모델과 세 가지 데이터 세트(사이버 멀미, 감정, 활동 분류)에 대한 XAI 기반 DP 접근 방식을 평가합니다. 우리의 결과는 제안된 방법이 Transformer 모델을 사용하여 최대 97% 정확도로 모델 유용성을 유지하면서 사이버 멀미 작업에 대해 MIA 및 RDA 성공률을 각각 최대 43% 및 39% 감소시키는 것을 보여줍니다. 또한 기존 DP 접근 방식에 비해 추론 시간을 최대 $\about 2 \times$ 향상시킵니다. 실용성을 입증하기 위해 XAI 기반 DP AI XR 모델을 HTC VIVE Pro 헤드셋에 배포하고 사용자 인터페이스(UI)인 PrivateXR을 개발하여 사용자가 실시간 작업 예측을 수신하면서 개인 정보 보호 수준(예: 낮음, 중간, 높음)을 조정할 수 있도록 하여 XR 게임 플레이 중에 사용자 개인 정보를 보호합니다. 마지막으로 참가자들이 PrivateXR UI가 만족스러운 유틸리티 및 사용자 경험과 함께 효과적이라는 것을 확인하는 사용자 연구를 통해 우리의 접근 방식을 검증합니다.",https://doi.org/10.1109/ISMAR67309.2025.00061,Interaction & Input,Deep Learning / Neural Networks,Technical Evaluation; User Study,Algorithm / Method; System / Framework
104,2025,Probabilistic Verification of Cybersickness in Virtual Reality Through Bayesian Networks,베이지안 네트워크를 통한 가상 현실의 사이버 멀미에 대한 확률적 검증,"Cybersickness remains a major challenge in virtual and mixed reality (VR/MR), yet existing methods primarily focus on predicting its onset without offering formal guarantees regarding its occurrence or effective mitigation. As VR/MR applications expand into safety-critical domains like healthcare, defense, verifiable safety assurances become essential to protect users from adverse physiological and psychological effects. This paper introduces a probabilistic verification framework leveraging Bayesian Networks (BN) to explicitly model the interactions among system parameters, human physiological responses, and cybersickness severity. Unlike deep learning approaches that lack interpretability and formal verification capabilities, the proposed BN model explicitly captures how environmental and system-level factors (e.g., luminance, spectral entropy, and image gradient complexity via HoG features) influence physiological responses (e.g., heart rate, reaction time, eye tracking), ultimately affecting cybersickness severity. By learning the joint probability distribution of these factors, our approach provides rigorous formal guarantees on cybersickness risk under specified operational conditions. If these guarantees are not met, automated adaptive adjustments are recommended to restore safe conditions. Experimental validation involving physiological and systemlevel data demonstrates that Bayesian Networks provide an interpretable and efficient framework, uniquely enabling formal probabilistic verification of cybersickness risks. This capability makes the proposed approach particularly suitable for designing and deploying VR/MR systems with explicitly verified safety constraints.","사이버 멀미는 가상 및 혼합 현실(VR/MR)의 주요 과제로 남아 있지만 기존 방법은 발생 또는 효과적인 완화에 대한 공식적인 보증을 제공하지 않고 주로 발병을 예측하는 데 중점을 둡니다. VR/MR 애플리케이션이 의료, 국방과 같이 안전이 중요한 영역으로 확장됨에 따라 생리학적, 심리적 부작용으로부터 사용자를 보호하기 위해 검증 가능한 안전 보장이 필수적이 되었습니다. 이 문서에서는 베이지안 네트워크(BN)를 활용하여 시스템 매개변수, 인간의 생리적 반응 및 사이버 멀미 심각도 간의 상호 작용을 명시적으로 모델링하는 확률적 검증 프레임워크를 소개합니다. 해석 가능성 및 형식적 검증 기능이 부족한 딥 러닝 접근 방식과 달리 제안된 BN 모델은 환경 및 시스템 수준 요소(예: HoG 기능을 통한 휘도, 스펙트럼 엔트로피 및 이미지 그라데이션 복잡성)가 생리적 반응(예: 심박수, 반응 시간, 시선 추적)에 어떻게 영향을 미치고 궁극적으로 사이버 멀미 심각도에 영향을 미치는지 명시적으로 포착합니다. 이러한 요소의 공동 확률 분포를 학습함으로써 우리의 접근 방식은 지정된 운영 조건에서 사이버 멀미 위험에 대한 엄격한 공식 보증을 제공합니다. 이러한 보증이 충족되지 않으면 안전한 상태를 복원하기 위해 자동화된 적응 조정을 권장합니다. 생리학적 및 시스템 수준 데이터와 관련된 실험적 검증은 베이지안 네트워크가 해석 가능하고 효율적인 프레임워크를 제공하여 사이버 멀미 위험에 대한 공식적인 확률론적 검증을 고유하게 가능하게 함을 보여줍니다. 이 기능을 통해 제안된 접근 방식은 명시적으로 검증된 안전 제약 조건을 갖춘 VR/MR 시스템을 설계하고 배포하는 데 특히 적합합니다.",https://doi.org/10.1109/ISMAR67309.2025.00087,Interaction & Input,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method; System / Framework
105,2025,RCLL-AR: Augmented Reality Support for Understanding Autonomous Processes in the RoboCup Logistics League,RCLL-AR: RoboCup Logistics League의 자율 프로세스를 이해하기 위한 증강 현실 지원,"Autonomous robot operations in the industry are becoming increasingly complex. It is therefore a significant challenge to comprehend the fundamental processes and to gain an understanding of the status of these systems. The RoboCup Logistics League (RCLL) represents a small smart factory environment with several workstations and operating robots. Despite its small scale the processes that occur within the league are very complex. Even with live commentary, observers have difficulties to follow the processes and game progress. This results in low interest in the RCLL and only few of visitors at competitions. To address this, we want to present RCLL-AR, an augmented reality (AR) solution visualizing highly relevant information of the RoboCup Logistics League. By using RCLL-AR, spectators of the game can see the current progress of the game, receive additional information about different workstations and understand future robot movements. To gain insights into the benefits of RCLL-AR for different stakeholders, we conducted expert interviews, a novice user study and an HMD study. Our findings showcase challenges AR faces in complex autonomous systems but also indicate benefits for novices and experts.","업계의 자율 로봇 운영은 점점 더 복잡해지고 있습니다. 따라서 기본 프로세스를 이해하고 이러한 시스템의 상태를 이해하는 것은 중요한 과제입니다. RCLL(RoboCup Logistics League)은 여러 워크스테이션과 작동 로봇이 있는 소규모 스마트 공장 환경을 나타냅니다. 작은 규모에도 불구하고 리그 내에서 발생하는 프로세스는 매우 복잡합니다. 실시간 해설이 있어도 관찰자는 프로세스와 게임 진행을 따라가는 데 어려움을 겪습니다. 이로 인해 RCLL에 대한 관심이 낮아지고 대회 방문객도 극소수에 불과합니다. 이를 해결하기 위해 로보컵 물류리그의 관련성이 높은 정보를 시각화하는 증강현실(AR) 솔루션 RCLL-AR을 선보이고자 합니다. RCLL-AR을 사용하여 게임 관중은 게임의 현재 진행 상황을 확인하고, 다양한 워크스테이션에 대한 추가 정보를 받고, 미래의 로봇 움직임을 이해할 수 있습니다. 다양한 이해관계자를 위한 RCLL-AR의 이점에 대한 통찰력을 얻기 위해 전문가 인터뷰, 초보 사용자 연구 및 HMD 연구를 실시했습니다. 우리의 연구 결과는 복잡한 자율 시스템에서 AR이 직면한 과제를 보여 주지만 초보자와 전문가에게도 이점이 있음을 나타냅니다.",https://doi.org/10.1109/ISMAR67309.2025.00045,Display & Optics,Other,User Study,User Study / Empirical Findings
106,2025,ReachVox: Clutter-Free Reachability Visualization for Robot Motion Planning in Virtual Reality,ReachVox: 가상 현실에서 로봇 동작 계획을 위한 깔끔한 도달 가능성 시각화,"Human-Robot-Collaboration can enhance workflows by leveraging the mutual strengths of human operators and robots. Planning and understanding robot movements remain major challenges in this domain. This problem is prevalent in dynamic environments that might need constant robot motion path adaptation. In this paper, we investigate whether a minimalistic encoding of the reachability of a point near an object of interest, which we call ReachVox, can aid the collaboration between a remote operator and a robotic arm in VR. Through a user study ($\mathrm{n}=20$), we indicate the strength of the visualization relative to a point-based reachability check-up.",인간-로봇-협력은 인간 작업자와 로봇의 상호 강점을 활용하여 작업 흐름을 향상시킬 수 있습니다. 로봇 움직임을 계획하고 이해하는 것은 이 영역에서 여전히 주요 과제로 남아 있습니다. 이 문제는 지속적인 로봇 동작 경로 적응이 필요할 수 있는 동적 환경에서 널리 퍼져 있습니다. 이 논문에서는 ReachVox라고 하는 관심 객체 근처 지점의 도달 가능성을 최소한으로 인코딩하는 것이 VR에서 원격 조작자와 로봇 팔 간의 협업을 도울 수 있는지 여부를 조사합니다. 사용자 연구($\mathrm{n}=20$)를 통해 포인트 기반 도달 가능성 점검과 관련된 시각화의 강도를 나타냅니다.,https://doi.org/10.1109/ISMAR67309.2025.00151,Collaboration & Social,Other,User Study,User Study / Empirical Findings
107,2025,Real-Time Photorealistic Style Transfer of Digital Humans for Immersive Virtual Reality,몰입형 가상 현실을 위한 디지털 휴먼의 실시간 사실적 스타일 전송,"We present a novel approach for real-time photorealistic style transfer of digital humans in virtual reality environments using a lightweight U-Net-based neural network architecture. Our method transforms rendered VR images into photorealistic images while maintaining temporal consistency. Unlike previous approaches that attempt to support arbitrary style transfer, we focus on predefined target styles, enabling significantly higher performance and visual fidelity in real-time applications. Our technique achieves high frame rates (104 FPS at $2 ~\mathrm{K} \times 2 ~\mathrm{K}$ resolution) through optimization and 8-bit integer quantization with NVIDIA's TensorRT. By incorporating foveated rendering techniques that prioritize processing in the center of vision, we further achieve $72+$ FPS at $2 \times 2064 \times 2208$ (stereo resolution) when integrated into a full PC-powered VR pipeline. The temporal artifacts such as flickering are eliminated through our direct image-to-image regression training, without additional temporal constraints. We evaluate two training methodologies: application-specific using sampled VR renderings, and generalized using diverse photorealistic datasets. Our experimental results demonstrate that our approach outperforms previous techniques in both quality and performance metrics for VR applications, enabling new possibilities for immersive photorealistic experiences.",우리는 경량 U-Net 기반 신경망 아키텍처를 사용하여 가상 현실 환경에서 디지털 휴먼의 실시간 사실적 스타일 전송을 위한 새로운 접근 방식을 제시합니다. 우리의 방법은 렌더링된 VR 이미지를 시간적 일관성을 유지하면서 사실적인 이미지로 변환합니다. 임의 스타일 전송을 지원하려는 이전 접근 방식과 달리 사전 정의된 대상 스타일에 중점을 두어 실시간 애플리케이션에서 훨씬 더 높은 성능과 시각적 충실도를 제공합니다. 우리의 기술은 NVIDIA의 TensorRT를 사용한 최적화 및 8비트 정수 양자화를 통해 높은 프레임 속도($2 ~\mathrm{K} \times 2 ~\mathrm{K}$ 해상도에서 104FPS)를 달성합니다. 시야 중심에서 처리를 우선시하는 포비티드 렌더링 기술을 통합함으로써 전체 PC 기반 VR 파이프라인에 통합될 때 $2 \times 2064 \times 2208$(스테레오 해상도)에서 $72+$ FPS를 더욱 달성합니다. 깜박임과 같은 시간적 아티팩트는 추가적인 시간적 제약 없이 직접적인 이미지 간 회귀 훈련을 통해 제거됩니다. 우리는 샘플링된 VR 렌더링을 사용하는 애플리케이션별 훈련 방법과 다양한 사실적 데이터 세트를 사용하여 일반화하는 두 가지 훈련 방법을 평가합니다. 우리의 실험 결과는 우리의 접근 방식이 VR 애플리케이션의 품질 및 성능 지표 모두에서 이전 기술을 능가하여 몰입감 있고 사실적인 경험을 위한 새로운 가능성을 가능하게 한다는 것을 보여줍니다.,https://doi.org/10.1109/ISMAR67309.2025.00088,Rendering & Visualization,Deep Learning / Neural Networks,Quantitative Experiment; Technical Evaluation,Algorithm / Method; System / Framework
108,2025,Real-Time Physically-Based Relighting and Composition of Radiance Fields with Proxy Meshes,프록시 메시를 사용한 실시간 물리 기반 재조명 및 래디언스 필드 구성,"Radiance fields, such as neural radiance fields (NeRFs) and 3D Gaussian splatting (3DGS), are the new primitives to represent 3D scenes. Relighting and composition of radiance fields are critical for modeling the complex 3D world in computer graphics. However, it is difficult to relight and composite radiance fields because traditional physically-based rendering techniques, such as path tracing, cannot be directly applied to radiance fields. We propose a physically-based relighting and composition method for radiance fields with proxy meshes. A unified framework is presented to enable us to use radiance fields as the traditional assets in computer graphics. We generate proxy meshes of the radiance fields by reconstructing the geometries of the scenes using Gaussian-based surface reconstruction and the materials using physically-based differentiable rendering. We leverage differential rendering, which is previously used in augmented reality (AR) and mixed reality (MR), to evaluate the radiance change on the proxy meshes introduced by the changing lighting condition, the inserted radiance fields, or the inserted mesh models. Proxy meshes can help us utilize hardwareaccelerated ray tracing to perform real-time path tracing. Experimental results show that our method outperforms the baselines in terms of relighting performance and can achieve photorealistic relighting and composition of radiance fields in real-time.","NeRF(신경 복사 필드) 및 3DGS(3D Gaussian splatting)와 같은 복사 필드는 3D 장면을 표현하는 새로운 기본 요소입니다. 복사 필드의 재조명 및 구성은 컴퓨터 그래픽에서 복잡한 3D 세계를 모델링하는 데 중요합니다. 그러나 경로 추적과 같은 기존의 물리적 기반 렌더링 기술은 복사장에 직접 적용할 수 없기 때문에 복사장을 다시 조명하고 합성하는 것이 어렵습니다. 우리는 프록시 메쉬를 사용하여 래디언스 필드에 대한 물리적 기반 재조명 및 구성 방법을 제안합니다. 래디언스 필드를 컴퓨터 그래픽의 전통적인 자산으로 사용할 수 있도록 통합된 프레임워크가 제공됩니다. 우리는 가우시안 기반 표면 재구성을 사용하여 장면의 형상을 재구성하고 물리 기반 미분 렌더링을 사용하여 재료를 재구성하여 복사 필드의 프록시 메시를 생성합니다. We leverage differential rendering, which is previously used in augmented reality (AR) and mixed reality (MR), to evaluate the radiance change on the proxy meshes introduced by the changing lighting condition, the inserted radiance fields, or the inserted mesh models. 프록시 메시는 하드웨어 가속 레이 트레이싱을 활용하여 실시간 경로 추적을 수행하는 데 도움이 될 수 있습니다. 실험 결과는 우리의 방법이 재조명 성능 측면에서 기준선을 능가하고 실시간으로 사실적인 재조명 및 복사 필드 구성을 달성할 수 있음을 보여줍니다.",https://doi.org/10.1109/ISMAR67309.2025.00046,Rendering & Visualization,3D Reconstruction,Technical Evaluation,Algorithm / Method
109,2025,Rendering Togetherness: Embodied Social Synchronization in Multi-User VR,렌더링 공생성: 다중 사용자 VR에서 구현된 소셜 동기화,"Implementing multi-person interactions in Virtual Reality (VR) poses two interrelated challenges: (1) accurately capturing and rendering the kinematics of social interactions, and (2) fostering social connectedness, a key component of effective group communication. In physical settings, social connectedness is closely linked to interpersonal motor coordination. Whether similar mechanisms operate in VR, and how best to render them remains an open question. This study investigated group synchronization in VR by manipulating two key factors: visual coupling and joint commitment to synchronize, both known to influence group synchrony and social connectedness. Data from this VR experiment were compared with a previous study conducted in a real-world setting. In both contexts, visual coupling and joint commitment enhanced group synchrony and the feeling of social connectedness. However, their effects on individual kinematic features (e.g., movement frequency and amplitude) diverged between real and virtual environments, suggesting different coordination strategies were employed. These findings demonstrate that while multi-user VR can support the emergence of collective movement and foster social bonds, technical constraints, such as limited motion fidelity and restricted field of view, can shape how users adapt their movements to achieve joint action. This has important theoretical and practical implications for the design and modeling of collective motion in VR.","가상 현실(VR)에서 다중 사용자 상호 작용을 구현하는 데에는 (1) 사회적 상호 작용의 운동학을 정확하게 포착하고 렌더링하는 것과 (2) 효과적인 그룹 커뮤니케이션의 핵심 구성 요소인 사회적 연결성을 육성하는 두 가지 상호 관련된 과제가 있습니다. 물리적 환경에서 사회적 연결은 대인 운동 조정과 밀접하게 연관되어 있습니다. 유사한 메커니즘이 VR에서 작동하는지 여부와 이를 가장 잘 렌더링하는 방법은 여전히 ​​열려 있는 질문으로 남아 있습니다. 이 연구는 그룹 동기화와 사회적 연결성에 영향을 미치는 것으로 알려진 시각적 결합과 동기화에 대한 공동 헌신이라는 두 가지 주요 요소를 조작하여 VR의 그룹 동기화를 조사했습니다. 이 VR 실험의 데이터는 실제 환경에서 수행된 이전 연구와 비교되었습니다. 두 가지 맥락 모두에서 시각적 결합과 공동 헌신은 그룹의 동시성과 사회적 연결감을 향상시켰습니다. 그러나 개별 운동학적 특징(예: 이동 빈도 및 진폭)에 대한 효과는 실제 환경과 가상 환경 간에 다양하여 서로 다른 조정 전략이 사용되었음을 시사합니다. 이러한 연구 결과는 다중 사용자 VR이 집단 운동의 출현을 지원하고 사회적 유대감을 조성할 수 있지만 제한된 모션 충실도 및 제한된 시야와 같은 기술적 제약이 사용자가 공동 작업을 달성하기 위해 움직임을 적응하는 방법을 형성할 수 있음을 보여줍니다. 이는 VR의 집단 동작 설계 및 모델링에 중요한 이론적, 실무적 의미를 갖습니다.",https://doi.org/10.1109/ISMAR67309.2025.00079,Collaboration & Social,Other,Technical Evaluation,Algorithm / Method
110,2025,Rhythmic Interaction Influences Synchrony Perception in VR,리듬 상호 작용은 VR의 동시성 인식에 영향을 미칩니다,"Shared synchronised activities, such as dancing and singing, can promote social bonding in both physical and virtual settings, but are susceptible to network latencies in mediated environments, such as social virtual reality (VR). Temporal delays can hinder interpersonal entrainment, causing users to feel out of sync, and reduce presence. While prior work has examined how stimulus type and frequency affect synchrony perception, the role of rhythmic interaction, central to many interactive synchronised experiences, remains unexplored. We conducted a within-subject study with 32 participants to systematically investigate how rhythmic interaction shapes synchrony perception of rhythmic audiovisual stimuli in networked VR environments. Motor behaviour and subjective synchrony ratings ($\mathrm{n}=32$) are analysed, complemented by eye-tracking metrics from a quality-controlled subset ($\mathrm{n}=25$). Through controlled stimulus onset asynchronies (SOAs) between audio and video stimuli, we simulate network latency scenarios wshere (1) audio precedes video (as in VR dance environments where music playback is synchronised to a global clock while remote dancer movements appear delayed), and (2) video precedes audio (as in orchestra experiences where a conductor's movements are perceived first, followed by a delayed auditory response from the orchestra). Participants either synchronised hand movements with a virtual avatar or passively observed it across two stimulus frequencies (0.5 Hz and 1 Hz), for both audio- and video-leading offsets. Our results show that rhythmic interaction significantly increases tolerance to audiovisual offsets, shifting the left decision criterion (LDC) by up to 66 ms, the point of subjective synchrony (PSS) by up to 31 ms, and widening the window of subjective synchrony (WSS) by up to 59 ms. These effects suggest that rhythmic interaction can reduce sensitivity to asynchronies in audio-leading scenarios, such as VR dance environments. We also found that temporal offsets influence rhythmic interaction behaviour and confirm that recent findings on the influence of stimulus frequency on PSS and temporal offsets on pupil dilation can to a certain extent be replicated in interactive VR.","춤, 노래 등 공유된 동기화된 활동은 물리적 환경과 가상 환경 모두에서 사회적 유대감을 촉진할 수 있지만 소셜 가상 현실(VR)과 같은 매개 환경에서는 네트워크 대기 시간이 발생하기 쉽습니다. 시간적 지연은 대인 관계 참여를 방해하여 사용자가 동기화되지 않은 느낌을 받고 존재감을 감소시킬 수 있습니다. 이전 연구에서는 자극 유형과 빈도가 동시 인식에 어떻게 영향을 미치는지 조사했지만, 많은 상호 작용 동기화 경험의 중심인 리듬 상호 작용의 역할은 아직 탐구되지 않은 상태로 남아 있습니다. 우리는 리드미컬한 상호 작용이 네트워크로 연결된 VR 환경에서 리드미컬한 시청각 자극의 동시 인식을 어떻게 형성하는지 체계적으로 조사하기 위해 32명의 참가자를 대상으로 피험자 내 연구를 수행했습니다. 모터 행동 및 주관적 동시성 등급($\mathrm{n}=32$)이 분석되고 품질 관리 하위 집합($\mathrm{n}=25$)의 시선 추적 측정항목으로 보완됩니다. 오디오와 비디오 자극 간의 제어된 자극 개시 비동기(SOA)를 통해 우리는 (1) 오디오가 비디오보다 앞서고(음악 재생이 글로벌 시계에 동기화되고 원격 댄서의 움직임이 지연되는 것처럼 보이는 VR 댄스 환경에서와 같이), (2) 비디오가 오디오보다 앞선다는 점(지휘자의 움직임이 먼저 인식된 후 오케스트라의 청각 반응이 지연되는 오케스트라 경험에서와 같이) 네트워크 대기 시간 시나리오를 시뮬레이션합니다. 참가자는 오디오 및 비디오 선도 오프셋 모두에 대해 가상 아바타와 손 움직임을 동기화하거나 두 개의 자극 주파수(0.5Hz 및 1Hz)에서 수동적으로 관찰했습니다. 우리의 결과는 리드미컬한 상호작용이 시청각 오프셋에 대한 내성을 크게 증가시켜 왼쪽 결정 기준(LDC)을 최대 66ms까지 이동시키고 주관적 동기화 지점(PSS)을 최대 31ms까지 이동시키며 주관적 동기화(WSS) 창을 최대 59ms까지 넓히는 것을 보여줍니다. 이러한 효과는 리드미컬한 상호 작용이 VR 댄스 환경과 같은 오디오 선도 시나리오에서 비동기성에 대한 민감도를 줄일 수 있음을 시사합니다. 우리는 또한 시간적 오프셋이 리드미컬한 상호 작용 동작에 영향을 미치고 PSS에 대한 자극 주파수의 영향과 동공 확장에 대한 시간적 오프셋에 대한 최근 연구 결과가 대화형 VR에서 어느 정도 복제될 수 있음을 확인했습니다.",https://doi.org/10.1109/ISMAR67309.2025.00049,Perception & Cognition; Collaboration & Social,Sensor Fusion,User Study,User Study / Empirical Findings
111,2025,Robustness of Self-Avatar Animation Beyond Sparse Tracking: Effects of Pose Estimator Discrepancies and Inaccuracies,희소 추적을 넘어선 자체 아바타 애니메이션의 견고성: 포즈 추정기 불일치 및 부정확성의 영향,"In immersive applications, reconstructing full-body self-avatar motion remains a challenging task, particularly in the absence of lowerbody tracking devices. A promising approach involves supplementing sparse upper-body motion signals with 3D Cartesian joint positions estimated from external RGB cameras. While effective, such pose estimators are prone to inaccuracies, jitter, and occlusions, which can negatively impact reconstruction quality. In this work, we investigate the robustness of self-avatar animation models to such artifacts and to variations across pose estimators. We evaluate two state-of-the-art models, HMD-Poser and AvatarJLM, and show that integrating RGB-based pose data improves lower-body accuracy over VR-only baselines, even for inaccurate 3D pose. However, model performance degrades when training and testing rely on different pose estimators. This highlights the sensitivity of current approaches to estimator variability and underscores the need for estimator-aware training to ensure robustness in real-world deployment.","몰입형 응용 프로그램에서 전신 자체 아바타 모션을 재구성하는 것은 특히 하체 추적 장치가 없는 경우 어려운 작업으로 남아 있습니다. 유망한 접근 방식은 외부 RGB 카메라에서 추정된 3D 데카르트 관절 위치로 희박한 상체 모션 신호를 보완하는 것입니다. 이러한 포즈 추정기는 효과적이지만 부정확성, 지터 및 폐색이 발생하기 쉬우며 이는 재구성 품질에 부정적인 영향을 미칠 수 있습니다. 이 작업에서 우리는 이러한 아티팩트와 포즈 추정기의 변형에 대한 자체 아바타 애니메이션 모델의 견고성을 조사합니다. 우리는 두 가지 최첨단 모델인 HMD-Poser와 AvatarJLM을 평가하고 RGB 기반 포즈 데이터를 통합하면 부정확한 3D 포즈에서도 VR 전용 기준선에 비해 하체 정확도가 향상된다는 것을 보여줍니다. 그러나 훈련과 테스트가 다른 포즈 추정기에 의존하는 경우 모델 성능이 저하됩니다. 이는 추정기 변동성에 대한 현재 접근 방식의 민감도를 강조하고 실제 배포의 견고성을 보장하기 위한 추정기 인식 교육의 필요성을 강조합니다.",https://doi.org/10.1109/ISMAR67309.2025.00039,Perception & Cognition,Other,Technical Evaluation,Algorithm / Method
112,2025,SAMR: A Spatial-Augmented Mixed Reality Method for Enhancing Vision-Language Models in 3D Scene Understanding,SAMR: 3D 장면 이해에서 시각-언어 모델을 향상시키기 위한 공간 증강 혼합 현실 방법,"Understanding 3D scenes in mixed reality (MR) is crucial for advancing human-computer interaction, especially in MR applications that demand spatial awareness and contextual reasoning. While Vision-Language Models (VLMs) perform well in 2D image interpretation, they struggle to incorporate spatial context from 3D settings, which limits their effectiveness in MR scenarios. To address this issue, we introduce SAMR, a Spatial-Augmented Mixed Reality method designed to enhance VLMs for 3D scene understanding. Our system consists of three key modules. The first module, a spatial-segmented fusion module, uses FastSAM-based segmentation to create objectlevel meshes from head-mounted display (HMD) images. It maps extracted feature points to 3D coordinates through ray casting on the HMD-captured mesh and applies triangular facet fitting. The second module, a multimodal interaction module, combines gestures, gaze, and voice commands to enable intuitive interaction with 3D meshes for annotating prompts. The third module, a VLM integration module, processes data by merging annotated 2D images with user queries to form standardized prompts for the VLM. The VLM then generates responses linked to user-specified object meshes. By enhancing VLMs with spatial context and multimodal capabilities, SAMR greatly improves 3D scene interpretation. We demonstrate SAMR's effectiveness across six key application scenarios: object identification, relationship analysis, distance estimation, targeted object questioning, and cognitive assistance. This approach provides a robust framework for MR applications with AI agents.","혼합 현실(MR)에서 3D 장면을 이해하는 것은 인간과 컴퓨터의 상호 작용을 발전시키는 데 매우 중요합니다. 특히 공간 인식과 상황별 추론이 필요한 MR 애플리케이션에서는 더욱 그렇습니다. VLM(시각 언어 모델)은 2D 이미지 해석에서는 잘 작동하지만 3D 설정의 공간적 맥락을 통합하는 데 어려움을 겪어 MR 시나리오에서의 효과가 제한됩니다. 이 문제를 해결하기 위해 3D 장면 이해를 위해 VLM을 향상시키도록 설계된 공간 증강 혼합 현실 방법인 SAMR을 소개합니다. 우리 시스템은 세 가지 핵심 모듈로 구성됩니다. 공간 분할 융합 모듈인 첫 번째 모듈은 FastSAM 기반 분할을 사용하여 HMD(헤드 마운트 디스플레이) 이미지에서 개체 수준 메시를 생성합니다. 추출된 특징점을 HMD로 캡처한 메쉬에 레이 캐스팅을 통해 3D 좌표로 매핑하고 삼각형 패싯 피팅을 적용합니다. 두 번째 모듈인 다중 모드 상호 작용 모듈은 제스처, 응시 및 음성 명령을 결합하여 프롬프트에 주석을 달기 위해 3D 메시와의 직관적인 상호 작용을 가능하게 합니다. 세 번째 모듈인 VLM 통합 모듈은 주석이 달린 2D 이미지를 사용자 쿼리와 병합하여 VLM에 대한 표준화된 프롬프트를 형성함으로써 데이터를 처리합니다. 그런 다음 VLM은 사용자가 지정한 개체 메시에 연결된 응답을 생성합니다. 공간적 맥락과 다중 모드 기능으로 VLM을 강화함으로써 SAMR은 3D 장면 해석을 크게 향상시킵니다. 객체 식별, 관계 분석, 거리 추정, 대상 객체 질문, 인지 지원 등 6가지 주요 응용 시나리오에서 SAMR의 효율성을 입증합니다. 이 접근 방식은 AI 에이전트가 포함된 MR 애플리케이션을 위한 강력한 프레임워크를 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00094,Interaction & Input; Display & Optics,Natural Language Processing,Case Study / Application Demo,Algorithm / Method; System / Framework
113,2025,Safeteleport: Potential Field-Guided Teleportation for Personal Space Protection in Social VR,Safeteleport: 소셜 VR에서 개인 공간 보호를 위한 잠재적인 현장 유도 순간이동,"In social virtual reality (VR), maintaining appropriate interpersonal distance is essential for user comfort and privacy. However, most existing locomotion methods provide limited support for respecting personal space, leaving users vulnerable to unintentional or socially inappropriate intrusions. To address this issue, we propose potential field-guided teleportation, a proactive locomotion framework consisting of two method implementations that dynamically adjust teleportation targets based on real-time interpersonal proximity, preventing entry into others' personal spaces without explicit user intervention. We evaluate our technique through two user studies: a preliminary study exploring energy-based constraint parameters, followed by a comparative study against conventional and negotiated teleportation methods. Experiments were conducted in socially interactive VR scenarios populated with simulated users exhibiting human-like behaviors. Results demonstrate that our methods reduce perceived social anxiety while maintaining locomotion efficiency and usability. This work presents a socially-aware locomotion strategy that balances personal space protection with effective and socially appropriate movement in shared virtual environments.","소셜 가상 현실(VR)에서는 사용자의 편안함과 개인 정보 보호를 위해 적절한 대인 거리를 유지하는 것이 필수적입니다. 그러나 대부분의 기존 이동 방법은 개인 공간 존중에 대한 지원이 제한되어 있어 사용자가 의도치 않거나 사회적으로 부적절한 침입에 취약하게 됩니다. 이 문제를 해결하기 위해 우리는 실시간 대인 근접성을 기반으로 순간 이동 목표를 동적으로 조정하여 명시적인 사용자 개입 없이 다른 사람의 개인 공간에 진입하는 것을 방지하는 두 가지 방법 구현으로 구성된 사전 예방적 이동 프레임워크인 잠재적 현장 유도 순간 이동을 제안합니다. 우리는 두 가지 사용자 연구, 즉 에너지 기반 제약 매개변수를 탐색하는 예비 연구와 기존 및 협상된 순간 이동 방법에 대한 비교 연구를 통해 기술을 평가합니다. 실험은 인간과 유사한 행동을 보이는 시뮬레이션된 사용자로 구성된 사회적 상호작용형 VR 시나리오에서 수행되었습니다. 결과는 우리의 방법이 운동 효율성과 유용성을 유지하면서 인식된 사회적 불안을 감소시키는 것을 보여줍니다. 이 작품은 공유된 가상 환경에서 개인 공간 보호와 효과적이고 사회적으로 적절한 움직임의 균형을 맞추는 사회적 인식 운동 전략을 제시합니다.",https://doi.org/10.1109/ISMAR67309.2025.00059,Interaction & Input; Collaboration & Social,Redirected Walking / Locomotion,Simulation,Algorithm / Method
114,2025,Scalable Object Detection in Mixed Reality Using Incremental Re-Training and One-Shot 3D Annotation,증분 재교육 및 원샷 3D 주석을 사용하여 혼합 현실에서 확장 가능한 객체 감지,"While object detection can be incredibly useful for a variety of augmented and mixed reality applications, achieving a large number of classifiable objects with high accuracy without extremely large deep learning (DL) or object recognition models is still difficult. More importantly, object recognition frameworks are often rigid in that they don't provide a direct means to add new classes to pretrained models in real time. In this paper, we introduce a novel approach that enables ondemand training of new object classes for consistent detection of in-situ objects for virtual labeling and interaction. By leveraging knowledge of the 3D location of an object in the scene taken from a mixed reality (MR) display's environment mesh, we are able to automate the labeling of subsequent 2D images taken from the frontfacing camera, which requires only a single, initial labeling interaction from an end-user. In addition, we have developed a continual learning approach that allows for on-the-fly retraining of the classifier and provides accurate classification quickly enough for the model to be practically usable in MR applications. We validate this approach by measuring the re-training time required for various object configurations, provide a comparison to other classification strategies, and analyze how the addition of object classes affect detection continuity across 3D scenes. We also demonstrate that labeling interactions work for practical applications in AR that are dependent on object detection, such as language learning, procedural instruction, or manufacturing guidance.","객체 감지는 다양한 증강 및 혼합 현실 애플리케이션에 매우 유용할 수 있지만, 대규모 딥 러닝(DL) 또는 객체 인식 모델 없이 분류 가능한 객체를 높은 정확도로 달성하는 것은 여전히 ​​어렵습니다. 더 중요한 것은 객체 인식 프레임워크가 사전 훈련된 모델에 실시간으로 새 클래스를 추가하는 직접적인 수단을 제공하지 않는다는 점에서 종종 경직된다는 것입니다. 본 논문에서는 가상 라벨링 및 상호 작용을 위해 현장 개체를 일관되게 감지하기 위해 새로운 개체 클래스에 대한 주문형 교육을 가능하게 하는 새로운 접근 방식을 소개합니다. 혼합 현실(MR) 디스플레이의 환경 메시에서 가져온 장면 내 객체의 3D 위치에 대한 지식을 활용하여 전면 카메라에서 가져온 후속 2D 이미지의 레이블 지정을 자동화할 수 있으며, 이는 최종 사용자의 단일 초기 레이블 지정 상호 작용만 필요합니다. 또한 분류기의 즉각적인 재교육을 허용하고 모델이 MR 애플리케이션에서 실제로 사용할 수 있을 만큼 빠르게 정확한 분류를 제공하는 지속적인 학습 접근 방식을 개발했습니다. 우리는 다양한 객체 구성에 필요한 재훈련 시간을 측정하여 이 접근 방식을 검증하고, 다른 분류 전략과의 비교를 제공하고, 객체 클래스 추가가 3D 장면 전반에 걸쳐 감지 연속성에 어떻게 영향을 미치는지 분석합니다. 또한 언어 학습, 절차 지침 또는 제조 지침과 같은 객체 감지에 의존하는 AR의 실제 응용 프로그램에 라벨링 상호 작용이 작동한다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR67309.2025.00040,Content Authoring; Education & Training,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method
115,2025,"Shared, Replicated, or Separated? a Comparative Study of Virtual Workspace Configurations for Collaborative Hands-on Learning","공유, 복제 또는 분리? 협업 실습 학습을 위한 가상 작업 공간 구성에 대한 비교 연구","Most work on collaborative immersive systems mimics real-world settings, using fully shared virtual workspaces that foster close collaboration. However, recent work in educational contexts using remote desktop environments suggests these shared approaches may not be optimal for learning, as it showed that individual workspaces lead to better learning outcomes. In this paper, we investigate whether individual workspaces also lead to better outcomes in a collaborative VR learning environment. We compare three distinct workspace configurations in a problem-solving task: (1) a fully shared environment where two users work on the same materials, (2) a replicated environment where each user has their own copy of the materials but can still see their partner and their workspace, and (3) a separated environment where users cannot see each other nor each other's workspace and each has individual materials. We evaluate how these configurations influenced collaborative interaction, problem-solving strategies, and learning. Our results suggest the replicated workspace reduced social experience and did not improve learning outcomes compared to the shared one, however, it allowed broader exploration of the problem space.","협업 몰입형 시스템에 대한 대부분의 작업은 긴밀한 협업을 촉진하는 완전히 공유된 가상 작업 공간을 사용하여 실제 설정을 모방합니다. 그러나 원격 데스크톱 환경을 사용하는 교육적 맥락에서 최근 연구에 따르면 이러한 공유 접근 방식은 개별 작업 공간이 더 나은 학습 결과로 이어지는 것으로 나타났기 때문에 학습에 적합하지 않을 수 있음을 시사합니다. 본 논문에서는 개별 작업 공간이 협업적인 VR 학습 환경에서 더 나은 결과로 이어지는지 조사합니다. 우리는 문제 해결 작업에서 세 가지 별개의 작업 공간 구성을 비교합니다. (1) 두 명의 사용자가 동일한 자료로 작업하는 완전히 공유된 환경, (2) 각 사용자가 자신의 자료 사본을 가지고 있지만 여전히 파트너와 작업 공간을 볼 수 있는 복제 환경, (3) 사용자가 서로 또는 서로의 작업 공간을 볼 수 없고 각자가 개별 자료를 갖는 분리된 환경입니다. 우리는 이러한 구성이 협업 상호 작용, 문제 해결 전략 및 학습에 어떤 영향을 미쳤는지 평가합니다. 우리의 결과는 복제된 작업 공간이 공유 작업 공간에 비해 사회적 경험을 감소시키고 학습 결과를 향상시키지는 않았지만 문제 공간을 더 광범위하게 탐색할 수 있음을 시사합니다.",https://doi.org/10.1109/ISMAR67309.2025.00081,Collaboration & Social; Education & Training,Other,Other,System / Framework
116,2025,Siren Song: Acoustic Attacks on Pose Estimation in XR Headsets,Siren Song: XR 헤드셋의 포즈 추정에 대한 음향 공격,"Extended Reality (XR) experiences involve interactions between users, the real world, and virtual content. A key step to enable these experiences is the XR headset sensing and estimating the user's pose in order to accurately place and render virtual content in the real world. XR headsets use multiple sensors (e.g., cameras, inertial measurement unit) to perform pose estimation and improve its robustness, but this provides an attack surface for adversaries to interfere with the pose estimation process. In this paper, we create and study the effects of acoustic attacks that create false signals in the inertial measurement unit (IMU) on XR headsets, leading to adverse downstream effects on XR applications. We generate resonant acoustic signals on a HoloLens 2 and measure the resulting perturbations in the IMU readings, and also demonstrate both finegrained and coarse attacks on the ORB-SLAM3 and an open-source XR system (ILLIXR). With the knowledge gleaned from attacking these open-source frameworks, we demonstrate four end-to-end proof-of-concept attacks on a HoloLens 2: manipulating user input, clickjacking, zone invasion, and denial of user interaction. Our experiments show that current commercial XR headsets are susceptible to acoustic attacks, raising concerns for their security.","확장 현실(XR) 경험에는 사용자, 현실 세계, 가상 콘텐츠 간의 상호 작용이 포함됩니다. 이러한 경험을 가능하게 하는 핵심 단계는 현실 세계에서 가상 콘텐츠를 정확하게 배치하고 렌더링하기 위해 XR 헤드셋이 사용자의 자세를 감지하고 추정하는 것입니다. XR 헤드셋은 여러 센서(예: 카메라, 관성 측정 장치)를 사용하여 자세 추정을 수행하고 견고성을 향상시키지만 이는 공격자가 포즈 추정 프로세스를 방해할 수 있는 공격 표면을 제공합니다. 본 논문에서는 XR 헤드셋의 관성 측정 장치(IMU)에서 잘못된 신호를 생성하여 XR 애플리케이션에 부정적인 다운스트림 영향을 미치는 음향 공격의 효과를 생성하고 연구합니다. HoloLens 2에서 공명 음향 신호를 생성하고 IMU 판독값에서 결과적인 섭동을 측정하며 ORB-SLAM3 및 오픈 소스 XR 시스템(ILLIXR)에 대한 세밀한 공격과 거친 공격을 모두 시연합니다. 이러한 오픈 소스 프레임워크를 공격하여 얻은 지식을 바탕으로 HoloLens 2에 대한 네 가지 엔드투엔드 개념 증명 공격(사용자 입력 조작, 클릭재킹, 영역 침입 및 사용자 상호 작용 거부)을 보여줍니다. 우리의 실험에 따르면 현재 상용 XR 헤드셋은 음향 공격에 취약하여 보안에 대한 우려가 커지고 있습니다.",https://doi.org/10.1109/ISMAR67309.2025.00158,Tracking & Localization; Interaction & Input,Sensor Fusion,Quantitative Experiment,Hardware / Device; System / Framework
117,2025,SliVR: A 360° VR-Hub for Fast Selections in Multiple Virtual Environments,SliVR: 여러 가상 환경에서 빠른 선택을 위한 360° VR 허브,"Current virtual reality (VR) systems limit users to a single virtual 3D environment (VE) at a time, which restricts their ability to engage with multiple VEs simultaneously. We present SLIVR, a VR-Hub designed to enhance multitasking capabilities by arranging multiple VEs in a regular polygon formation around the user. Each VE is visually represented by life-sized 2D previews that facilitate hub-based interactions. Direct Selection enables users to select objects within a VE directly from the preview, allowing them to remain within the hub. Additionally, Focus-mode enlarges a chosen 2D preview, providing access to objects beyond the initial viewport. In a user study ($\mathrm{N}=18$), we evaluated SLIVR against a conventional grid-based VR app launcher and a hub that utilized SLIVR's visualization but only supported Teleportation interactions. Our findings revealed that SLIVR improved target selection performance by 16.8% overall and by 30.5% when engaging with six VEs, compared to the grid-based app launcher. Notably, hub-based interactions accounted for 94.9% of selections, with Focus-mode being utilized six times more frequently than Teleportation.","현재 가상 현실(VR) 시스템은 사용자를 한 번에 하나의 가상 3D 환경(VE)으로 제한하므로 여러 VE를 동시에 사용할 수 있는 능력이 제한됩니다. 사용자 주변에 다수의 VE를 정다각형 형태로 배치하여 멀티태스킹 능력을 강화하도록 설계된 VR-Hub인 SLIVR을 소개합니다. 각 VE는 허브 기반 상호 작용을 용이하게 하는 실물 크기의 2D 미리보기로 시각적으로 표현됩니다. 직접 선택을 사용하면 사용자가 미리 보기에서 직접 VE 내의 개체를 선택하여 허브 내에 유지할 수 있습니다. 또한 초점 모드는 선택한 2D 미리보기를 확대하여 초기 뷰포트 너머의 개체에 대한 액세스를 제공합니다. 사용자 연구($\mathrm{N}=18$)에서 우리는 SLIVR의 시각화를 활용했지만 순간 이동 상호 작용만 지원하는 허브와 기존 그리드 기반 VR 앱 실행 프로그램에 대해 SLIVR을 평가했습니다. 우리의 연구 결과에 따르면 SLIVR은 그리드 기반 앱 런처에 비해 대상 선택 성능이 전반적으로 16.8% 향상되었고 6개의 VE를 사용할 때 30.5% 향상되었습니다. 특히 허브 기반 상호 작용이 선택의 94.9%를 차지했으며, 집중 모드는 순간 이동보다 6배 더 자주 활용되었습니다.",https://doi.org/10.1109/ISMAR67309.2025.00065,Interaction & Input,Sensor Fusion,User Study,User Study / Empirical Findings
118,2025,Sonify Anything: Towards Context-Aware Sonic Interactions in AR,Sonify Anything: AR의 상황 인식 음향 상호작용을 향하여,"In Augmented Reality (AR), virtual objects interact with real objects. However, the lack of physicality of virtual objects leads to the absence of natural sonic interactions. When virtual and real objects collide, either no sound or a generic sound is played. Both lead to an incongruent multisensory experience reducing interaction and object realism. Unlike in Virtual Reality (VR) and games, where predefined scenes and interactions allow for the playback of prerecorded sound samples, AR requires real-time sound synthesis that dynamically adapts to novel contexts and objects to provide audiovisual congruence during interaction. To enhance real-virtual object interactions in AR, we propose a framework for context-aware sounds using methods from computer vision to recognize and segment the materials of real objects. The material's physical properties and the impact dynamics of the interaction are used to generate material-based sounds in real-time using physical modelling synthesis. In a user study with 24 participants, we compared our congruent material-based sounds to a generic sound effect, mirroring the current standard of non-context-aware sounds in AR applications. The results showed that material-based sounds led to significantly more realistic sonic interactions. Material-based sounds also enabled participants to distinguish visually similar materials with significantly greater accuracy and confidence. These findings show that context-aware, material-based sonic interactions in AR foster a stronger sense of realism and enhance our perception of real-world surroundings.","증강 현실(AR)에서는 가상 객체가 실제 객체와 상호 작용합니다. 그러나 가상 객체의 물리적 특성 부족으로 인해 자연스러운 음향 상호 작용이 발생하지 않습니다. 가상 개체와 실제 개체가 충돌하면 소리가 들리지 않거나 일반 소리가 재생됩니다. 둘 다 부적합한 다감각 경험으로 이어져 상호 작용과 객체 현실성을 감소시킵니다. 미리 정의된 장면과 상호 작용을 통해 미리 녹음된 사운드 샘플을 재생할 수 있는 가상 현실(VR) 및 게임과 달리 AR에는 상호 작용 중에 시청각적 일치를 제공하기 위해 새로운 상황과 객체에 동적으로 적응하는 실시간 사운드 합성이 필요합니다. AR에서 실제-가상 개체 상호 작용을 향상시키기 위해 컴퓨터 비전의 방법을 사용하여 실제 개체의 재료를 인식하고 분할하는 상황 인식 사운드 프레임워크를 제안합니다. 재료의 물리적 특성과 상호 작용의 충격 역학은 물리적 모델링 합성을 통해 실시간으로 재료 기반 사운드를 생성하는 데 사용됩니다. 24명의 참가자를 대상으로 한 사용자 연구에서 우리는 일치하는 재료 기반 사운드를 일반적인 사운드 효과와 비교하여 AR 애플리케이션의 상황을 인식하지 못하는 사운드의 현재 표준을 반영했습니다. 결과는 물질 기반 사운드가 훨씬 더 사실적인 음향 상호작용을 가져온다는 것을 보여주었습니다. 또한 재료 기반 사운드를 통해 참가자들은 시각적으로 유사한 재료를 훨씬 더 정확하고 확실하게 구별할 수 있었습니다. 이러한 연구 결과는 AR의 상황 인식, 재료 기반 음향 상호 작용이 더욱 강력한 현실감을 조성하고 실제 환경에 대한 인식을 향상시킨다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR67309.2025.00030,Interaction & Input; Perception & Cognition,Computer Vision,User Study,Algorithm / Method; User Study / Empirical Findings
119,2025,"The Awe-Some Spectrum: Self-Reported Awe Varies by Eliciting Scenery and Presence in Virtual Reality, and the User's Nationality","경이로운 스펙트럼: 가상 현실의 풍경과 존재, 사용자의 국적에 따라 자체 보고된 경외감이 달라집니다.","Awe is a multifaceted emotion often associated with the perception of vastness, that challenges existing mental frameworks. Despite its growing relevance in affective computing and psychological research, awe remains difficult to elicit and measure. This raises the research questions of how awe can be effectively elicited, which factors are associated with the experience of awe, and whether it can reliably be measured using biosensors. For this study, we designed 10 immersive Virtual Reality (VR) scenes with dynamic transitions from narrow to vast environments. These scenes were used to explore how awe relates to environmental features (abstract, human-made, nature), personality traits, and country of origin. We collected skin conductance, respiration, self-reported awe and presence data from participants from Germany, Japan, and Jordan. Our results indicate that self-reported awe varies significantly across countries and scene types. In particular, a scene depicting outer space elicited the strongest awe. Scenes that elicited high selfreported awe also induced a stronger sense of presence. However, we found no evidence that awe ratings are correlated with physiological responses. These findings challenge the assumption that awe is reliably reflected in autonomic arousal and underscore the importance of cultural and perceptual context. Our study offers new insights into how immersive VR can be designed to elicit awe, and suggests that subjective reports - rather than physiological signals - remain the most consistent indicators of emotional impact.","경외감은 기존의 정신적 틀에 도전하는 광대함에 대한 인식과 종종 관련된다면적인 감정입니다. 감성 컴퓨팅과 심리학 연구에서 관련성이 높아지고 있음에도 불구하고 경외감을 유발하고 측정하는 것은 여전히 ​​어렵습니다. 이는 경외심을 어떻게 효과적으로 이끌어낼 수 있는지, 어떤 요인이 경외감 경험과 연관되어 있는지, 바이오센서를 사용하여 경외심을 안정적으로 측정할 수 있는지에 대한 연구 질문을 제기합니다. 이 연구를 위해 우리는 좁은 환경에서 넓은 환경으로 역동적으로 전환되는 10개의 몰입형 가상 현실(VR) 장면을 설계했습니다. 이러한 장면은 경외감이 환경적 특징(추상적, 인간이 만든, 자연), 성격 특성 및 출신 국가와 어떻게 관련되는지 탐구하는 데 사용되었습니다. 우리는 독일, 일본, 요르단 참가자로부터 피부 전도도, 호흡, 자가 보고된 경외심 및 존재감 데이터를 수집했습니다. 우리의 결과는 스스로 보고한 경외감이 국가와 장면 유형에 따라 크게 다르다는 것을 나타냅니다. In particular, a scene depicting outer space elicited the strongest awe. 높은 자기보고적 경외감을 불러일으킨 장면은 또한 더 강한 존재감을 불러일으켰습니다. 그러나 우리는 경외감 평가가 생리적 반응과 상관 관계가 있다는 증거를 찾지 못했습니다. 이러한 발견은 경외감이 자율 신경 각성에 확실하게 반영된다는 가정에 도전하고 문화적, 지각적 맥락의 중요성을 강조합니다. 우리의 연구는 몰입형 VR이 어떻게 경외심을 불러일으키도록 설계할 수 있는지에 대한 새로운 통찰력을 제공하고, 생리적 신호가 아닌 주관적인 보고서가 감정적 영향에 대한 가장 일관된 지표로 남아 있음을 시사합니다.",https://doi.org/10.1109/ISMAR67309.2025.00132,Perception & Cognition,Other,User Study,System / Framework
120,2025,The Cost of Virtuality Switching: Searching for Physical and Virtual Targets in Optical-See-Through Augmented Reality,가상 전환 비용: 광학 투명 증강 현실에서 물리적 및 가상 대상 검색,"As AR applications expand across our daily lives, understanding user interactions within mixed environments-where virtual and physical objects coexist-has become increasingly important. This work investigates human performance and behavior during visual search and selection tasks across three object conditions: (1) virtual objects only, (2) physical objects only, and (3) a combination of virtual and physical objects (Mixed) requiring frequent virtuality switching. We also vary the distance to the target plane while maintaining subtended visual angle: a ‘near’ condition at the headset's focal plane and a ‘far’ condition at a mid-zone action space distance of 3 meters. Results indicate that, while there are some small effects that can be linked back to established display phenomena such as Vergence-Accommodation Conflict, a main cause for performance differences among the object conditions comes from people adjusting their search and selection behavior to the challenges of virtuality switching, resulting in Mixed conditions requiring significant longer completion times, associated with significantly larger head motion, eye movement, and controller movement. Mixed conditions also resulted in significantly lower accuracy for target selection. Virtual-to-Physical transitions took the longest to complete, followed by Physical-to-Virtual transitions, both significantly longer than transitions to targets within the same virtuality. Participants also reported increased Eye Strain, Fatigue, and Task Load with the Mixed conditions. This work provides insight into the complexities of mixed object interaction and presents quantitative assessments of pronounced virtuality switching, with implications for designing effective AR interfaces.","AR 애플리케이션이 일상 생활 전반으로 확장됨에 따라 가상 개체와 실제 개체가 공존하는 혼합 환경 내에서 사용자 상호 작용을 이해하는 것이 점점 더 중요해지고 있습니다. 이 작업은 (1) 가상 객체만, (2) 물리적 객체만, (3) 빈번한 가상 전환이 필요한 가상 및 물리적 객체의 조합(혼합)이라는 세 가지 객체 조건에서 시각적 검색 및 선택 작업 중 인간의 성능과 행동을 조사합니다. 또한 적절한 시각적 각도를 유지하면서 대상 평면까지의 거리를 변경합니다. 즉, 헤드셋 초점면의 '근거리' 조건과 3미터의 중간 영역 작업 공간 거리의 '원거리' 조건입니다. 결과는 Vergence-Accommodation Contribute와 같은 확립된 디스플레이 현상과 다시 연결될 수 있는 몇 가지 작은 효과가 있지만 개체 조건 간의 성능 차이의 주요 원인은 사람들이 가상 전환 문제에 대한 검색 및 선택 동작을 조정하여 훨씬 더 긴 머리 움직임, 눈 움직임 및 컨트롤러 움직임과 관련된 훨씬 더 긴 완료 시간이 필요한 혼합 조건에서 비롯된다는 것을 나타냅니다. 또한 혼합 조건으로 인해 표적 선택의 정확도가 크게 낮아졌습니다. 가상에서 물리적으로의 전환이 완료하는 데 가장 오랜 시간이 걸렸고, 물리적에서 가상으로의 전환이 그 뒤를 이었습니다. 둘 다 동일한 가상 내에서 대상으로 전환하는 것보다 훨씬 더 오래 걸렸습니다. 참가자들은 또한 혼합 조건에서 눈의 피로, 피로 및 작업 부하가 증가했다고 보고했습니다. 이 작업은 혼합 객체 상호 작용의 복잡성에 대한 통찰력을 제공하고 효과적인 AR 인터페이스 설계에 대한 의미와 함께 뚜렷한 가상 전환에 대한 정량적 평가를 제시합니다.",https://doi.org/10.1109/ISMAR67309.2025.00168,Interaction & Input; Display & Optics,Other,Quantitative Experiment,Hardware / Device
121,2025,The Effect of Crowds on Peripersonal Space and Interpersonal Distance in Immersive Virtual Environments,몰입형 가상 환경에서 군중이 개인 주변 공간과 개인 간 거리에 미치는 영향,"Maintaining physical distance is a crucial aspect of social interactions in the real world. This paper investigates how people embodied in self-avatars, manage such spacing in immersive virtual environments. Specifically, we measured how interpersonal distance (IPD) and peripersonal space (PPS) were affected by a surrounding virtual crowd. We designed three crowd density conditions — no crowd, a medium crowd, and a dense crowd — where participants interacted with a virtual human. Additionally, based on the results of a questionnaire, participants were grouped into one of three groups depending on their self-reported comfort level with crowds: some found crowds relaxing, some were indifferent, and some found crowds stressful. Our findings revealed that IPD decreased as crowd density increased, regardless of the participants' groupings. However, PPS exhibited a more nuanced response, with those stressed by a crowd showing heightened sensitivity to crowd density, reflected by larger PPS boundaries, faster reaction times, and less confidence when estimating their PPS boundary. The study also identified significant interactions between crowd density and participants' comfort levels, particularly among the group stressed by a crowd, where heightened stress led to expanded personal space. These results show the critical role of social and environmental factors in shaping spatial perception and behavior in virtual environments.","물리적 거리를 유지하는 것은 현실 세계의 사회적 상호 작용에서 중요한 측면입니다. 본 논문은 자아 아바타에 구현된 사람들이 몰입형 가상 환경에서 그러한 간격을 어떻게 관리하는지 조사합니다. 구체적으로, 우리는 주변 가상 군중이 개인간 거리(IPD)와 개인 주변 공간(PPS)에 어떤 영향을 미치는지 측정했습니다. 우리는 참가자가 가상 ​​인간과 상호 작용하는 세 가지 군중 밀도 조건(군중 없음, 중간 군중, 밀집 군중)을 설계했습니다. 또한, 설문지 결과에 따라 참가자들은 군중에 대한 자체 보고된 편안함 수준에 따라 세 그룹 중 하나로 그룹화되었습니다. 일부는 군중이 편안하다고 답했고, 일부는 무관심했으며, 일부는 군중이 스트레스를 받는다고 답했습니다. 우리의 연구 결과에 따르면 참가자 그룹에 관계없이 군중 밀도가 증가함에 따라 IPD가 감소하는 것으로 나타났습니다. 그러나 PPS는 군중에 의해 스트레스를 받는 사람들이 군중 밀도에 대한 민감도가 높아지는 등 더 미묘한 반응을 보였으며, PPS 경계를 추정할 때 더 큰 PPS 경계, 더 빠른 반응 시간 및 낮은 신뢰도로 반영되었습니다. 이 연구는 또한 군중 밀도와 참가자의 편안함 수준 사이의 중요한 상호 작용을 확인했습니다. 특히 군중으로 인해 스트레스를 받는 그룹에서 스트레스가 높아지면 개인 공간이 확장됩니다. 이러한 결과는 가상 환경에서 공간 인식과 행동을 형성하는 데 있어 사회적, 환경적 요인의 중요한 역할을 보여줍니다.",https://doi.org/10.1109/ISMAR67309.2025.00141,Perception & Cognition,Optical / Display Technology,Questionnaire / Survey,User Study / Empirical Findings
122,2025,The Effect of Haptic Feedback in an Immersive Microsurgery Simulator on VR Training and Skill Transfer,몰입형 미세수술 시뮬레이터의 햅틱 피드백이 VR 훈련 및 기술 이전에 미치는 영향,"With the increasing use of immersive simulators in surgical training, there is an emerging emphasis on providing realistic haptic feedback. Although haptic feedback is recognized as necessary for surgical skill acquisition, its role in transferring those skills to real-life situations still needs to be explored. Our study aims to investigate the impact of haptic feedback on basic microsurgery skill acquisition, transfer, and retention, as well as on the user experience during VR training. An immersive simulator was developed to practice a microgrid task under magnification with and without haptic feedback. A similar physical setup with a binocular microscope was also designed to measure skills transfer to the real world. Thirty-three volunteers ($N=33$) were randomly divided into three participant groups: the haptic feedback group (HG), the no-haptic feedback group (NG), and the control group (CG). All participants performed the task on the physical setup during the pre-post-retention tests. After the pre-test, the first two groups performed six training trials on the immersive simulator, with the HG group receiving haptic feedback while performing the task. The control group did not receive any training. The results show that the HG and NG groups significantly improved their learning curve during training, with no significant differences between them. On the Other hand, the haptic feedback led to significantly higher usability and possibility of examination scores than those without haptic feedback. Finally, all the groups improved their time performance on the physical simulator. In addition, the haptic group participants showed a more significant gain in performance in terms of accuracy and error rates. These findings confirm the effectiveness of immersive environments combined with haptic feedback as a valuable training tool, facilitating the transfer of technical skills to realworld applications. Moreover, the results indicate that haptic feedback can also improve the user experience in immersive surgical simulators.","수술 훈련에서 몰입형 시뮬레이터의 사용이 증가함에 따라 현실적인 촉각 피드백을 제공하는 것이 강조되고 있습니다. 햅틱 피드백은 수술 기술 습득에 필요한 것으로 인식되지만, 이러한 기술을 실제 상황으로 전달하는 역할은 여전히 ​​탐구되어야 합니다. 우리 연구의 목표는 햅틱 피드백이 기본적인 미세 수술 기술 습득, 전달 및 유지뿐만 아니라 VR 교육 중 사용자 경험에 미치는 영향을 조사하는 것입니다. 햅틱 피드백 유무에 관계없이 확대된 마이크로그리드 작업을 연습하기 위해 몰입형 시뮬레이터가 개발되었습니다. 쌍안 현미경을 사용한 유사한 물리적 설정도 실제 세계로의 기술 이전을 측정하기 위해 설계되었습니다. 33명의 자원봉사자($N=33$)를 햅틱 피드백 그룹(HG), 비햅틱 피드백 그룹(NG), 제어 그룹(CG)의 세 가지 참가자 그룹으로 무작위로 나누었습니다. 모든 참가자는 사전 사후 보존 테스트 중에 물리적 설정에 대한 작업을 수행했습니다. 사전 테스트 후 처음 두 그룹은 몰입형 시뮬레이터에서 6번의 훈련 시험을 수행했으며, HG 그룹은 작업을 수행하는 동안 햅틱 피드백을 받았습니다. 통제그룹은 어떠한 교육도 받지 않았습니다. 결과는 HG 그룹과 NG 그룹이 훈련 중에 학습 곡선을 크게 향상시켰으며 두 그룹 사이에는 큰 차이가 없음을 보여줍니다. 반면, 햅틱 피드백은 햅틱 피드백이 없는 경우에 비해 사용성과 시험점수 가능성이 현저히 높은 것으로 나타났다. 마지막으로 모든 그룹은 물리적 시뮬레이터에서 시간 성능을 향상시켰습니다. 또한 햅틱 그룹 참가자들은 정확도와 오류율 측면에서 더 큰 성과를 보였습니다. 이러한 연구 결과는 귀중한 교육 도구로서 햅틱 피드백과 결합된 몰입형 환경의 효과를 확인하여 기술을 실제 응용 프로그램으로 쉽게 이전할 수 있음을 확인합니다. 또한 결과는 햅틱 피드백이 몰입형 수술 시뮬레이터의 사용자 경험을 향상시킬 수도 있음을 나타냅니다.",https://doi.org/10.1109/ISMAR67309.2025.00099,Education & Training; Interaction & Input,Sensor Fusion,User Study; Quantitative Experiment,User Study / Empirical Findings
123,2025,The Honest Virtual Self? Effects of Avatar Personalization and Motor Control on Physiological Responses to Deceptive Behaviours,정직한 가상 자아? 사기 행위에 대한 생리적 반응에 대한 아바타 개인화 및 운동 제어의 효과,"With the advent of social virtual reality (VR), understanding how avatar personalization impacts users' social behaviour and physiological responses in VR is increasingly important. Deceptive behaviour is particularly relevant, as users can often hide their identity behind generic avatars, which can facilitate deception. We investigated whether avatar personalization and motor control impacted physiological reactions in a Detection-of-Deception task. Twenty participants performed the task with different levels of avatar personalization and motor control over the avatar. While personalization did not impact skin conductance responses, it led to a significant decrease in heart rate, which is an established physiological response associated with deceptive behaviour. This effect was exclusive to personalized avatars and not observed in generic avatars. Personalization and motor control led to increased embodiment, body ownership, agency and presence ratings. Overall, personalized avatars can preserve users’ physiological reactions to key social events, and thus enhance the realism of VR simulations.","소셜 가상 현실(VR)의 출현으로 아바타 개인화가 VR에서 사용자의 사회적 행동과 생리적 반응에 어떤 영향을 미치는지 이해하는 것이 점점 더 중요해지고 있습니다. 사용자는 종종 자신의 신원을 일반 아바타 뒤에 숨길 수 있어 사기를 촉진할 수 있으므로 사기 행위는 특히 관련이 있습니다. 우리는 사기 탐지 작업에서 아바타 개인화 및 운동 제어가 생리적 반응에 영향을 미치는지 여부를 조사했습니다. 20명의 참가자는 다양한 수준의 아바타 개인화 및 아바타에 대한 운동 제어를 통해 작업을 수행했습니다. 개인화는 피부 전도 반응에 영향을 미치지 않았지만 사기적인 행동과 관련된 확립된 생리적 반응인 심박수를 크게 감소시켰습니다. 이 효과는 개인화된 아바타에만 국한되었으며 일반 아바타에서는 관찰되지 않았습니다. 개인화 및 운동 제어로 인해 구체화, 신체 소유권, 기관 및 존재 등급이 향상되었습니다. 전반적으로 개인화된 아바타는 주요 사회적 이벤트에 대한 사용자의 생리적 반응을 보존하여 VR 시뮬레이션의 현실감을 향상시킬 수 있습니다.",https://doi.org/10.1109/ISMAR67309.2025.00092,Perception & Cognition,Sensor Fusion,User Study,User Study / Empirical Findings
124,2025,The Immersive Debriefing: Comparative Evaluation of Full and Segmented Redo Methods in Virtual Reality,몰입형 디브리핑: 가상 현실에서 전체 재실행 방법과 분할 재실행 방법의 비교 평가,"This study investigates the impact of two immersive debriefing strategies on learning outcomes in a virtual reality (VR) simulation focused on mobile cybersecurity. The simulation highlights everyday mistakes to raise awareness of risky behaviors in public settings. We developed and evaluated an immersive debriefing system that enables participants to review their performance and re-engage with the scenario through a redo phase. This redo was implemented in two formats: a full scenario redo (F-REDO) and a segmented redo targeting specific moments (S-REDO). We found that both redo formats were equally effective in learning outcomes and user satisfaction, using a mixed-methods approach combining standardized questionnaires (motivation, cognitive load, usability, knowledge retention) and qualitative trainer feedback. However, S-REDO demonstrated greater time efficiency without increasing cognitive load and was perceived by the trainer as more engaging and pedagogically relevant. These results support the integration of personalized, interactive debriefing tools in VR learning environments, particularly in domains requiring targeted remediation and critical decision-making. Abstract Abstract","이 연구는 모바일 사이버 보안에 초점을 맞춘 가상 현실(VR) 시뮬레이션의 학습 결과에 대한 두 가지 몰입형 디브리핑 전략의 영향을 조사합니다. 시뮬레이션은 공공 장소에서 위험한 행동에 대한 인식을 높이기 위해 일상적인 실수를 강조합니다. 우리는 참가자가 자신의 성과를 검토하고 다시 실행 단계를 통해 시나리오에 다시 참여할 수 있도록 하는 몰입형 디브리핑 시스템을 개발하고 평가했습니다. 이 redo는 전체 시나리오 redo(F-REDO)와 특정 순간을 대상으로 한 분할 redo(S-REDO)의 두 가지 형식으로 구현되었습니다. 우리는 표준화된 설문지(동기 부여, 인지 부하, 유용성, 지식 유지)와 질적 트레이너 피드백을 결합한 혼합 방법 접근 방식을 사용하여 두 가지 다시 실행 형식이 모두 학습 결과와 사용자 만족도에 똑같이 효과적이라는 것을 발견했습니다. 그러나 S-REDO는 인지 부하를 증가시키지 않으면서 더 큰 시간 효율성을 보여 주었으며 트레이너는 더 매력적이고 교육학적으로 관련성이 있다고 인식했습니다. 이러한 결과는 VR 학습 환경, 특히 목표 교정 및 중요한 의사 결정이 필요한 영역에서 개인화된 대화형 디브리핑 도구의 통합을 지원합니다. 추상 추상",https://doi.org/10.1109/ISMAR67309.2025.00024,Perception & Cognition; Education & Training,Sensor Fusion,User Study,Algorithm / Method
125,2025,The Impact of Performance-Specific Feedback from a Virtual Coach in a Virtual Reality Exercise Application,가상 현실 운동 애플리케이션에서 가상 코치의 성과별 피드백이 미치는 영향,"Virtual reality (VR) exercise applications are promising tools, e.g., for at-home training and rehabilitation. However, existing applications vary significantly in key design choices such as environments, embodiment, and virtual coaching, making it difficult to derive clear design guidelines. A prominent design choice is the use of embodied virtual coaches, which guide user interaction and provide feedback. In a user study with 76 participants, we investigated how different levels of performance specificity in feedback from an embodied virtual coach affect intermediate factors, such as VR experience, motivation, and coach perception. Participants performed lower-body movement exercises, i.e., Leg Raises and Knee Extensions, commonly used in knee rehabilitation. We found that highly performance-specific feedback led to higher scores compared to medium specificity for perceived realism, as well as the anthropomorphism and sympathy of the virtual coach, but did not affect motivation. Based on our findings, we propose the design suggestion to include precise, performance-specific details when creating feedback for a virtual coach. We observed a descriptive pattern of higher scores in the low specificity condition compared to the medium condition on most measures, which raises the possibility that less specific feedback may, in some cases, be perceived more positively than moderately specific feedback. These findings provide valuable insights into how design choices impact relevant intermediate factors that are crucial for maximizing both workout effectiveness and the quality of the virtual coaching experience.","가상 현실(VR) 운동 애플리케이션은 예를 들어 재택 훈련 및 재활을 위한 유망한 도구입니다. 그러나 기존 애플리케이션은 환경, 구현, 가상 코칭 등 주요 설계 선택이 크게 다르기 때문에 명확한 설계 지침을 도출하기가 어렵습니다. 눈에 띄는 디자인 선택은 사용자 상호 작용을 안내하고 피드백을 제공하는 내장된 가상 코치를 사용하는 것입니다. 76명의 참가자를 대상으로 한 사용자 연구에서 우리는 구현된 가상 코치의 피드백에서 다양한 수준의 성과 특이성이 VR 경험, 동기 부여, 코치 인식과 같은 중간 요인에 어떻게 영향을 미치는지 조사했습니다. 참가자들은 무릎 재활에 흔히 사용되는 하체 운동(레그 올리기, 무릎 확장)을 실시했습니다. 우리는 높은 성과 관련 피드백이 가상 코치의 의인화 및 공감뿐만 아니라 인지된 현실감에 대해 중간 특이도에 비해 더 높은 점수로 이어졌지만 동기에는 영향을 미치지 않는다는 것을 발견했습니다. 우리는 연구 결과를 바탕으로 가상 코치에 대한 피드백을 생성할 때 정확하고 성능별 세부 사항을 포함하는 디자인 제안을 제안합니다. 우리는 대부분의 측정값에서 중간 조건에 비해 낮은 특이성 조건에서 더 높은 점수의 설명 패턴을 관찰했는데, 이는 덜 구체적인 피드백이 어떤 경우에는 적당히 구체적인 피드백보다 더 긍정적으로 인식될 수 있는 가능성을 높입니다. 이러한 결과는 디자인 선택이 운동 효과와 가상 코칭 경험의 품질을 극대화하는 데 중요한 관련 중간 요소에 어떻게 영향을 미치는지에 대한 귀중한 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00110,Perception & Cognition; Interaction & Input,Optical / Display Technology,User Study,User Study / Empirical Findings; Design Guidelines
126,2025,The Relationship Between Time and Distance Perception in Egocentric Target-Selection-Based Teleportation,자기 중심적 목표 선택 기반 순간 이동의 시간과 거리 인식의 관계,"Traveling distances in the real world inherently involves the passage of time, as reaching a desired location is a continuous process. This temporal component, for instance, influences how distances are perceived and estimated. However, in virtual environments, this relationship is often altered or entirely absent. This can make locomotion faster, as is the case with point-and-click teleportation, but depending on the application, it may diminish the user experience or negatively affect distance perception. Instantaneous movement is often chosen due to its lack of negative effects on cybersickness rather than its efficiency. In this research, we explore methods to incorporate temporal elements into different stages of point-and-click teleportation. We investigate how these adjustments influence participants' perception of distance. Our findings show a reduction in distance underestimation when using a time-delayed teleportation method, confirming that temporal factors can impact distance perception in virtual environments.","현실 세계에서 이동 거리는 본질적으로 시간의 흐름을 수반합니다. 원하는 위치에 도달하는 것은 연속적인 과정이기 때문입니다. 예를 들어, 이러한 시간적 구성 요소는 거리가 인식되고 추정되는 방식에 영향을 미칩니다. 그러나 가상 환경에서는 이러한 관계가 변경되거나 완전히 없는 경우가 많습니다. 이렇게 하면 포인트 앤 클릭 순간 이동의 경우처럼 이동 속도가 빨라질 수 있지만 애플리케이션에 따라 사용자 경험이 저하되거나 거리 인식에 부정적인 영향을 미칠 수 있습니다. 즉각적인 이동은 효율성보다는 사이버 멀미에 대한 부정적인 영향이 부족하기 때문에 선택되는 경우가 많습니다. 이 연구에서 우리는 포인트 앤 클릭 순간이동의 다양한 단계에 시간적 요소를 통합하는 방법을 탐구합니다. 우리는 이러한 조정이 참가자의 거리 인식에 어떤 영향을 미치는지 조사합니다. 우리의 연구 결과는 시간 지연 순간 이동 방법을 사용할 때 거리 과소 평가가 감소함을 보여 주며, 시간적 요인이 가상 환경에서 거리 인식에 영향을 미칠 수 있음을 확인합니다.",https://doi.org/10.1109/ISMAR67309.2025.00036,Interaction & Input; Perception & Cognition,Redirected Walking / Locomotion,User Study,Algorithm / Method
127,2025,The State of Replication at IEEE ISMAR and IEEE VR: A Scoping Literature Review (2010 - 2024) and Online Survey,IEEE ISMAR 및 IEEE VR의 복제 상태: 범위 지정 문헌 검토(2010~2024) 및 온라인 설문조사,"A replication attempts to confirm outcomes of earlier research, which is critical in validating and generalizing scientific findings. Yet, its prevalence and practices remain underexplored in Augmented and Virtual Reality (AR/VR) research. To address this, we present a scoping literature review of replication studies within IEEE ISMAR and IEEE VR, spanning 15 years from 2010 to 2024. Our analysis revealed that replication in AR/VR research is rare. Of 2167 total papers reviewed, less than 4 % were identified as replication studies. Of these, conceptual replications were the predominant type (57 %). Most of those were studies in VR (67 %) and within-subject designs (66 %). Complementing the literature survey results, we conducted an online survey with 61 participants about their experiences with replication studies and found that 39 % of them had conducted a replication study. However, limited resources and external motivation hamper the execution of replication studies among these AR/VR researchers. Combining the findings from our literature review and online survey, we discuss the current state of replication research and the factors contributing to its infrequency. We provide recommendations to improve AR/VR research replication practices, focusing on research culture and reporting, and discuss ongoing challenges.","복제는 과학적 발견을 검증하고 일반화하는 데 중요한 초기 연구 결과를 확인하려고 시도합니다. 그러나 증강 및 가상 현실(AR/VR) 연구에서는 그 보급률과 사례가 아직 충분히 탐구되지 않은 상태입니다. 이 문제를 해결하기 위해 우리는 2010년부터 2024년까지 15년에 걸쳐 IEEE ISMAR 및 IEEE VR 내의 복제 연구에 대한 범위 지정 문헌 검토를 제시합니다. 우리의 분석에 따르면 AR/VR 연구에서 복제는 드물다는 것이 밝혀졌습니다. 검토된 총 2,167개의 논문 중 4% 미만이 복제 연구로 확인되었습니다. 이 중 개념적 복제가 주요 유형이었습니다(57%). 그 중 대부분은 VR(67%)과 피험자 내 디자인(66%)에 대한 연구였습니다. 문헌 조사 결과를 보완하여 61명의 참가자를 대상으로 반복 연구 경험에 대한 온라인 설문 조사를 실시한 결과, 그 중 39%가 반복 연구를 수행한 것으로 나타났습니다. 그러나 제한된 자원과 외부 동기로 인해 AR/VR 연구자들의 복제 연구 실행이 방해를 받습니다. 문헌 검토와 온라인 설문 조사 결과를 결합하여 복제 연구의 현재 상태와 빈도가 낮은 요인에 대해 논의합니다. 우리는 연구 문화 및 보고에 중점을 두고 AR/VR 연구 복제 관행을 개선하기 위한 권장 사항을 제공하고 지속적인 과제에 대해 논의합니다.",https://doi.org/10.1109/ISMAR67309.2025.00086,Other,Optical / Display Technology,User Study,Survey / Review
128,2025,Three Techniques for Enhancing Emotional Expression on Embodied Avatar Face in VR,VR에 구현된 아바타 얼굴의 감정 표현을 향상시키는 세 가지 기술,"People often attempt to mask their true emotions through deliberate facial expressions, but such efforts are not always successful. In contrast, emotional concealment could be more easily achieved in Virtual Reality (VR) when appropriate functionalities are available. This study introduces three techniques in VR that enable users to manually adjust emotional facial expression while still reflecting real-time facial tracking results. The Ekman (Ek) technique allows users to select six discrete emotions via button interaction, while the Scrollable-Ekman (SEk) technique extends this by allowing users to scale the intensity of the selected emotion. The Arousal-Valence (AV) technique offers nuanced control within a two-dimensional arousal-valence space. We evaluated these techniques against a baseline condition that synchronizes users' natural facial expressions, focusing on the expression of happiness, sadness, and anger. In most measurements, the Ek and SEk showed better results compared to the baseline and the AV techniques. Notably, the SEk technique was particularly effective in enhancing hedonic quality. During the free-flowing conversation, the most critical factor was the timely and well-synchronized coordination between speech and controlled facial expressions. Participant satisfaction also varied by usage style: those who tried to use the techniques continuously and naturally to mimic real-life communication reported lower satisfaction, while those who used them occasionally for playful or exaggerated expressions tended to report higher satisfaction.","사람들은 종종 고의적인 표정을 통해 자신의 진정한 감정을 가리려고 시도하지만, 그러한 노력이 항상 성공적인 것은 아닙니다. 대조적으로, 적절한 기능을 사용할 수 있는 가상 현실(VR)에서는 감정적 은폐가 더 쉽게 달성될 수 있습니다. 본 연구에서는 실시간 얼굴 추적 결과를 반영하면서 사용자가 감정적인 표정을 수동으로 조정할 수 있는 VR의 세 가지 기술을 소개합니다. Ek(Ek) 기술을 사용하면 사용자는 버튼 상호 작용을 통해 6가지 개별 감정을 선택할 수 있으며, SEk(Scrollable-Ekman) 기술을 사용하면 선택한 감정의 강도를 조정할 수 있습니다. AV(Arousal-Valence) 기술은 2차원 각성-유가 공간 내에서 미묘한 제어를 제공합니다. 우리는 행복, 슬픔, 분노의 표현에 초점을 맞춰 사용자의 자연스러운 표정을 동기화하는 기본 조건에 대해 이러한 기술을 평가했습니다. 대부분의 측정에서 Ek와 SEk는 기준선과 AV 기술에 비해 더 나은 결과를 보여주었습니다. 특히, SEk 기법은 쾌락 품질을 향상시키는 데 특히 효과적이었습니다. 자유로운 대화가 진행되는 동안 가장 중요한 요소는 말과 얼굴 표정의 시기적절하고 잘 동기화된 조정이었습니다. 참가자의 만족도는 사용 스타일에 따라 다양했습니다. 실제 의사소통을 모방하기 위해 지속적으로 자연스럽게 기술을 사용하려는 참가자는 만족도가 더 낮은 것으로 보고된 반면, 장난스럽거나 과장된 표현을 위해 가끔 사용하는 참가자는 더 높은 만족도를 보고하는 경향이 있었습니다.",https://doi.org/10.1109/ISMAR67309.2025.00016,Tracking & Localization,Optical / Display Technology,Quantitative Experiment,Algorithm / Method
129,2025,Through the Expert's Eyes: Exploring Asynchronous Expert Perspectives and Gaze Visualizations in XR,전문가의 눈을 통해: XR에서 비동기식 전문가 관점 및 시선 시각화 탐색,"Transferring knowledge across generations is fundamental to human civilization, yet the challenge of passing on complex practical skills persists. Methods without a physically present instructor, such as videos, often fail to explain complex manual tasks, where spatial and social factors are critical. Technologies such as eXtended Reality and Artificial Intelligence hold the potential to retain expert knowledge and facilitate the creation of tailored, contextualized, and asynchronous explanations regardless of time and place. In contrast to videos, the learner's perspective can be different from the recorded perspective in XR. This paper investigates the impact of asynchronous first- and third-person perspectives and gaze visualizations on efficiency, feeling of embodiment, and connectedness during manual tasks. The empirical results of our study ($\mathrm{N}=36$) show that the first-person perspective is better in quantitative measures and preferred by users. We identify best practices for presenting preserved knowledge and provide guidelines for designing future systems.","여러 세대에 걸쳐 지식을 전달하는 것은 인류 문명의 기본이지만, 복잡한 실무 기술을 전달하려는 과제는 여전히 남아 있습니다. 비디오와 같이 실제로 강사가 없는 방법은 공간적, 사회적 요인이 중요한 복잡한 수동 작업을 설명하지 못하는 경우가 많습니다. 확장 현실(eXtended Reality) 및 인공 지능(AI)과 같은 기술은 전문 지식을 유지하고 시간과 장소에 관계없이 맞춤형, 상황별, 비동기식 설명 생성을 촉진할 수 있는 잠재력을 가지고 있습니다. 비디오와 달리, 학습자의 관점은 XR에서 녹화된 관점과 다를 수 있습니다. 본 논문에서는 비동기식 1인칭 및 3인칭 관점과 시선 시각화가 수동 작업 중 효율성, 구체화된 느낌, 연결성에 미치는 영향을 조사합니다. 우리 연구의 실증적 결과($\mathrm{N}=36$)에 따르면 1인칭 시점이 정량적 측면에서 더 좋고 사용자가 선호하는 것으로 나타났습니다. 우리는 보존된 지식을 제시하기 위한 모범 사례를 식별하고 미래 시스템 설계를 위한 지침을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00136,Interaction & Input; Perception & Cognition,Optical / Display Technology,Quantitative Experiment,User Study / Empirical Findings
130,2025,ThumbShift: Modulating Perceived Object Properties Through Dynamic Thumb Repositioning,ThumbShift: 동적 Thumb 위치 조정을 통해 인식된 개체 속성 조정,"Inspired by the observation that humans naturally adjust finger configurations based on object size and weight, we present ThumbShift, a novel haptic controller that physically moves and rotates the user's thumb to render subtle shifts in finger collaboration and affect whole-hand grasp perception. Unlike prior work focused on grasp type or global haptic feedback, our approach uniquely targets finger collaboration variation through localised, real-time finger repositioning during grasp - enabling Dynamic Digit Positioning (DDP) to modulate haptic and perceptual experience. Results of our user studies show that while size perception changes only slightly, by about 5%, in a two-alternative forced choice (2AFC) task, perceived weight shifts significantly—by approximately 19%—in magnitude estimation tasks. We also report on the influence of mass centre position which extends the weight perception changing ability to about 56%, and how finger force distribution works in altering users' perception. These findings demonstrate that dynamic thumb movement can reconfigure force distribution across the hand and substantially alter haptic experience. By highlighting the underexplored role of digit motion in object perception, our work opens new directions for perception-aware haptic devices in VR, AR, and physical interaction design.","인간이 물체의 크기와 무게에 따라 자연스럽게 손가락 구성을 조정한다는 관찰에서 영감을 받아 사용자의 엄지손가락을 물리적으로 움직이고 회전시켜 손가락 협업의 미묘한 변화를 렌더링하고 손 전체의 파악 인식에 영향을 미치는 새로운 햅틱 컨트롤러인 ThumbShift를 제시합니다. 파악 유형 또는 글로벌 촉각 피드백에 초점을 맞춘 이전 작업과 달리, 당사의 접근 방식은 파악 중 국소화된 실시간 손가락 위치 조정을 통해 손가락 협업 변형을 고유하게 목표로 삼아 DDP(Dynamic Digit Positioning)를 통해 촉각 및 지각 경험을 조절할 수 있습니다. 사용자 연구 결과에 따르면 크기 인식은 2AFC(두 가지 대체 강제 선택) 작업에서 약 5% 정도만 변경되는 반면 크기 추정 작업에서는 인식된 체중 이동이 약 19% 정도 크게 변경되는 것으로 나타났습니다. 또한 무게 인식 변화 능력을 약 56%로 확장하는 질량 중심 위치의 영향과 손가락 힘 분포가 사용자의 인식 변화에 어떻게 작용하는지 보고합니다. 이러한 발견은 역동적인 엄지손가락의 움직임이 손 전체의 힘 분포를 재구성하고 촉각 경험을 실질적으로 변경할 수 있음을 보여줍니다. 물체 인식에서 아직 탐구되지 않은 손가락 동작의 역할을 강조함으로써 우리의 작업은 VR, AR 및 물리적 상호 작용 디자인에서 인식 인식 촉각 장치에 대한 새로운 방향을 제시합니다.",https://doi.org/10.1109/ISMAR67309.2025.00067,Interaction & Input,Haptic / Tactile Feedback,Qualitative Analysis,Algorithm / Method; User Study / Empirical Findings
131,2025,TouchWalker: Real-Time Avatar Locomotion from Touchscreen Finger Walking,TouchWalker: 터치스크린 손가락 걷기를 통한 실시간 아바타 이동,"We present TouchWalker, a real-time system for controlling fullbody avatar locomotion using finger-walking gestures on a touchscreen. The system comprises two main components: TouchWalker-MotionNet, a neural motion generator that synthesizes full-body avatar motion on a per-frame basis from temporally sparse twofinger input, and TouchWalker-UI, a compact touch interface that interprets user touch input to avatar-relative foot positions. Unlike prior systems that rely on symbolic gesture triggers or predefined motion sequences, TouchWalker uses its neural component to generate continuous, context-aware full-body motion on a per-frame basis-including airborne phases such as running, even without input during mid-air steps-enabling more expressive and immediate interaction. To ensure accurate alignment between finger contacts and avatar motion, it employs a MoE-GRU architecture with a dedicated foot-alignment loss. We evaluate TouchWalker in a user study comparing it to a virtual joystick baseline with predefined motion across diverse locomotion tasks. Results show that TouchWalker improves users' sense of embodiment, enjoyment, and immersion.","터치스크린에서 손가락 걷기 제스처를 사용하여 전신 아바타 운동을 제어하는 ​​실시간 시스템인 TouchWalker를 소개합니다. 이 시스템은 두 가지 주요 구성 요소로 구성됩니다. 일시적으로 희박한 두 손가락 입력에서 프레임별로 전신 아바타 모션을 합성하는 신경 모션 생성기인 TouchWalker-MotionNet과 사용자 터치 입력을 아바타 관련 발 위치로 해석하는 컴팩트 터치 인터페이스인 TouchWalker-UI입니다. 기호 제스처 트리거 또는 사전 정의된 모션 시퀀스에 의존하는 이전 시스템과 달리 TouchWalker는 신경 구성 요소를 사용하여 공중 단계 중 입력 없이도 달리기와 같은 공중 단계를 포함하여 프레임별로 지속적이고 상황을 인식하는 전신 모션을 생성하여 더욱 표현적이고 즉각적인 상호 작용을 가능하게 합니다. 손가락 접촉과 아바타 모션 간의 정확한 정렬을 보장하기 위해 전용 발 정렬 손실이 있는 MoE-GRU 아키텍처를 사용합니다. 우리는 사용자 연구를 통해 TouchWalker를 다양한 이동 작업 전반에 걸쳐 사전 정의된 모션이 있는 가상 조이스틱 기준선과 비교하여 평가했습니다. 결과는 TouchWalker가 사용자의 체화감, 즐거움, 몰입감을 향상시키는 것으로 나타났습니다.",https://doi.org/10.1109/ISMAR67309.2025.00068,Interaction & Input; Perception & Cognition,Redirected Walking / Locomotion,User Study,System / Framework
132,2025,Touching the Virtual Dog: Effects of Active and Passive Haptic Feedback on Social Presence and Emotional Bonding in Virtual Pet Interaction,가상 개 만지기: 가상 애완동물 상호작용에서 사회적 존재감과 정서적 유대감에 대한 능동 및 수동 햅틱 피드백의 효과,"Immersive technologies such as virtual and augmented reality (VR/AR) enable people to own and interact with virtual pets, offering an alternative for those who are unable to care for real pets due to spatial, physical, or financial constraints. However, how users form social bonds and emotional connections with virtual pets remains unclear and underexplored. Most virtual pet systems rely primarily on visual and auditory cues, overlooking one critical modality for emotional bonding�touch. Touch plays a fundamental role in emotional communication and social presence, particularly in human-animal interactions. Haptic feedback can fill the role of providing touch cues for users of virtual pet systems. This paper investigates how different haptic feedback modalities can influence emotional communication between users and virtual pets. We compare active haptic feedback�more specifically vibrotactile feedback�delivered through haptic gloves that respond to user interactions with a virtual dog, and passive haptic feedback, provided through a physical plush toy dog that represents the dog's body in physical space. In a within-subjects study with 32 participants, results revealed that passive haptic feedback significantly enhanced emotional bonding, social presence, and perceptions of realism, while active vibrotactile feedback contributed meaningfully in the absence of passive cues, especially in increasing behavioral engagement. These findings offer valuable design insights for emotionally resonant virtual pet systems, suggesting that passive haptics anchor affective realism, while active vibrotactile haptics enhance interactivity and compensate for reduced physical embodiment in immersive environments.","가상 및 증강 현실(VR/AR)과 같은 몰입형 기술을 통해 사람들은 가상 애완동물을 소유하고 상호작용할 수 있으며, 공간적, 물리적, 재정적 제약으로 인해 실제 애완동물을 돌볼 수 없는 사람들에게 대안을 제공합니다. 그러나 사용자가 가상 ​​애완동물과 사회적 유대감 및 정서적 연결을 형성하는 방법은 아직 불분명하고 탐구가 부족한 상태입니다. 대부분의 가상 애완동물 시스템은 주로 시각적, 청각적 단서에 의존하며 정서적 유대감을 위한 중요한 양식 중 하나인 접촉을 간과합니다. 접촉은 정서적 의사소통과 사회적 존재감, 특히 인간과 동물의 상호작용에서 근본적인 역할을 합니다. 햅틱 피드백은 가상 애완동물 시스템 사용자에게 터치 신호를 제공하는 역할을 수행할 수 있습니다. 이 논문에서는 다양한 촉각 피드백 양식이 사용자와 가상 애완동물 간의 감정적 의사소통에 어떻게 영향을 미칠 수 있는지 조사합니다. 우리는 가상 개와의 사용자 상호 작용에 반응하는 햅틱 장갑을 통해 전달되는 능동 햅틱 피드백, 특히 진동 촉각 피드백과 물리적 공간에서 강아지의 몸을 나타내는 실제 플러시 장난감 개를 통해 제공되는 수동 촉각 피드백을 비교합니다. 32명의 참가자를 대상으로 한 피험자 내 연구에서 결과에 따르면 수동적 햅틱 피드백은 정서적 유대감, 사회적 존재감 및 현실감에 대한 인식을 크게 향상시키는 반면, 능동적 진동 촉각 피드백은 수동적 단서가 없는 경우, 특히 행동 참여 증가에 의미 있는 기여를 하는 것으로 나타났습니다. 이러한 연구 결과는 감정적으로 공명하는 가상 애완 동물 시스템에 대한 귀중한 설계 통찰력을 제공하며, 수동적 햅틱이 정서적 사실감을 고정하는 반면, 능동형 진동 촉각 햅틱은 상호 작용을 향상시키고 몰입형 환경에서 감소된 물리적 구현을 ​​보완한다는 것을 시사합니다.",https://doi.org/10.1109/ISMAR67309.2025.00102,Perception & Cognition; Interaction & Input,Haptic / Tactile Feedback,User Study,User Study / Empirical Findings
133,2025,Toward a More Standardized Multi-Directional Tapping Task in VR: the Effect of Target Depth,VR에서 보다 표준화된 다방향 태핑 작업을 향하여: 표적 깊이의 영향,"The multi-directional tapping task has long served as a foundational tool for evaluating pointing performance in human-computer interaction research. However, its transition from 2D interfaces to virtual reality (VR) raises challenges, especially in standardizing target depth. This study explores how target depth influences performance in VR, focusing on two common techniques: Raycasting and Virtual Hand. We conducted two controlled experiments (each with $\mathrm{n}=20$) to isolate depth effects. In Experiment 1, fixed target size led to visual angle (VA) shifts across depths, affecting performance. Both techniques performed best when VA was between $1-4^{\circ}$; Raycasting peaked at 2 m, Virtual Hand at $0.4-0.5 ~\mathrm{m}$. In Experiment 2, we controlled VA to isolate depth itself. Raycasting remained stable beyond 2 m but degraded at close range due to biomechanical limits. Virtual Hand remained sensitive to depth despite fixed VA, but differences were smaller, with throughput unaffected. These results suggest VA should be the primary parameter for standardizing the task in VR. Depth-specific evaluation remains necessary, except for Raycasting beyond 2 m. We provide depth-aware guidelines to improve standardization and comparability while aligning with ISO protocols.",다방향 태핑 작업은 오랫동안 인간-컴퓨터 상호 작용 연구에서 포인팅 성능을 평가하기 위한 기본 도구로 사용되어 왔습니다. 그러나 2D 인터페이스에서 가상 현실(VR)로 전환하면 특히 대상 깊이를 표준화하는 데 어려움이 따릅니다. 이 연구에서는 레이캐스팅(Raycasting)과 가상 손(Virtual Hand)이라는 두 가지 일반적인 기술에 중점을 두고 대상 깊이가 VR 성능에 어떻게 영향을 미치는지 탐구합니다. 깊이 효과를 분리하기 위해 두 가지 통제된 실험(각각 $\mathrm{n}=20$)을 수행했습니다. 실험 1에서는 고정된 목표 크기로 인해 시야각(VA)이 깊이에 따라 이동하여 성능에 영향을 미쳤습니다. 두 기술 모두 VA가 $1-4^{\circ}$ 사이일 때 가장 잘 수행되었습니다. Raycasting은 2m로 정점에 이르렀고 Virtual Hand는 $0.4-0.5 ~\mathrm{m}$로 정점에 달했습니다. 실험 2에서는 깊이 자체를 분리하도록 VA를 제어했습니다. 레이캐스팅은 2m 이상에서는 안정적으로 유지되었지만 생체 역학적 한계로 인해 가까운 거리에서는 성능이 저하되었습니다. Virtual Hand는 고정된 VA에도 불구하고 깊이에 민감하게 반응했지만 처리량에는 영향을 주지 않고 차이가 더 작았습니다. 이러한 결과는 VA가 VR에서 작업을 표준화하기 위한 기본 매개변수여야 함을 시사합니다. 2m를 초과하는 Raycasting을 제외하고 깊이별 평가는 여전히 필요합니다. 우리는 ISO 프로토콜에 맞춰 표준화와 비교 가능성을 향상시키기 위한 깊이 있는 지침을 제공합니다.,https://doi.org/10.1109/ISMAR67309.2025.00047,Interaction & Input,Optical / Display Technology,User Study,Algorithm / Method
134,2025,Transforming Avalanche Rescue Training: Evaluating the Effectiveness of Virtual Reality Training in High-Stakes Environments,눈사태 구조 훈련 혁신: 위험이 큰 환경에서 가상 현실 훈련의 효과 평가,"As winter sports continue to grow in popularity, the need for effective avalanche rescue training becomes increasingly critical. While effective, traditional training methods face challenges such as logistical constraints and limited accessibility. This study investigates the potential of virtual reality (VR) as an alternative approach to enhance avalanche rescue training. We conducted a comparative analysis between traditional ($\mathrm{N}=21$) and VR ($\mathrm{N}=22$) training methods, focusing on participants' knowledge retention, procedural adherence, and confidence in handling real-life scenarios and equipment. Our findings indicate that VR training offers comparable learning outcomes to traditional methods, with the added benefits of increased engagement and accessibility. However, challenges in handling equipment suggest that VR should complement, rather than replace, hands-on training. This research contributes valuable insights into integrating immersive technologies in safety education and the potential for VR to improve training effectiveness in highrisk environments.","겨울 스포츠의 인기가 계속 높아짐에 따라 효과적인 눈사태 구조 훈련의 필요성이 점점 더 중요해지고 있습니다. 기존의 교육 방법은 효과적이기는 하지만 물류적 제약과 제한된 접근성 등의 문제에 직면해 있습니다. 본 연구에서는 눈사태 구조 훈련을 강화하기 위한 대안적 접근 방식으로 가상 현실(VR)의 잠재력을 조사합니다. 우리는 참가자의 지식 보유, 절차 준수, 실제 시나리오 및 장비 처리에 대한 자신감에 중점을 두고 기존($\mathrm{N}=21$) 및 VR($\mathrm{N}=22$) 교육 방법 간의 비교 분석을 수행했습니다. 우리의 연구 결과에 따르면 VR 교육은 참여도와 접근성이 향상되는 추가 이점과 함께 기존 방법과 유사한 학습 결과를 제공합니다. 그러나 장비 취급에 따른 어려움으로 인해 VR은 실습 교육을 대체하기보다는 보완해야 합니다. 이 연구는 안전 교육에 몰입형 기술을 통합하고 VR이 고위험 환경에서 교육 효과를 향상시킬 수 있는 잠재력에 대한 귀중한 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00139,Education & Training,Optical / Display Technology,User Study,Algorithm / Method
135,2025,Unwinding Rotations Reduces VR Sickness in Nonsimulated Immersive Telepresence,풀린 회전은 시뮬레이션되지 않은 몰입형 텔레프레즌스에서 VR 멀미를 줄입니다.,"Immersive telepresence, when a user views the video stream of a 360° camera in a remote environment using a Head Mounted Display (HMD), has great potential to improve the sense of being in a remote environment. In most cases of immersive robotic telepresence, the camera is mounted on a mobile robot which increases the portion of the environment that the remote user can explore. However, robot motions can induce unpleasant symptoms associated with Virtual Reality (VR) sickness, degrading the overall user experience. Previous research has shown that unwinding the rotations of the robot, that is, decoupling the rotations that the camera undergoes due to robot motions from what is seen by the user, can increase user comfort and reduce VR sickness. However, that work considered a virtual environment and a simulated robot. In this work, to test whether the same hypotheses hold when the video stream from a real camera is used, we carried out a user study ($n=36$) in which the unwinding rotations method was compared against coupled rotations in a task completed through a panoramic camera mounted on a robotic arm. Furthermore, within an inspection task which involved translations and rotations in three dimensions, we tested whether unwinding the robot rotations impacted the performance of users. The results show that the users found the unwinding rotations method to be more comfortable and preferable, and that a reduced level of VR sickness can be achieved without a significant impact on task performance.","사용자가 HMD(Head Mounted Display)를 사용하여 원격 환경에서 360° 카메라의 비디오 스트림을 볼 때 몰입형 텔레프레즌스는 원격 환경에 있는 느낌을 향상시킬 수 있는 큰 잠재력을 가지고 있습니다. 몰입형 로봇 텔레프레즌스의 대부분의 경우 카메라는 모바일 로봇에 장착되어 원격 사용자가 탐색할 수 있는 환경의 부분이 늘어납니다. 그러나 로봇 동작은 가상 현실(VR) 멀미와 관련된 불쾌한 증상을 유발하여 전반적인 사용자 경험을 저하시킬 수 있습니다. 이전 연구에서는 로봇의 회전을 풀면, 즉 로봇 동작으로 인해 카메라가 겪는 회전을 사용자가 보는 것과 분리하면 사용자의 편안함을 높이고 VR 멀미를 줄일 수 있는 것으로 나타났습니다. 그러나 그 작업은 가상환경과 시뮬레이션 로봇을 고려한 것이다. 본 연구에서는 실제 카메라의 비디오 스트림을 사용할 때 동일한 가설이 유지되는지 테스트하기 위해 로봇 팔에 장착된 파노라마 카메라를 통해 완료되는 작업에서 풀기 회전 방법을 결합 회전과 비교하는 사용자 연구($n=36$)를 수행했습니다. 또한 3차원의 변환 및 회전이 포함된 검사 작업 내에서 로봇 회전 풀기가 사용자의 성능에 영향을 미치는지 여부를 테스트했습니다. 결과는 사용자가 풀기 회전 방법이 더 편안하고 바람직하다고 생각했으며 작업 성능에 큰 영향을 주지 않고 VR 멀미 수준을 줄일 수 있음을 보여줍니다.",https://doi.org/10.1109/ISMAR67309.2025.00034,Display & Optics; Collaboration & Social,Sensor Fusion,Simulation,Algorithm / Method
136,2025,VR Onboarding Procedures for Multiple Collocated Users: See-Through Tutorials and Group Transitions,함께 배치된 여러 사용자를 위한 VR 온보딩 절차: 투명 튜토리얼 및 그룹 전환,"Making virtual reality (VR) more accessible for novice and nondigital users has largely focused on single-user solutions, such as tutorials, tooltips, and visual guidance cues. While structured onboarding protocols have proven effective, they do not scale well for multiple collocated users due to varying assistance needs, different learning speeds, and guidance preferences. Additionally, the sudden isolation upon entering VR can cause disorientation, particularly in group settings where users may lose awareness of their physical and social surrounding. This paper proposes a novel onboarding framework that extends traditional procedures with four phases: 1) entering video see-through mixed reality after putting on the HMD, 2) interactive tutorials in mixed reality, 3) adaptive group transition strategies into VR, and 4) structured exit mechanisms. In our implementation of the framework, we explore three concepts for group transitions: individual, sequential, and collective. In a user study with 36 participants, we collected feedback on our mixedreality onboarding approach and evaluate the usability, co-presence, agency, and continuity of our three proposed transition techniques and compare it to the common direct transition into virtual reality. Our results indicate a clear preference for onboarding through an additional mixed reality stage and provide valuable insights into advantages and trade-offs of the different group transition strategies.","초보자와 디지털 사용자가 아닌 사용자가 가상 ​​현실(VR)에 더 쉽게 접근할 수 있도록 하기 위해 주로 튜토리얼, 도구 설명, 시각적 안내 신호와 같은 단일 사용자 솔루션에 중점을 두었습니다. 구조화된 온보딩 프로토콜은 효과적인 것으로 입증되었지만 다양한 지원 요구 사항, 다양한 학습 속도 및 안내 기본 설정으로 인해 함께 배치된 여러 사용자에 맞게 확장되지 않습니다. 또한 VR에 들어갈 때 갑자기 고립되면 방향 감각 상실이 발생할 수 있습니다. 특히 사용자가 물리적, 사회적 주변 환경에 대한 인식을 잃을 수 있는 그룹 환경에서는 더욱 그렇습니다. 본 논문에서는 1) HMD 착용 후 비디오 시스루 혼합 현실 진입, 2) 혼합 현실의 대화형 튜토리얼, 3) VR로의 적응형 그룹 전환 전략, 4) 구조화된 종료 메커니즘의 4단계로 기존 절차를 확장하는 새로운 온보딩 프레임워크를 제안합니다. 프레임워크 구현에서 우리는 그룹 전환에 대한 세 가지 개념인 개별, 순차적 및 집단을 탐구합니다. 36명의 참가자를 대상으로 한 사용자 연구에서 우리는 혼합 현실 온보딩 접근 방식에 대한 피드백을 수집하고 제안된 세 가지 전환 기술의 유용성, 공존, 대행사 및 연속성을 평가하고 이를 가상 현실로의 일반적인 직접 전환과 비교했습니다. 우리의 결과는 추가 혼합 현실 단계를 통한 온보딩에 대한 명확한 선호를 나타내며 다양한 그룹 전환 전략의 장점과 장단점에 대한 귀중한 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00043,Display & Optics; Perception & Cognition,Other,User Study,Algorithm / Method
137,2025,"VR as a ""Drop-In"" Well-Being Tool for Knowledge Workers","지식 근로자를 위한 ""드롭인(Drop-In)"" 웰빙 도구로서의 VR","Virtual Reality (VR) is increasingly being used to support workplace well-being, but many interventions focus narrowly on a single activity or goal. Our work explores how VR can meet the diverse physical and mental needs of knowledge workers. We developed Tranquil Loom, a VR app offering stretching, guided meditation, and open exploration across four environments. The app includes an AI assistant that suggests activities based on users emotional states. We conducted a two-phase mixed-methods study: (1) interviews with 10 knowledge workers to guide the apps design, and (2) deployment with 35 participants gathering usage data, well-being measures, and interviews. Results showed increases in mindfulness and reductions in anxiety. Participants enjoyed both structured and open-ended activities, often using the app playfully. While AI suggestions were used infrequently, they prompted ideas for future personalization. Overall, participants viewed VR as a flexible, “dropin” tool, highlighting its value for situational rather than prescriptive well-being support.","가상 현실(VR)은 직장 웰빙을 지원하기 위해 점점 더 많이 사용되고 있지만, 많은 개입은 단일 활동이나 목표에만 초점을 맞추고 있습니다. 우리의 작업은 VR이 지식 근로자의 다양한 신체적, 정신적 요구를 어떻게 충족시킬 수 있는지 탐구합니다. 우리는 네 가지 환경에서 스트레칭, 명상 안내, 개방형 탐색을 제공하는 VR 앱인 Tranquil Loom을 개발했습니다. 이 앱에는 사용자의 감정 상태에 따라 활동을 제안하는 AI 도우미가 포함되어 있습니다. 우리는 2단계 혼합 방법 연구를 수행했습니다. (1) 앱 설계를 안내하기 위해 10명의 지식 근로자와의 인터뷰, (2) 사용 데이터, 웰빙 측정 및 인터뷰를 수집하는 35명의 참가자를 대상으로 배포했습니다. 결과는 마음챙김이 증가하고 불안이 감소한 것으로 나타났습니다. 참가자들은 구조화된 활동과 개방형 활동을 모두 즐겼으며 종종 앱을 장난스럽게 사용했습니다. AI 제안은 자주 사용되지 않았지만 향후 개인화에 대한 아이디어를 촉발했습니다. 전반적으로 참가자들은 VR을 유연한 ""드롭인(dropin)"" 도구로 보았으며 규범적인 웰빙 지원보다는 상황에 따른 가치를 강조했습니다.",https://doi.org/10.1109/ISMAR67309.2025.00127,Rendering & Visualization,Optical / Display Technology,User Study,Algorithm / Method
138,2025,VRTennis: Forehand Training in Virtual Reality with Rule-Based Motion Analysis and Multimodal Feedback,VRTennis: 규칙 기반 모션 분석 ​​및 다중 모드 피드백을 사용한 가상 현실의 포핸드 교육,"Improper technique is detrimental to a tennis player's performance and increases the risk of injury. Regular training routines are necessary but take effort and motivation, and on-court training requires resources that pose additional barriers. Virtual reality paves the way for engaging self-training applications that address these barriers, but without a coach, motion errors may go uncorrected. We present a complementary way of practicing aspects of proper tennis forehand technique in virtual reality, utilizing automated motion analysis for immediate post-action multimodal feedback. Our VR tennis training utilizes motor learning principles and motion analysis to reinforce proper movement patterns and provide timely corrections. We overcome the problem of complex motion analysis by breaking the motion into distinct phases and utilizing the concept of coaching rules. After each shot, auditory and visual feedback is given, focusing on one aspect at a time. The exclusive use of the Meta Quest's partial motion capture poses technical challenges, restricting the set of applicable coaching rules and feedback due to the limited number of tracked joints. However, it allows us to present a more accessible and flexible alternative to on-court training and existing self-training setups. We conducted a user study $(\mathrm{N} = 26)$ following a within-subjects pretest-posttest design to evaluate short-term effects of our VR tennis training. Results demonstrate significant improvements in motivation, performance metrics, and participants' self-reported confidence in technique from the pre- to posttest, suggesting a potential short-term learning effect. Qualitative insights reveal that participants believe our VR training can complement traditional tennis training to a certain degree.","부적절한 기술은 테니스 선수의 경기력에 해롭고 부상 위험을 증가시킵니다. 정기적인 훈련 루틴이 필요하지만 노력과 동기가 필요하며, 코트 내 훈련에는 추가적인 장벽을 초래하는 자원이 필요합니다. 가상 현실은 이러한 장벽을 해결하는 자가 훈련 애플리케이션을 활용할 수 있는 길을 열었지만 코치가 없으면 동작 오류가 수정되지 않을 수 있습니다. 우리는 즉각적인 액션 후 다중 모드 피드백을 위한 자동화된 모션 분석을 활용하여 가상 현실에서 적절한 테니스 포핸드 기술 측면을 연습하는 보완적인 방법을 제시합니다. 우리의 VR 테니스 훈련은 운동 학습 원리와 동작 분석을 활용하여 올바른 움직임 패턴을 강화하고 시기적절한 교정을 제공합니다. 동작을 별개의 단계로 나누고 코칭 규칙 개념을 활용하여 복잡한 동작 분석 문제를 극복합니다. 각 촬영 후에는 한 번에 한 가지 측면에 초점을 맞춰 청각 및 시각적 피드백이 제공됩니다. Meta Quest의 부분 모션 캡처를 독점적으로 사용하면 기술적인 문제가 발생하여 추적되는 관절 수가 제한되어 적용 가능한 코칭 규칙 및 피드백 세트가 제한됩니다. 그러나 이를 통해 우리는 코트 내 훈련과 기존 자가 훈련 설정에 대해 보다 접근하기 쉽고 유연한 대안을 제시할 수 있습니다. 우리는 VR 테니스 훈련의 단기 효과를 평가하기 위해 피험자 내 사전 테스트-사후 테스트 설계에 따라 $(\mathrm{N} = 26)$ 사용자 연구를 수행했습니다. 결과는 사전 테스트부터 사후 테스트까지 동기 부여, 성과 지표 및 기술에 대한 참가자의 자체 보고 자신감이 크게 향상되었음을 보여 주며 잠재적인 단기 학습 효과를 시사합니다. 질적 통찰에 따르면 참가자들은 우리의 VR 훈련이 전통적인 테니스 훈련을 어느 정도 보완할 수 있다고 믿고 있습니다.",https://doi.org/10.1109/ISMAR67309.2025.00115,Education & Training,Other,User Study,User Study / Empirical Findings
139,2025,VRtalk: Real-Time Interactive Intelligent Anime Avatars in Virtual Reality,VRtalk: 가상 현실의 실시간 대화형 지능형 애니메이션 아바타,"The convergence of virtual reality live streaming and AI-driven avatars has emerged as a significant technological trend. However, current integration attempts remain in the proof-of-concept stage, with the primary challenge of automatic interaction system establishment. To build interactive intelligence anime avatars within VR frameworks, we have developed a multimodal interaction architecture centered on dialogue agents, realizing comprehensive understanding, reasoning, and response. Our approach 1).proposes high granularity explicit-implicit understanding and a dual-center switchable reasoning mechanism to support flexible responses. 2).innovates a dual-source animation mechanism for co-speech face-body visualization and a textual command module for supervising crossmodal animation, and 3).enhances expressiveness through mapping persona, content, voice, and motion to anime style. Experimental results demonstrate the state-of-the-art performance of VRtalk, highlighting its practical significance and future potential.","가상현실 라이브 스트리밍과 AI 기반 아바타의 융합이 중요한 기술 트렌드로 떠올랐습니다. 그러나 현재 통합 시도는 개념 증명 단계에 머물러 있으며 자동 상호 작용 시스템 구축이 주요 과제입니다. VR 프레임워크 내에서 대화형 지능 애니메이션 아바타를 구축하기 위해 대화 에이전트 중심의 다중 모드 상호 작용 아키텍처를 개발하여 포괄적인 이해, 추론 및 응답을 실현했습니다. 우리의 접근 방식 1)은 유연한 응답을 지원하기 위해 고도로 세분화된 명시적-암시적 이해와 이중 중심 전환 가능한 추론 메커니즘을 제안합니다. 2) 공동 음성 얼굴-신체 시각화를 위한 듀얼 소스 애니메이션 메커니즘과 크로스모달 애니메이션 감독을 위한 텍스트 명령 모듈을 혁신하고, 3) 페르소나, 콘텐츠, 음성 및 모션을 애니메이션 스타일에 매핑하여 표현력을 향상시킵니다. 실험 결과는 VRtalk의 최첨단 성능을 보여주며 VRtalk의 실질적인 중요성과 미래 잠재력을 강조합니다.",https://doi.org/10.1109/ISMAR67309.2025.00125,Interaction & Input; Rendering & Visualization,Natural Language Processing,Technical Evaluation,System / Framework
140,2025,Virtual Museum Tour Agent: Effects of Responsiveness and Awareness,가상 박물관 투어 에이전트: 반응성과 인식의 효과,"We explored how responsiveness (i.e., the ability to answer questions) and awareness (i.e., the ability to navigate toward the user in the virtual environment) of a virtual agent acting as a tour guide impact study participants in a virtual museum. We followed a 2 (responsiveness: non-responsive vs. responsive virtual agent) $\times 2$ (awareness: unaware vs. aware virtual agent) within-group ($N=29$) study design and conducted a study to explore several variables spanning: agent credibility and intelligence (i.e., perceived intelligence, perceived knowledge), social interaction and presence (i.e., co-presence, rapport), human-likeness (i.e., uncanny valley, anthropomorphism), awareness dimensions (i.e., private, public, and surrounding awareness), desire for future interaction, and behavioral responses (i.e., distance traveled, dwell gazes). We found that the responsive virtual agent positively impacted participants' perceived intelligence, perceived knowledgeability, co-presence, rapport, anthropomorphism, surrounding awareness, desire for future interaction, and dwell gaze on surroundings. However, the awareness factor did not impact our participants. Instead, we found responsiveness $\times$ awareness interaction effects on distance traveled, dwell gaze on the virtual agent, and dwell gaze on surroundings. These findings offer valuable insights into designing intelligent virtual agents that act as museum tour guides, enhancing user experience in virtual museum settings.","우리는 투어 가이드 역할을 하는 가상 에이전트의 반응성(즉, 질문에 답하는 능력)과 인식(즉, 가상 환경에서 사용자를 향해 탐색하는 능력)이 가상 박물관의 연구 참가자에게 어떤 영향을 미치는지 조사했습니다. 우리는 그룹 내($N=29$) 연구 설계에 따라 2(응답성: 무응답 대 반응형 가상 에이전트) $\times 2$(인식: 무인식 대 인식 가상 에이전트)에 따라 에이전트 신뢰성 및 인텔리전스(예: 인지된 지능, 인지된 지식), 사회적 상호 작용 및 존재(예: 공동 존재, 교감), 인간 유사성(예: 기괴함)에 이르는 여러 변수를 탐색하는 연구를 수행했습니다. 계곡, 의인화), 인식 차원(예: 개인, 공공 및 주변 인식), 미래 상호 작용에 대한 욕구 및 행동 반응(예: 이동 거리, 거주 시선). 우리는 반응형 가상 에이전트가 참가자의 인지된 지능, 인지된 지식 능력, 공존, 교감, 의인화, 주변 인식, 미래 상호 작용에 대한 욕구 및 주변에 대한 시선에 긍정적인 영향을 미친다는 것을 발견했습니다. 그러나 인식 요인은 참가자에게 영향을 미치지 않았습니다. 대신, 우리는 이동 거리, 가상 에이전트에 대한 체류 시선, 주변에 대한 체류 시선에 대한 반응성 $\times$ 인식 상호 작용 효과를 발견했습니다. 이러한 연구 결과는 박물관 투어 가이드 역할을 하는 지능형 가상 에이전트를 설계하고 가상 박물관 환경에서 사용자 경험을 향상시키는 데 귀중한 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00021,Interaction & Input; Perception & Cognition,Optical / Display Technology,User Study,User Study / Empirical Findings
141,2025,Virtual Pass-Through: Evaluating 3D Gaussian Splatting as an Alternative to Conventional Video Pass-Through in Static Environments,가상 패스스루: 정적 환경에서 기존 비디오 패스스루의 대안으로 3D 가우스 스플래팅 평가,"Video pass-through (VPT) techniques often cause visual distortions like the ‘telescope eye' effect, which leads to less immersion and spatial awareness in Extended Reality (XR). This study investigates the potential of 3D Gaussian Splatting (3DGS) as a rendering alternative to video pass-through, particularly focusing on its ability to mitigate common perceptual artifacts and enhance immersion. To evaluate the efficacy of 3DGS in an XR context, we conducted a user study comparing two visualization types: (1) conventional VPT and (2) 3DGS-based scene representation. Three distinct scenes with high detail, natural depth perception, and reflective surfaces were included in the study. An empirical study was conducted with a structured questionnaire that inquired about the level of immersion and the impact of the visual stimuli. Our results provide insights into the scalability and hardware considerations for implementing 3DGS in realtime interactive environments. The evaluation results indicated that users with a 3DGS visualization experienced a reduction in visual distortion and an enhancement in the perceived realism of objects. The findings show that 3DGS can serve as an effective alternative to VPT, potentially improving realism and reducing perceptual artifacts in XR applications.","VPT(비디오 패스스루) 기술은 종종 '망원경 눈' 효과와 같은 시각적 왜곡을 유발하여 확장 현실(XR)에서 몰입감과 공간 인식이 저하됩니다. 이 연구에서는 비디오 통과에 대한 렌더링 대안으로서 3D GS(3D Gaussian Splatting)의 잠재력을 조사하고, 특히 일반적인 지각 아티팩트를 완화하고 몰입감을 향상시키는 기능에 중점을 둡니다. XR 맥락에서 3DGS의 효율성을 평가하기 위해 우리는 (1) 기존 VPT와 (2) 3DGS 기반 장면 표현이라는 두 가지 시각화 유형을 비교하는 사용자 연구를 수행했습니다. 높은 세부 묘사, 자연스러운 깊이 인식 및 반사 표면을 갖춘 세 가지 뚜렷한 장면이 연구에 포함되었습니다. 몰입도와 시각적 자극의 영향을 묻는 구조화된 설문지를 사용하여 실증적 연구를 진행하였다. 우리의 결과는 실시간 대화형 환경에서 3DGS를 구현하기 위한 확장성과 하드웨어 고려 사항에 대한 통찰력을 제공합니다. 평가 결과에 따르면 3DGS 시각화를 사용하는 사용자는 시각적 왜곡이 감소하고 객체의 현실감이 향상되는 것을 경험했습니다. 연구 결과에 따르면 3DGS는 VPT의 효과적인 대안이 될 수 있으며 잠재적으로 현실성을 향상하고 XR 애플리케이션에서 지각 아티팩트를 줄일 수 있습니다.",https://doi.org/10.1109/ISMAR67309.2025.00131,Perception & Cognition; Rendering & Visualization,3D Reconstruction,User Study,User Study / Empirical Findings
142,2025,Virtual Roomie: Immersive Layout Co-Design With a Virtual Agent,Virtual Roomie: 가상 에이전트를 사용한 몰입형 레이아웃 공동 설계,"We explored human-virtual agent collaboration during a layout design task in a virtual reality environment. Specifically, we developed a human-in-the-loop optimization-based method that drives the decision-making of the virtual agent. Our algorithm accounts for spatial constraints in furniture placement by evaluating boundary proximity, collision costs, and relationships between furniture items in real-time. It also considers the current configuration of the living room, as modified by the user during the co-design process, to guide the virtual agent's furniture placement decisions in the virtual living room. We compared our method (i.e., optimization) against two other co-design strategies (i.e., template and random) following a within-group ($N=24)$ study design. We found the proposed optimization co-design strategy significantly enhanced perceived collaboration compared to the other two co-design strategies. Moreover, our participants attributed higher private and public awareness to the virtual agent in the optimization condition. In addition, the analysis of the logged data showed that participants placed more furniture items and made fewer corrections when codesigning the living room with a virtual agent whose decisions were based on the optimization method. Our results demonstrate that a virtual agent's behavior, which dynamically responds to user actions while maintaining spatial coherence, creates more effective collaborative experiences in an immersive co-design task.","가상 현실 환경에서 레이아웃 디자인 작업을 진행하는 동안 인간과 가상 에이전트의 협업을 살펴보았습니다. 구체적으로 우리는 가상 에이전트의 의사 결정을 주도하는 인간 참여형(Human-In-The-Loop) 최적화 기반 방법을 개발했습니다. 우리의 알고리즘은 경계 근접성, 충돌 비용 및 가구 항목 간의 관계를 실시간으로 평가하여 가구 배치의 공간적 제약을 설명합니다. 또한 공동 디자인 과정에서 사용자가 수정한 거실의 현재 구성을 고려하여 가상 거실에서 가상 에이전트의 가구 배치 결정을 안내합니다. 우리는 그룹 내($N=24)$ 연구 설계에 따라 우리의 방법(즉, 최적화)을 두 가지 다른 공동 설계 전략(즉, 템플릿 및 무작위)과 비교했습니다. 우리는 제안된 최적화 공동 설계 전략이 다른 두 가지 공동 설계 전략에 비해 인식된 협업을 크게 향상시키는 것을 발견했습니다. 또한 참가자들은 최적화 조건에서 가상 에이전트에 대한 개인 및 대중의 인식이 더 높다고 생각했습니다. 또한, 로깅된 데이터를 분석한 결과, 최적화 방법에 따라 결정을 내리는 가상 에이전트와 함께 거실을 공동설계할 때 참여자들이 가구 항목을 더 많이 배치하고 수정 작업을 더 적게 한 것으로 나타났습니다. 우리의 결과는 공간적 일관성을 유지하면서 사용자 행동에 동적으로 반응하는 가상 에이전트의 동작이 몰입형 공동 설계 작업에서 보다 효과적인 협업 경험을 창출한다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR67309.2025.00026,Perception & Cognition; Collaboration & Social,Other,User Study,Algorithm / Method
143,2025,"Visual and Auditory Feedback of Vibration, and Particle Effects for Enhancing Pseudo-Haptic Button Interaction in VR",VR에서 의사 햅틱 버튼 상호 작용을 향상시키기 위한 진동 및 입자 효과의 시각 및 청각 피드백,"Mid-air interaction in virtual reality (VR) offers a device-free and natural interface, but the absence of physical haptic feedback limits user experience. Pseudo-haptics, which leverage visual and auditory cues to induce the illusion of touch, provide a promising alternative. This study investigates the individual and combined effects of three pseudo-haptic feedback modalities: visual feedback of vibration (with varying frequency, amplitude, and duration), auditory feedback of vibration, and particle effects, on the perception and experience of mid-air button interactions in VR. An experiment with 36 participants subjectively evaluated 54 different virtual button conditions. The results show that visual vibration cues and particle effects significantly enhanced various aspects of the user experience, including haptic illusion, while auditory vibration feedback improved spatiotemporal perception and pragmatic quality. Among the modalities, particle effects had the strongest overall impact, and among the vibration parameters, duration was the most influential. Participant interviews revealed that excessive or incongruent multimodal feedback reduced the effectiveness and appeal of pseudo-haptic sensations. Furthermore, individual differences in preference suggest that personalized feedback settings could be advantageous. Based on the findings, we provide empirical guidelines for designing perceptually optimized pseudo-haptic feedback in VR button interfaces.","가상 현실(VR)의 공중 상호 작용은 장치가 필요 없는 자연스러운 인터페이스를 제공하지만 물리적인 햅틱 피드백이 없기 때문에 사용자 경험이 제한됩니다. 시각 및 청각 단서를 활용하여 촉각의 착각을 유도하는 의사 햅틱은 유망한 대안을 제공합니다. 이 연구에서는 VR의 공중 버튼 상호 작용에 대한 인식 및 경험에 대한 진동의 시각적 피드백(다양한 주파수, 진폭 및 지속 시간 포함), 진동의 청각 피드백 및 입자 효과의 세 가지 의사 햅틱 피드백 방식의 개별 및 결합 효과를 조사합니다. 36명의 참가자를 대상으로 한 실험에서는 54개의 서로 다른 가상 버튼 조건을 주관적으로 평가했습니다. 결과는 시각적 진동 단서와 입자 효과가 햅틱 환상을 포함한 사용자 경험의 다양한 측면을 크게 향상시키는 반면 청각 진동 피드백은 시공간 인식과 실용적인 품질을 향상시키는 것으로 나타났습니다. 양식 중에서 입자 효과가 전체적으로 가장 큰 영향을 미쳤으며, 진동 매개변수 중에서 지속 시간이 가장 큰 영향을 미쳤습니다. 참가자 인터뷰에서는 과도하거나 부적합한 다중 모드 피드백이 의사 햅틱 감각의 효과와 매력을 감소시키는 것으로 나타났습니다. 또한 선호도의 개인차는 개인화된 피드백 설정이 유리할 수 있음을 시사합니다. 연구 결과를 바탕으로 VR 버튼 인터페이스에서 지각적으로 최적화된 의사 햅틱 피드백을 설계하기 위한 경험적 지침을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00109,Interaction & Input,Haptic / Tactile Feedback,User Study,User Study / Empirical Findings
144,2025,Weighted-Area Based Alignment Metric for Redirected Walking in VR,VR에서 방향 전환된 걷기를 위한 가중 영역 기반 정렬 측정법,"Redirected walking allows users to naturally walk in virtual environments that are larger than their physical surroundings by subtly steering them around real-world obstacles. Alignment-based controllers, such as Vis.-Poly. and Alignment-Optimized, have demonstrated potential in minimizing collision occurrences. However, the assessment of shape similarity using polygonal area has weakened the performance of these controllers since area similarity fails to differentiate between regions with same area but different shapes. This paper presents a weighted area method for approximating the area of a walkable region in front of the user. The area of a walkable region can be estimated by accumulating the subdivided unit areas that intersect the region. These unit areas are assigned weights based on the user's location and orientation. Moreover, Artificial potential fields (APF) value at the center of each unit can be easily incorporated into the weight computation. In a simulation-based evaluation, the Alignment-Optimized controller, implemented using the weighted area method, both without and with the APF value-dubbed as Alignment-Optimized-WA-(AD) and Alignment-Optimized-WA-(AD+APF), respectively-is compared against top-notch controllers including Alignment-Optimized, Vis.Poly., ARC, Alignment+APF-Optimized, APF-S2T, and APF-S2G. The results reveal that both Alignment-Optimized-WA-(AD) and Alignment-Optimized-WA-(AD+APF) exceed the performance of these cutting-edge controllers.","리디렉션 걷기를 사용하면 사용자는 실제 장애물 주위로 미묘하게 방향을 조정하여 실제 주변보다 더 큰 가상 환경에서 자연스럽게 걸을 수 있습니다. Vis.-Poly와 같은 정렬 기반 컨트롤러. 및 Alignment-Optimized는 충돌 발생을 최소화하는 잠재력을 입증했습니다. 그러나 다각형 영역을 사용하여 모양 유사성을 평가하면 영역 유사성이 동일한 영역이지만 모양이 다른 영역을 구별하지 못하기 때문에 이러한 컨트롤러의 성능이 약화되었습니다. 본 논문에서는 사용자 앞의 보행 가능한 영역의 면적을 근사화하기 위한 가중치 적용 영역 방법을 제시합니다. 보행 가능한 지역의 면적은 해당 지역과 교차하는 세분화된 단위 면적을 누적하여 추정할 수 있습니다. 이러한 단위 영역에는 사용자의 위치와 방향에 따라 가중치가 할당됩니다. 또한 각 장치 중앙의 인공 전위장(APF) 값을 가중치 계산에 쉽게 통합할 수 있습니다. 시뮬레이션 기반 평가에서는 각각 Alignment-Optimized-WA-(AD) 및 Alignment-Optimized-WA-(AD+APF)로 명명된 APF 값이 없는 경우와 없는 경우 모두 가중치 영역 방법을 사용하여 구현된 Alignment-Optimized 컨트롤러를 Alignment-Optimized, Vis.Poly., ARC, Alignment+APF-Optimized, APF-S2T, 및 APF-S2G. 결과는 Alignment-Optimized-WA-(AD)와 Alignment-Optimized-WA-(AD+APF)가 모두 이러한 최첨단 컨트롤러의 성능을 초과한다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR67309.2025.00023,Interaction & Input,Sensor Fusion,Simulation,Algorithm / Method
145,2025,What Makes Object Referencing Clear? Multimodal Strategies for Shared Understanding in XR Collaboration,객체 참조를 명확하게 만드는 것은 무엇입니까? XR 협업의 이해 공유를 위한 다중 모드 전략,"Effective shared object referencing is essential for collaboration in multi-user Extended Reality (XR) environments. While prior work has explored gaze, pointing, and speech, most studies focused on individual selection tasks, leaving a gap in understanding how these modalities support mutual comprehension. This study investigates optimal combinations of gaze, pointing, and speech under varying distances ($1 ~\mathrm{m}, 8 ~\mathrm{m}, 15 ~\mathrm{m}$), viewpoints, and object arrangements. Results show that multimodal combinations significantly improved referencing accuracy and reduced cognitive load compared to gaze-only interaction. Pointing gestures were highly effective in resolving viewpoint discrepancies. Speech provided high referencing accuracy but increased selection time, while gaze enabled fast referencing with reduced accuracy at longer distances. The integration of all three modalities achieved the best balance of speed, accuracy, and cognitive load. These findings offer practical guidance for designing XR systems that support clear and efficient shared referencing across diverse collaborative conditions.","다중 사용자 확장 현실(XR) 환경에서의 협업에는 효과적인 공유 객체 참조가 필수적입니다. 이전 작업에서는 시선, 포인팅 및 음성을 탐구했지만 대부분의 연구는 개별 선택 작업에 ​​중점을 두어 이러한 양식이 상호 이해를 지원하는 방법을 이해하는 데 공백을 남겼습니다. 본 연구에서는 다양한 거리($1 ~\mathrm{m}, 8 ~\mathrm{m}, 15 ~\mathrm{m}$), 시점 및 개체 배열에서 시선, 포인팅 및 음성의 최적 조합을 조사합니다. 결과는 다중 모드 조합이 시선 전용 상호 작용에 비해 참조 정확도를 크게 향상시키고 인지 부하를 감소시키는 것으로 나타났습니다. 포인팅 제스처는 관점 불일치를 해결하는 데 매우 효과적이었습니다. 음성은 높은 참조 정확도를 제공했지만 선택 시간이 증가한 반면 시선은 더 먼 거리에서 정확도가 떨어지면서 빠른 참조를 가능하게 했습니다. 세 가지 방식을 모두 통합하여 속도, 정확성, 인지 부하의 균형이 가장 잘 맞춰졌습니다. 이러한 연구 결과는 다양한 협업 조건에서 명확하고 효율적인 공유 참조를 지원하는 XR 시스템을 설계하기 위한 실용적인 지침을 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00014,Interaction & Input; Collaboration & Social,Optical / Display Technology,Quantitative Experiment,User Study / Empirical Findings
146,2025,What if Virtual Agents Had Scents? Users' Judgments of Virtual Agent Personality and Appeals in Encounters,가상 에이전트에 향기가 있다면 어떨까요? 가상 에이전트 성격에 대한 사용자의 판단과 만남에 대한 호소,"Incorporating multi-sensory cues into Virtual Reality (VR) can significantly enhance user experiences, mirroring the multi-sensory interactions we encounter in the real-world. Olfaction plays a crucial role in shaping impressions when engaging with others. This study examines how non-verbal cues from virtual agents-specifically olfactory cues, emotional expressions, and gender-influence user perceptions during encounters with virtual agents. Our findings indicate that in unscented, woodsy, and floral scent conditions, participants primarily relied on visually observable cues to form their impressions of virtual agents. Positive emotional expressions, conveyed through facial expressions and gestures, contributed to more favorable impressions, with this effect being stronger for the female agent than the male agent. However, in the unpleasant scent condition, participants consistently formed negative impressions, which overpowered the influence of emotional expressions and gender, suggesting that aversive olfactory stimuli can detrimentally impact user perceptions. Our results emphasize the importance of carefully selecting olfactory stimuli when designing immersive and engaging VR interactions. Finally, we present our findings and outline future research directions for effectively integrating olfactory cues into virtual agents.","다중 감각 단서를 가상 현실(VR)에 통합하면 현실 세계에서 접하는 다중 감각 상호 작용을 반영하여 사용자 경험을 크게 향상시킬 수 있습니다. 후각은 다른 사람과 소통할 때 인상을 형성하는 데 중요한 역할을 합니다. 이 연구에서는 가상 에이전트의 비언어적 단서, 특히 후각 단서, 감정 표현 및 성별이 가상 에이전트와의 만남 중 사용자 인식에 어떻게 영향을 미치는지 조사합니다. 우리의 연구 결과에 따르면 무향, 나무향, 꽃향기 조건에서 참가자들은 가상 에이전트에 대한 인상을 형성하기 위해 주로 시각적으로 관찰 가능한 단서에 의존했습니다. 얼굴 표정과 몸짓을 통해 전달되는 긍정적인 감정 표현은 더 호의적인 인상에 기여했으며, 이러한 효과는 남성 에이전트보다 여성 에이전트에게 더 강하게 나타났습니다. 그러나 불쾌한 냄새 조건에서는 참가자들이 지속적으로 부정적인 인상을 형성하여 감정 표현과 성별의 영향을 압도하여 혐오스러운 후각 자극이 사용자 인식에 해로운 영향을 미칠 수 있음을 시사합니다. 우리의 결과는 몰입감 있고 매력적인 VR 상호 작용을 디자인할 때 후각 자극을 신중하게 선택하는 것이 중요하다는 점을 강조합니다. 마지막으로, 후각 신호를 가상 에이전트에 효과적으로 통합하기 위한 연구 결과를 제시하고 향후 연구 방향의 개요를 설명합니다.",https://doi.org/10.1109/ISMAR67309.2025.00028,Interaction & Input,Sensor Fusion,User Study,User Study / Empirical Findings
147,2025,When Senses Collide: Investigating Modality Congruence and Interference Between Task and Notification in Augmented Reality,감각이 충돌할 때: 증강 현실에서 작업과 알림 간의 양식 일치 및 간섭 조사,"As augmented reality (AR) technologies become more integrated into everyday tasks, the design of notification systems that minimize disruption while enhancing user awareness is increasingly important. This study investigates how crossmodal interference between notification modality (visual, audio, tactile) and primary task modality (also visual, auditory, tactile) affects user perception and performance in AR environments. Through a controlled user study ($N=36$), participants engaged in modality-specific pattern recognition tasks while responding to spatial directional notifications delivered via different sensory channels. By analyzing notification awareness time, reaction time, task accuracy, and subjective measures, such as cognitive load and user preference, the study reveals how congruency or mismatch between task and notification modalities can either facilitate or hinder attention redirection and multi-tasking efficiency. The findings contribute to the design of adaptive AR notification systems that are less intrusive and better aligned with users' perceptual and cognitive states.","증강 현실(AR) 기술이 일상 작업에 더욱 통합됨에 따라 사용자 인식을 향상시키면서 방해를 최소화하는 알림 시스템의 설계가 점점 더 중요해지고 있습니다. 본 연구에서는 알림 양식(시각, 청각, 촉각)과 기본 작업 양식(시각, 청각, 촉각) 간의 교차 모드 간섭이 AR 환경에서 사용자 인식과 성능에 어떤 영향을 미치는지 조사합니다. 통제된 사용자 연구($N=36$)를 통해 참가자들은 다양한 감각 채널을 통해 전달된 공간 방향 알림에 반응하면서 양식별 패턴 인식 작업에 참여했습니다. 알림 인식 시간, 반응 시간, 작업 정확도, 인지 부하 및 사용자 선호도와 같은 주관적 측정값을 분석함으로써 이 연구는 작업과 알림 양식 간의 일치 또는 불일치가 어떻게 주의 방향 전환 및 멀티태스킹 효율성을 촉진하거나 방해할 수 있는지를 보여줍니다. 이번 연구 결과는 덜 방해적이고 사용자의 지각 및 인지 상태에 더 잘 맞는 적응형 AR 알림 시스템의 설계에 기여합니다.",https://doi.org/10.1109/ISMAR67309.2025.00117,Perception & Cognition; Audio & Sound,Haptic / Tactile Feedback,User Study,User Study / Empirical Findings
148,2025,When is Self-Gaze Helpful? Examining Uni- vs Bi-Directional Gaze Visualization in Collocated AR Tasks,Self-Gaze는 언제 도움이 되나요? 함께 배치된 AR 작업에서 단방향 및 양방향 시선 시각화 검사,"Shared-gaze visualizations (SGVs) in augmented reality enable collaborators to share focus and intentions through gaze interactions. Most prior research has examined bi-directional visualizations, where both users see their own and their partner's gaze, to provide feedback on how their gaze is communicated to their partner. However, bi-directional SGV approaches are largely based on research for remote collaboration. In collocated settings, bi-directional SGVs can obstruct views and cause distractions. Additionally, collocated applications differ from remote ones. We propose that if eye-tracking is well-calibrated, bi-directional visualizations may be unnecessary in collocated settings. To explore this, we conducted a user study comparing perceptions of uni- and bi-directional gaze visualizations in a virtual collaborative sorting task. Our results suggest that self-gaze may not always be necessary for users; however, there are cases in which self-gaze helps them feel more confident in the task. We offer a deeper understanding for future collaborative gaze interaction systems.",증강 현실의 공유 시선 시각화(SGV)를 통해 공동 작업자는 시선 상호 작용을 통해 초점과 의도를 공유할 수 있습니다. 대부분의 이전 연구에서는 사용자 모두 자신과 파트너의 시선을 보는 양방향 시각화를 조사하여 시선이 파트너에게 전달되는 방식에 대한 피드백을 제공했습니다. 그러나 양방향 SGV 접근 방식은 주로 원격 협업에 대한 연구를 기반으로 합니다. 함께 배치된 환경에서는 양방향 SGV가 시야를 방해하고 주의를 산만하게 할 수 있습니다. 또한 함께 배치된 애플리케이션은 원격 애플리케이션과 다릅니다. 시선 추적이 잘 보정된 경우 공동 배치된 설정에서는 양방향 시각화가 불필요할 수 있다고 제안합니다. 이를 탐색하기 위해 우리는 가상 협업 정렬 작업에서 단방향 및 양방향 시선 시각화에 대한 인식을 비교하는 사용자 연구를 수행했습니다. 우리의 결과는 사용자에게 자기 응시가 항상 필요한 것은 아닐 수 있음을 시사합니다. 그러나 자기 응시가 작업에 대해 더 자신감을 갖는 데 도움이 되는 경우가 있습니다. 우리는 미래의 협업 시선 상호 작용 시스템에 대한 더 깊은 이해를 제공합니다.,https://doi.org/10.1109/ISMAR67309.2025.00096,Collaboration & Social; Interaction & Input,Other,User Study,User Study / Empirical Findings
149,2025,Why Slow Feels Fast and Fast Feels Slow: Evaluating and Predicting Speed Misperception,느린 것이 빠르게 느껴지는 이유와 빠른 것이 느리게 느껴지는 이유: 속도에 대한 오해 평가 및 예측,"Human perception of speed is largely driven by visual cues. However, our subjective estimations of speed are influenced by several factors that can lead to deceptive cues and speed misperception. While some prior studies have explored individual effects on speed perception, such as contrast, spatial frequency, and temporal frequency, their combined influence remains underexamined, particularly in immersive VR environments. In this work, we systematically investigate the influence and interplay of four visual factors—contrast, spatial frequency, temporal frequency, and eccentricity—on human perception of speed. To this end, we conduct a psychophysical study measuring subjective speed judgments across controlled stimuli and reveal significant perceptual biases induced by these factors. Based on our collected data, we learn a model to predict the underestimation or overestimation of perceived speed from visual scene properties. We apply and validate our findings in three immersive environments and demonstrate their influence on common VR scenarios. Finally, we discuss how understanding the factors that shape speed perception can drive the design of perceptually aligned virtual environments, with potential future applications such as correcting speed misperception and conceivably mitigating visual-vestibular conflicts by modulating perceived speed.","속도에 대한 인간의 인식은 주로 시각적 단서에 의해 좌우됩니다. 그러나 속도에 대한 우리의 주관적인 추정은 기만적인 단서와 속도에 대한 잘못된 인식으로 이어질 수 있는 여러 요인의 영향을 받습니다. 일부 이전 연구에서는 대비, 공간 주파수 및 시간 주파수와 같은 속도 인식에 대한 개별 영향을 조사했지만, 특히 몰입형 VR 환경에서 이들의 결합된 영향은 과소평가된 상태로 남아 있습니다. 이 연구에서 우리는 인간의 속도 인식에 대한 네 가지 시각적 요소인 대비, 공간 주파수, 시간 주파수 및 편심의 영향과 상호 작용을 체계적으로 조사합니다. 이를 위해 우리는 통제된 자극에 대한 주관적인 속도 판단을 측정하는 정신물리학적 연구를 수행하고 이러한 요인에 의해 유발된 중요한 지각 편향을 밝힙니다. 수집된 데이터를 기반으로 시각적 장면 속성으로부터 인지 속도의 과소평가 또는 과대평가를 예측하는 모델을 학습합니다. 우리는 세 가지 몰입형 환경에서 연구 결과를 적용하고 검증하며 일반적인 VR 시나리오에 미치는 영향을 보여줍니다. 마지막으로, 속도 인식을 형성하는 요소를 이해하는 것이 인식적으로 정렬된 가상 환경의 설계를 어떻게 추진할 수 있는지 논의하고, 속도 인식 오류를 수정하고 인식된 속도를 조절하여 시각-전정 충돌을 완화하는 등 잠재적인 미래 응용 프로그램을 사용합니다.",https://doi.org/10.1109/ISMAR67309.2025.00114,Perception & Cognition,Sensor Fusion,Simulation,Algorithm / Method; User Study / Empirical Findings
150,2025,Will you be Aware? Eye Tracking-Based Modeling of Situational Awareness in Augmented Reality,당신은 인식할 것인가? 증강 현실에서 상황 인식의 시선 추적 기반 모델링,"Augmented Reality (AR) systems, while enhancing task performance through real-time guidance, pose risks of inducing cognitive tunneling—a hyperfocus on virtual content that compromises situational awareness (SA) in safety-critical scenarios. This paper investigates SA in AR-guided cardiopulmonary resuscitation (CPR), where responders must balance effective compressions with vigilance to unpredictable hazards (e.g., patient vomiting). We developed an AR app on a Magic Leap 2 that overlays real-time CPR feedback (compression depth and rate) and conducted a user study with simulated unexpected incidents (e.g., bleeding) to evaluate SA, in which SA metrics were collected via observation and questionnaires administered during freeze-probe events. Eye tracking analysis revealed that higher SA levels were associated with greater saccadic amplitude and velocity, and with reduced proportion and frequency of fixations on virtual content. To predict SA, we propose FixGraphPool, a graph neural network that structures gaze events (fixations, saccades) into spatiotemporal graphs, effectively capturing dynamic attentional patterns. Our model achieved 83.0 % accuracy ($\mathrm{F} 1=81.0 \%$), outperforming feature-based machine learning and state-of-the-art time-series models by leveraging domain knowledge and spatial-temporal information encoded in ET data. These findings demonstrate the potential of eye tracking for SA modeling in AR and highlight its utility in designing AR systems that ensure user safety and situational awareness.","증강 현실(AR) 시스템은 실시간 안내를 통해 작업 성능을 향상시키는 동시에 인지 터널링(안전이 중요한 시나리오에서 상황 인식(SA)을 손상시키는 가상 콘텐츠에 지나치게 집중하는 현상)을 유발할 위험이 있습니다. 이 논문에서는 응답자가 예측할 수 없는 위험(예: 환자 구토)에 대한 경계와 효과적인 압박의 균형을 맞춰야 하는 AR 유도 심폐소생술(CPR)의 SA를 조사합니다. 우리는 실시간 CPR 피드백(압박 깊이 및 속도)을 오버레이하는 Magic Leap 2에서 AR 앱을 개발하고 시뮬레이션된 예상치 못한 사건(예: 출혈)으로 사용자 연구를 수행하여 SA를 평가했습니다. 여기서 SA 지표는 동결 프로브 이벤트 중에 시행된 관찰 및 설문지를 통해 수집되었습니다. 안구 추적 분석에 따르면 SA 수준이 높을수록 단속성 진폭 및 속도가 증가하고 가상 콘텐츠에 대한 고정 비율 및 빈도가 감소하는 것으로 나타났습니다. SA를 예측하기 위해 시선 이벤트(고정, 단속운동)를 시공간 그래프로 구성하여 동적 주의 패턴을 효과적으로 포착하는 그래프 신경망인 FixGraphPool을 제안합니다. 우리 모델은 83.0%의 정확도($\mathrm{F} 1=81.0 \%$)를 달성했으며, ET 데이터에 인코딩된 도메인 지식과 시공간 정보를 활용하여 기능 기반 기계 학습 및 최첨단 시계열 모델을 능가했습니다. 이러한 연구 결과는 AR에서 SA 모델링을 위한 시선 추적의 잠재력을 보여주고 사용자 안전과 상황 인식을 보장하는 AR 시스템 설계에 있어 시선 추적의 유용성을 강조합니다.",https://doi.org/10.1109/ISMAR67309.2025.00051,Perception & Cognition; Interaction & Input,Eye / Gaze Tracking,Questionnaire / Survey,Algorithm / Method; User Study / Empirical Findings
151,2025,X-Mask: Improving Soft-Edge Occlusion in Optical See-Through Displays with Cross-Shaped Pinholes,X-Mask: 십자형 핀홀이 있는 광학 투명 디스플레이의 소프트 에지 폐색 개선,"Placing a transparent liquid crystal display (LCD) into the light path is a simple approach to create occlusion-capable optical seethrough head-mounted displays (OST-HMDs) that suffers from defocused (soft-edge) occlusion where the mask leakage partially occludes surrounding content as well. Creating a focused (hard-edge) occlusion that does not suffer from mask leakage requires complicated, bulky optical setups. We present X-Mask, a pinhole-arraybased OST-HMD that creates a sharp occlusion mask without the need for a bulky setup requiring only two transparent LCD layers. By rendering a pinhole array on the layer closer to the user's eye, our system functions as a programmable aperture layer that extends the effective depth of field and improves the sharpness of the occlusion mask rendered on the second LCD layer. Utilizing a conventional circular pinhole would result in non-uniform brightness and contrast. By changing the pinhole shape to a cross enables nearoptimal retinal tiling with reduced overlaps and gaps. To accommodate pupil size variation, focus distance, and gaze direction, our system design allows for gaze-contingent adjustment of both LCD layers. We validate X-Mask in simulations and a physical prototype showing improved occlusion sharpness and visual uniformity.","투명 LCD(액정 디스플레이)를 광 경로에 배치하는 것은 마스크 누출로 인해 주변 콘텐츠도 부분적으로 가려지는 초점이 흐려진(소프트 에지) 폐색 문제를 겪는 폐색 가능 광학 투명 머리 장착형 디스플레이(OST-HMD)를 만드는 간단한 접근 방식입니다. 마스크 누출이 발생하지 않는 집중된(하드 에지) 폐색을 생성하려면 복잡하고 부피가 큰 광학 설정이 필요합니다. 우리는 두 개의 투명 LCD 레이어만 필요한 부피가 큰 설정 없이 선명한 폐색 마스크를 생성하는 핀홀 배열 기반 OST-HMD인 X-Mask를 소개합니다. 사용자의 눈에 더 가까운 레이어에 핀홀 어레이를 렌더링함으로써 우리 시스템은 유효 피사계 심도를 확장하고 두 번째 LCD 레이어에 렌더링된 폐색 마스크의 선명도를 향상시키는 프로그래밍 가능한 조리개 레이어로 기능합니다. 기존의 원형 핀홀을 활용하면 밝기와 대비가 균일하지 않게 됩니다. 핀홀 모양을 십자 모양으로 변경하면 겹침과 간격이 줄어들면서 거의 최적의 망막 타일링이 가능해집니다. 동공 크기 변화, 초점 거리 및 시선 방향을 수용하기 위해 우리 시스템 설계에서는 두 LCD 레이어의 시선 조건에 따른 조정이 가능합니다. 향상된 교합 선명도와 시각적 균일성을 보여주는 시뮬레이션과 물리적 프로토타입에서 X-Mask를 검증합니다.",https://doi.org/10.1109/ISMAR67309.2025.00077,Display & Optics; Interaction & Input,Sensor Fusion,Simulation,System / Framework
152,2025,XR Reality Check: What Commercial Devices Deliver for Spatial Tracking,XR 현실 점검: 상업용 장치가 공간 추적을 위해 제공하는 것,"Inaccurate spatial tracking in extended reality (XR) devices leads to virtual object jitter, misalignment, and user discomfort, fundamentally limiting immersive experiences and natural interactions. In this work, we introduce a novel testbed that enables simultaneous, synchronized evaluation of multiple XR devices under identical environmental and kinematic conditions. Leveraging this platform, we present the first comprehensive empirical benchmarking of five state-of-the-art XR devices across 16 diverse scenarios. Our results reveal substantial intra-device performance variation, with individual devices exhibiting up to 101% increases in error when operating in featureless environments. We also demonstrate that tracking accuracy strongly correlates with visual conditions and motion dynamics. We also observe significant inter-device disparities, with performance differences of up to $2.8 \times$, which are closely linked to hardware specifications such as sensor configurations and dedicated processing units. Finally, we explore the feasibility of substituting a motion capture system with the Apple Vision Pro as a practical ground truth reference. While the Apple Vision Pro delivers highly accurate relative pose error estimates ($R^{2}=0.830$), its absolute pose error estimation remains limited ($R^{2}=0.387$), highlighting both its potential and its constraints for rigorous XR evaluation. This work establishes the first standardized framework for comparative XR tracking evaluation, providing the research community with reproducible methodologies, comprehensive benchmark datasets, and open-source tools that enable systematic analysis of tracking performance across devices and conditions, thereby accelerating the development of more robust spatial sensing technologies for XR systems.","확장 현실(XR) 장치의 부정확한 공간 추적은 가상 물체의 흔들림, 정렬 불량 및 사용자 불편을 초래하여 몰입형 경험과 자연스러운 상호 작용을 근본적으로 제한합니다. 이 연구에서는 동일한 환경 및 운동학적 조건에서 여러 XR 장치를 동시에 동기화하여 평가할 수 있는 새로운 테스트베드를 소개합니다. 이 플랫폼을 활용하여 우리는 16가지 다양한 시나리오에 걸쳐 5가지 최첨단 XR 장치에 대한 최초의 포괄적인 경험적 벤치마킹을 제시합니다. 우리의 결과는 기능이 없는 환경에서 작동할 때 개별 장치의 오류가 최대 101% 증가하는 등 상당한 장치 내 성능 변화를 보여줍니다. 또한 추적 정확도가 시각적 조건 및 모션 역학과 밀접한 관련이 있음을 보여줍니다. 또한 센서 구성 및 전용 처리 장치와 같은 하드웨어 사양과 밀접하게 연결되어 있는 최대 $2.8 \times$의 성능 차이로 상당한 장치 간 차이를 관찰했습니다. 마지막으로, 실제적인 실제 참조로서 모션 캡처 시스템을 Apple Vision Pro로 대체하는 타당성을 탐구합니다. Apple Vision Pro는 매우 정확한 상대 포즈 오류 추정($R^{2}=0.830$)을 제공하지만 절대 포즈 오류 추정은 여전히 ​​제한되어($R^{2}=0.387$) 엄격한 XR 평가에 대한 잠재력과 제약을 모두 강조합니다. 이 작업은 비교 XR 추적 평가를 위한 최초의 표준화된 프레임워크를 확립하여 연구 커뮤니티에 장치 및 조건 전반에 걸쳐 추적 성능을 체계적으로 분석할 수 있는 재현 가능한 방법론, 포괄적인 벤치마크 데이터 세트 및 오픈 소스 도구를 제공함으로써 XR 시스템을 위한 보다 강력한 공간 감지 기술 개발을 가속화합니다.",https://doi.org/10.1109/ISMAR67309.2025.00101,Tracking & Localization,Sensor Fusion,Technical Evaluation,System / Framework; Hardware / Device
153,2025,Xstudio - a Collaborative Metaverse-Based Tool for Automotive Design and Engineering,Xstudio - 자동차 디자인 및 엔지니어링을 위한 협업 메타버스 기반 도구,"This work addresses the limitations of traditional 2D collaboration tools in automotive design, where complex 3D model reviews require immersive spatial interaction. We developed a metaverse-based collaborative platform for automotive design processes using Unreal Engine 5, addressing three research directions: (1) core requirements for automotive design collaboration, (2) effectiveness of immersive technologies in such professional workflows, and (3) an empirical user study of test scenarios comparing the platform as a conventional PC application to an immersive Virtual Reality (VR) experience. Xstudio enables real-time interaction with 3D models and advanced functionalities for design teams. We evaluated the effectiveness of the tool through user studies and technical performance analysis. The expert study of 11 employees, who were part of the engineering department of BMW (a European car manufacturer), was designed to evaluate the usability of the features using the System Usability Score (SUS), which scored a promising value of 76 (VR) and 74 (PC). The NASA Task Load Index (NASA-TLX) resulted in 40.1 for VR and 37.7 in PC mode. A set of quantitative analyses with factors such as task completion time (TCT), error rates (ER), and user interaction metrics (UIM) were applied in the comparison of VR and PC modes. The result shows that the VR mode has higher TCT than the PC mode, but in contrast, VR has a higher interaction rate to complete the task, input precision, and the multi-step manipulation can be optimized. We summarize the findings in guidelines for future research in the collaborative car design process. Key technical contributions include a modular Entity-Component-System architecture for dynamic tool customization and hybrid VR/PC interactions. In the domain-specific automotive engineering space, both application types complement each other, offering efficiency (PC) or immersion (VR) for specific tasks. Future work will focus on UI/UX refinements with rigorous validation in follow-up studies. Specific improvements will include streamlined menus, snap-based selection aids, and pointer-locking tools to address precision challenges.","이 작업은 복잡한 3D 모델 검토에 몰입형 공간 상호 작용이 필요한 자동차 설계의 기존 2D 협업 도구의 한계를 해결합니다. 우리는 Unreal Engine 5를 사용하여 자동차 디자인 프로세스를 위한 메타버스 기반 협업 플랫폼을 개발했으며, 세 가지 연구 방향을 다루었습니다: (1) 자동차 디자인 협업을 위한 핵심 요구 사항, (2) 이러한 전문 워크플로우에서 몰입형 기술의 효율성, (3) 플랫폼을 기존 PC 애플리케이션과 몰입형 가상 현실(VR) 경험으로 비교하는 테스트 시나리오에 대한 경험적 사용자 연구입니다. Xstudio를 사용하면 디자인 팀이 3D 모델 및 고급 기능과 실시간 상호 작용할 수 있습니다. 우리는 사용자 연구와 기술적 성능 분석을 통해 도구의 효율성을 평가했습니다. BMW(유럽 자동차 제조업체)의 엔지니어링 부서 직원 11명을 대상으로 한 전문가 연구는 시스템 사용성 점수(SUS)를 사용하여 기능의 사용성을 평가하기 위해 설계되었으며, 유망한 값인 76(VR)과 74(PC)를 기록했습니다. NASA 작업 부하 지수(NASA-TLX)는 VR의 경우 40.1, PC 모드의 경우 37.7을 나타냈습니다. VR과 PC 모드의 비교에는 작업 완료 시간(TCT), 오류율(ER), 사용자 상호 작용 지표(UIM)와 같은 요소를 사용한 일련의 정량적 분석이 적용되었습니다. 그 결과 VR 모드는 PC 모드에 비해 TCT가 더 높은 것으로 나타났으나, 이에 비해 VR은 작업 완료를 위한 상호작용 비율, 입력 정밀도 및 다단계 조작을 최적화할 수 있는 비율이 더 높습니다. 우리는 협업 자동차 설계 프로세스의 향후 연구를 위한 지침에 결과를 요약합니다. 주요 기술 기여에는 동적 도구 사용자 정의 및 하이브리드 VR/PC 상호 작용을 위한 모듈식 엔터티-구성 요소-시스템 아키텍처가 포함됩니다. 도메인별 자동차 엔지니어링 공간에서 두 애플리케이션 유형은 서로를 보완하여 특정 작업에 대한 효율성(PC) 또는 몰입도(VR)를 제공합니다. 향후 작업은 후속 연구에서 엄격한 검증을 통해 UI/UX 개선에 중점을 둘 것입니다. 구체적인 개선 사항에는 간소화된 메뉴, 스냅 기반 선택 보조 기능 및 정밀도 문제를 해결하기 위한 포인터 잠금 도구가 포함됩니다.",https://doi.org/10.1109/ISMAR67309.2025.00166,Interaction & Input; Perception & Cognition,Other,Quantitative Experiment; Questionnaire / Survey,User Study / Empirical Findings; System / Framework
154,2025,Zeitgebers-Based User Experience Analysis and Time Perception Modeling via Transformer in VR,VR에서 Transformer를 통한 Zeitgebers 기반 사용자 경험 분석 및 시간 인식 모델링,"Virtual Reality (VR) creates a highly realistic and controllable simulation environment that can easily manipulate users' perception of space and time. However, while the sensation of “losing track of time” is often associated with enjoyable experiences, both the relationship between time perception and user experience in VR, and the underlying mechanisms of time perception itself, remain largely unexplored. In this study, we first investigated how different zeitgebers—such as light color, music tempo, and VR task—affect time perception. We then introduced the Relative Subjective Time Change (RSTC) method to explore the link between time perception and user experience quantitatively. Furthermore, to uncover the mechanisms underlying time perception in VR, we propose a computational model based on CNN and Transformer, named the Time Perception Modeling Network (TPM-Net), which leverages multimodal physiological data to infer users' time perception states in VR. In a between-subject experiment with 56 participants, our results indicate that the VR task factor significantly influences time perception, with red light and slow-tempo music contributing to an underestimation of time. The RSTC method effectively demonstrates that a relative underestimation of time in VR is strongly associated with enhanced user experience, presence, and engagement. Moreover, the TPM-Net shows great potential in modeling time perception, enabling further inference of relative changes in both time perception and user experience. Our study comprehensively elucidates the mechanisms of time perception in VR. It provides valuable insights and promising methodologies for exploring the relationship between time perception and user experience. Modeling time perception through physiological data marks a first step toward objectively assessing users' temporal perception states, offering a promising tool for VR-based therapy and training systems that require precise temporal awareness.","가상 현실(VR)은 공간과 시간에 대한 사용자의 인식을 쉽게 조작할 수 있는 매우 현실적이고 제어 가능한 시뮬레이션 환경을 만듭니다. 그러나 ""시간 추적을 잃는다""는 느낌은 종종 즐거운 경험과 관련되어 있지만, VR에서 시간 인식과 사용자 경험 사이의 관계, 그리고 시간 인식 자체의 기본 메커니즘은 대부분 아직 탐구되지 않은 상태로 남아 있습니다. 본 연구에서 우리는 먼저 빛의 색상, 음악 템포, VR 작업과 같은 다양한 시대정신이 시간 인식에 어떤 영향을 미치는지 조사했습니다. 그런 다음 시간 인식과 사용자 경험 간의 연관성을 정량적으로 탐색하기 위해 RSTC(상대적 주관적 시간 변화) 방법을 도입했습니다. 또한 VR에서 시간 인식의 기본 메커니즘을 밝히기 위해 다중 모드 생리학적 데이터를 활용하여 VR에서 사용자의 시간 인식 상태를 추론하는 Time Perception Modeling Network(TPM-Net)라는 CNN 및 Transformer 기반 계산 모델을 제안합니다. 56명의 참가자를 대상으로 한 피험자 간 실험에서 우리의 결과는 VR 작업 요소가 시간 인식에 큰 영향을 미치고 빨간색 조명과 느린 템포의 음악이 시간을 과소평가하는 데 기여한다는 것을 나타냅니다. RSTC 방법은 VR에서 시간을 상대적으로 과소평가하는 것이 사용자 경험, 존재감 및 참여도 향상과 밀접한 관련이 있음을 효과적으로 보여줍니다. 또한 TPM-Net은 시간 인식 모델링에 큰 잠재력을 보여 시간 인식과 사용자 경험 모두에서 상대적인 변화를 추가로 추론할 수 있습니다. 우리의 연구는 VR의 시간 인식 메커니즘을 포괄적으로 설명합니다. 이는 시간 인식과 사용자 경험 사이의 관계를 탐구하기 위한 귀중한 통찰력과 유망한 방법론을 제공합니다. 생리학적 데이터를 통한 시간 인식 모델링은 사용자의 시간 인식 상태를 객관적으로 평가하기 위한 첫 번째 단계이며, 정확한 시간 인식이 필요한 VR 기반 치료 및 훈련 시스템을 위한 유망한 도구를 제공합니다.",https://doi.org/10.1109/ISMAR67309.2025.00017,Perception & Cognition; Education & Training,Deep Learning / Neural Networks,User Study; Technical Evaluation,Algorithm / Method
155,2025,Zero-Sum vs. Positive-Sum: Effects of Inter-Team Competition Modes and Haptic Feedback on Team Flow in Multi-Team VR,제로섬 대 포지티브섬: 다중 팀 VR의 팀 흐름에 대한 팀 간 경쟁 모드 및 햅틱 피드백의 효과,"Virtual reality (VR), particularly through multi-user VR systems, enhances collaboration and immersion. However, multi-team VR systems (MTVR), which allow several teams to interact, either by cooperating or competing, are less studied. Drawing inspiration from concepts in game theory, specifically the positive-sum game (PSG) and zero-sum game (ZSG), this study investigated the impact of inter-team competition modes at the team level and haptic feedback at the individual level on team flow and individual flow for the first time. An experimental MTVR that supports inter-team competition in a two-person vs two-person pattern was implemented first. Then, a $2 \times 2$ within-subject experiment was conducted to examine the effects of different inter-team competition modes (PSG/ZSG) and haptic feedback (on/off). The results based on 188 participants indicate that the PSG mode leads to significantly higher levels of collective ambition and improved team relationships compared to the ZSG mode. Furthermore, providing haptic feedback can significantly enhance awareness of shared and personal task goals, resulting in more cautious performance during teamwork.","특히 다중 사용자 VR 시스템을 통한 가상 현실(VR)은 협업과 몰입감을 향상시킵니다. 그러나 여러 팀이 협력하거나 경쟁하여 상호 작용할 수 있도록 하는 MTVR(다중 팀 VR 시스템)에 대한 연구는 거의 없습니다. 게임 이론의 개념, 특히 포지티브섬 게임(PSG)과 제로섬 게임(ZSG)에서 영감을 얻은 이 연구에서는 팀 수준의 팀 간 경쟁 모드와 개인 수준의 햅틱 피드백이 팀 흐름과 개인 흐름에 미치는 영향을 처음으로 조사했습니다. 2인 대 2인 패턴의 팀 간 경쟁을 지원하는 실험적인 MTVR이 먼저 구현되었습니다. 그런 다음 다양한 팀 간 경쟁 모드(PSG/ZSG)와 햅틱 피드백(켜기/끄기)의 효과를 조사하기 위해 $2 \times 2$ 피험자 내 실험을 수행했습니다. 188명의 참가자를 대상으로 한 결과에 따르면 PSG 모드는 ZSG 모드에 비해 훨씬 더 높은 수준의 집단적 야망과 향상된 팀 관계로 이어지는 것으로 나타났습니다. 또한 햅틱 피드백을 제공하면 공유 및 개인 작업 목표에 대한 인식이 크게 향상되어 팀워크 중에 더욱 신중한 성과를 낼 수 있습니다.",https://doi.org/10.1109/ISMAR67309.2025.00121,Collaboration & Social; Perception & Cognition,Haptic / Tactile Feedback,User Study,User Study / Empirical Findings
156,2025,cLock: Single-Handed Two-Factor Authentication in VR Using Wrist Rotation and Multi-Finger Tapping,시계 잠금: 손목 회전 및 여러 손가락 탭핑을 사용한 VR의 한 손 2단계 인증,"As Virtual Reality (VR) devices become increasingly shared among users, there is a pressing need for authentication methods that balance security, usability, and privacy while accommodating VR's unique interaction constraints. This paper presents cLock, a novel single-handed two-factor authentication technique in VR that allows users to enter PINs with multiple cursors on a virtual circular numpad through wrist rotation and finger tapping. We first optimized the UI design of cLock by comparing participants' input performance with different UI parameters. We then extracted spatiotemporal behavioral features of both fingers and palm during PIN entry, which facilitated cLock's authentication algorithm. In the usability evaluation with four input postures, cLock achieved significantly faster authentication speed than laser and touch-based baselines, without sacrificing accuracy. Meanwhile, it was most preferred by participants in terms of privacy, social acceptance and physical effort. A following evaluation of security demonstrated that cLock achieved a deciphering rate of only $1 / 8$ of the baselines against shoulder-surfing within 1 m. Even in scenarios of password leakage, cLock could still achieve an FAR of 2.3% and FRR of 3.2% with 20 registered users. A final 11-day study verified the longitudinal stability of cLock.","가상 현실(VR) 장치가 사용자들 사이에서 점점 더 많이 공유됨에 따라 VR의 고유한 상호 작용 제약 조건을 수용하면서 보안, 유용성, 개인 정보 보호의 균형을 맞추는 인증 방법이 절실히 필요합니다. 본 논문에서는 사용자가 손목 회전과 손가락 탭을 통해 가상 원형 숫자 패드에 여러 개의 커서로 PIN을 입력할 수 있는 VR의 새로운 한 손 2단계 인증 기술인 cLock을 제시합니다. 먼저 참가자의 입력 성능을 다양한 UI 매개변수와 비교하여 cLock의 UI 디자인을 최적화했습니다. 그런 다음 PIN 입력 중 손가락과 손바닥의 시공간적 행동 특징을 추출하여 cLock의 인증 알고리즘을 활성화했습니다. 4가지 입력 자세에 대한 사용성 평가에서 cLock은 정확도 저하 없이 레이저 및 터치 기반 기준선보다 훨씬 빠른 인증 속도를 달성했습니다. 한편, 사생활 보호, 사회적 수용, 육체적 노력 측면에서는 참가자들이 가장 선호하는 것으로 나타났습니다. 보안에 대한 다음 평가에서는 cLock이 1m 이내의 숄더 서핑에 대해 기준선의 단 $1/8$ 해독 속도를 달성했음을 보여주었습니다. 비밀번호 유출 시나리오에서도 cLock은 등록된 사용자가 20명인 경우 FAR 2.3%, FRR 3.2%를 달성할 수 있습니다. 마지막 11일간의 연구에서는 시계의 종방향 안정성이 검증되었습니다.",https://doi.org/10.1109/ISMAR67309.2025.00064,Interaction & Input; Privacy & Security,Other,Technical Evaluation,Algorithm / Method
157,2024,"""I Feel Myself So Small!"": Designing and Evaluating VR Awe Experiences Based on Theories Related to Sublime","""나는 나 자신이 너무 작다고 느낀다!"": Sublime과 관련된 이론을 기반으로 한 VR 경외 경험 설계 및 평가","Research suggests the potential of employing VR to elicit awe experiences, thereby promoting well-being. Building upon theories related to the sublime and embodiment, we designed three VR scenes to evaluate the effectiveness of sublime and embodied design elements in invoking awe experiences. We conducted a within-subject study involving 28 young adults who experienced the three VR designs. Results demonstrated that the VR design with sublime elements significantly elicited more intense awe experiences compared to the one without, while adding embodied elements did not enhance the intensity of awe. Qualitative interviews revealed critical design elements (e.g., the obscure event should be reasonable) and their underlying mechanisms (e.g., leading to feelings of enlightenment) in invoking awe experiences. We further discuss considerations and implications for the design of effective awe-inspiring VR applications.","연구에 따르면 VR을 활용해 경이로운 경험을 이끌어내고 이를 통해 웰빙을 증진할 수 있는 가능성이 있다고 합니다. 숭고함과 구현에 관련된 이론을 바탕으로 우리는 경외감을 불러일으키는 숭고하고 구체화된 디자인 요소의 효과를 평가하기 위해 세 가지 VR 장면을 디자인했습니다. 우리는 세 가지 VR 디자인을 경험한 28명의 청년을 대상으로 피험자 내 연구를 실시했습니다. 결과는 숭고한 요소가 포함된 VR 디자인이 없는 디자인에 비해 훨씬 더 강렬한 경외감을 불러일으키는 반면, 구체화된 요소를 추가해도 경외심의 강도는 향상되지 않는 것으로 나타났습니다. 질적 인터뷰를 통해 경외감을 불러일으키는 중요한 디자인 요소(예: 모호한 사건은 합리적이어야 함)와 그 기본 메커니즘(예: 깨달음의 느낌으로 이어짐)이 드러났습니다. 우리는 경외심을 불러일으키는 효과적인 VR 애플리케이션 설계에 대한 고려 사항과 의미에 대해 더 자세히 논의합니다.",https://doi.org/10.1109/ISMAR62088.2024.00071,Interaction & Input,Optical / Display Technology,Qualitative Analysis,Other
158,2024,"""Looking"" into Attention Patterns in Extended Reality: An Eye Tracking-Based Study","확장 현실에서 주의 패턴을 ""살펴보기"": 시선 추적 기반 연구","Virtual reality (VR) simulations have been adopted to provide controllable environments for running augmented reality (AR) experiments in diverse scenarios. However, insufficient research has explored the impact of AR applications on users, especially their attention patterns, and whether VR simulations accurately replicate these effects. In this work, we propose to analyze user attention patterns via eye tracking during XR usage. To represent applications that provide both helpful guidance and irrelevant information, we built a Sudoku Helper app that includes visual hints and potential distractions during the puzzle-solving period. We conducted two user studies with 19 different users each in AR and VR, in which we collected eye tracking data, conducted gaze-based analysis, and trained machine learning (ML) models to predict user attentional states and attention control ability. Our results show that the AR app had a statistically significant impact on enhancing attention by increasing the fixated proportion of time, while the VR app reduced fixated time and made the users less focused. Results indicate that there is a discrepancy between VR simulations and the AR experience. Our ML models achieve 99.3% and 96.3% accuracy in predicting user attention control ability in AR and VR, respectively. A noticeable performance drop when transferring models trained on one medium to the other further highlights the gap between the AR experience and the VR simulation of it.","다양한 시나리오에서 증강 현실(AR) 실험을 실행하기 위한 제어 가능한 환경을 제공하기 위해 가상 현실(VR) 시뮬레이션이 채택되었습니다. 그러나 AR 애플리케이션이 사용자, 특히 주의 패턴에 미치는 영향과 VR 시뮬레이션이 이러한 효과를 정확하게 복제하는지에 대한 연구는 충분하지 않습니다. 본 연구에서는 XR 사용 중 시선추적을 통해 사용자의 주의 패턴을 분석할 것을 제안합니다. 유용한 안내와 관련 없는 정보를 모두 제공하는 애플리케이션을 나타내기 위해 퍼즐을 푸는 동안 시각적 힌트와 방해 요소가 포함된 Sudoku Helper 앱을 구축했습니다. 우리는 AR과 VR에서 각각 19명의 서로 다른 사용자를 대상으로 두 가지 사용자 연구를 수행했습니다. 이 연구에서는 시선 추적 데이터를 수집하고, 시선 기반 분석을 수행하고, 기계 학습(ML) 모델을 훈련하여 사용자 주의 상태와 주의 제어 능력을 예측했습니다. 연구 결과, AR 앱은 집중 시간을 늘려 집중력을 높이는 데 통계적으로 유의미한 영향을 미친 반면, VR 앱은 집중 시간을 줄여 집중력을 떨어뜨리는 데 통계적으로 유의미한 영향을 미치는 것으로 나타났습니다. 결과는 VR 시뮬레이션과 AR 경험 사이에 불일치가 있음을 나타냅니다. 우리의 ML 모델은 AR과 VR에서 사용자 주의 제어 능력을 각각 99.3%와 96.3%의 정확도로 예측합니다. 한 매체에서 훈련된 모델을 다른 매체로 전송할 때 눈에 띄는 성능 저하로 인해 AR 경험과 VR 시뮬레이션 간의 격차가 더욱 두드러집니다.",https://doi.org/10.1109/ISMAR62088.2024.00101,Interaction & Input; Education & Training,Sensor Fusion,Quantitative Experiment,Algorithm / Method
159,2024,"""Not my fault!"" ""All thanks to me!"": Studying Agency, Satisfaction and Self-Serving Attributional Bias with Rigged Archery in Virtual Reality","""내 잘못이 아니야!"" ""모두 나 덕분입니다!"": 가상 현실에서 조작된 궁술을 사용한 주체, 만족 및 이기적 귀인 편향 연구","User’s experience in virtual reality includes different cognitive, emotional, and behavioural factors. One key factor that characterises the interaction between humans and their environment is the sense of Agency. In virtual reality, the sense of Agency is defined as the sensation of having a virtual body which moves and acts accordingly to one’s intentions. However, it is usually measured with self-assessed questionnaires, focusing mainly on motor control within the virtual environment. We designed an experiment based on a game of rigged archery, manipulating avatar control and distant consequences of user’s actions. This allowed us to separately study both dimensions of Agency: the Feeling of Agency and the Judgement of Agency. Through this, we studied how they are linked with other psychological factors such as Locus of Control, Satisfaction and Self-Serving Attributional Bias. Our results suggest an influence of Internal Locus of Control on the Feeling of Agency. We observed that participant’s satisfaction levels was correlated with their assessment of Agency, and more prominently their Judgement of Agency. In addition, our results show that Feeling of Agency and Judgement of Agency can be understood as two distinct parameters of one’s subjective experience in virtual reality. Lastly, we observed that participants immersed in our virtual environment did show a Self-Serving Attributional Bias, as in real life. Indeed, they tended to claim authorship on successful actions while blaming failure on external factors. We believe that these results offer a better understanding of the different factors that could impact the sense of Agency in virtual reality.","가상 현실에서 사용자의 경험에는 다양한 인지적, 정서적, 행동적 요인이 포함됩니다. 인간과 환경 사이의 상호 작용을 특징짓는 주요 요소 중 하나는 주체 의식입니다. 가상현실에서 주체감은 자신의 의도에 따라 움직이고 행동하는 가상의 신체를 갖는 감각으로 정의됩니다. 그러나 일반적으로 가상 환경 내 운동 제어에 주로 초점을 맞춘 자체 평가 설문지로 측정됩니다. 우리는 조작된 양궁 게임을 기반으로 아바타 제어와 사용자 행동의 먼 결과를 조작하는 실험을 설계했습니다. 이를 통해 우리는 선택 의지의 두 가지 측면, 즉 선택 의지의 느낌과 선택 의지의 판단을 별도로 연구할 수 있었습니다. 이를 통해 통제중심, 만족, 자기중심적 귀인편향 등 다른 심리적 요인과 어떻게 연관되어 있는지 연구하였다. 우리의 결과는 대리인의 느낌에 대한 내부 통제 소재의 영향을 시사합니다. 우리는 참가자의 만족도 수준이 기관 평가, 특히 기관 판단과 상관관계가 있음을 관찰했습니다. 또한, 우리의 결과는 주체의 느낌과 주체의 판단이 가상 현실에서의 주관적 경험의 두 가지 별개의 매개변수로 이해될 수 있음을 보여줍니다. 마지막으로, 우리는 가상 환경에 몰입한 참가자들이 실제 생활에서와 마찬가지로 자기중심적인 귀인 편향을 보이는 것을 관찰했습니다. 실제로 그들은 성공적인 행동에 대해 저자임을 주장하는 반면, 실패를 외부 요인에 탓하는 경향이 있었습니다. 우리는 이러한 결과가 가상 현실에서 에이전시 감각에 영향을 미칠 수 있는 다양한 요소에 대한 더 나은 이해를 제공한다고 믿습니다.",https://doi.org/10.1109/ISMAR62088.2024.00059,Perception & Cognition,Other,User Study,User Study / Empirical Findings
160,2024,A Multi-Layout Design For Immersive Visualization of Hierarchical Network Data,계층적 네트워크 데이터의 몰입형 시각화를 위한 다중 레이아웃 디자인,"Visualization plays a vital role in making sense of complex network data. Recent studies have shown the potential of using extended reality (XR) for the immersive exploration of networks. The additional depth cues offered by XR help users perform better in certain tasks when compared to using traditional desktop setups. However, prior works on immersive network visualization rely mostly on singular, static graph layouts to present the data to the user. This poses a problem since there is no optimal layout for all possible tasks. The choice of layout heavily depends on the type of network and the task at hand. We introduce a multi-layout design that promotes more efficient use of the available space in VR environments and allows users to explore hierarchical network data in immersive space effectively. We implement our design with a choice of four distinct views on the network. The resulting system leverages various existing layout techniques to efficiently use the available space in VR and provide an optimal view of the data depending on the task and the level of detail required to solve it. To evaluate our approach, we conducted a user study comparing it against the state of the art for immersive network visualization. Participants performed tasks at varying spatial scopes. The results show that our approach outperforms the baseline in spatially focused scenarios as well as when the whole network needs to be considered.",시각화는 복잡한 네트워크 데이터를 이해하는 데 중요한 역할을 합니다. 최근 연구에서는 네트워크의 몰입형 탐색을 위해 확장 현실(XR)을 사용할 수 있는 가능성이 나타났습니다. XR이 제공하는 추가적인 깊이 단서는 사용자가 기존 데스크탑 설정을 사용할 때와 비교할 때 특정 작업에서 더 나은 성능을 발휘하도록 도와줍니다. 그러나 몰입형 네트워크 시각화에 대한 이전 작업은 사용자에게 데이터를 제공하기 위해 대부분 단일 정적 그래프 레이아웃에 의존했습니다. 가능한 모든 작업에 대한 최적의 레이아웃이 없기 때문에 문제가 발생합니다. 레이아웃 선택은 네트워크 유형과 현재 작업에 따라 크게 달라집니다. VR 환경에서 사용 가능한 공간을 보다 효율적으로 활용하고 사용자가 몰입형 공간에서 계층적 네트워크 데이터를 효과적으로 탐색할 수 있도록 하는 다중 레이아웃 디자인을 도입합니다. 우리는 네트워크에서 네 가지 뚜렷한 보기를 선택하여 디자인을 구현합니다. 결과 시스템은 다양한 기존 레이아웃 기술을 활용하여 VR에서 사용 가능한 공간을 효율적으로 사용하고 작업 및 문제 해결에 필요한 세부 수준에 따라 최적의 데이터 보기를 제공합니다. 우리의 접근 방식을 평가하기 위해 우리는 몰입형 네트워크 시각화를 위한 최신 기술과 이를 비교하는 사용자 연구를 수행했습니다. 참가자들은 다양한 공간 범위에서 작업을 수행했습니다. 결과는 우리의 접근 방식이 공간적으로 집중된 시나리오뿐만 아니라 전체 네트워크를 고려해야 할 때 기준선보다 성능이 우수하다는 것을 보여줍니다.,https://doi.org/10.1109/ISMAR62088.2024.00120,Rendering & Visualization,Other,User Study,Algorithm / Method
161,2024,APPEAR: Adaptive Pose Estimation for Mobile Augmented Reality,APPEAR: 모바일 증강 현실을 위한 적응형 자세 추정,"In Mobile Augmented Reality (MAR) applications, rendering virtual objects accurately on the user’s screen relies on knowing the poses of the user and the virtual objects. Typically, Simultaneous Localization and Mapping (SLAM) is used for pose estimation, but SLAM consumes lots of space and time. Many MAR applications such as gaming and navigation do not need continuous mapping, hence Visual Odometry (VO) is developed that provides localization without map generation. This paper evaluates VO’s accuracy compared to SLAM by specifically considering feature points at varying depths which distinguishes indoor and outdoor environments. Our findings suggest VO performs better with distant feature points (i.e., commonly outdoors), while SLAM excels with closer feature points (i.e., typically indoors). These findings inspire us to design APPEAR, an adaptive approach that switches between VO and SLAM as needed. We implemented and evaluated APPEAR on a computer rather than a smartphone because the chosen VO method was not available on a phone. APPEAR demonstrates memory savings of up to 100 MB and a 1.67 x speed boost on medium-sized datasets without losing accuracy. Additionally, while Absolute Trajectory Error (ATE) has been the primary metric for evaluating pose estimation, we argue for more comprehensive quality metrics for MAR applications: accuracy, precision, and recall. We also define selective ATE where ATE is only calculated in the region where the virtual objects are visible.","MAR(모바일 증강 현실) 애플리케이션에서 가상 개체를 사용자 화면에 정확하게 렌더링하려면 사용자의 자세와 가상 개체를 알아야 합니다. 일반적으로 자세 추정에는 SLAM(Simultaneous Localization and Mapping)이 사용되지만 SLAM은 많은 공간과 시간을 소비합니다. 게임 및 내비게이션과 같은 많은 MAR 애플리케이션에는 지속적인 매핑이 필요하지 않으므로 지도 생성 없이 위치 파악을 제공하는 VO(Visual Odometry)가 개발되었습니다. 본 논문에서는 실내와 실외 환경을 구별하는 다양한 깊이의 특징점을 구체적으로 고려하여 SLAM과 비교하여 VO의 정확도를 평가합니다. 우리의 연구 결과에 따르면 VO는 먼 특징점(예: 일반적으로 실외)에서 더 나은 성능을 발휘하는 반면 SLAM은 더 가까운 특징점(예: 일반적으로 실내)에서 탁월한 성능을 발휘합니다. 이러한 발견은 필요에 따라 VO와 SLAM 사이를 전환하는 적응형 접근 방식인 APPEAR를 설계하도록 영감을 주었습니다. 선택한 VO 방식을 휴대폰에서는 사용할 수 없었기 때문에 스마트폰이 아닌 컴퓨터에서 APPEAR를 구현하고 평가했습니다. APPEAR는 정확도 저하 없이 중간 규모 데이터세트에서 최대 100MB의 메모리 절약과 1.67배의 속도 향상을 보여줍니다. 또한 ATE(Absolute Trajectory Error)가 자세 추정을 평가하기 위한 기본 측정 기준인 반면, 우리는 MAR 애플리케이션에 대한 보다 포괄적인 품질 측정 기준인 정확성, 정밀도 및 재현율을 주장합니다. 또한 가상 객체가 보이는 영역에서만 ATE가 계산되는 선택적 ATE를 정의합니다.",https://doi.org/10.1109/ISMAR62088.2024.00077,Tracking & Localization; Interaction & Input,SLAM / Spatial Mapping,Quantitative Experiment,Algorithm / Method
162,2024,ARthroNeRF: Field of View Enhancement of Arthroscopic Surgeries using Augmented Reality and Neural Radiance Fields,ARthroNeRF: 증강 현실 및 신경 복사장을 사용한 관절경 수술의 시야 강화,"Arthroscopy is a minimally invasive orthopedic procedure commonly used to treat joints such as the shoulder, hip, or knee. A major difficulty for surgeons in arthroscopic procedures is the simultaneous coordination of the surgical tools used for manipulation and the arthroscopic camera. To further complicate this task, the narrow space where arthroscopic procedures are performed limits the ability to move the arthroscope inside the patient’s body, restricting the field of view. In this work, to overcome these limitations, we introduce ARthroNeRF, a novel framework that combines Neural Radiance Fields (NeRF) and Augmented Reality (AR). This framework allows for the generation of synthetic viewpoints from the perspective of surgical tools without the need for an additional camera. To evaluate the feasibility of the proposed framework, we conducted a user study with 18 participants. In this study, participants were tasked to touch hidden targets assisted by a synthetic view generated from the perspective of a surgical tool. The results of this study demonstrate that ARthroNeRF provides accurate supplementary visual information and suggest that ARthroNeRF has the potential to streamline the learning process in arthroscopic surgery. In addition, we built a system capable of presenting the reconstructed scenes using the Microsoft HoloLens 2. The incorporation of AR, overlaying synthesized images alongside the original arthroscopic footage and within the surgeon’s visual field, represents a viable alternative to enhance perception in spatially constrained scenarios.","관절경술은 어깨, 엉덩이, 무릎과 같은 관절을 치료하는 데 일반적으로 사용되는 최소 침습 정형외과 시술입니다. 관절경 수술에서 외과의사에게 가장 큰 어려움은 조작에 사용되는 수술 도구와 관절경 카메라를 동시에 조정하는 것입니다. 이 작업을 더욱 복잡하게 만드는 것은 관절경 시술이 수행되는 좁은 공간으로 인해 관절경을 환자 신체 내부로 이동하는 능력이 제한되어 시야가 제한된다는 것입니다. 본 연구에서는 이러한 한계를 극복하기 위해 NeRF(Neural Radiance Fields)와 AR(Augmented Reality)을 결합한 새로운 프레임워크인 ARthroNeRF를 소개합니다. 이 프레임워크를 사용하면 추가 카메라 없이도 수술 도구의 관점에서 합성 시점을 생성할 수 있습니다. 제안된 프레임워크의 타당성을 평가하기 위해 우리는 18명의 참가자를 대상으로 사용자 연구를 수행했습니다. 이 연구에서 참가자들은 수술 도구의 관점에서 생성된 종합 보기의 도움을 받아 숨겨진 대상을 터치하라는 임무를 받았습니다. The results of this study demonstrate that ARthroNeRF provides accurate supplementary visual information and suggest that ARthroNeRF has the potential to streamline the learning process in arthroscopic surgery. 또한 Microsoft HoloLens 2를 사용하여 재구성된 장면을 표시할 수 있는 시스템을 구축했습니다. 원본 관절경 영상과 외과의사의 시야 내에 합성 이미지를 오버레이하는 AR의 통합은 공간적으로 제한된 시나리오에서 인식을 향상시킬 수 있는 실행 가능한 대안을 나타냅니다.",https://doi.org/10.1109/ISMAR62088.2024.00136,Interaction & Input; Rendering & Visualization,3D Reconstruction,User Study,System / Framework
163,2024,ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D Pose Estimation,ASDF: 6D 자세 추정을 통합하여 Late Fusion을 활용하는 조립 상태 감지,"In medical and industrial domains, providing guidance for assembly processes can be critical to ensure efficiency and safety. Errors in assembly can lead to significant consequences such as extended surgery times and prolonged manufacturing or maintenance times in industry. Assembly scenarios can benefit from in-situ augmented reality visualization, i.e., augmentations in close proximity to the target object, to provide guidance, reduce assembly times, and minimize errors. In order to enable in-situ visualization, 6D pose estimation can be leveraged to identify the correct location for an augmentation. Existing 6D pose estimation techniques primarily focus on individual objects and static captures. However, assembly scenarios have various dynamics, including occlusion during assembly and dynamics in the appearance of assembly objects. Existing work focus either on object detection combined with state detection, or focus purely on the pose estimation. To address the challenges of 6D pose estimation in combination with assembly state detection, our approach ASDF builds upon the strengths of YOLOv8, a real-time capable object detection framework. We extend this framework, refine the object pose, and fuse pose knowledge with network-detected pose information. Utilizing our late fusion in our Pose2State module results in refined 6D pose estimation and assembly state detection. By combining both pose and state information, our Pose2State module predicts the final assembly state with precision. The evaluation of our ASDF dataset shows that our Pose2State module leads to an improved assembly state detection and that the improvement of the assembly state further leads to a more robust 6D pose estimation. Moreover, on the GBOT dataset, we outperform the pure deep learning-based network and even outperform the hybrid and pure tracking-based approaches.","의료 및 산업 분야에서는 효율성과 안전을 보장하기 위해 조립 공정에 대한 지침을 제공하는 것이 중요할 수 있습니다. 조립 오류로 인해 수술 시간이 연장되고 업계에서 제조 또는 유지 관리 시간이 연장되는 등 심각한 결과가 발생할 수 있습니다. 조립 시나리오는 현장 증강 현실 시각화(예: 대상 물체에 근접한 증강)의 이점을 활용하여 지침을 제공하고 조립 시간을 단축하며 오류를 최소화할 수 있습니다. 현장 시각화를 활성화하기 위해 6D 자세 추정을 활용하여 증강을 위한 올바른 위치를 식별할 수 있습니다. 기존 6D 포즈 추정 기술은 주로 개별 개체와 정적 캡처에 중점을 둡니다. 그러나 조립 시나리오에는 조립 중 폐색 및 조립 개체 모양의 역학을 포함하여 다양한 역학이 있습니다. 기존 작업은 상태 감지와 결합된 객체 감지에 중점을 두거나 순수하게 포즈 추정에만 중점을 둡니다. 어셈블리 상태 감지와 결합된 6D 포즈 추정 문제를 해결하기 위해 우리의 접근 방식 ASDF는 실시간 가능 객체 감지 프레임워크인 YOLOv8의 강점을 기반으로 구축되었습니다. 우리는 이 프레임워크를 확장하고, 객체 포즈를 개선하고, 포즈 지식을 네트워크에서 감지된 포즈 정보와 융합합니다. Pose2State 모듈의 후기 융합을 활용하면 세련된 6D 포즈 추정 및 어셈블리 상태 감지가 가능해집니다. 포즈와 상태 정보를 결합하여 Pose2State 모듈은 최종 어셈블리 상태를 정확하게 예측합니다. ASDF 데이터세트를 평가한 결과 Pose2State 모듈이 어셈블리 상태 감지를 개선하고 어셈블리 상태 개선으로 더욱 강력한 6D 포즈 추정이 가능하다는 사실이 나타났습니다. 또한 GBOT 데이터 세트에서 우리는 순수 딥 러닝 기반 네트워크보다 성능이 뛰어나고 하이브리드 및 순수 추적 기반 접근 방식보다 성능이 뛰어납니다.",https://doi.org/10.1109/ISMAR62088.2024.00033,Education & Training; Tracking & Localization,Deep Learning / Neural Networks,Quantitative Experiment,Algorithm / Method
164,2024,AViSal360: Audiovisual Saliency Prediction for 360° Video,AViSal360: 360° 비디오에 대한 시청각 현저성 예측,"Saliency prediction in 360° video plays an important role in modeling visual attention, and can be leveraged for content creation, compression techniques, or quality assessment methods, among others. Visual attention in immersive environments depends not only on visual input, but also on inputs from other sensory modalities, primarily audio. Despite this, only a minority of saliency prediction models have incorporated auditory inputs, and much remains to be explored about what auditory information is relevant and how to integrate it in the prediction. In this work, we propose an audiovisual saliency model for 360° video content, AViSal360. Our model integrates both spatialized and semantic audio information, together with visual inputs. We perform exhaustive comparisons to demonstrate both the actual relevance of auditory information in saliency prediction, and the superior performance of our model when compared to previous approaches.","360° 비디오의 돌출성 예측은 시각적 주의를 모델링하는 데 중요한 역할을 하며 무엇보다도 콘텐츠 제작, 압축 기술 또는 품질 평가 방법에 활용될 수 있습니다. 몰입형 환경에서 시각적 주의는 시각적 입력뿐만 아니라 주로 오디오와 같은 다른 감각 양식의 입력에도 의존합니다. 그럼에도 불구하고, 현저성 예측 모델 중 소수만이 청각 입력을 통합했으며, 어떤 청각 정보가 관련성이 있고 이를 예측에 통합하는 방법에 대해 많은 연구가 남아 있습니다. 본 연구에서는 360° 비디오 콘텐츠를 위한 시청각 돌출 모델인 AViSal360을 제안합니다. 우리 모델은 공간화된 오디오 정보와 의미론적인 오디오 정보를 시각적 입력과 함께 통합합니다. 우리는 돌출성 예측에서 청각 정보의 실제 관련성과 이전 접근 방식과 비교할 때 우리 모델의 우수한 성능을 모두 입증하기 위해 철저한 비교를 수행합니다.",https://doi.org/10.1109/ISMAR62088.2024.00141,Perception & Cognition; Audio & Sound,Other,Technical Evaluation,Algorithm / Method
165,2024,Accented Character Entry Using Physical Keyboards in Virtual Reality,가상 현실에서 물리적 키보드를 사용한 악센트 문자 입력,"Research on text entry in Virtual Reality (VR) has gained popularity but the efficient entry of accented characters, characters with diacritical marks, in VR remains underexplored. Entering accented characters is supported on most capacitive touch keyboards through a long press on a base character and a subsequent selection of the accented character. However, entering those characters on physical keyboards is still challenging, as they require a recall and an entry of respective numeric codes. To address this issue this paper investigates three techniques to support accented character entry on physical keyboards in VR. Specifically, we compare a contextaware numeric code technique that does not require users to recall a code, a key-press-only condition in which the accented characters are dynamically remapped to physical keys next to a base character, and a multimodal technique, in which eye gaze is used to select the accented version of a base character previously selected by keypress on the keyboard. The results from our user study $(n=18)$ reveal that both the key-press-only and the multimodal technique outperform the baseline technique in terms of text entry speed.","가상 현실(VR)의 텍스트 입력에 대한 연구가 인기를 얻었지만 VR에서 악센트가 있는 문자, 분음 기호가 있는 문자를 효율적으로 입력하는 방법은 아직 연구가 부족합니다. 악센트 문자 입력은 기본 문자를 길게 누른 후 악센트 문자를 선택하는 방식으로 대부분의 정전식 터치 키보드에서 지원됩니다. 그러나 실제 키보드에 해당 문자를 입력하려면 각 숫자 코드를 불러오고 입력해야 하기 때문에 여전히 어렵습니다. 이 문제를 해결하기 위해 이 문서에서는 VR의 실제 키보드에서 악센트 문자 입력을 지원하는 세 가지 기술을 조사합니다. 구체적으로, 우리는 사용자가 코드를 기억할 필요가 없는 상황 인식 숫자 코드 기술, 악센트 문자가 기본 문자 옆의 물리적 키에 동적으로 다시 매핑되는 키 누름 전용 조건, 그리고 시선을 사용하여 키보드의 키 누름으로 이전에 선택한 기본 문자의 악센트 버전을 선택하는 다중 모드 기술을 비교합니다. 사용자 연구 $(n=18)$의 결과는 키 누르기 전용 기술과 다중 모드 기술 모두 텍스트 입력 속도 측면에서 기본 기술보다 성능이 우수하다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR62088.2024.00081,Interaction & Input,Other,User Study,Algorithm / Method
166,2024,Adaptive Portals: Enhancing Virtual Reality Interaction Spaces With Real-Time Self-Adjusting Portals,적응형 포털: 실시간 자체 조정 포털을 통해 가상 현실 상호 작용 공간 향상,"This paper introduces foundational techniques for Adaptive Portals (AP); portals that adjust to users’ needs in Virtual Reality (VR). At the forefront of our contributions is the AP-Reach technique, with Entry and Exit strategies, designed to overcome the spatial limitations of portals and VR, enabling users to comfortably extend their reach to objects beyond their immediate vicinity. In parallel, we present PH-Reach as an alternative to the AP concept, also aimed at extending user reach inside portals. The effectiveness of these reach techniques is evaluated through a comparative user study, which compared Entry AP-Reach, Exit AP-Reach, and PHReach techniquesagainst traditional natural reach as a baseline. Results show Exit AP-Reach surpassed both its counterparts and the baseline to various degrees in terms of reducing task completion time, physical movement/demand, effort, and frustration. Additionally, this paper introduces AP-Bounds and AP-Restriction.","이 문서에서는 적응형 포털(AP)의 기본 기술을 소개합니다. 가상 현실(VR)에서 사용자의 요구에 맞게 조정되는 포털입니다. 우리가 기여한 것의 최전선에는 포털과 VR의 공간적 한계를 극복하도록 설계된 진입 및 퇴출 전략을 갖춘 AP-Reach 기술이 있으며, 이를 통해 사용자는 바로 근처를 넘어 물체에 편안하게 도달할 수 있습니다. 동시에 우리는 포털 내부에서 사용자 도달 범위를 확장하는 것을 목표로 하는 AP 개념의 대안으로 PH-Reach를 제시합니다. 이러한 도달 기술의 효과는 Entry AP-Reach, Exit AP-Reach 및 PHReach 기술을 기존의 자연 도달을 기준으로 비교한 비교 사용자 연구를 통해 평가됩니다. 결과에 따르면 Exit AP-Reach는 작업 완료 시간, 물리적 이동/요구, 노력 및 좌절감을 줄이는 측면에서 다양한 수준에서 대응 제품과 기준선을 모두 능가했습니다. 또한 이 문서에서는 AP 경계 및 AP 제한을 소개합니다.",https://doi.org/10.1109/ISMAR62088.2024.00117,Interaction & Input,Other,User Study,Algorithm / Method
167,2024,"Analysis of Immersive Mid-Air Sketching Behavior, Sketch Quality, and User Experience in Design Ideation Tasks","디자인 아이디어 작업에서 몰입형 공중 스케치 동작, 스케치 품질 및 사용자 경험 분석","Immersive 3D sketching systems empower users with tools to create sketches directly in the air around themselves, in all three dimensions, using only simple hand gestures. These sketching systems have the potential to greatly extend the interactive capabilities of immersive learning environments. The perceptual challenges of Virtual Reality (VR), however, combined with the ergonomic and cognitive challenges of creating mid-air 3D sketches reduce the effectiveness of immersive sketching used for problem-solving, reflection, and to capture fleeting ideas. We contribute to the understanding of the potential challenges of mid-air sketching systems in educational settings, where expression is valued higher than accuracy, and sketches are used to support problem-solving and to explain abstract concepts. We conducted an empirical study with 36 participants with different spatial abilities to investigate if the way that people sketch in mid-air is dependent on the goal of the sketch. We compare the technique, quality, efficiency, and experience of participants as they create 3D mid-air sketches in three different tasks. We examine how users approach mid-air sketching when the sketches they create serve to convey meaning and when sketches are merely reproductions of geometric models created by someone else. We found that in tasks aimed at expressing personal design ideas, between starting and ending strokes, participants moved their heads more and their controllers at higher velocities and created strokes in faster times than in tasks aimed at recreating 3D geometric figures. They reported feeling less time pressure to complete sketches but redacted a larger percentage of strokes. These findings serve to inform the design of creative virtual environments that support reasoning and reflection through mid-air sketching. With this work, we aim to strengthen the power of immersive systems that support midair 3D sketching by exploiting natural user behavior to assist users to more quickly and faithfully convey their meaning in sketches.","몰입형 3D 스케치 시스템은 사용자에게 간단한 손 동작만으로 주변 공중에서 3차원 스케치를 직접 만들 수 있는 도구를 제공합니다. 이러한 스케치 시스템은 몰입형 학습 환경의 대화형 기능을 크게 확장할 수 있는 잠재력을 가지고 있습니다. 그러나 가상 현실(VR)의 지각적 과제는 공중 3D 스케치 제작에 따른 인체공학적, 인지적 과제와 결합되어 문제 해결, 반영 및 순간적인 아이디어 캡처에 사용되는 몰입형 스케치의 효율성을 감소시킵니다. 우리는 표현이 정확성보다 더 중요하고 스케치가 문제 해결을 지원하고 추상적 개념을 설명하는 데 사용되는 교육 환경에서 공중 스케치 시스템의 잠재적인 과제를 이해하는 데 기여합니다. 우리는 사람들이 공중에서 스케치하는 방식이 스케치의 목적에 따라 달라지는지 조사하기 위해 서로 다른 공간 능력을 가진 36명의 참가자를 대상으로 실증적 연구를 수행했습니다. 참가자들이 세 가지 작업을 통해 3D 공중 스케치를 만들면서 기술, 품질, 효율성 및 경험을 비교합니다. 우리는 사용자가 만든 스케치가 의미를 전달하는 역할을 하고 스케치가 단순히 다른 사람이 만든 기하학적 모델의 복제일 때 사용자가 공중 스케치에 어떻게 접근하는지 조사합니다. 우리는 개인적인 디자인 아이디어를 표현하기 위한 작업에서 시작과 끝 스트로크 사이에서 참가자들이 3D 기하학적 도형을 재현하기 위한 작업보다 머리와 컨트롤러를 더 빠른 속도로 더 많이 움직이고 더 빠른 시간에 스트로크를 생성한다는 것을 발견했습니다. 그들은 스케치를 완성해야 한다는 시간적 압박감을 덜 느꼈지만 획의 상당 부분을 수정했다고 보고했습니다. 이러한 연구 결과는 공중 스케치를 통해 추론과 성찰을 지원하는 창의적인 가상 환경의 디자인을 알리는 데 도움이 됩니다. 본 작업을 통해 자연스러운 사용자 행동을 활용하여 사용자가 스케치에서 의미를 보다 빠르고 충실하게 전달할 수 있도록 지원함으로써 공중 3D 스케치를 지원하는 몰입형 시스템의 위력을 강화하는 것을 목표로 합니다.",https://doi.org/10.1109/ISMAR62088.2024.00041,Interaction & Input; Perception & Cognition,Hand / Gesture Recognition,User Study,Algorithm / Method; User Study / Empirical Findings
168,2024,Audience Amplified: Virtual Audiences in Asynchronously Performed AR Theater,관객 증폭: 비동기식으로 공연되는 AR 극장의 가상 관객,"Audience reactions can considerably enhance live experiences; conversely, in anytime/anywhere augmented reality (AR) experiences, large crowds of people might not always be available to congregate. To get closer to simulating live events with large audiences, we created a mobile AR experience where users can wander around naturally and engage in AR theater with virtual audiences trained from real audiences using imitation learning. This allows us to carefully capture the essence of human imperfections and behavior in artificial intelligence (AI) audiences. The result is a novel mobile AR experience in which solitary AR users experience an augmented performance in a physical space with a virtual audience. Virtual dancers emerge from the surroundings, accompanied by a digitally simulated audience, to provide a community experience akin to immersive theater. In a pilot study, simulated human avatars were vastly preferred over just audience audio commentary. We subsequently engaged 20 participants as attendees of an AR dance performance, comparing a no-audience condition with a simulated audience of six onlookers. Through questionnaires and experience reports, we investigated user reactions and behavior. Our results demonstrate that the presence of virtual audience members caused attendees to perceive the performance as a social experience with increased interest and involvement in the event. On the other hand, for some attendees, the dance performances without the virtual audience evoked a stronger positive sentiment.","청중의 반응은 라이브 경험을 상당히 향상시킬 수 있습니다. 반대로, 언제 어디서나 증강 현실(AR) 경험에서는 많은 사람들이 항상 모일 수 있는 것은 아닙니다. 대규모 청중이 참여하는 라이브 이벤트 시뮬레이션에 더 가까워지기 위해 우리는 모방 학습을 통해 사용자가 자연스럽게 돌아다니며 실제 청중으로부터 훈련받은 가상 청중과 함께 AR 극장에 참여할 수 있는 모바일 AR 경험을 만들었습니다. 이를 통해 우리는 인공 지능(AI) 청중의 인간 불완전성과 행동의 본질을 주의 깊게 포착할 수 있습니다. 그 결과, 단독 AR 사용자가 가상 ​​관객과 함께 물리적 공간에서 증강된 성능을 경험하는 새로운 모바일 AR 경험이 탄생했습니다. 가상 댄서들이 디지털 방식으로 시뮬레이션된 청중과 함께 주변에서 나타나 몰입형 연극과 유사한 커뮤니티 경험을 제공합니다. 파일럿 연구에서는 청중의 오디오 해설보다 시뮬레이션된 인간 아바타가 훨씬 더 선호되었습니다. 이후 우리는 20명의 참가자를 AR 댄스 공연의 참석자로 참여시켜 관객이 없는 조건과 6명의 구경꾼으로 구성된 시뮬레이션된 청중을 비교했습니다. 설문조사와 경험 보고서를 통해 사용자 반응과 행동을 조사했습니다. 우리의 결과는 가상 관객의 존재로 인해 참석자들이 공연을 이벤트에 대한 관심과 참여가 증가하는 사회적 경험으로 인식하게 되었음을 보여줍니다. 반면, 일부 참석자들에게는 가상 관객이 없는 댄스 공연이 더욱 강한 긍정적인 감정을 불러일으켰다.",https://doi.org/10.1109/ISMAR62088.2024.00062,Perception & Cognition,Sensor Fusion,User Study,Other
169,2024,Augmented Reality Annotations for Assisting with Decision-Making and Time-Critical Tasking,의사결정 및 시간이 중요한 작업 수행을 지원하기 위한 증강 현실 주석,"This research explores various Augmented Reality (AR) annotations for supporting performance and decision-making in a time-critical task. The task was designed in collaboration with defence contacts to mimic a command and control scenario, requiring rapid decisions under time pressure. A set of annotations was derived from previous literature, and modifications were made to suit the tasking requirements. The annotations chosen were based on their level of abstraction from least to most abstract: Animations, StickyNotes, Icons, and Highlights. The study compared five conditions (Animations, StickyNotes, Icons, Highlights, and no annotations) for their effectiveness in supporting performance and decision-making in a time-critical task. StickyNotes was found to be the most effective for supporting performance. The study’s results also identify that annotations for assisting with time-critical tasking should be easy and quick to understand, contain sufficient complexity, and not be too abstract in detail.","이 연구에서는 시간이 중요한 작업에서 성능과 의사 결정을 지원하기 위한 다양한 증강 현실(AR) 주석을 탐색합니다. 이 작업은 시간 압박 속에서 신속한 결정이 필요한 명령 및 제어 시나리오를 모방하기 위해 국방 담당자와 협력하여 설계되었습니다. 일련의 주석은 이전 문헌에서 파생되었으며 작업 요구 사항에 맞게 수정되었습니다. 선택한 주석은 추상화 수준이 가장 낮은 것부터 가장 추상적인 것까지(애니메이션, StickyNotes, 아이콘 및 강조 표시)에 따라 결정되었습니다. 이 연구에서는 시간이 중요한 작업에서 성능 및 의사 결정을 지원하는 데 있어 5가지 조건(애니메이션, StickyNotes, 아이콘, 하이라이트 및 주석 없음)을 비교했습니다. StickyNotes는 성능 지원에 가장 효과적인 것으로 나타났습니다. 연구 결과는 또한 시간이 중요한 작업을 지원하기 위한 주석은 이해하기 쉽고 빠르며, 충분한 복잡성을 포함하고, 세부적으로 너무 추상적이어서는 안 된다는 점을 확인했습니다.",https://doi.org/10.1109/ISMAR62088.2024.00102,Interaction & Input,Other,Other,Dataset / Benchmark
170,2024,Augmented Reality Visualization Techniques for Attention Guidance to Out-of-View Objects: A Systematic Review,시야 밖의 물체에 대한 주의 유도를 위한 증강 현실 시각화 기법: 체계적 검토,"Recent advancements in augmented reality (AR) hardware, software and application capabilities have introduced exciting benefits and advantages, especially in industrial and occupational fields. However, many new challenges have arisen with the increasing use of AR in real work settings, such as increased visual capture and attention demanded by graphics presented within a relatively small field of view (FOV) (as compared to users’ natural FOV). This systematic review paper addresses how to effectively use and design AR visualization techniques to cue objects located outside a user’s FOV. We posit that new visualization techniques are needed to cue out-of-view objects while maintaining user attention and minimizing distraction from the user’s primary task. A significant amount of research has been done to examine effective visualizations for guiding attention in AR, specifically how to encode direction and distance of out-of-view objects. Our review compares the performance associated with existing techniques, as well as what characteristics have been implemented and studied. In this work, we also present design guidelines derived from our analysis and synthesis to understand what visualization technique characteristics may be effective. Our final recommendations describe the value of reference lines, feedback to users, location indicators, best practices to encode direction and distance, and the use of subtle cues.","증강 현실(AR) 하드웨어, 소프트웨어 및 애플리케이션 기능의 최근 발전으로 인해 특히 산업 및 직업 분야에서 흥미로운 이점과 이점이 도입되었습니다. 그러나 실제 작업 환경에서 AR 사용이 증가함에 따라 (사용자의 자연 FOV에 비해) 상대적으로 작은 시야(FOV) 내에 표시되는 그래픽에 필요한 시각적 캡처 및 주의가 증가하는 등 많은 새로운 과제가 발생했습니다. 이 체계적 검토 논문에서는 사용자의 FOV 외부에 있는 물체에 신호를 주기 위해 AR 시각화 기술을 효과적으로 사용하고 설계하는 방법을 다룹니다. 우리는 사용자의 주의를 유지하고 사용자의 주요 작업에 방해가 되는 것을 최소화하면서 시야 밖의 개체에 신호를 주기 위해서는 새로운 시각화 기술이 필요하다고 가정합니다. AR에서 주의를 유도하기 위한 효과적인 시각화, 특히 시야 밖에 있는 물체의 방향과 거리를 인코딩하는 방법을 조사하기 위해 상당한 양의 연구가 수행되었습니다. 우리의 검토에서는 기존 기술과 관련된 성능과 구현 및 연구된 특성을 비교합니다. 본 연구에서는 어떤 시각화 기술 특성이 효과적일 수 있는지 이해하기 위해 분석 및 종합을 통해 도출된 설계 지침도 제시합니다. 최종 권장 사항에서는 참조선의 가치, 사용자에 대한 피드백, 위치 표시기, 방향과 거리를 인코딩하는 모범 사례, 미묘한 단서 사용에 대해 설명합니다.",https://doi.org/10.1109/ISMAR62088.2024.00098,Rendering & Visualization,Other,Literature Review / Meta-analysis,Design Guidelines; Survey / Review
171,2024,Avatar-Centered Feedback: Dynamic Avatar Alterations Can Induce Avoidance Behaviors to Virtual Dangers,아바타 중심 피드백: 동적 아바타 변경으로 인해 가상 위험에 대한 회피 행동이 유도될 수 있음,"Abstract One singularity of VR is its capacity to generate a strong sensation of being present in a dangerous environment, without risking the physical consequences. However, this absence of consequences when interacting with virtual dangers might also limit the induction of realistic responses, such as avoidance behaviors, which are a key factor of various VR applications (e.g., training, journalism, or exposure therapies). To address this limitation, we propose avatar-centered feedback, a novel, device-free approach, consisting of dynamically altering the avatar’s appearance and movements to offer coherent feedback from interactions with the virtual environment, including dangers. To begin with, we present a design space clarifying the range of potential implementations for this approach. Then, we tested this approach in a metallurgy scenario, where participants’ virtual hands would redden and display burns as they got closer to a virtual fire (appearance alteration), and simulate short withdrawal reflexes movements when a spark burst next to them (movement alteration). Our results show that in comparison to a control group, participants receiving avatar-centered feedback demonstrated significantly more avoidance behaviors to the virtual fire. Interestingly, we also found that experiencing avatar-centered feedback of fire significantly increased avoidance behaviors toward a following danger of different nature (a circular saw). These results suggest that avatar-centered feedback can also impact the general perception of the avatar vulnerability to the virtual environment.","Abstract VR의 한 가지 특이점은 물리적 결과를 감수하지 않고도 위험한 환경에 있는 듯한 강한 감각을 생성할 수 있다는 것입니다. 그러나 가상 위험과 상호작용할 때 이러한 결과가 없으면 다양한 VR 애플리케이션(예: 훈련, 저널리즘 또는 노출 요법)의 핵심 요소인 회피 행동과 같은 현실적인 반응의 유도가 제한될 수도 있습니다. 이러한 한계를 해결하기 위해 우리는 위험을 포함하여 가상 환경과의 상호 작용에서 일관된 피드백을 제공하기 위해 아바타의 외모와 움직임을 동적으로 변경하는 새로운 장치 없는 접근 방식인 아바타 중심 피드백을 제안합니다. 우선, 이 접근 방식의 잠재적 구현 범위를 명확히 하는 설계 공간을 제시합니다. 그런 다음 참가자의 가상 손이 가상 불에 가까워질수록 화상을 입히고(외모 변경) 옆에서 스파크가 터질 때 짧은 철수 반사 움직임을 시뮬레이션하는 야금 시나리오에서 이 접근 방식을 테스트했습니다(움직임 변경). 우리의 결과는 통제 그룹과 비교하여 아바타 중심 피드백을 받은 참가자가 가상 ​​화재에 대해 훨씬 더 많은 회피 행동을 보여주었다는 것을 보여줍니다. 흥미롭게도 우리는 아바타 중심의 불 피드백을 경험하면 다음과 같은 다양한 성격의 위험(원형 톱)에 대한 회피 행동이 크게 증가한다는 사실도 발견했습니다. 이러한 결과는 아바타 중심 피드백이 가상 환경에 대한 아바타 취약성에 대한 일반적인 인식에도 영향을 미칠 수 있음을 시사합니다.",https://doi.org/10.1109/ISMAR62088.2024.00023,Perception & Cognition,Sensor Fusion,Technical Evaluation,Algorithm / Method
172,2024,Avoiding Virtual Characters: The Effects of Proximity and Gesture,가상 캐릭터 피하기: 근접성과 제스처의 효과,"We explored how study participants interacted with virtual characters in a virtual reality study. Specifically, we developed a 3 (proximity: close vs. middle vs. far) $\times 2$ (gesture: passive vs. active) experimental design (N = 26) to understand how combinations of proximity between two virtual characters and gestures assigned to them influence study participants’ self-reported ratings (co-presence, attentional allocation, behavioral interdependence, emotional reactivity, and perceived politeness). We also examined their avoidance movements (duration, trajectory length, and speed) and their avoidance decisions (passing through/around and minimum distance side). We collected both survey responses and our participants’ trajectories. Our study revealed that 1) the proximity factor impacted how our participants rated their co-presence and behavioral interdependence, as well as whether they decided to pass through or around the virtual characters, and 2) the gesture factor impacted how participants rated their behavioral interdependence, emotional reactivity, perceived politeness, and also affected their duration, trajectory length, and speed. Our research contributes to understanding personal space and social norms in virtual environments, offering valuable insights for virtual reality developers on the importance of social dynamics in designing interactions with virtual characters.","우리는 가상 현실 연구에서 연구 참가자가 가상 ​​캐릭터와 어떻게 상호 작용하는지 탐구했습니다. 구체적으로 우리는 두 가상 캐릭터 간의 근접성과 그들에게 할당된 제스처의 조합이 연구 참가자의 자체 보고 평가(공존, 주의 할당, 행동 상호 의존성, 정서적 반응성 및 인지된 공손함)에 어떻게 영향을 미치는지 이해하기 위해 3(근접성: 가까움 vs. 중간 vs. 원거리) $\times 2$(제스처: 수동적 vs. 활성) 실험 설계(N = 26)를 개발했습니다. 또한 회피 이동(지속 시간, 궤적 길이 및 속도)과 회피 결정(통과/주변 및 최소 거리 측)을 조사했습니다. 우리는 설문조사 응답과 참가자의 궤적을 모두 수집했습니다. 우리의 연구에 따르면 1) 근접 요인은 참가자가 가상 ​​캐릭터를 통과하거나 주변을 통과하기로 결정했는지 여부뿐만 아니라 참가자가 공동 존재 및 행동 상호 의존성을 평가하는 방식에 영향을 미쳤으며, 2) 제스처 요인은 참가자가 행동 상호 의존성, 감정적 반응성, 인지된 예의를 평가하는 방식에 영향을 미쳤으며 지속 시간, 궤적 길이 및 속도에도 영향을 미쳤습니다. 우리의 연구는 가상 환경에서 개인 공간과 사회적 규범을 이해하는 데 기여하며 가상 현실 개발자에게 가상 캐릭터와의 상호 작용을 디자인할 때 사회적 역학의 중요성에 대한 귀중한 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR62088.2024.00018,Interaction & Input; Perception & Cognition,Sensor Fusion,Questionnaire / Survey,User Study / Empirical Findings
173,2024,Becoming An Animal? Exploring Proteus Effect Based on Human-avatar Hand Gesture Consistency,동물이 되다? 인간-아바타 손 제스처 일관성을 기반으로 한 프로테우스 효과 탐색,"Human cognition and behavior can be unconsciously affected by personal avatars in the virtual world, a phenomenon known as the Proteus Effect. When using first-person non-human avatars, the characteristics of virtual hands may also induce relevant cognitive and even behavioral patterns in real human hands. Therefore, evaluating human-avatar gesture consistency may be a potentially effective method for objectively assessing the Proteus Effect when using non-human avatars. To explore this question, we first created human and non-human avatars, including three animals. Then, we constructed a dataset of hand gestures and trained a model to dynamically recognize real-hand gestures that were consistent with corresponding avatar hands. Next, we designed a virtual reality experimental task involving grasping objects with intuitive gestures and performed a 2 (avatar type: human/non-human) $* 2$ (virtual hand: presence/absence of spontaneous movement) within-subject experiment to examine the effects of avatar characteristics on self-illusion and human-avatar gesture consistency. The results showed that participants performed a significantly larger percentage of gestures that were consistent with their currently used avatars. Additionally, participants did experience self-illusion when using non-human avatars, although the levels were significantly lower than those when using human avatars. Therefore, self-illusion may serve as a perceptual antecedent of the Proteus Effect, even with non-human avatars, inadvertently altering the behavioral gestures of their real hands. In conclusion, detecting human-avatar gesture consistency can help evaluate the Proteus Effect.","인간의 인지와 행동은 가상 세계에서 개인 아바타에 의해 무의식적으로 영향을 받을 수 있는데, 이는 프로테우스 효과(Proteus Effect)로 알려진 현상입니다. 인간이 아닌 1인칭 아바타를 사용할 때 가상 손의 특성은 실제 인간 손에서 관련 인지 패턴과 심지어 행동 패턴까지 유도할 수도 있습니다. 따라서 인간-아바타 제스처 일관성을 평가하는 것은 인간이 아닌 아바타를 사용할 때 프로테우스 효과를 객관적으로 평가하는 데 잠재적으로 효과적인 방법이 될 수 있습니다. 이 질문을 탐구하기 위해 우리는 먼저 세 마리의 동물을 포함하여 인간 및 인간이 아닌 아바타를 만들었습니다. 그런 다음 손 제스처 데이터 세트를 구축하고 해당 아바타 손과 일치하는 실제 손 제스처를 동적으로 인식하도록 모델을 훈련했습니다. 다음으로 직관적인 제스처로 물체를 잡는 가상 현실 실험 작업을 설계하고 아바타 특성이 자기 환상과 인간-아바타 제스처 일관성에 미치는 영향을 조사하기 위해 2(아바타 유형: 인간/비인간)$* 2$(가상 손: 자발적인 움직임의 유무) 피험자 내 실험을 수행했습니다. 결과는 참가자들이 현재 사용하는 아바타와 일치하는 제스처를 훨씬 더 많이 수행한 것으로 나타났습니다. 또한 참가자들은 인간이 아닌 아바타를 사용할 때 자기 환상을 경험했지만 그 수준은 인간 아바타를 사용할 때보다 상당히 낮았습니다. 따라서 자기 환상은 인간이 아닌 아바타의 경우에도 프로테우스 효과의 지각적 선행 요소로 작용하여 실수로 실제 손의 행동 제스처를 변경할 수 있습니다. 결론적으로 인간-아바타 제스처 일관성을 감지하면 프로테우스 효과를 평가하는 데 도움이 될 수 있습니다.",https://doi.org/10.1109/ISMAR62088.2024.00093,Perception & Cognition; Interaction & Input,Hand / Gesture Recognition,User Study; Technical Evaluation,Algorithm / Method; User Study / Empirical Findings
174,2024,Big Feet for Little People: Scaling Gap Affordance Judgments of Children and Adults with Virtual Feet,작은 사람들을 위한 큰 발: 가상 발을 이용한 어린이와 성인의 격차 여유도 판단 확장,"Virtual reality (VR) has become widely accessible through the development of more commercially available head-mounted displays (HMDs). This accessibility has particularly increased the use of VR in children. However, much of the previous research on understanding perception and action in virtual reality has only been conducted on adults, leaving many open questions about how children perceive and interact with virtual environments. In this paper, we examine whether there are age-related differences in judging the ability to step over a gap using immersive VR. Affordances are a useful measure for understanding objective perceptions of the actions that can be performed in an immersive virtual environment. Such measures are particularly well suited for children given they can easily respond yes or no as to whether they perceive that they can step over a gap. Further, manipulations of the size of virtual body parts could allow us to ascertain how much children rely on the perceived size of their bodies to make decisions about actions. In Experiment 1, adults and children saw motion-tracked virtual feet that were either larger or smaller than their actual foot size. They had to respond as to whether they could step over gaps that varied in width. They also gave perceptual estimates of the width of the gap in feet or meters. The results showed that adults who experience the smaller virtual feet underestimated their stepping ability more than adults with the larger feet. However, children had the opposite effect, such that seeing smaller virtual feet led to an overestimation of their stepping ability. To test whether this age-related difference in body scaling was due to misperception of foot size, adults and children matched virtual shoes to their actual feet size in Experiment 2. This matching task showed no perceptual differences between the age groups. Across our two experiments, we showed that children scale gap affordance judgments differently than adults and this difference cannot be explained by difference in perceptions of the size of virtual feet. The results suggest that children can effectively make perception and action judgments in virtual reality, but do not always do so in the same manner as adults. Such a finding has implications for the design of virtual reality games, educational tools, and training systems that are becoming increasingly common for children.","가상 현실(VR)은 보다 상업적으로 이용 가능한 헤드 마운트 디스플레이(HMD)의 개발을 통해 널리 접근 가능해졌습니다. 이러한 접근성으로 인해 특히 어린이의 VR 사용이 증가했습니다. 그러나 가상 현실의 인식과 행동을 이해하기 위한 이전 연구의 대부분은 성인을 대상으로만 수행되었기 때문에 어린이가 가상 환경을 어떻게 인식하고 상호 작용하는지에 대한 많은 질문이 남아 있습니다. 본 논문에서는 몰입형 VR을 활용하여 격차를 뛰어넘는 능력을 판단하는데 연령에 따른 차이가 있는지 살펴보았다. 어포던스는 몰입형 가상 환경에서 수행할 수 있는 행동에 대한 객관적인 인식을 이해하는 데 유용한 척도입니다. 이러한 조치는 어린이가 격차를 극복할 수 있다고 인식하는지 여부에 대해 쉽게 예 또는 아니오로 응답할 수 있다는 점에서 어린이에게 특히 적합합니다. 또한 가상 신체 부위의 크기를 조작하면 어린이가 행동에 대한 결정을 내릴 때 인식된 신체 크기에 얼마나 의존하는지 확인할 수 있습니다. 실험 1에서 성인과 어린이는 실제 발 크기보다 크거나 작은 모션 추적 가상 발을 보았습니다. 그들은 폭이 다양한 격차를 넘어갈 수 있는지에 대해 응답해야 했습니다. 그들은 또한 피트나 미터 단위로 간격의 폭에 대한 지각적 추정을 제공했습니다. 결과는 더 작은 가상 발을 경험한 성인이 더 큰 발을 가진 성인보다 걷기 능력을 더 과소평가하는 것으로 나타났습니다. 그러나 어린이들은 가상의 발이 더 작을수록 걷기 능력을 과대평가하는 등 반대 효과를 나타냈습니다. 신체 스케일링의 이러한 연령 관련 차이가 발 크기에 대한 잘못된 인식으로 인한 것인지 테스트하기 위해 성인과 어린이는 실험 2에서 가상 신발을 실제 발 크기와 일치시켰습니다. 이 일치 작업은 연령 그룹 간에 지각 차이가 없음을 보여주었습니다. 두 번의 실험을 통해 우리는 아이들이 성인과 다르게 격차 여유 판단을 확장하며 이러한 차이는 가상 발 크기에 대한 인식의 차이로 설명될 수 없음을 보여주었습니다. 결과는 어린이가 가상 현실에서 효과적으로 인식 및 행동 판단을 내릴 수 있지만 항상 성인과 같은 방식으로 수행하는 것은 아니라는 점을 시사합니다. 이러한 발견은 어린이에게 점점 더 일반화되고 있는 가상 현실 게임, 교육 도구 및 훈련 시스템의 설계에 영향을 미칩니다.",https://doi.org/10.1109/ISMAR62088.2024.00039,Display & Optics; Interaction & Input,Optical / Display Technology,Technical Evaluation,System / Framework
175,2024,Calibration of Augmented Reality Headset with External Tracking System Using AX=YB,AX=YB를 이용한 외부 추적 시스템을 갖춘 증강현실 헤드셋의 보정,"In Augmented Reality, a robust virtual-to-real calibration that aligns the virtual and real spaces is crucial to ensure accurate overlays. Inspired by popular methods in robotics, we propose establishing the virtual-to-real calibration as a hand-eye/robot-world calibration problem using an $A X=Y B$ formulation. This formulation uses both the self-localization of a head-mounted display and the tracking functionality of an external tracking system. Additional techniques are also provided to address the data synchronization issue between the two measurement systems. To further improve the results, we integrate a post-acquisition outlier filter based on the $A X-Y B$ Frobenius norm. Improvements resulting from the filter were first validated by simulation. For the assessment of the complete pipeline, both subjective evaluation based on human perception and objective evaluation based on computer vision methods were used. The results show that the proposed method is accurate and noise-resistant. With only 100 measurement samples collected, the overlay of a tracked object at the farthest distance (1300 mm) in front of the tracking system has an average rotation error of $1.77 \pm 0.54^{\circ}$ and an average translation error of $4.82 \pm 1.71 \mathrm{~mm}$.",증강 현실에서는 정확한 오버레이를 보장하기 위해 가상 공간과 실제 공간을 정렬하는 강력한 가상-실제 보정이 중요합니다. 로봇 공학의 인기 있는 방법에서 영감을 받아 $A X=Y B$ 공식을 사용하여 손-눈/로봇 세계 교정 문제로 가상-실제 교정을 설정하는 것을 제안합니다. 이 공식은 머리 장착형 디스플레이의 자체 위치 파악과 외부 추적 시스템의 추적 기능을 모두 사용합니다. 두 측정 시스템 간의 데이터 동기화 문제를 해결하기 위한 추가 기술도 제공됩니다. 결과를 더욱 향상시키기 위해 $A X-Y B$ Frobenius 표준을 기반으로 획득 후 이상치 필터를 통합했습니다. 필터로 인한 개선 사항은 먼저 시뮬레이션을 통해 검증되었습니다. 전체 파이프라인의 평가에는 인간의 인식을 기반으로 한 주관적 평가와 컴퓨터 비전 방법을 기반으로 한 객관적인 평가가 모두 사용되었습니다. 결과는 제안된 방법이 정확하고 잡음에 강하다는 것을 보여줍니다. 100개의 측정 샘플만 수집한 경우 추적 시스템 앞에서 가장 먼 거리(1300mm)에 있는 추적 개체의 오버레이에는 $1.77 \pm 0.54^{\circ}$의 평균 회전 오류와 $4.82 \pm 1.71 \mathrm{~mm}$의 평균 변환 오류가 있습니다.,https://doi.org/10.1109/ISMAR62088.2024.00035,Tracking & Localization; Display & Optics,Computer Vision,Quantitative Experiment,System / Framework; Algorithm / Method
176,2024,Clothes Vibration Device for Producing Tactile Stimuli to Evoke the Perception of Strong Wind,강풍 인지를 불러일으키는 촉각 자극을 생성하는 의류 진동 장치,"In virtual environments, wind displays can reproduce the sensation of wind to enhance user experience such as presence. Enabling users to perceive strong winds can benefit several applications, such as disaster education and entertainment content. However, the large and bulky equipment required to blow strong winds across the entire body of the user limit its application. Although cross-modal effects are proposed to alter wind perceptions through multisensory stimuli, but methods to induce the perception of strong winds without generating a strong wind need further exploration. In this study, we propose a method to create an illusion of strong wind by providing tactile stimuli through clothes. We developed a compact and lightweight wearable device that vibrates the fabric by a motor to simulate its fluttering motion under a strong wi nd. Two user studies were conducted to investigate the effect of vibrating the clothes on the perceived wind velocity, realness of the wind, and presence. The results indicated that tactile stimuli provided by vibrating the clothes increased the perceived wind velocity to a similar level as the strong wind exceeding $10 \mathrm{~m} / \mathrm{s}$ while the parameters of the physical wind were unchanged. It was also revealed that the vibration of the clothes decreases the realness of the actual wind and a visual scene of the clothes of the avatar flutter increases presence. Thus, the proposed wearable device is useful for creating the experience of strong winds while keeping the wind displays compact.","가상 환경에서 바람 디스플레이는 바람의 감각을 재현하여 현장감과 같은 사용자 경험을 향상시킬 수 있습니다. 사용자가 강풍을 인지할 수 있게 하면 재난 교육 및 엔터테인먼트 콘텐츠와 같은 여러 애플리케이션에 도움이 될 수 있습니다. 그러나 사용자의 몸 전체에 강한 바람을 불어넣어야 하는 크고 부피가 큰 장비는 적용에 한계가 있다. 다감각 자극을 통해 바람 인식을 변경하기 위해 교차 모드 효과가 제안되었지만 강한 바람을 생성하지 않고 강한 바람에 대한 인식을 유도하는 방법은 추가 탐색이 필요합니다. 본 연구에서는 옷을 통해 촉각자극을 제공하여 강풍의 환영을 만들어내는 방법을 제안한다. 우리는 모터로 직물을 진동시켜 강한 바람에 펄럭이는 움직임을 시뮬레이션하는 작고 가벼운 웨어러블 장치를 개발했습니다. 옷의 진동이 인지된 풍속, 바람의 현실성 및 존재감에 미치는 영향을 조사하기 위해 두 가지 사용자 연구가 수행되었습니다. 결과는 옷을 진동시켜 제공되는 촉각 자극이 감지 풍속을 $10 \mathrm{~m} / \mathrm{s}$를 초과하는 강풍과 유사한 수준으로 증가시키는 반면 물리적 바람의 매개변수는 변하지 않음을 나타냅니다. 또한, 옷의 진동으로 인해 실제 바람의 현실감이 감소하고, 아바타의 옷이 흔들리는 시각적 장면이 현장감을 높이는 것으로 나타났다. 따라서 제안된 웨어러블 디바이스는 바람 디스플레이를 컴팩트하게 유지하면서 강한 바람의 경험을 만드는 데 유용합니다.",https://doi.org/10.1109/ISMAR62088.2024.00143,Perception & Cognition,Sensor Fusion,Simulation,Hardware / Device
177,2024,Common Cues? Toward the Relationship of Spatial Presence and the Sense of Embodiment,일반적인 단서? 공간적 현존과 체현감의 관계를 중심으로,"The sense of presence and the sense of embodiment are two fundamental qualia, pivotal to many virtual reality experiences. Empirical research indicates a notable interdependence between these two qualia, where manipulations designed to affect one often exhibit a concurrent influence on the other. Existing theories on the development of qualia in virtual reality make no or only insufficient statements on this deep interdependence. In this work, we present a novel theoretical perspective on this connection. Based on existing theories, we argue that all the fundamental cues influencing one quale have the potential to impact the other one too. We present three studies ($n=42, n=42, n=32$) that generally support this novel perspective. Among other things, they show that traditional spatial presence cues such as head-tracking and passive depth cues (stereoscopy, linear perspective, etc.) can potentially also serve as embodiment cues. Conversely, they show that typical embodiment cues such as the visuotactile and visuoproprioceptive synchrony of a virtual hand are also spatial presence cues. The cues only differ in terms of how strongly they influence the respective quale. This novel perspective not only enhances our understanding of fundamental mechanics of virtual reality but it can also guide the development of more effective measurement instruments.","존재감과 구체화 감각은 많은 가상 현실 경험의 중추적인 두 가지 기본 특성입니다. 경험적 연구에 따르면 이 두 감각질 사이의 주목할만한 상호 의존성은 하나에 영향을 미치도록 고안된 조작이 종종 다른 감각질에도 동시에 영향을 미치는 것으로 나타납니다. 가상 현실의 감각질 발달에 관한 기존 이론은 이러한 깊은 상호의존성에 대해 전혀 설명하지 않거나 불충분하게 설명합니다. 본 연구에서는 이러한 연관성에 대한 새로운 이론적 관점을 제시합니다. 기존 이론을 바탕으로 우리는 한 특성에 영향을 미치는 모든 기본 단서가 다른 특성에도 영향을 미칠 가능성이 있다고 주장합니다. 우리는 일반적으로 이러한 새로운 관점을 뒷받침하는 세 가지 연구($n=42, n=42, n=32$)를 제시합니다. 무엇보다도 그들은 머리 추적 및 수동적 깊이 단서(입체경, 선형 원근법 등)와 같은 전통적인 공간 존재 단서가 잠재적으로 구체화 단서 역할을 할 수 있음을 보여줍니다. 반대로, 그들은 가상 손의 시각촉각 및 시각고유수용성 동시성과 같은 전형적인 실시예 단서가 공간적 존재 단서이기도 함을 보여줍니다. 단서는 각 특성에 얼마나 강하게 영향을 미치는지에 따라 다릅니다. 이 새로운 관점은 가상 현실의 기본 메커니즘에 대한 이해를 향상시킬 뿐만 아니라 보다 효과적인 측정 도구의 개발을 안내할 수도 있습니다.",https://doi.org/10.1109/ISMAR62088.2024.00128,Perception & Cognition; Interaction & Input,Haptic / Tactile Feedback,Quantitative Experiment,User Study / Empirical Findings
178,2024,Crafting Virtual Realities: Designing a VR End-User Authoring Platform for Personalised Exposure Therapy,가상 현실 제작: 맞춤형 노출 치료를 위한 VR 최종 사용자 저작 플랫폼 설계,"Abstract Exposure therapy (ET) gradually introduces people to the objects, animals, or situations they fear to help them overcome the angst with that source of anxiety. VR(ET) enables exposure to various triggers in the safety of the clinical or home environments. While prior work has explored how specific VR environments support therapy in contexts such as social anxiety or arachnophobia, therapy’s individualised nature is often overlooked. We used an iterative participatory design approach to develop an authoring platform for therapists, enabling them to tailor VR environments during exposure by changing and parameterising elements and reapplying past scenes. We used this platform as a design probe in a study with ten therapists to elicit discussions about the design of VRET experiences and therapists’ authoring needs. Findings highlight the value of controlling the stimuli presented to patients and deviating from stereotypical scenarios, and the importance of further investigating the therapist’s virtual representation.","추상 노출 치료(ET)는 사람들이 두려워하는 물체, 동물 또는 상황을 점차적으로 소개하여 불안의 원인이 되는 불안을 극복하도록 돕습니다. VR(ET)은 임상이나 가정 환경의 안전 속에서 다양한 유발 요인에 노출될 수 있도록 해줍니다. 이전 연구에서는 특정 VR 환경이 사회적 불안이나 거미공포증과 같은 맥락에서 치료를 어떻게 지원하는지 탐구했지만, 치료의 개별화된 특성은 종종 간과되었습니다. 우리는 치료사를 위한 저작 플랫폼을 개발하기 위해 반복적인 참여 디자인 접근 방식을 사용하여 요소를 변경 및 매개변수화하고 과거 장면을 다시 적용함으로써 노출 중에 VR 환경을 맞춤화할 수 있도록 했습니다. 우리는 VRET 경험의 디자인과 치료사의 저작 요구 사항에 대한 토론을 이끌어내기 위해 10명의 치료사와 함께한 연구에서 이 플랫폼을 디자인 프로브로 사용했습니다. 연구 결과는 환자에게 제시되는 자극을 제어하고 고정관념에서 벗어나는 것의 가치와 치료사의 가상 표현을 추가로 조사하는 것의 중요성을 강조합니다.",https://doi.org/10.1109/ISMAR62088.2024.00109,Medical & Healthcare; Content Authoring,Sensor Fusion,Other,User Study / Empirical Findings
179,2024,Cross-Domain Gender Identification Using VR Tracking Data,VR 추적 데이터를 활용한 도메인 간 성별 식별,"Recently, much work has been done to research personal identifiability of extended reality (XR) users. Many of these prior studies are task-specific and involve identifying users completing a specific XR task. On the other hand, some studies have been domainspecific and focus on identifying users completing different XR tasks from the same domain, such as watching 360° videos or assembling structures. In this paper, we present one of the few studies to investigate cross-domain identification (i.e., identifying users completing XR tasks from different domains). To facilitate our investigation, we used open-source datasets from two different virtual reality (VR) studies-one from an assembly domain and one from a gaming domain-to investigate the feasibility of cross-domain gender identification, as personal identification is not possible between these datasets. The results of our machine learning experiments clearly demonstrate that cross-domain gender identification is more difficult than domain-specific gender identification. Furthermore, our results indicate that head position is important for gender identification and demonstrate that the k-nearest neighbors (kNN) algorithm is not suitable for cross-domain gender identification, which future researchers should be aware of.","최근에는 확장 현실(XR) 사용자의 개인 식별 가능성을 연구하기 위해 많은 작업이 수행되었습니다. 이러한 이전 연구 중 다수는 작업별로 이루어졌으며 특정 XR 작업을 완료하는 사용자를 식별하는 것과 관련됩니다. 반면, 일부 연구는 도메인별로 진행되었으며 360° 비디오 시청, 구조물 조립 등 동일한 도메인에서 다양한 XR 작업을 완료하는 사용자를 식별하는 데 중점을 두었습니다. 본 논문에서는 교차 도메인 식별(즉, 다른 도메인에서 XR 작업을 완료하는 사용자 식별)을 조사하는 몇 안 되는 연구 중 하나를 제시합니다. 조사를 용이하게 하기 위해 우리는 두 개의 서로 다른 가상 현실(VR) 연구(어셈블리 도메인과 게임 도메인의 하나)의 오픈 소스 데이터세트를 사용하여 도메인 간 성별 식별의 타당성을 조사했습니다. 이러한 데이터세트 간에는 개인 식별이 불가능하기 때문입니다. 우리의 기계 학습 실험 결과는 교차 도메인 성별 식별이 도메인별 성별 식별보다 더 어렵다는 것을 분명히 보여줍니다. 또한, 우리의 결과는 머리 위치가 성별 식별에 중요하다는 것을 나타내며 kNN(k-nearest neighbor) 알고리즘이 향후 연구자가 알아야 할 교차 도메인 성별 식별에 적합하지 않음을 보여줍니다.",https://doi.org/10.1109/ISMAR62088.2024.00032,Education & Training,Other,Technical Evaluation,Algorithm / Method
180,2024,Crossing Rays: Evaluation of Bimanual Mid-air Selection Techniques in an Immersive Environment,교차 광선: 몰입형 환경에서 양방향 공중 선택 기술 평가,"Mid-air navigation offers a method of aerial travel that mitigates the constraints associated with continuous navigation. A mid-air selection technique is essential to enable such navigation. In this paper, we consider four variations of intersection-based bimanual midair selection techniques with visual aids and supporting features: Simple-Ray, Simple-Stripe, Precision-Stripe, and Cursor-Sync. We evaluate their performance and user experience compared to an unimanual mid-air selection technique using two tasks that require selecting a mid-air position with or without a reference object. Our findings indicate that the bimanual techniques generally demonstrate faster selection times compared to the unimanual technique. With a supporting feature, the bimanual techniques can provide a more accurate selection than the unimanual technique. Based on our results, we discuss the effect of selection technique’s visual aids and supporting features on performance and user experience for mid-air selection.","공중 항법은 지속적인 항법과 관련된 제약을 완화하는 항공 여행 방법을 제공합니다. 이러한 탐색을 위해서는 공중 선택 기술이 필수적입니다. 본 논문에서는 Simple-Ray, Simple-Stripe, Precision-Stripe 및 Cursor-Sync라는 시각적 도구와 지원 기능을 갖춘 교차 기반 양손 공중 선택 기술의 네 가지 변형을 고려합니다. 우리는 참조 객체 유무에 관계없이 공중 위치를 선택해야 하는 두 가지 작업을 사용하여 단일 수동 공중 선택 기술과 비교하여 성능과 사용자 경험을 평가합니다. 우리의 연구 결과에 따르면 양손 기술은 일반적으로 단방향 기술에 비해 선택 시간이 더 빠르다는 것을 나타냅니다. 지원 기능을 사용하면 양손 기술이 단방향 기술보다 더 정확한 선택을 제공할 수 있습니다. 우리의 결과를 바탕으로 선택 기법의 시각적 도구와 지원 기능이 공중 선택의 성능과 사용자 경험에 미치는 영향을 논의합니다.",https://doi.org/10.1109/ISMAR62088.2024.00047,Interaction & Input,Other,Quantitative Experiment,Algorithm / Method; User Study / Empirical Findings
181,2024,Crowd Data-driven Artwork Placement in Virtual Exhibitions for Visitor Density Distribution Planning,방문객 밀도 분포 계획을 위한 가상 전시회에 군중 데이터 기반 예술 작품 배치,"We propose a novel crowd data-driven optimization approach for artwork placement in virtual exhibitions. With the emerging concept of Metaverse, a multitude of users can engage with content contemporaneously in virtual exhibitions. Yet, few studies have suggested a method to resolve crowd density concentration in multiuser Mixed Reality (MR) and Virtual Reality (VR) environments. In this study, our approach leverages crowd data engaged with artworks to predict optimal placement of artworks to distribute crowd density in virtual exhibitions, prior to exhibition planning. To investigate the requirement and validity of our approach, we conducted focus group interviews and an artwork relocation experiment as preliminary studies. In the generation of solution scenes for optimal placement, our optimizer adaptively scrutinizes placeable areas with considerations of crowd density distribution, scene rationality, and artwork similarity. Through a performance comparison analysis between optimization results, we confirmed that the optimizer successfully fulfilled the intended objectives with respect to the design considerations, resolving practical scenarios in exhibition planning.","우리는 가상 전시회에서 작품 배치를 위한 새로운 군중 데이터 기반 최적화 접근 방식을 제안합니다. 메타버스(Metaverse)라는 새로운 개념이 등장하면서 다수의 사용자가 가상 ​​전시를 통해 동시에 콘텐츠에 참여할 수 있게 되었습니다. 그러나 다중 사용자 혼합 현실(MR) 및 가상 현실(VR) 환경에서 군중 밀도 집중을 해결하는 방법을 제안한 연구는 거의 없습니다. 본 연구에서 우리의 접근 방식은 미술품과 관련된 군중 데이터를 활용하여 미술품의 최적 배치를 예측하여 전시 계획 이전에 가상 전시의 군중 밀도를 분산시킵니다. 우리 접근 방식의 요구 사항과 타당성을 조사하기 위해 예비 연구로 포커스 그룹 인터뷰와 작품 재배치 실험을 수행했습니다. 최적의 배치를 위한 솔루션 장면 생성에서 당사의 최적화 프로그램은 군중 밀도 분포, 장면 합리성 및 작품 유사성을 고려하여 배치 가능한 영역을 적응적으로 면밀히 조사합니다. 최적화 결과 간의 성능 비교 분석을 통해 최적화 프로그램이 디자인 고려 사항에 대해 의도한 목표를 성공적으로 달성하고 전시 기획의 실제 시나리오를 해결했음을 확인했습니다.",https://doi.org/10.1109/ISMAR62088.2024.00075,Interaction & Input,Optical / Display Technology,Qualitative Analysis,Algorithm / Method
182,2024,DataliVR: Transformation of Data Literacy Education through Virtual Reality with ChatGPT-Powered Enhancements,DataliVR: ChatGPT 기반 강화 기능을 갖춘 가상 현실을 통한 데이터 활용 능력 교육의 혁신,"Data literacy is essential in today’s data-driven world, emphasizing individuals’ abilities to effectively manage data and extract meaningful insights. However, traditional classroom-based educational approaches often struggle to fully address the multifaceted nature of data literacy. As education undergoes digital transformation, innovative technologies such as Virtual Reality (VR) offer promising avenues for immersive and engaging learning experiences. This paper introduces DataliVR, a pioneering VR application aimed at enhancing the data literacy skills of university students within a contextual and gamified virtual learning environment. By integrating Large Language Models (LLMs) like ChatGPT as a conversational artificial intelligence (AI) chatbot embodied within a virtual avatar, DataliVR provides personalized learning assistance, enriching user learning experiences. Our study employed an experimental approach, with chatbot availability as the independent variable, analyzing learning experiences and outcomes as dependent variables with a sample of thirty participants. Our approach underscores the effectiveness and user-friendliness of ChatGPT-powered DataliVR in fostering data literacy skills. Moreover, our study examines the impact of the ChatGPT-based AI chatbot on users’ learning, revealing significant effects on both learning experiences and outcomes. Our study presents a robust tool for fostering data literacy skills, contributing significantly to the digital advancement of data literacy education through cutting-edge VR and AI technologies. Moreover, our research provides valuable insights and implications for future research endeavors aiming to integrate LLMs (e.g., ChatGPT) into educational VR platforms.","데이터 활용 능력은 오늘날의 데이터 중심 세계에서 필수적이며, 데이터를 효과적으로 관리하고 의미 있는 통찰력을 추출하는 개인의 능력을 강조합니다. 그러나 전통적인 교실 기반 교육 접근 방식은 데이터 활용 능력의 다면적 특성을 완전히 해결하는 데 어려움을 겪는 경우가 많습니다. 교육이 디지털 전환을 거치면서 가상 현실(VR)과 같은 혁신적인 기술은 몰입감 있고 매력적인 학습 경험을 위한 유망한 방법을 제공합니다. 이 문서에서는 상황에 맞는 게임화된 가상 학습 환경에서 대학생의 데이터 활용 능력을 향상시키는 것을 목표로 하는 선구적인 VR 애플리케이션인 DataliVR을 소개합니다. ChatGPT와 같은 대형 언어 모델(LLM)을 가상 아바타 내에 구현된 대화형 인공 지능(AI) 챗봇으로 통합함으로써 DataliVR은 맞춤형 학습 지원을 제공하여 사용자 학습 경험을 풍부하게 합니다. 우리 연구에서는 챗봇 가용성을 독립 변수로 하는 실험적 접근 방식을 사용하여 30명의 참가자 샘플을 사용하여 학습 경험과 결과를 종속 변수로 분석했습니다. 우리의 접근 방식은 데이터 활용 능력 육성에 있어 ChatGPT 기반 DataliVR의 효율성과 사용자 친화성을 강조합니다. 또한, 우리의 연구에서는 ChatGPT 기반 AI 챗봇이 사용자의 학습에 미치는 영향을 조사하여 학습 경험과 결과 모두에 중요한 영향을 미치는 것으로 나타났습니다. 우리의 연구는 데이터 활용 능력 육성을 위한 강력한 도구를 제시하며, 최첨단 VR 및 AI 기술을 통해 데이터 활용 능력 교육의 디지털 발전에 크게 기여합니다. 또한, 우리의 연구는 LLM(예: ChatGPT)을 교육용 VR 플랫폼에 통합하려는 향후 연구 노력에 귀중한 통찰력과 시사점을 제공합니다.",https://doi.org/10.1109/ISMAR62088.2024.00026,Education & Training,Deep Learning / Neural Networks; Natural Language Processing,User Study,Algorithm / Method
183,2024,Design and Clinical Evaluation of ARAS: An Augmented Reality Assistance System for Open Pancreatic Surgery,ARAS의 설계 및 임상 평가: 개복췌장수술을 위한 증강현실 지원 시스템,"The integration of Augmented Reality (AR) technology into surgical procedures offers significant potential to enhance clinical outcomes. While there are plenty of lab-proven prototypes, systems employed in actual clinical settings require specialized design and rigorous clinical evaluation of these AR-based solutions to meet the high demands of complex medical fields. Our research exposes these complex requirements emerging from clinical environments, such as operation theaters. To address the challenges, we introduce ARAS, an operational AR assistance system for live open pancreatic surgery. Employing a user-centric design methodology, we designed and refined ARAS through several iterations, ensuring its practical applicability and effectiveness in a real-world surgical setting during clinical trials. ARAS enables in situ and precise visualization of the patient’s 3D reconstructed vascular system and tumor during the surgical procedure. We evaluated ARAS through clinical trials (N=7) involving patients diagnosed with pancreatic cancer. Our interviews with the surgeons underscored the utility of ARAS for open pancreatic surgery, especially in critical and highly time-pressured phases of the surgery, as it proved to be exceptionally beneficial in aiding surgeons during in their decision-making process. In a post-surgery evaluation, the surgeons also certified the precise visualization accuracy of ARAS during those critical phases. Our findings showcased the heightened requirements for AR-based solutions in operational clinical use and proved that ARAS met the challenges emerging during live surgeries. Consequently, surgical AR assistance systems do have a transformative potential to revolutionize traditional practices, but their applicability is subject to high design constraints during critical medical procedures.","증강 현실(AR) 기술을 수술 절차에 통합하면 임상 결과를 향상시킬 수 있는 상당한 잠재력이 제공됩니다. 실험실에서 입증된 프로토타입이 많이 있지만 실제 임상 환경에 사용되는 시스템에는 복잡한 의료 분야의 높은 요구 사항을 충족하기 위해 이러한 AR 기반 솔루션에 대한 전문적인 설계와 엄격한 임상 평가가 필요합니다. 우리의 연구는 수술실과 같은 임상 환경에서 나타나는 이러한 복잡한 요구 사항을 보여줍니다. 이러한 문제를 해결하기 위해 라이브 개복 췌장 수술을 위한 운영 AR 지원 시스템인 ARAS를 소개합니다. 사용자 중심 설계 방법론을 사용하여 ARAS를 여러 번의 반복을 통해 설계하고 개선하여 임상 시험 중 실제 수술 환경에서 실제 적용 가능성과 효율성을 보장했습니다. ARAS를 사용하면 수술 과정에서 환자의 3D 재구성 혈관계와 종양을 현장에서 정밀하게 시각화할 수 있습니다. 췌장암 진단을 받은 환자를 대상으로 한 임상시험(N=7)을 통해 ARAS를 평가했습니다. 외과 의사와의 인터뷰를 통해 ARAS가 개복 췌장 수술, 특히 중요하고 시간 압박이 심한 수술 단계에서 ARAS의 유용성을 강조했습니다. 이는 외과 의사의 의사 결정 과정을 지원하는 데 매우 유용한 것으로 입증되었기 때문입니다. 수술 후 평가에서 외과의사는 중요한 단계에서 ARAS의 정확한 시각화 정확도를 인증했습니다. 우리의 연구 결과는 수술적 임상 사용에서 AR 기반 솔루션에 대한 요구 사항이 높아졌음을 보여주고 ARAS가 실제 수술 중에 나타나는 과제를 충족한다는 것을 입증했습니다. 결과적으로, 수술용 AR 보조 시스템은 전통적인 관행을 혁신할 수 있는 혁신적인 잠재력을 가지고 있지만, 중요한 의료 절차 중에 적용 가능성에는 높은 설계 제약이 따릅니다.",https://doi.org/10.1109/ISMAR62088.2024.00052,Medical & Healthcare,Other,Quantitative Experiment,System / Framework
184,2024,Design and Evaluation of Controller-based Raycasting Methods for Secure and Efficient Text Entry in Virtual Reality,가상 현실에서 안전하고 효율적인 텍스트 입력을 위한 컨트롤러 기반 레이캐스팅 방법의 설계 및 평가,"With the exponential growth of digital information, ensuring text security, a fundamental component of information security, becomes increasingly paramount. While authentication remains a primary focus for data access control and protection, the rich sensor ecosystem and immersive experiences of virtual reality (VR) environments introduce new privacy risks, particularly with inconspicuous sensors like motion and location sensors. In this context, protecting the security of text entered by users poses a unique challenge. This paper explores the feasibility of enhancing text security by introducing variability in virtual input tools during typing processes. Specifically, we investigate the impact of introducing successive and random intermittent variations to the virtual ray (start point and direction) with controller-based raycasting techniques on text security and typing experience. The results demonstrate that introducing variability in virtual ray effectively protects regular text and passwords. Random intermittent introducing variability balances security and user experience for regular text. These findings provide insights into enhancing text security beyond authentication and defending against the potential risks in VR environments.","With the exponential growth of digital information, ensuring text security, a fundamental component of information security, becomes increasingly paramount. 인증은 데이터 액세스 제어 및 보호의 주요 초점으로 남아 있지만, 풍부한 센서 생태계와 가상 현실(VR) 환경의 몰입형 경험은 특히 모션 및 위치 센서와 같이 눈에 띄지 않는 센서의 경우 새로운 개인 정보 보호 위험을 초래합니다. 이러한 맥락에서 사용자가 입력한 텍스트의 보안을 보호하는 것은 독특한 과제를 안겨줍니다. 이 문서에서는 입력 프로세스 중 가상 입력 도구에 가변성을 도입하여 텍스트 보안을 강화하는 타당성을 탐구합니다. 특히 컨트롤러 기반 레이캐스팅 기술을 사용하여 가상 광선(시작점 및 방향)에 연속적이고 무작위 간헐적인 변형을 도입하는 것이 텍스트 보안 및 타이핑 경험에 미치는 영향을 조사합니다. 결과는 Virtual Ray에 가변성을 도입하면 일반 텍스트와 비밀번호를 효과적으로 보호한다는 것을 보여줍니다. 무작위로 간헐적으로 도입되는 가변성은 일반 텍스트에 대한 보안과 사용자 경험의 균형을 유지합니다. 이러한 결과는 인증을 넘어 텍스트 보안을 강화하고 VR 환경의 잠재적 위험을 방어하는 방법에 대한 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR62088.2024.00049,Privacy & Security; Interaction & Input,Other,Other,Algorithm / Method; User Study / Empirical Findings
185,2024,Detectability of ETHD Position and Speed Redirection for VR Haptics,VR 햅틱을 위한 ETHD 위치 및 속도 리디렉션 감지 가능성,"Abstract An encountered-type haptic device (ETHD) moves a physical object to align it with the virtual object with which the user of a virtual reality application makes contact, providing haptic feedback. One limitation of ETHDs is their limited reachability due to mechanical constraints. One approach to extending the reachability of an ETHD involves redirection, achieved by altering the pose of the user’s body, hand, or handheld prop in the virtual world. While previous studies have quantified the detection thresholds of redirection in the context of stationary objects, ETHD’s present the opportunity and the need to study redirection in the context of dynamic objects. This paper presents a user study $(\mathrm{N}=25)$ with two experiments aimed at investigating whether dynamic object properties, such as direction and speed, significantly affect redirection detection thresholds. The first experiment finds that the mere presence of dynamic objects does not decrease the detection threshold. Consequently, previously measured detection thresholds remain applicable for ETHD reachability extension. However, the second experiment unveils a crucial relationship between the speed of dynamic objects and the original reachability of the ETHD. Although the virtual object can move $15 \mathrm{~cm} / \mathrm{s}$ faster than the ETHD, this increase in speed is insufficient to compensate for the extended reachability enabled by the detection threshold.","Abstract ETHD(조우형 촉각 장치)는 가상 현실 애플리케이션 사용자가 접촉하는 가상 객체에 물리적 객체를 이동시켜 촉각 피드백을 제공합니다. ETHD의 한 가지 한계는 기계적 제약으로 인해 접근이 제한된다는 것입니다. ETHD의 도달 가능성을 확장하는 한 가지 접근 방식은 가상 세계에서 사용자의 신체, 손 또는 휴대용 소품의 자세를 변경하여 달성되는 리디렉션을 포함합니다. 이전 연구에서는 고정 개체의 맥락에서 리디렉션의 감지 임계값을 정량화했지만 ETHD는 동적 개체의 맥락에서 리디렉션을 연구할 기회와 필요성을 제시합니다. This paper presents a user study $(\mathrm{N}=25)$ with two experiments aimed at investigating whether dynamic object properties, such as direction and speed, significantly affect redirection detection thresholds. 첫 번째 실험에서는 단순히 동적 개체가 존재한다고 해서 감지 임계값이 감소하지 않는다는 사실을 발견했습니다. 결과적으로, 이전에 측정된 탐지 임계값은 ETHD 도달 가능성 확장에 계속 적용됩니다. 그러나 두 번째 실험에서는 동적 객체의 속도와 ETHD의 원래 도달 가능성 사이의 중요한 관계가 밝혀졌습니다. 가상 객체가 ETHD보다 $15 \mathrm{~cm} / \mathrm{s}$ 더 빠르게 이동할 수 있지만 이러한 속도 증가는 감지 임계값에 의해 활성화된 확장된 도달 가능성을 보상하기에는 충분하지 않습니다.",https://doi.org/10.1109/ISMAR62088.2024.00028,Interaction & Input,Haptic / Tactile Feedback,User Study,Algorithm / Method; User Study / Empirical Findings
186,2024,Detecting in-car VR Motion Sickness from Lower Face Action Units,하부 얼굴 액션 유닛을 통해 차량 내 VR 멀미 감지,"This paper presents the first in-car VR motion sickness (VRMS) detection model based on lower face action units (LF-AUs). Initially developed in a simulated in-car environment with 78 participants, the model’s generalizability was later tested in realworld driving conditions. Motion sickness was induced using visual linear motion in the VR headset and physical horizontal rotation via a rotating chair. We used a convolutional neural network (MobileNetV3) to automatically extract LF-AUs from images of the users’ mouth region, captured by the VR headset’s built-in camera. These LF-AUs were then used to train a Support Vector Regression (SVR) model to estimate motion sickness scores. We compared the SVR model’s performance using LF-AUs, pupil diameters, and physiological features (individually and in combination) from the same VR headset. Results showed that both individual LF-AU (right dimple) and combined LF-AUs had significant Pearson correlations with self-reported motion sickness scores and achieved lower root mean squared error compared to pupil diameters. The best detection results were obtained by combining LF-AUs and pupil diameters, while physiological features alone did not yield significant results. The LF-AUs-based model demonstrated encouraging generalizability across different settings in the independent studies.","본 논문에서는 LF-AU(Lower Face Action Unit)를 기반으로 한 최초의 차량 내 VR 멀미(VRMS) 감지 모델을 제시합니다. 처음에는 78명의 참가자가 참여하는 시뮬레이션된 차량 내 환경에서 개발되었으며, 나중에 실제 운전 조건에서 모델의 일반화 가능성을 테스트했습니다. VR 헤드셋의 시각적 선형 운동과 회전 의자를 통한 물리적 수평 회전을 사용하여 멀미를 유발했습니다. 우리는 VR 헤드셋에 내장된 카메라로 촬영한 사용자 입 영역의 이미지에서 자동으로 LF-AU를 추출하기 위해 컨볼루셔널 신경망(MobileNetV3)을 사용했습니다. 그런 다음 이러한 LF-AU를 사용하여 SVR(지원 벡터 회귀) 모델을 훈련하여 멀미 점수를 추정했습니다. 동일한 VR 헤드셋의 LF-AU, 동공 직경, 생리적 특징(개별 및 조합)을 사용하여 SVR 모델의 성능을 비교했습니다. 결과는 개별 LF-AU(오른쪽 보조개)와 결합된 LF-AU 모두 자체 보고된 멀미 점수와 Pearson 상관관계가 유의미하며 동공 직경에 비해 더 낮은 평균 제곱근 오차를 달성한 것으로 나타났습니다. LF-AU와 동공 직경을 결합하여 최상의 검출 결과를 얻었지만, 생리학적 특징만으로는 유의미한 결과를 얻지 못했습니다. LF-AU 기반 모델은 독립적인 연구에서 다양한 설정에 걸쳐 일반화 가능성을 장려하는 것으로 나타났습니다.",https://doi.org/10.1109/ISMAR62088.2024.00118,Interaction & Input,Deep Learning / Neural Networks,User Study,Algorithm / Method
187,2024,Direction-Based Authentication: Combining Symbolic Input and Contextual Cues for Virtual Reality Password Entry,방향 기반 인증: 가상 현실 비밀번호 입력을 위한 기호 입력과 상황별 단서 결합,"This paper presents Direction-Based Authentication (DBA), a novel authentication method for virtual reality that combines symbolic input and contextual information to balance efficiency with memorability. In DBA, the password consists of four view directions selected by the user across four virtual environments. The user can either physically turn their head or use buttons to select directions, and remember the directions by either symbol or visual context. We conducted a within-subjects study (N=32) to evaluate the efficiency, memorability, and security compared to methods based on symbols or context only. The results demonstrated that password entry with DBA was more efficient than the contextual-only approach. While recall rate was not significantly higher than the symbolic-only method, participants’ subjective ratings indicated that DBA better supported memorability. All three methods were highly resistant to observational attack, although the user-defined passwords appeared homogeneous in certain cases. Overall, the study shows that combining symbolic and contextual information is promising to balance efficiency with memorability for VR authentication, but potential usability and security issues can arise without careful consideration. Based on these findings, we discussed future directions for optimizing the usability and security of DBA and insights regarding participants’ reliance on symbolic vs. visual information.","본 논문에서는 기호 입력과 상황 정보를 결합하여 효율성과 기억력의 균형을 맞추는 가상 현실을 위한 새로운 인증 방법인 방향 기반 인증(DBA)을 제시합니다. In DBA, the password consists of four view directions selected by the user across four virtual environments. 사용자는 물리적으로 머리를 돌리거나 버튼을 사용하여 방향을 선택할 수 있으며 기호나 시각적 맥락을 통해 방향을 기억할 수 있습니다. 우리는 기호나 맥락에만 기반한 방법과 비교하여 효율성, 기억성 및 보안을 평가하기 위해 과목 내 연구(N=32)를 수행했습니다. 결과는 DBA를 사용한 비밀번호 입력이 상황별 접근 방식보다 더 효율적이라는 것을 보여주었습니다. 회상률은 기호 전용 방법보다 크게 높지는 않았지만 참가자의 주관적 평가에서는 DBA가 기억력을 더 잘 지원하는 것으로 나타났습니다. 세 가지 방법 모두 관찰 공격에 대한 저항력이 매우 높았지만 특정 경우에는 사용자 정의 비밀번호가 동일하게 나타났습니다. 전반적으로, 이 연구는 상징적 정보와 맥락적 정보를 결합하면 VR 인증의 효율성과 기억력 사이의 균형을 유지할 수 있지만 신중한 고려 없이 잠재적인 사용성 및 보안 문제가 발생할 수 있음을 보여줍니다. 이러한 결과를 바탕으로 우리는 DBA의 유용성과 보안을 최적화하기 위한 향후 방향과 참가자의 상징적 정보와 시각적 정보에 대한 의존도에 대한 통찰력을 논의했습니다.",https://doi.org/10.1109/ISMAR62088.2024.00083,Privacy & Security,Other,User Study,Algorithm / Method
188,2024,Do you read me? (E)motion Legibility of Virtual Reality Character Representations,내 말을 읽어요? (E)가상현실 캐릭터 표현의 모션 가독성,"We compared the body movements of five virtual reality (VR) avatar representations in a user study $(\mathrm{N}=53)$ to ascertain how well these representations could convey body motions associated with different emotions: one head-and-hands representation using only tracking data, one upper-body representation using inverse kinematics (IK), and three full-body representations using IK, motioncapture, and the state-of-the-art deep-learning model AGRoL. Participants’ emotion detection accuracies were similar for the IK and AGRoL representations, highest for the full-body motion-capture representation and lowest for the head-and-hands representation. Our findings suggest that from the perspective of emotion expressivity, connected upper-body parts that provide visual continuity improve clarity, and that current techniques for algorithmically animating the lower-body are ineffective. In particular, the deep-learning technique studied did not produce more expressive results, suggesting the need for training data specifically made for social VR applications.","우리는 사용자 연구 $(\mathrm{N}=53)$에서 5가지 가상 현실(VR) 아바타 표현의 신체 움직임을 비교하여 이러한 표현이 다양한 감정과 관련된 신체 동작을 얼마나 잘 전달할 수 있는지 확인했습니다. 하나는 추적 데이터만 사용한 머리와 손 표현, 역운동학(IK)을 사용한 하나의 상체 표현, IK, 모션 캡처 및 최첨단 딥 러닝 모델 AGRoL을 사용한 세 가지 전신 표현입니다. 참가자의 감정 감지 정확도는 IK 및 AGRoL 표현에서 유사했으며 전신 모션 캡처 표현에서 가장 높았고 머리와 손 표현에서 가장 낮았습니다. 우리의 연구 결과는 감정 표현의 관점에서 시각적 연속성을 제공하는 연결된 상체 부분이 명확성을 향상시키고 하체를 알고리즘적으로 애니메이션화하는 현재 기술이 효과적이지 않다는 것을 시사합니다. 특히, 연구된 딥러닝 기술은 더 표현적인 결과를 생성하지 못했으며, 이는 소셜 VR 애플리케이션을 위해 특별히 제작된 훈련 데이터의 필요성을 시사합니다.",https://doi.org/10.1109/ISMAR62088.2024.00044,Education & Training,Other,User Study; Technical Evaluation,Algorithm / Method; User Study / Empirical Findings
189,2024,Does Voice Matter? The Effect of Verbal Communication and Asymmetry on the Experience of Collaborative Social XR,목소리가 중요합니까? 언어적 의사소통과 비대칭성이 협업 소셜 XR 경험에 미치는 영향,"This work evaluates how the asymmetry of device configurations and verbal communication influence the user experience of social eXtended Reality (XR) for self-perception, other-perception, and task perception. We developed an application that enables social collaboration between two users with varying device configurations.We compare the conditions of one symmetric interaction, where both device configurations are Head-Mounted Displays (HMDs) with tracked controllers, with the conditions of one asymmetric interaction, where one device configuration is an HMD with tracked controllers and the other device configuration is a desktop screen with a mouse. In our study, 52 participants collaborated in a dyadic interaction on a sorting task while talking to each other. We compare our results to previous work that evaluated the same scenario without verbal communication. In line with prior research, self-perception is influenced by the immersion of the used device configuration and verbal communication. While co-presence was not affected by the device configuration or the inclusion of verbal communication, social presence was only higher for HMD configurations that allowed verbal communication. Task perception was hardly affected by the device configuration or verbal communication. We conclude that the device in social XR is important for self-perception with or without verbal communication. However, the results indicate that the device configuration only affects the qualities of social interaction in collaborative scenarios when verbal communication is enabled. To sum up, asymmetric collaboration maintains the high quality of self-perception and interaction for highly immersed users while still enabling the participation of less immersed users.","이 작업은 장치 구성과 언어 의사소통의 비대칭성이 자기 인식, 타인 인식 및 작업 인식에 대한 사회적 확장 현실(XR)의 사용자 경험에 어떻게 영향을 미치는지 평가합니다. 우리는 다양한 장치 구성을 가진 두 사용자 간의 소셜 협업을 가능하게 하는 애플리케이션을 개발했습니다. 두 장치 구성이 모두 추적 컨트롤러가 있는 머리 장착형 디스플레이(HMD)인 하나의 대칭 상호 작용 조건과 하나의 장치 구성이 추적 컨트롤러가 있는 HMD이고 다른 장치 구성이 마우스가 있는 데스크톱 화면인 하나의 비대칭 상호 작용 조건을 비교합니다. 우리 연구에서 52명의 참가자는 서로 대화하면서 분류 작업에 대한 일대일 상호 작용으로 협력했습니다. 우리는 구두 의사소통 없이 동일한 시나리오를 평가한 이전 작업과 결과를 비교합니다. 선행연구와 마찬가지로 자기지각은 사용된 장치 구성의 몰입도와 언어적 의사소통에 의해 영향을 받는다. 공존은 장치 구성이나 언어 의사소통 포함에 의해 영향을 받지 않았지만, 언어 의사소통을 허용하는 HMD 구성에서만 사회적 존재감이 더 높았습니다. 작업 인식은 장치 구성이나 언어 의사소통에 거의 영향을 받지 않았습니다. 우리는 소셜 XR의 장치가 언어적 의사소통 유무에 관계없이 자기 인식에 중요하다는 결론을 내렸습니다. 그러나 결과는 장치 구성이 음성 통신이 활성화된 경우 협업 시나리오에서 사회적 상호 작용의 품질에만 영향을 미친다는 것을 나타냅니다. 요약하자면, 비대칭 협업은 몰입도가 높은 사용자의 자기 인식과 상호 작용의 높은 품질을 유지하는 동시에 몰입도가 낮은 사용자의 참여도 가능하게 합니다.",https://doi.org/10.1109/ISMAR62088.2024.00129,Collaboration & Social; Perception & Cognition,Natural Language Processing,User Study,User Study / Empirical Findings
190,2024,Don't Record My Private pARts: Understanding The Role of Sensitive Contexts and Privacy Perceptions in Influencing Attitudes Towards Everyday Augmented Reality Sensor Usage,내 사적인 부분을 기록하지 마세요: 일상적인 증강 현실 센서 사용에 대한 태도에 영향을 미치는 민감한 상황과 개인 정보 보호 인식의 역할 이해,"Everyday Augmented Reality (AR) headsets come with an array of sensing capabilities. Users wearing these headsets for extended periods may prefer specific sensors to remain inactive in some contexts for privacy and sensitivity reasons. Currently, the contexts in which users wish to limit sensor data collection are unclear. To explore this, we conducted a survey ($\mathrm{N}=100$), collecting 552 scenarios to understand which situations users wish to restrict or completely block data collection by specific sensors or combinations on their AR headset. Our results show the sensitive contexts can be classified into seven categories: 1) presence of confidential information; 2) risk of data quantification; 3) expectation of solitude; 4) rules prohibiting data collection; 5) modesty and nudity; 6) home environments; and 7) outdoor public locations. Our results provide insights into privacy-invasive contexts when people want to limit and restrict their AR sensors, building towards automating permission configurations during the prolonged use of everyday AR headsets.",일상적인 증강 현실(AR) 헤드셋에는 다양한 감지 기능이 함께 제공됩니다. 장기간 이러한 헤드셋을 착용하는 사용자는 개인 정보 보호 및 민감도상의 이유로 특정 센서가 특정 상황에서 비활성 상태로 유지되는 것을 선호할 수 있습니다. 현재 사용자가 센서 데이터 수집을 제한하려는 맥락이 불분명합니다. 이를 탐색하기 위해 우리는 사용자가 AR 헤드셋의 특정 센서 또는 조합에 의한 데이터 수집을 제한하거나 완전히 차단하려는 상황을 이해하기 위해 552개의 시나리오를 수집하는 설문 조사($\mathrm{N}=100$)를 수행했습니다. 우리의 결과는 민감한 상황이 7가지 범주로 분류될 수 있음을 보여줍니다. 1) 기밀 정보의 존재; 2) 데이터 정량화의 위험; 3) 고독에 대한 기대; 4) 데이터 수집을 금지하는 규칙 5) 정숙함과 과도한 노출; 6) 가정 환경; 7) 야외 공공 장소. 우리의 결과는 사람들이 일상적인 AR 헤드셋을 장기간 사용하는 동안 권한 구성을 자동화하기 위해 AR 센서를 제한하고 싶어할 때 개인 정보 침해 상황에 대한 통찰력을 제공합니다.,https://doi.org/10.1109/ISMAR62088.2024.00090,Perception & Cognition,Other,Questionnaire / Survey,Hardware / Device
191,2024,Effects of Organizational and Behavioral Reactions of Virtual Crowds on Users' Affect and Behavior in a Simulated Stressful Situation,시뮬레이션된 스트레스 상황에서 가상 군중의 조직적 및 행동적 반응이 사용자의 영향과 행동에 미치는 영향,"Evidence from the behavioral psychology literature suggests that in unexpected stressful situations, crowd behavior can have a direct impact on people’s emotions and behaviors. This paper investigates whether this effect is elicited in immersive virtual reality by a crowd of virtual humans (VHs) in an external stressful situation induced by a fire. We conducted a mixed-design study in an experimental scenario where users were asked to collect 10 items from a virtual market. The between-subjects factors were reaction type and agent type, while the within-subjects factor was the simulation phase, which included the normal and stressful situations. There were two reaction types where the VHs showed either active (agitated) or passive (calm) behavior towards the external threat situation. For the agent type, the VHs exhibited either individualistic or group (3-4 member group) behaviors. In a mixed factorial 2(crowd type)x 2(reaction type) x 2(simulation phase) design, we examined the influences of these factors on participants’ emotional and behavioral responses. The results showed that the participants’ task performance and social interaction were affected by the simulated stressor. Negative emotions were elicited when they encountered active VHs and small groups of VHs. Furthermore, a greater change in their interpersonal closeness was experienced when interacting with small groups of VHs, than when interacting with individual VHs.","행동심리학 문헌의 증거에 따르면 예상치 못한 스트레스 상황에서 군중의 행동은 사람들의 감정과 행동에 직접적인 영향을 미칠 수 있습니다. 본 논문에서는 화재로 인한 외부 스트레스 상황에서 가상 인간(VH) 군중이 몰입형 가상 현실에서 이러한 효과를 유발하는지 조사합니다. 우리는 사용자에게 가상 시장에서 10개의 아이템을 수집하도록 요청하는 실험 시나리오에서 혼합 디자인 연구를 수행했습니다. 피험자 간 요인은 반응 유형과 행위자 유형이었고, 피험자 내 요인은 정상 상황과 스트레스 상황을 포함하는 시뮬레이션 단계였습니다. VH가 외부 위협 상황에 대해 적극적(흥분) 또는 수동적(차분) 행동을 보이는 두 가지 반응 유형이 있었습니다. 에이전트 유형의 경우 VH는 개인주의적 또는 그룹(3~4명 그룹) 행동을 나타냈습니다. 혼합 요인 2(군중 유형) x 2(반응 유형) x 2(시뮬레이션 단계) 설계에서 이러한 요소가 참가자의 정서적 및 행동 반응에 미치는 영향을 조사했습니다. 결과는 참가자의 작업 수행과 사회적 상호 작용이 시뮬레이션된 스트레스 요인에 의해 영향을 받는 것으로 나타났습니다. 활동적인 VH와 소규모 VH 그룹을 만났을 때 부정적인 감정이 유발되었습니다. 또한, 개별 VH와 상호 작용할 때보다 소규모 VH 그룹과 상호 작용할 때 대인 친밀감의 변화가 더 크게 나타났습니다.",https://doi.org/10.1109/ISMAR62088.2024.00127,Interaction & Input,Deep Learning / Neural Networks,User Study; Simulation,User Study / Empirical Findings
192,2024,Efficacy of Virtual Reality Distraction for Reducing Chronic Pain,만성 통증 감소를 위한 가상 현실 주의 분산의 효능,"Chronic pain is a major health problem that requires the development of novel treatment strategies to address this growing issue. This paper introduces the application of virtual reality technology for managing chronic pain in outpatient settings. Virtual reality is unique in its ability to engage users through a multisensory experience merging visual, auditory, and sometimes tangible stimuli. We conducted a study involving patients experiencing chronic lumbar and cervical pain to assess its effectiveness in reducing chronic pain. Participants were asked to report their pain scores before, during, and immediately after the virtual reality session. Additionally, the study automatically recorded each participant’s completion time and error rate. These metrics were then analyzed to investigate the potential relationship between pain intensity and task performance. The results demonstrated a significant decrease in pain ratings both during and after the virtual reality session. The findings also revealed an interaction between pain intensity levels and the time spent playing the virtual reality game. Specifically, higher pain levels were associated with shorter completion times for tasks. Additionally, as pain increased, patients’ accuracy in performing tasks decreased. The study we conducted showed a high potential for enhancing distraction levels for patients with chronic pain using virtual reality.","만성 통증은 점점 커지는 문제를 해결하기 위한 새로운 치료 전략의 개발이 필요한 주요 건강 문제입니다. 본 논문에서는 외래 환자 환경에서 만성 통증을 관리하기 위한 가상 현실 기술의 적용을 소개합니다. 가상 현실은 시각적, 청각적, 때로는 유형의 자극을 통합하는 다감각적 경험을 통해 사용자의 참여를 유도하는 능력이 독특합니다. 우리는 만성 요추 및 경추 통증을 겪고 있는 환자를 대상으로 만성 통증 감소 효과를 평가하기 위해 연구를 수행했습니다. 참가자들은 가상 현실 세션 전, 도중, 직후에 통증 점수를 보고하도록 요청 받았습니다. 또한 연구는 각 참가자의 완료 시간과 오류율을 자동으로 기록했습니다. 그런 다음 통증 강도와 작업 수행 사이의 잠재적인 관계를 조사하기 위해 이러한 측정항목을 분석했습니다. 그 결과, 가상 현실 세션 도중과 이후에 통증 평가가 크게 감소한 것으로 나타났습니다. 연구 결과는 또한 통증 강도 수준과 가상 현실 게임을 플레이하는 데 소요된 시간 사이의 상호 작용을 보여주었습니다. 특히 통증 수준이 높을수록 작업 완료 시간이 짧아졌습니다. 또한 통증이 증가할수록 환자의 작업 수행 정확도가 감소했습니다. 우리가 수행한 연구는 가상 현실을 사용하여 만성 통증이 있는 환자의 주의 산만 수준을 향상시킬 수 있는 높은 잠재력을 보여주었습니다.",https://doi.org/10.1109/ISMAR62088.2024.00119,Interaction & Input,Sensor Fusion,Quantitative Experiment,User Study / Empirical Findings
193,2024,Efficient Mid-Air Text Input Correction in Virtual Reality,가상 현실에서 효율적인 공중 텍스트 입력 수정,"The task of inputting text within virtual reality has attracted significant research attention over the last five years. Less well explored is the related task of correcting inputted text when errors are made. This is despite the fact that considerable time and frustration stems from efforts to correct text. In this paper, we bridge this gap in prior research and explore efficient methods for supporting text input correction in virtual reality. We present a characterization of the types and frequencies of errors encountered when inputting text in virtual reality and an analysis of effective editing strategies. We also present the results of a user study evaluating the performance and usability trade-offs for several interaction methods leveraging the unique capabilities of modern head-mounted displays.",가상 현실 내에서 텍스트를 입력하는 작업은 지난 5년 동안 상당한 연구 관심을 끌었습니다. 오류가 발생했을 때 입력된 텍스트를 수정하는 관련 작업은 덜 잘 탐구되었습니다. 이는 텍스트를 수정하려는 노력으로 인해 상당한 시간과 좌절이 발생한다는 사실에도 불구하고 발생합니다. 본 논문에서는 이전 연구의 이러한 격차를 해소하고 가상 현실에서 텍스트 입력 수정을 지원하는 효율적인 방법을 탐색합니다. 가상현실에서 텍스트를 입력할 때 발생하는 오류의 유형과 빈도에 대한 특성화와 효과적인 편집 전략에 대한 분석을 제시합니다. 또한 최신 헤드 마운트 디스플레이의 고유한 기능을 활용하는 여러 상호 작용 방법의 성능과 유용성 절충점을 평가한 사용자 연구 결과도 제시합니다.,https://doi.org/10.1109/ISMAR62088.2024.00105,Interaction & Input; Display & Optics,Other,User Study,Algorithm / Method
194,2024,Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception,인코딩-저장-검색: 언어로 인코딩된 자기중심적 인식을 통해 인간 기억 강화,"We depend on our own memory to encode, store, and retrieve our experiences. However, memory lapses can occur. One promising avenue for achieving memory augmentation is through the use of augmented reality head-mounted displays to capture and preserve egocentric videos, a practice commonly referred to as lifelogging. However, a significant challenge arises from the sheer volume of video data generated through lifelogging, as the current technology lacks the capability to encode and store such large amounts of data efficiently. Further, retrieving specific information from extensive video archives requires substantial computational power, further complicating the task of quickly accessing desired content. To address these challenges, we propose a memory augmentation agent that involves leveraging natural language encoding for video data and storing them in a vector database. This approach harnesses the power of large vision language models to perform the language encoding process. Additionally, we propose using large language models to facilitate natural language querying. Our agent underwent extensive evaluation using the QA-Ego4D dataset and achieved state-of-the-art results with a BLEU score of 8.3, outperforming conventional machine learning models that scored between 3.4 and 5.8. Additionally, we conducted a user study in which participants interacted with the human memory augmentation agent through episodic memory and open-ended questions. The results of this study show that the agent results in significantly better recall performance on episodic memory tasks compared to human participants. The results also highlight the agent’s practical applicability and user acceptance.","우리는 경험을 인코딩하고, 저장하고, 검색하기 위해 자신의 기억에 의존합니다. 그러나 기억 상실이 발생할 수 있습니다. 기억 증강을 달성하기 위한 유망한 방법 중 하나는 증강 현실 머리 장착 디스플레이를 사용하여 자기중심적인 비디오를 캡처하고 보존하는 것입니다. 이는 일반적으로 라이프로깅이라고 불리는 관행입니다. 그러나 라이프로깅을 통해 생성된 비디오 데이터의 양이 엄청나게 크다는 점에서 심각한 문제가 발생합니다. 현재 기술로는 이러한 대용량 데이터를 효율적으로 인코딩하고 저장할 수 있는 기능이 부족하기 때문입니다. 또한 광범위한 비디오 아카이브에서 특정 정보를 검색하려면 상당한 컴퓨팅 성능이 필요하므로 원하는 콘텐츠에 빠르게 액세스하는 작업이 더욱 복잡해집니다. 이러한 문제를 해결하기 위해 우리는 비디오 데이터에 대한 자연어 인코딩을 활용하고 이를 벡터 데이터베이스에 저장하는 메모리 증강 에이전트를 제안합니다. 이 접근 방식은 대규모 비전 언어 모델의 기능을 활용하여 언어 인코딩 프로세스를 수행합니다. 또한 자연어 쿼리를 용이하게 하기 위해 대규모 언어 모델을 사용할 것을 제안합니다. 우리 에이전트는 QA-Ego4D 데이터 세트를 사용하여 광범위한 평가를 거쳤으며 BLEU 점수 8.3으로 최첨단 결과를 달성하여 3.4에서 5.8 사이의 점수를 얻은 기존 기계 학습 모델을 능가했습니다. 또한 우리는 참가자들이 일화 기억과 개방형 질문을 통해 인간 기억 증강 에이전트와 상호 작용하는 사용자 연구를 수행했습니다. 이 연구의 결과는 에이전트가 인간 참가자에 비해 일화 기억 작업에서 훨씬 더 나은 회상 성능을 나타냄을 보여줍니다. 결과는 또한 에이전트의 실제 적용 가능성과 사용자 수용성을 강조합니다.",https://doi.org/10.1109/ISMAR62088.2024.00108,Display & Optics,Natural Language Processing,User Study; Technical Evaluation,Algorithm / Method; Dataset / Benchmark
195,2024,Enhancement of Co-located Shared VR Experiences: Representing Non-HMD Observers on Both HMD and 2D Screens,함께 배치된 공유 VR 경험 향상: HMD 및 2D 화면 모두에서 HMD가 아닌 관찰자를 표현,"Virtual reality (VR) not only allows head-mounted display (HMD) users to immerse themselves in virtual worlds but also to share them with others. When designed correctly, this shared experience can be enjoyable. However, in typical scenarios, HMD users are isolated by their devices, and non-HMD observers lack connection with the virtual world. To address this, our research investigates visually representing observers on both HMD and 2D screens to enhance shared experiences. The study, including five representation conditions, reveals that incorporating observer representation positively impacts both HMD users and observers. For how to design and represent them, our work shows that HMD users prefer methods displaying real-world visuals, while observers exhibit diverse preferences regarding being represented with real or virtual images. We provide design guidelines tailored to both displays, offering valuable insights to enhance co-located shared VR experiences for HMD users and non-HMD observers.","가상 현실(VR)을 통해 HMD(헤드 마운트 디스플레이) 사용자는 가상 세계에 몰입할 수 있을 뿐만 아니라 이를 다른 사람들과 공유할 수도 있습니다. 올바르게 설계하면 이러한 공유 경험이 즐거울 수 있습니다. 그러나 일반적인 시나리오에서 HMD 사용자는 장치에 의해 격리되고 HMD가 아닌 관찰자는 가상 세계와의 연결이 부족합니다. 이 문제를 해결하기 위해 우리 연구에서는 HMD와 2D 화면 모두에서 관찰자를 시각적으로 표현하여 공유 경험을 향상시키는 방법을 조사합니다. 5가지 표현 조건을 포함한 이 연구는 관찰자 표현을 통합하는 것이 HMD 사용자와 관찰자 모두에게 긍정적인 영향을 미친다는 것을 보여줍니다. 이를 디자인하고 표현하는 방법에 대해 우리의 작업은 HMD 사용자가 실제 시각적인 방법을 선호하는 반면, 관찰자는 실제 이미지 또는 가상 이미지로 표현되는 것과 관련하여 다양한 선호도를 보인다는 것을 보여줍니다. 우리는 두 디스플레이 모두에 맞춤화된 디자인 지침을 제공하여 HMD 사용자와 HMD가 아닌 관찰자를 위해 같은 위치에 있는 공유 VR 경험을 향상시킬 수 있는 귀중한 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR62088.2024.00038,Display & Optics; Collaboration & Social,Other,Other,Design Guidelines
196,2024,"Enhancing Eye-Hand Coordination in Volleyball Players: A Comparative Analysis of VR, AR, and 2D Display Technologies and Task Instructions","배구선수의 눈-손 협응력 향상: VR, AR, 2D 디스플레이 기술 및 작업 지침 비교 분석","Previous studies analyzed user motor performance with Virtual Reality (VR) and Augmented Reality (AR) Eye-Hand Coordination Training Systems (EHCTSs) while asking participants to follow specific task instructions. Although these studies suggested VR & AR EHCTSs as potential training systems for sports players, they recruited participants for their user studies among general population. In this paper, we examined the training performance of 16 professional volleyball players over 8 days using EHCTSs with three display technologies (VR, AR, and 2D touchscreen) and with four distinct task instructions (prioritizing speed, error rate, accuracy, or none). Our results indicate that volleyball players performed best with 2D touchscreen in terms of time, error rate, accuracy, precision, and throughput. Moreover, their performance was superior when using VR over AR. They also successfully followed the task instructions given to them and consistently improved their throughput performance. These findings underscore the potential of EHCTS in volleyball training and highlight the need for further research to optimize VR & AR user experience and performance.","이전 연구에서는 참가자들에게 특정 작업 지침을 따르도록 요청하면서 가상 현실(VR) 및 증강 현실(AR) 눈-손 조정 훈련 시스템(EHCTS)을 사용하여 사용자 운동 성능을 분석했습니다. 이러한 연구에서는 VR 및 AR EHCTS가 스포츠 선수를 위한 잠재적인 훈련 시스템으로 제안되었지만 일반 대중을 대상으로 사용자 연구를 위해 참가자를 모집했습니다. 본 논문에서는 세 가지 디스플레이 기술(VR, AR, 2D 터치스크린)과 네 가지 고유한 작업 지침(속도, 오류율, 정확도 우선 순위 또는 없음)을 갖춘 EHCTS를 사용하여 8일 동안 16명의 프로 배구 선수의 훈련 성과를 조사했습니다. 우리의 결과는 배구 선수가 시간, 오류율, 정확성, 정밀도 및 처리량 측면에서 2D 터치스크린을 사용할 때 가장 좋은 성적을 냈다는 것을 나타냅니다. 또한 AR보다 VR을 사용할 때 성능이 더 뛰어났습니다. 또한 그들은 주어진 작업 지침을 성공적으로 따랐으며 지속적으로 처리량 성능을 향상시켰습니다. 이러한 발견은 배구 훈련에서 EHCTS의 잠재력을 강조하고 VR 및 AR 사용자 경험과 성능을 최적화하기 위한 추가 연구가 필요함을 강조합니다.",https://doi.org/10.1109/ISMAR62088.2024.00036,Interaction & Input; Education & Training,Other,Quantitative Experiment; User Study,System / Framework
197,2024,Enhancing Human Task Performance through Audiovisual Augmentation,시청각 증강을 통해 인간 작업 성능 향상,"In unusual environments and situations like extreme sports, underground environments, underwater, conflict zones or outer space, human sensory perception is often limited, which can adversely affect task performance, situational awareness, and the feeling of presence. This paper explores the efficacy of augmenting human perception with audiovisual information to support individuals in such contexts. Specifically, we investigate the impact of audiovisual augmentation on presence, situational awareness, and task performance. To conduct this research, we designed a virtual reality (VR) simulation of a space mission aboard the International Space Station (ISS). Within this simulation, participants were tasked with performing maintenance activities while receiving artificial augmentations. We conducted a user study involving 43 participants who performed the same maintenance task under four augmentation conditions: audio cues, visual cues, audiovisual cues, and no cues. Our findings reveal that adding audiovisual information significantly enhances performance. Participants with audiovisual augmentations had a 60% improvement in task completion speed compared to those without augmentations. Moreover, the workload was substantially reduced, and the sense of presence was increased when audiovisual support was used. The results highlight the potential of audiovisual augmentation as a valuable tool for individuals engaging in situations with reduced audiovisual perception. The insights demonstrating the versatility and promise of audiovisual augmentation in enhancing task performance.","익스트림 스포츠, 지하 환경, 수중, 분쟁 지역 또는 우주 공간과 같은 비정상적인 환경 및 상황에서는 인간의 감각 지각이 제한되는 경우가 많으며, 이는 작업 수행, 상황 인식 및 존재감에 부정적인 영향을 미칠 수 있습니다. 본 논문에서는 그러한 맥락에서 개인을 지원하기 위해 시청각 정보를 통해 인간의 인식을 강화하는 것의 효능을 탐구합니다. 구체적으로 우리는 시청각 증강이 현재 상태, 상황 인식 및 작업 수행에 미치는 영향을 조사합니다. 이 연구를 수행하기 위해 우리는 국제 우주 정거장(ISS)에서의 우주 임무에 대한 가상 현실(VR) 시뮬레이션을 설계했습니다. 이 시뮬레이션 내에서 참가자들은 인공 강화를 받는 동시에 유지 관리 활동을 수행하는 임무를 받았습니다. 우리는 오디오 신호, 시각적 신호, 시청각 신호 및 신호 없음의 네 가지 강화 조건에서 동일한 유지 관리 작업을 수행한 43명의 참가자를 대상으로 사용자 연구를 수행했습니다. 우리의 연구 결과에 따르면 시청각 정보를 추가하면 성능이 크게 향상되는 것으로 나타났습니다. 시청각 증강을 받은 참가자는 증강이 없는 참가자에 비해 작업 완료 속도가 60% 향상되었습니다. 또한 시청각 지원을 활용하면 작업량이 대폭 줄어들고 현장감도 높아졌습니다. 결과는 시청각 인식이 감소된 상황에 참여하는 개인을 위한 귀중한 도구로서 시청각 증강의 잠재력을 강조합니다. 작업 성능을 향상시키는 데 있어 시청각 증강의 다양성과 가능성을 보여주는 통찰력입니다.",https://doi.org/10.1109/ISMAR62088.2024.00095,Perception & Cognition; Education & Training,Sensor Fusion,User Study,User Study / Empirical Findings
198,2024,Error Management for Augmented Reality Assembly Instructions,증강현실 조립 지침에 대한 오류 관리,"Augmented reality (AR) lends itself to presenting visual instructions on how to assemble or disassemble an object. Splitting the assembly procedure into shorter steps and presenting the corresponding instructions in AR supports their comprehension. However, one can still misinterpret instructions and make errors while manipulating the object. While previous work supports detecting the occurrence of errors, we investigate handling such errors. This requires knowledge of the error at runtime of the application. Starting from a categorization of the errors, we investigate how to automatically derive common error states to generate training data. We introduce an extension to a state-of-the-art deep-learning-based object detector for supporting the detection of assembly states at real-time update rates, based on contrastive learning. We evaluated the proposed detector, showing that it outperforms the state-of-the-art, and we demonstrate our work with an AR application that alerts the user if errors occur and provides visual help to correct the error.","증강 현실(AR)은 물체를 조립하거나 분해하는 방법에 대한 시각적 지침을 제시하는 데 적합합니다. 조립 절차를 더 짧은 단계로 나누고 해당 지침을 AR로 제시하면 이해력이 향상됩니다. 그러나 개체를 조작하는 동안 지침을 잘못 해석하고 오류가 발생할 수 있습니다. 이전 작업에서는 오류 발생 감지를 지원했지만 우리는 이러한 오류 처리를 조사했습니다. 이를 위해서는 애플리케이션 런타임 시 오류에 대한 지식이 필요합니다. 오류 분류부터 시작하여 일반적인 오류 상태를 자동으로 도출하여 훈련 데이터를 생성하는 방법을 조사합니다. 대조 학습을 기반으로 실시간 업데이트 속도로 어셈블리 상태 감지를 지원하는 최첨단 딥러닝 기반 객체 감지기의 확장 기능을 소개합니다. We evaluated the proposed detector, showing that it outperforms the state-of-the-art, and we demonstrate our work with an AR application that alerts the user if errors occur and provides visual help to correct the error.",https://doi.org/10.1109/ISMAR62088.2024.00084,Education & Training,Optical / Display Technology,Technical Evaluation,Algorithm / Method
199,2024,Evaluating Transferable Emotion Expressions for Zoomorphic Social Robots using VR Prototyping',VR 프로토타이핑을 이용한 줌모픽 소셜 로봇의 전달 가능한 감정 표현 평가',"Zoomorphic robots have the potential to offer companionship and well-being as accessible, low-maintenance alternatives to pet ownership. Many such robots, however, feature limited emotional expression, restricting their potential for rich affective relationships with everyday domestic users. Additionally, exploring this design space using hardware prototyping is obstructed by physical and logistical constraints. We leveraged virtual reality rapid prototyping with passive haptic interaction to conduct a broad mixed-methods evaluation of emotion expression modalities and participatory prototyping of multimodal expressions. We found differences in recognisability, effectiveness and user empathy between modalities while highlighting the importance of facial expressions and the benefits of combining animal-like and unambiguous modalities. We use our findings to inform promising directions for the affective zoomorphic robot design and potential implementations via hardware modification or augmented reality, then discuss how VR prototyping makes this field more accessible to designers and researchers.","줌모픽(Zoomorphic) 로봇은 애완동물 소유에 대한 접근 가능하고 유지 관리가 적은 대안으로 동반자 관계와 웰빙을 제공할 수 있는 잠재력을 가지고 있습니다. 그러나 그러한 로봇 중 상당수는 감정 표현이 제한되어 있어 일상적인 국내 사용자와의 풍부한 정서적 관계에 대한 잠재력이 제한됩니다. 또한 하드웨어 프로토타입을 사용하여 이 설계 공간을 탐색하는 것은 물리적, 물류적 제약으로 인해 방해를 받습니다. 우리는 감정 표현 양식에 대한 광범위한 혼합 방법 평가와 다중 모드 표현의 참여형 프로토타이핑을 수행하기 위해 수동적 햅틱 상호 작용을 갖춘 가상 현실 신속한 프로토타이핑을 활용했습니다. 우리는 표정의 중요성과 동물적 양식과 모호하지 않은 양식을 결합하는 것의 이점을 강조하면서 양식 간 인식성, 효율성 및 사용자 공감의 차이를 발견했습니다. 우리는 연구 결과를 사용하여 감성적 확대형 로봇 설계에 대한 유망한 방향과 하드웨어 수정 또는 증강 현실을 통한 잠재적 구현을 ​​알리고 VR 프로토타이핑을 통해 디자이너와 연구원이 이 분야에 더 쉽게 접근할 수 있는 방법에 대해 논의합니다.",https://doi.org/10.1109/ISMAR62088.2024.00125,Interaction & Input,Haptic / Tactile Feedback,Technical Evaluation,User Study / Empirical Findings
200,2024,"Exploring Augmented Reality User Interface Transitions Across Mid-Air, On-Body and Physical Surfaces","공중, 신체 및 물리적 표면 전반에 걸친 증강 현실 사용자 인터페이스 전환 탐색","Augmented Reality User Interfaces commonly stay floated in midair unless explicitly moved by users. This often results in suboptimal performance due to the absence of haptic feedback and arm fatigue. We explore the transition of AR Interfaces to leverage onbody and physical surfaces, e.g., the arm and desk, in addition to mid-air. We begin with a user study to assess the potential of these surfaces for transitioning virtual UIs. Study results indicate a strong user preference for transitioning interfaces from mid-air to physical surfaces when they are available. We further explore three UI transition mechanisms: manual, semi-automatic, and automatic, each with varying levels of automation and user control. Results from a user study reveal that semi-automatic transition is the most preferred method, as it offers a good balance between automation and user control. We conclude with design guidelines for transitioning AR UIs across mid-air, on-body, and physical surfaces.","증강 현실 사용자 인터페이스는 일반적으로 사용자가 명시적으로 움직이지 않는 한 공중에 떠 있는 상태를 유지합니다. 이로 인해 촉각 피드백이 부족하고 팔 피로가 발생하여 최적이 아닌 성능을 발휘하는 경우가 많습니다. 우리는 공중뿐 아니라 팔, 책상 등 신체 및 물리적 표면을 활용하기 위한 AR 인터페이스의 전환을 탐구합니다. 우리는 가상 UI 전환을 위한 이러한 표면의 잠재력을 평가하기 위한 사용자 연구부터 시작합니다. 연구 결과에 따르면 인터페이스가 사용 가능한 경우 공중에서 물리적 표면으로 전환하는 것을 사용자가 선호하는 것으로 나타났습니다. 우리는 세 가지 UI 전환 메커니즘(수동, 반자동, 자동)을 자세히 살펴봅니다. 각 메커니즘에는 다양한 자동화 수준과 사용자 제어 기능이 있습니다. 사용자 연구 결과에 따르면 반자동 전환은 자동화와 사용자 제어 간의 적절한 균형을 제공하므로 가장 선호되는 방법입니다. 공중, 신체 및 물리적 표면 전반에 걸쳐 AR UI를 전환하기 위한 디자인 지침으로 마무리합니다.",https://doi.org/10.1109/ISMAR62088.2024.00134,Interaction & Input,Haptic / Tactile Feedback,User Study,User Study / Empirical Findings
201,2024,Exploring Communication among people with Neurodevelopmental Disorder during Cooperative Play in Augmented and Virtual Reality,증강 현실과 가상 현실에서 협동 플레이를 하는 동안 신경 발달 장애가 있는 사람들 간의 의사소통 탐구,"The article investigates cooperative interaction in Virtual Reality (VR) and Augmented Reality (AR) for individuals with neurodevelopmental disorders (NDDs) and compares the potential of these interaction paradigms to enhance the communication skills of this population. Two versions of a cooperative game called SMUP (Social Match UP) were created in AR and VR, respectively, involving NDD specialists in the co-design process. During a within-subjects empirical study involving 30 subjects with NDD, we measured both the usability and verbal interaction of the two versions, known as SMUP-AR and SMUP-VR. The findings suggest that both SMUP-AR and SMUP-VR exhibited a good degree of usability for individuals with NDD, with no statistically significant differences between the two versions. However, the evaluation of communication skills, based on the analysis of the verbal production of users under the two experimental conditions, indicates that SMUP-AR had a stronger effect in promoting dialogic interaction compared to SMUP-VR. In addition, the paper discusses insights from the study on the UX design of cooperative AR and VR technology for people with NDD.",이 기사에서는 신경발달 장애(NDD)가 있는 개인을 위한 가상 현실(VR) 및 증강 현실(AR)의 협력적 상호 작용을 조사하고 이러한 상호 작용 패러다임의 잠재력을 비교하여 이 집단의 의사 소통 기술을 향상시킵니다. SMUP(Social Match UP)라는 협동 게임의 두 가지 버전이 각각 AR과 VR로 제작되었으며 공동 디자인 과정에 NDD 전문가가 참여했습니다. NDD가 있는 30명의 피험자를 대상으로 한 피험자 내 경험적 연구에서 우리는 SMUP-AR 및 SMUP-VR로 알려진 두 가지 버전의 유용성과 언어적 상호 작용을 모두 측정했습니다. 연구 결과에 따르면 SMUP-AR과 SMUP-VR은 모두 NDD 환자에게 좋은 수준의 유용성을 나타냈으며 두 버전 간에 통계적으로 유의미한 차이는 없었습니다. 그러나 두 가지 실험 조건에서 사용자의 언어 생성 분석을 기반으로 한 의사소통 능력 평가에서는 SMUP-AR이 SMUP-VR에 비해 대화 상호 작용을 촉진하는 데 더 강한 효과가 있음을 나타냅니다. 또한 이 논문에서는 NDD 환자를 위한 협력적 AR 및 VR 기술의 UX 디자인에 대한 연구에서 얻은 통찰력을 논의합니다.,https://doi.org/10.1109/ISMAR62088.2024.00051,Collaboration & Social,Optical / Display Technology,User Study,User Study / Empirical Findings
202,2024,Exploring Finger-Worn Solutions for Transitioning between the Reality-Virtuality Continuum,현실-가상 연속체 간 전환을 위한 손가락 착용 솔루션 탐색,"Head-mounted displays (HMDs) enable users to navigate the Reality-Virtuality Continuum, facilitating transitions between the Real world, Augmented Reality, Augmented Virtuality, and the Virtual world. Traditional transition methods use double taps on HMDs or buttons on handheld controllers to transition between the worlds. However, this can often disrupt hands-free interaction and hinder the overall immersion. Although prior work explored transitioning within a reality, little is known about solutions facilitating transitioning across multiple worlds. In this paper, we investigate index finger-based solutions for transitioning between multiple realities. We designed and fabricated finger-worn button configurations of 2 × 2, 2 × 1, and 4 × 1, and compared them with finger-worn solutions such as Joystick, Rotary wheel, and Slider. The results showed that the 2 × 2 button configuration is the most effective technique, minimizing trial time and ensuring user comfort. Overall, this research enhances VR user experiences by improving interaction techniques for fluid switching between realities in the Reality-Virtuality Continuum.","머리 장착형 디스플레이(HMD)를 통해 사용자는 현실-가상 연속체를 탐색하여 현실 세계, 증강 현실, 증강 가상 및 가상 세계 간의 전환을 촉진할 수 있습니다. 기존의 전환 방법은 HMD의 두 번 탭이나 휴대용 컨트롤러의 버튼을 사용하여 세계 간 전환을 수행합니다. 그러나 이는 종종 핸즈프리 상호 작용을 방해하고 전반적인 몰입을 방해할 수 있습니다. 이전 작업에서는 현실 내 전환을 탐구했지만 여러 세계 간의 전환을 촉진하는 솔루션에 대해서는 알려진 바가 거의 없습니다. 본 논문에서는 여러 현실 간 전환을 위한 검지 기반 솔루션을 조사합니다. 2×2, 2×1, 4×1의 손가락 착용형 버튼 구성을 설계 및 제작하고 이를 조이스틱, 로터리 휠, 슬라이더 등 손가락 착용형 솔루션과 비교했습니다. 그 결과, 2×2 버튼 구성이 시행 시간을 최소화하고 사용자 편의성을 보장하는 가장 효과적인 기술인 것으로 나타났습니다. 전반적으로, 이 연구는 현실-가상 연속체에서 현실 간 유동적 전환을 위한 상호 작용 기술을 개선하여 VR 사용자 경험을 향상시킵니다.",https://doi.org/10.1109/ISMAR62088.2024.00133,Interaction & Input; Display & Optics,Other,Other,Algorithm / Method
203,2024,Exploring Implicit and Explicit Stimuli of Virtual Agents with Increased Group Size in VR,VR에서 그룹 크기가 증가한 가상 에이전트의 암시적 및 명시적 자극 탐색,"It remains unknown whether the virtual agents’ implicit behaviors (e.g., the agent’s eye-gaze shifting) would also cause such an observable effect on the users’ experiences even when the users are not consciously aware of the implicit behaviors of the agents. More importantly, it is unclear whether the effect of the implicit behaviors of agents can be amplified as the agents’ group size increases. To answer these questions, we asked two participant groups ($\mathrm{N}=12$ each) to deliver a public speech in front of the virtual agents showing either an explicit (head movement) or implicit (eye movement) behavior as we manipulated the agents’ group sizes (small, medium, and large). We measured the users’ perceptions, emotions, and behavior (head and eye movements) in the study. A posterior psychophysical experiment confirmed the implicit nature of the eye-movement behavior of the virtual agents (i.e., participants’ detection rate of the eye movements was less than the chance level). The results showed that the explicit behavior of the virtual agents caused the expected participant behavior changes along with negative emotions. In contrast, the implicit behavior of the agents caused largely positive emotions, yet, without significant behavior changes of the participants. Most importantly, as the group size of the virtual agents increased, the degree of their effect on users increased in both the explicit head-movement condition and the implicit eye-movement condition.","사용자가 에이전트의 암시적 행동을 의식적으로 인식하지 못하는 경우에도 가상 에이전트의 암시적 행동(예: 에이전트의 시선 이동)이 사용자 경험에 관찰 가능한 영향을 미칠지 여부는 아직 알려지지 않았습니다. 더 중요한 것은 에이전트의 그룹 규모가 커짐에 따라 에이전트의 암묵적 행동 효과가 증폭될 수 있는지 여부가 불분명하다는 것입니다. 이러한 질문에 대답하기 위해 우리는 두 참가자 그룹(각각 $\mathrm{N}=12$)에게 에이전트의 그룹 크기(소형, 중간 및 대형)를 조작하면서 명시적(머리 움직임) 또는 암시적(눈 움직임) 동작을 보여주는 가상 에이전트 앞에서 공개 연설을 하도록 요청했습니다. 우리는 연구를 통해 사용자의 인식, 감정, 행동(머리와 눈의 움직임)을 측정했습니다. 후방 정신물리학적 실험을 통해 가상 에이전트의 안구 운동 동작의 암묵적 특성이 확인되었습니다(즉, 참가자의 안구 운동 감지 비율이 우연 수준보다 낮았습니다). 결과는 가상 에이전트의 명시적인 행동이 부정적인 감정과 함께 예상되는 참가자 행동 변화를 유발한다는 것을 보여주었습니다. 대조적으로, 에이전트의 암묵적인 행동은 대체로 긍정적인 감정을 유발했지만 참가자의 행동에는 큰 변화가 없었습니다. 가장 중요한 점은 가상 에이전트의 그룹 규모가 커짐에 따라 명시적 머리 움직임 조건과 암시적 안구 움직임 조건 모두에서 사용자에게 미치는 영향의 정도가 증가했다는 것입니다.",https://doi.org/10.1109/ISMAR62088.2024.00122,Interaction & Input,Sensor Fusion,User Study,User Study / Empirical Findings
204,2024,Exploring Spatial Cognitive Residue and Methods to Clear Users' Minds When Transitioning Between Virtual Environments,가상 환경 간 전환 시 사용자의 마음을 깨끗하게 하는 공간적 인지 잔여물 및 방법 탐색,"In most cases, retaining memories of things we have experienced in the past is desirable, but in some cases, we want to clear our minds so that we may focus completely on subsequent activities. When someone switches from one task to another, they commonly incur some “cognitive residue” where some of their cognitive resources such as working memory and attention remain devoted to their previous task even after they try to switch their focus to their new task. This residue could have a negative impact on their performance in the next task, and in such circumstances, it is important to reduce that residue. In this paper, we explore the concept of cognitive residue in the context of switching between virtual reality (VR) environments. We conducted a human-subject experiment (N=24) with a spatial recall task to investigate how different visual transitions might reduce participants’ spatial cognitive residue. In this instance, more errors on the recall task corresponds to less spatial cognitive residue. We found that transitions that lasted one minute successfully reduced spatial cognitive residue: they significantly reduced participants’ abilities to recall the positions of objects in their previous VE compared to an instantaneous cut transition. Additionally, for transitions that showed a nature scene, greater head movement significantly correlated with more spatial memory errors (i.e., less spatial cognitive residue). We discuss how these findings can be applied to support users transitioning between virtual tasks and environments in VR task switching scenarios.","대부분의 경우 과거에 경험한 일에 대한 기억을 간직하는 것이 바람직하지만 어떤 경우에는 마음을 비워서 다음 활동에 전적으로 집중하고 싶습니다. 누군가가 한 작업에서 다른 작업으로 전환하면 일반적으로 작업 기억 및 주의력과 같은 인지 자원 중 일부가 새로운 작업으로 초점을 전환하려고 시도한 후에도 이전 작업에 전념하는 ""인지 잔여물""이 발생합니다. 이 잔여물은 다음 작업 수행에 부정적인 영향을 미칠 수 있으므로 이러한 상황에서는 해당 잔여물을 줄이는 것이 중요합니다. 본 논문에서는 가상 현실(VR) 환경 간 전환의 맥락에서 인지 잔여물의 개념을 탐구합니다. 우리는 다양한 시각적 전환이 참가자의 공간 인지 잔여물을 어떻게 줄일 수 있는지 조사하기 위해 공간 회상 작업을 통해 인간 피험자 실험(N=24)을 수행했습니다. 이 경우, 회상 작업에서 오류가 많을수록 공간적 인지 잔여물이 줄어듭니다. 우리는 1분 동안 지속된 전환이 공간적 인지 잔여물을 성공적으로 감소시키는 것을 발견했습니다. 이는 즉각적인 컷 전환에 비해 이전 VE에서 객체의 위치를 ​​기억하는 참가자의 능력을 크게 감소시켰습니다. 또한 자연 장면을 보여주는 전환의 경우 머리 움직임이 클수록 공간 기억 오류가 많을수록(즉, 공간 인지 잔여물이 적음) 유의미한 상관관계가 있었습니다. 우리는 VR 작업 전환 시나리오에서 가상 작업과 환경 간 사용자 전환을 지원하기 위해 이러한 결과를 어떻게 적용할 수 있는지 논의합니다.",https://doi.org/10.1109/ISMAR62088.2024.00116,Perception & Cognition,Other,User Study,User Study / Empirical Findings
205,2024,Exploring the Effects of Spatial Constraints and Curvature for 3D Piloting in Virtual Environments,가상 환경에서 3D 조종을 위한 공간 제약 조건 및 곡률의 효과 탐색,"Piloting requires users to control and navigate the aircraft within a designated pathway, with a controller that utilizes two joysticks to control the aircraft. This task is representative of various daily and gaming scenarios, such as controlling the aircraft to capture the photo or navigating an object in a game from the start position to the end via a trajectory. In this work, we explore a model (based on the Steering Law) that predicts the piloting time required in spatial-constrained environments. Thus, two user studies are conducted to help us understand the relationship between path complexity (curvature) and spatial constraints (width and height). According to the results, we propose a model that can achieve $52.6 \%$ and $60.6 \%$ improvement in R-square and the Akaike Information Criterion (AIC), respectively. Next, an additional study was conducted to further verify the performance and efficiency of our proposed model with the change of movement direction and orientation. Our model and experimental results can benefit both game and interface designers of applications that require controlling moving objects along specific trajectories in virtual reality environments.","조종하려면 사용자가 두 개의 조이스틱을 활용하여 항공기를 제어하는 ​​컨트롤러를 사용하여 지정된 경로 내에서 항공기를 제어하고 탐색해야 합니다. 이 작업은 사진을 캡처하기 위해 항공기를 제어하거나 궤적을 통해 게임의 개체를 시작 위치에서 끝까지 탐색하는 등 다양한 일상 및 게임 시나리오를 대표합니다. 이 연구에서는 공간이 제한된 환경에서 필요한 조종 시간을 예측하는 모델(조향 법칙 기반)을 탐색합니다. 따라서 경로 복잡성(곡률)과 공간적 제약(너비 및 높이) 사이의 관계를 이해하는 데 도움이 되는 두 가지 사용자 연구가 수행되었습니다. 결과에 따라 우리는 R-square와 AIC(Akaike Information Criterion)에서 각각 $52.6 \%$ 및 $60.6 \%$ 개선을 달성할 수 있는 모델을 제안합니다. 다음으로, 이동 방향과 방향의 변화에 ​​따라 제안된 모델의 성능과 효율성을 추가로 검증하기 위해 추가 연구를 수행했습니다. 우리의 모델과 실험 결과는 가상 현실 환경에서 특정 궤적을 따라 움직이는 물체를 제어해야 하는 게임 및 인터페이스 설계자 모두에게 도움이 될 수 있습니다.",https://doi.org/10.1109/ISMAR62088.2024.00065,Interaction & Input,Other,Technical Evaluation,User Study / Empirical Findings
206,2024,Exploring the Effects of Viewpoint Height & Fluctuation on Walking Perception in Panoramic Tours,파노라마 투어에서 시점 높이 및 변동이 보행 인식에 미치는 영향 탐색,"Panoramic roaming with HMDs provides an immersive VR experience and the realism of user’s navigation in panoramic virtual environments (VEs) can be further enhanced by real-walking locomotion technology. However, the mismatched viewpoint height & fluctuation with the user’s visual senses may lead to perceptual conflicts during real-walking interaction. We conducted a series of user studies to investigate how viewpoint height & fluctuation affect the walking perception of users by real-walking interaction to navigate panoramic VEs and constructed a virtual camera motion model of viewpoint fluctuation and eyetracking compensation for walking behavior in panoramic virtual roaming. It was found that mismatched viewpoint fluctuation triggered generalized discomfort in participants and that participants preferred virtual viewpoint heights below eye level when roaming in panoramic VEs by real-walking. Considering these findings, we proposed viewpoint adaptive adjustment method based on visual manipulation and conducted a preliminary evaluation, which showed that the proposed method can effectively mitigate the conflict between the visual system and other sensory systems. It made that panoramic VEs constructed at a certain fixed height are rendered universal for users with different eyelevel heights and viewpoint fluctuation is matched to the undulation of their center of gravity during real-walking interaction.","HMD를 이용한 파노라마 로밍은 몰입형 VR 경험을 제공하며, 실제 걷기 이동 기술을 통해 파노라마 가상 환경(VE)에서 사용자 탐색의 현실감을 더욱 향상시킬 수 있습니다. 그러나 사용자의 시각적 감각과 일치하지 않는 시점 높이 및 변동으로 인해 실제 걷는 상호 작용 중에 지각 충돌이 발생할 수 있습니다. 파노라마 VE를 탐색하기 위해 실제 보행 상호 작용을 통해 시점 높이 및 변동이 사용자의 보행 인식에 어떤 영향을 미치는지 조사하기 위해 일련의 사용자 연구를 수행하고 파노라마 가상 로밍에서 보행 동작에 대한 시점 변동 및 시선 추적 보상에 대한 가상 카메라 모션 모델을 구축했습니다. 일치하지 않는 시점 변동은 참가자의 일반화된 불편함을 유발했으며 참가자는 실제 걷기로 파노라마 VE에서 로밍할 때 눈높이보다 낮은 가상 시점 높이를 선호하는 것으로 나타났습니다. 이러한 결과를 고려하여 우리는 시각적 조작을 기반으로 한 시점 적응 조정 방법을 제안하고 예비 평가를 수행하여 제안한 방법이 시각 시스템과 다른 감각 시스템 간의 충돌을 효과적으로 완화할 수 있음을 보여주었습니다. 특정 고정 높이에 구축된 파노라마 VE는 다양한 눈높이를 가진 사용자에게 보편적으로 제공되며 시점 변동은 실제 보행 상호 작용 중 무게 중심의 기복과 일치합니다.",https://doi.org/10.1109/ISMAR62088.2024.00072,Interaction & Input; Perception & Cognition,Redirected Walking / Locomotion,User Study,User Study / Empirical Findings; Algorithm / Method
207,2024,Exploring the Impact of Passthrough on VR Exergaming in Public Environments: A Field Study,공공 환경에서 VR 엑서게이밍에 대한 패스스루의 영향 탐색: 현장 연구,"Sedentary behavior is becoming increasingly prevalent in daily work and study environments. VR exergaming has emerged as a promising solution in these places of work and study. However, private spaces in these environments are not easy, and engaging in VR exergaming in public settings presents its own set of challenges (e.g., safety, social acceptance, isolation, and privacy protection). The recent development of Passthrough functionality in VR headsets allows users to maintain awareness of their surroundings, enhancing safety and convenience. Despite its potential benefits, little is known about how Passthrough could affect user performance and experience and solve the challenges of playing VR exergames in real-world public environments. To our knowledge, this work is the first to conduct a field study in an underground passageway on a university campus to explore the use of Passthrough in a realworld public environment, with a disturbance-free closed room as a baseline. Results indicate that enabling Passthrough in a public environment improves performance without compromising presence. Moreover, Passthrough can increase social acceptance, especially among individuals with higher levels of self-consciousness. These findings highlight Passthrough’s potential to encourage VR exergaming adoption in public environments, with promising implications for overall health and well-being.","일상 업무 및 학습 환경에서 앉아서 생활하는 행동이 점점 더 널리 퍼지고 있습니다. VR 엑서게이밍은 이러한 업무 및 학습 장소에서 유망한 솔루션으로 떠올랐습니다. 그러나 이러한 환경의 개인 공간은 쉽지 않으며, 공공 장소에서 VR 엑서게이밍에 참여하는 것은 안전, 사회적 수용, 고립, 개인정보 보호 등 고유한 과제를 안겨줍니다. 최근 VR 헤드셋의 패스스루(Passthrough) 기능이 개발되면서 사용자는 주변 환경에 대한 인식을 유지할 수 있어 안전성과 편의성이 향상되었습니다. 잠재적인 이점에도 불구하고 Passthrough가 어떻게 사용자 성능과 경험에 영향을 미치고 실제 공공 환경에서 VR 엑서게임을 플레이할 때 발생하는 문제를 해결할 수 있는지에 대해서는 알려진 바가 거의 없습니다. 우리가 아는 바로는, 이 작업은 방해 없는 밀폐 공간을 기준으로 실제 공공 환경에서 Passthrough의 사용을 탐구하기 위해 대학 캠퍼스의 지하 통로에서 현장 연구를 수행한 최초의 작업입니다. 결과에 따르면 공용 환경에서 패스스루를 활성화하면 존재감을 저하시키지 않으면서 성능이 향상되는 것으로 나타났습니다. 더욱이 패스스루(Passthrough)는 특히 자의식 수준이 높은 개인들 사이에서 사회적 수용성을 높일 수 있습니다. 이러한 연구 결과는 공공 환경에서 VR 엑서게이밍 채택을 장려하고 전반적인 건강과 웰빙에 유망한 영향을 미칠 수 있는 Passthrough의 잠재력을 강조합니다.",https://doi.org/10.1109/ISMAR62088.2024.00037,Perception & Cognition,Optical / Display Technology,Technical Evaluation,User Study / Empirical Findings
208,2024,Expressing the Social Intent of Touch Initiator in Virtual Reality Using Multimodal Haptics,다중 모드 햅틱을 사용하여 가상 현실에서 터치 개시자의 사회적 의도 표현,"Touch is crucial in communicating different social intents in our everyday lives. However, we lack methods to bring the capability of touch to express various social signals into VR. This paper aims to bridge this gap by exploring the effectiveness of haptic feedback in conveying the social intent of a touch initiator (a user touching a virtual agent). In User Study 1, we observe the touch gestures that users employ to express different social intents and collect the haptic feedback parameters appropriate for representing these touch gestures. User Study 2 analyzes how social intent-dependent multimodal (pressure, thermal, and texture) haptic feedback affects the touch initiator’s virtual social experience. The results indicate that haptic feedback representing social intent strengthens the social expression of touch and fosters emotional closeness with virtual agents. Combining it with a physical proxy makes the effects even more effective, enhancing copresence and avatar embodiment.","접촉은 일상 생활에서 다양한 사회적 의도를 전달하는 데 중요합니다. 그러나 다양한 사회적 신호를 표현하는 터치 기능을 VR로 구현하는 방법은 부족합니다. 본 논문은 터치 개시자(가상 에이전트를 터치하는 사용자)의 사회적 의도를 전달하는 데 있어 햅틱 피드백의 효과를 탐구함으로써 이러한 격차를 해소하는 것을 목표로 합니다. 사용자 연구 1에서는 사용자가 다양한 사회적 의도를 표현하기 위해 사용하는 터치 제스처를 관찰하고 이러한 터치 제스처를 나타내는 데 적합한 햅틱 피드백 매개변수를 수집합니다. 사용자 연구 2에서는 사회적 의도에 따른 다중 모드(압력, 열, 질감) 햅틱 피드백이 터치 개시자의 가상 소셜 경험에 어떻게 영향을 미치는지 분석합니다. 결과는 사회적 의도를 나타내는 햅틱 피드백이 터치의 사회적 표현을 강화하고 가상 에이전트와의 정서적 친밀감을 조성한다는 것을 나타냅니다. 이를 물리적 프록시와 결합하면 효과가 더욱 효과적이게 되어 공존성과 아바타 구현이 향상됩니다.",https://doi.org/10.1109/ISMAR62088.2024.00056,Interaction & Input; Perception & Cognition,Haptic / Tactile Feedback,User Study,User Study / Empirical Findings
209,2024,"Extended Reality and Digital Twin in the Oil and Gas Pipeline Industry: A Systematic Review on Applications, Trends, and Future Directions","석유 및 가스 파이프라인 산업의 확장 현실과 디지털 트윈: 애플리케이션, 동향 및 미래 방향에 대한 체계적인 검토","Immersive and simulation technologies, such as Augmented Reality (AR), Virtual Reality (VR), Mixed Reality (MR), and Digital Twins (DTs), have proven effective in enhancing decision-making, streamlining operations, and improving safety through intuitive visualizations. Unsurprisingly, the oil and gas (O&G) pipeline industry is increasingly turning to these technologies to tackle complex construction and resource management challenges. This paper reviews the applications, benefits, and future trends of these technologies, focusing on integration and emerging trends in monitoring, training, maintenance, and testing. It identifies potential gaps, offers guidance for future research, and emphasizes adapting to technological evolution, providing insights to improve system reliability and sustainability in O&G pipeline operations using immersion and simulation methods.","증강 현실(AR), 가상 현실(VR), 혼합 현실(MR), 디지털 트윈(DT)과 같은 몰입형 및 시뮬레이션 기술은 직관적인 시각화를 통해 의사 결정을 강화하고 운영을 간소화하며 안전성을 향상시키는 데 효과적인 것으로 입증되었습니다. 당연히 석유 및 가스(O&G) 파이프라인 산업은 복잡한 건설 및 자원 관리 문제를 해결하기 위해 이러한 기술을 점점 더 많이 활용하고 있습니다. 본 백서에서는 모니터링, 교육, 유지 관리 및 테스트 분야의 통합 및 새로운 추세에 중점을 두고 이러한 기술의 적용, 이점 및 미래 추세를 검토합니다. 잠재적인 격차를 식별하고, 미래 연구를 위한 지침을 제공하며, 기술 발전에 대한 적응을 강조하고, 몰입 및 시뮬레이션 방법을 사용하여 O&G 파이프라인 운영의 시스템 신뢰성과 지속 가능성을 향상시키기 위한 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR62088.2024.00086,Education & Training,Sensor Fusion,Questionnaire / Survey,System / Framework; Survey / Review
210,2024,FactoredSweeper: Optical See-Through Display Integrating Light Attenuation and Addition with Single Spatial Light Modulator,FactoredSweeper: 단일 공간 광 변조기와 광 감쇠 및 추가를 통합한 광학 투명 디스플레이,"Light Attenuation Displays (LADs), a subset of Optical See-Through Head-Mounted Displays (OST-HMDs), enable image display in bright environments by filtering incident light at the pixel level. Although recent methods have proposed single-DMD light attenuation, they do not consider additive color display and background compensation, limiting their applicability in real-world scenarios. We present FactoredSweeper, a single digital micromirror device (DMD) system that incorporates both light attenuation and addition. By synchronizing the DMD, color filter, and light source, our system generates an additive virtual image, light attenuation, and occlusion through time multiplexing. To produce the target image while compensating for the background, we optimize time-multiplexed binary DMD patterns and LED/color filter schedules using perceptually-driven non-negative matrix factorization. Simulations and prototypes demonstrate that our integrated attenuation-addition single-SLM system achieves superior dynamic range and perceptual image quality compared to conventional occlusion-capable OST-HMDs using grayscale occlusion masks.","OST-HMD(Optical See-Through Head-Mounted Display)의 하위 세트인 LAD(광 감쇠 디스플레이)는 입사광을 픽셀 수준에서 필터링하여 밝은 환경에서 이미지 표시를 가능하게 합니다. 최근 방법에서는 단일 DMD 광 감쇠를 제안했지만 추가 색상 디스플레이 및 배경 보상을 고려하지 않아 실제 시나리오에서의 적용 가능성이 제한됩니다. 우리는 빛 감쇠와 추가를 모두 통합한 단일 디지털 마이크로미러 장치(DMD) 시스템인 FactoredSweeper를 소개합니다. DMD, 컬러 필터 및 광원을 동기화함으로써 우리 시스템은 시간 다중화를 통해 추가 가상 이미지, 광 감쇠 및 폐색을 생성합니다. 배경을 보정하면서 대상 이미지를 생성하기 위해 지각 기반 비음수 행렬 분해를 사용하여 시간 다중화된 이진 DMD 패턴과 LED/색상 필터 일정을 최적화합니다. 시뮬레이션과 프로토타입은 통합 감쇠 추가 단일 SLM 시스템이 그레이스케일 폐색 마스크를 사용하는 기존 폐색 가능 OST-HMD에 비해 우수한 동적 범위와 지각 이미지 품질을 달성한다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR62088.2024.00020,Display & Optics; Diminished Reality,Optical / Display Technology,Simulation,Hardware / Device
211,2024,First or Third-Person Hearing? A Controlled Evaluation of Auditory Perspective on Embodiment and Sound Localization Performance,1인칭 청각 또는 3인칭 청각? 구현 및 소리 정위 성능에 대한 청각적 관점의 통제된 평가,"Virtual Reality (VR) allows users to flexibly choose the perspective through which they interact with a synthetic environment. Users can either adopt a first-person perspective, in which they see through the eyes of their virtual avatar, or a third-person perspective, in which their viewpoint is detached from the virtual avatar. Prior research has shown that the visual perspective affects different interactions and influences core experiential factors, such as the user’s sense of embodiment. However, there is limited understanding of how auditory perspective mediates user experience in immersive virtual environments. In this paper, we conducted a controlled experiment $(N=24)$ on the effect of the user’s auditory perspective on their performance in a sound localization task and their sense of embodiment. Our results showed that when viewing a virtual avatar from a third-person visual perspective, adopting the auditory perspective of the avatar may increase agency and self-avatar merging, even when controlling for variations in task difficulty caused by shifts in auditory perspective. Additionally, our findings suggest that differences in auditory perspective generally have a smaller effect than differences in visual perspective. We discuss the implications of our empirical investigation of audio perspective for designing embodied auditory experiences in VR.","가상 현실(VR)을 통해 사용자는 합성 환경과 상호 작용하는 관점을 유연하게 선택할 수 있습니다. 사용자는 가상 아바타의 눈을 통해 보는 1인칭 관점이나 가상 아바타에서 분리된 3인칭 관점을 채택할 수 있습니다. 이전 연구에 따르면 시각적 관점은 다양한 상호 작용에 영향을 미치고 사용자의 체화 감각과 같은 핵심 경험 요소에 영향을 미치는 것으로 나타났습니다. 그러나 몰입형 가상 환경에서 청각적 관점이 사용자 경험을 어떻게 중재하는지에 대한 이해는 제한적입니다. 본 논문에서 우리는 소리 위치 파악 작업 수행 및 구체화 감각에 대한 사용자의 청각적 관점의 영향에 대해 $(N=24)$ 통제된 실험을 수행했습니다. 우리의 결과는 제3자의 시각적 관점에서 가상 아바타를 볼 때 아바타의 청각적 관점을 채택하면 청각적 관점의 변화로 인한 작업 난이도의 변화를 제어할 때에도 주체와 자기 아바타 병합이 증가할 수 있음을 보여주었습니다. 또한, 우리의 연구 결과는 청각적 관점의 차이가 일반적으로 시각적 관점의 차이보다 작은 영향을 미친다는 것을 시사합니다. 우리는 VR에서 구체화된 청각 경험을 디자인하기 위한 오디오 관점에 대한 실증적 조사의 의미를 논의합니다.",https://doi.org/10.1109/ISMAR62088.2024.00022,Audio & Sound; Perception & Cognition,Optical / Display Technology,User Study,User Study / Empirical Findings
212,2024,From Research to Practice: Survey and Taxonomy of Object Selection in Consumer VR Applications,연구에서 실습까지: 소비자 VR 애플리케이션의 객체 선택에 대한 조사 및 분류,"Object selection has been explored extensively in the VR research literature. However, the research is typically conducted in constrained experimental setups. It remains unclear whether the designed selection techniques fit the prevalent practical uses and whether the experimental tasks represent important challenges in real applications. To identify and help bridge these gaps, we surveyed current consumer VR applications, containing 206 popular VR game and 3D modeling applications. We extracted 1300+ selection scenarios based on video analyses of these applications and derived a taxonomy to understand common patterns on where and how selections occur. Our findings reveal significant gaps in selection tasks and techniques between research and consumer applications. We also present an interactive visualization tool to help researchers explore the VR object selection scenarios. Finally, we discuss how our work can help researchers and developers evaluate techniques in meaningful tasks and drive the design of techniques.","객체 선택은 VR 연구 문헌에서 광범위하게 연구되었습니다. 그러나 연구는 일반적으로 제한된 실험 설정에서 수행됩니다. 설계된 선택 기술이 널리 사용되는 실제 용도에 적합한지, 실험 작업이 실제 응용 분야에서 중요한 과제를 나타내는지 여부는 여전히 불분명합니다. 이러한 격차를 식별하고 메우기 위해 우리는 206개의 인기 VR 게임 및 3D 모델링 애플리케이션이 포함된 현재 소비자 VR 애플리케이션을 조사했습니다. 우리는 이러한 애플리케이션의 비디오 분석을 기반으로 1300개 이상의 선택 시나리오를 추출하고 선택이 발생하는 위치와 방법에 대한 일반적인 패턴을 이해하기 위한 분류법을 도출했습니다. 우리의 연구 결과는 연구와 소비자 적용 사이의 선택 작업과 기술에 상당한 격차가 있음을 보여줍니다. 또한 연구원들이 VR 객체 선택 시나리오를 탐색하는 데 도움이 되는 대화형 시각화 도구를 제시합니다. 마지막으로 우리의 작업이 연구원과 개발자가 의미 있는 작업에서 기술을 평가하고 기술 설계를 추진하는 데 어떻게 도움이 될 수 있는지 논의합니다.",https://doi.org/10.1109/ISMAR62088.2024.00115,Rendering & Visualization,Other,Questionnaire / Survey,Algorithm / Method; Survey / Review
213,2024,GazeRing: Enhancing Hand-Eye Coordination with Pressure Ring in Augmented Reality,GazeRing: 증강 현실에서 압력 링을 사용하여 손과 눈의 조정 강화,"Hand-eye coordination techniques find widespread utility in augmented reality and virtual reality headsets, as they retain the speed and intuitiveness of eye gaze while leveraging the precision of hand gestures. However, in contrast to obvious interactive gestures, users prefer less noticeable interactions in public settings due to concerns about social acceptance. To address this, we propose GazeRing, a multimodal interaction technique that combines eye gaze with a smart ring, enabling private and subtle hand-eye coordination while allowing users’ hands complete freedom of movement. Specifically, we design a pressure-sensitive ring that supports sliding interactions in eight directions to facilitate efficient 3D object manipulation. Additionally, we introduce two control modes for the ring: finger-tap and finger-slide, to accommodate diverse usage scenarios. Through user studies involving object selection and translation tasks under two eye-tracking accuracy conditions, with two degrees of occlusion, GazeRing demonstrates significant advantages over existing techniques that do not require obvious hand gestures (e.g., gaze-only and gaze-speech interactions). Our GazeRing technique achieves private and subtle interactions, potentially improving the user experience in public settings. A demo video can be found at zhimin-wang.github.io/GazeRing.html.","손-눈 조정 기술은 손 제스처의 정확성을 활용하면서 시선의 속도와 직관성을 유지하므로 증강 현실 및 가상 현실 헤드셋에서 널리 유용합니다. 그러나 명백한 상호 작용 제스처와 달리 사용자는 사회적 수용에 대한 우려로 인해 공개 환경에서 눈에 띄지 않는 상호 작용을 선호합니다. 이를 해결하기 위해 우리는 시선과 스마트 링을 결합하여 사용자의 손이 완전히 자유롭게 움직일 수 있도록 하면서 개인적이고 미묘한 손과 눈의 조정을 가능하게 하는 다중 모드 상호 작용 기술인 GazeRing을 제안합니다. 특히 효율적인 3D 개체 조작을 용이하게 하기 위해 8방향의 슬라이딩 상호 작용을 지원하는 압력 감지 링을 설계합니다. 또한 다양한 사용 시나리오를 수용할 수 있도록 링에 손가락 탭과 손가락 슬라이드라는 두 가지 제어 모드를 도입했습니다. GazeRing은 두 가지 시선 추적 정확도 조건, 두 가지 교합 정도에서 개체 선택 및 번역 작업과 관련된 사용자 연구를 통해 명백한 손 제스처(예: 시선 전용 및 시선 음성 상호 작용)가 필요하지 않은 기존 기술에 비해 상당한 이점을 보여줍니다. 우리의 GazeRing 기술은 비공개적이고 미묘한 상호 작용을 달성하여 잠재적으로 공공 장소에서 사용자 경험을 향상시킵니다. 데모 비디오는 zhimin-wang.github.io/GazeRing.html에서 찾을 수 있습니다.",https://doi.org/10.1109/ISMAR62088.2024.00068,Interaction & Input,Hand / Gesture Recognition,Quantitative Experiment,Algorithm / Method
214,2024,Gender Differences in Perceiving Avatar Face and Interpersonal Distance: Exploring Realism and Social Presence in Mixed Reality,아바타 얼굴과 대인 거리를 인식하는 성별 차이: 혼합 현실에서 현실감과 사회적 존재 탐구,"Understanding gender differences in facial and spatial recognition is crucial for enhancing avatar-mediated communication. However, there remains a gap in understanding how participant gender influences perceptions of avatar facial expressions and spatial dynamics in Mixed Reality communication. Therefore, our study investigates how avatar non-verbal cues interact with gender differences to affect user experience and understanding in MR environments. To examine these complex relationships, we conducted a user study comparing the effects of various avatar facial expressions (Full, Mouth-Only, and Emotion-based) and interpersonal distances (Closer vs. Farther) on facial animation realism and social presence, with a focus on gender-balanced participant groups. Our findings revealed that female participants were particularly sensitive to the avatar’s proximity and facial expressions, reporting significantly higher perceptions of facial animation realism, copresence, message understanding, and affective understanding at farther distances compared to male participants. They also perceived higher copresence and message understanding when exposed to emotion-based facial expressions, as opposed to a mouth-only condition-a distinction not observed among male participants. Based on our findings, we advocate for avatar design strategies that accommodate gender differences in perception and preference, potentially through customizable levels of expressiveness to cater to diverse user needs and contexts.","얼굴 및 공간 인식의 성별 차이를 이해하는 것은 아바타를 통한 의사소통을 향상시키는 데 중요합니다. 그러나 참가자의 성별이 혼합 현실 통신에서 아바타 얼굴 표정과 공간적 역학에 대한 인식에 어떻게 영향을 미치는지 이해하는 데에는 여전히 격차가 남아 있습니다. 따라서 우리 연구에서는 아바타의 비언어적 신호가 성별 차이와 어떻게 상호 작용하여 MR 환경에서 사용자 경험과 이해에 영향을 미치는지 조사합니다. 이러한 복잡한 관계를 조사하기 위해 우리는 성별 균형 참가자 그룹에 초점을 맞춰 얼굴 애니메이션 현실감과 사회적 존재감에 대한 다양한 아바타 얼굴 표정(완전한, 입만 있는 및 감정 기반)과 대인 거리(가까운 vs. 먼)의 효과를 비교하는 사용자 연구를 수행했습니다. 우리의 연구 결과에 따르면 여성 참가자는 특히 아바타의 근접성과 얼굴 표정에 민감하여 남성 참가자에 비해 얼굴 애니메이션 현실감, 공존, 메시지 이해 및 먼 거리에서의 정서적 이해에 대해 훨씬 더 높은 인식을 보고한 것으로 나타났습니다. 그들은 또한 입으로만 말하는 조건과 달리 감정 기반 얼굴 표정에 노출되었을 때 더 높은 상호존재감과 메시지 이해도를 인식했는데, 이는 남성 참가자들 사이에서는 관찰되지 않은 차이였습니다. 연구 결과를 바탕으로 우리는 다양한 사용자 요구와 상황에 맞춰 표현 수준을 사용자 정의할 수 있어 인식과 선호도의 성별 차이를 수용하는 아바타 디자인 전략을 옹호합니다.",https://doi.org/10.1109/ISMAR62088.2024.00024,Perception & Cognition; Collaboration & Social,Optical / Display Technology,User Study,User Study / Empirical Findings
215,2024,Generating Haptic Motion Effects for General Scenes to Improve 4D Experiences,4D 경험 개선을 위해 일반 장면에 대한 햅틱 모션 효과 생성,"Motion effects are vital for enhancing 4D experiences in various applications, e.g., rides, films, and games, by physically moving the user. Recent research introduced algorithms for generating effective motion effects from the audiovisual stream, but these approaches either require object motion trajectories or miss important scene components, such as other objects, particles, and camera motion. This paper proposes a fully automatic algorithm synthesizing motion effects from all scene components. Our method leverages computer vision technologies to capture salient movements among multiple components, enabling the creation of holistic motion effects without the need for human intervention. Then, our method computes a motion proxy representing the movements of the scene components by compressing the captured movements into a singlepoint motion. The motion proxy is converted into a motion command through a motion cueing algorithm and delivered to the user. The results of a user study show that our algorithm can generate compelling motion effects that enhance the 4D experience better than semi-manually authored effects. Our approach can facilitate the production of captivating motion effects for many applications.","모션 효과는 사용자의 물리적 움직임을 통해 놀이기구, 영화, 게임 등 다양한 애플리케이션에서 4D 경험을 향상시키는 데 필수적입니다. 최근 연구에서는 시청각 스트림에서 효과적인 모션 효과를 생성하기 위한 알고리즘을 도입했지만 이러한 접근 방식은 객체 모션 궤적을 요구하거나 다른 객체, 입자, 카메라 모션과 같은 중요한 장면 구성 요소를 놓치게 됩니다. 본 논문에서는 모든 장면 구성 요소의 모션 효과를 합성하는 완전 자동 알고리즘을 제안합니다. 우리의 방법은 컴퓨터 비전 기술을 활용하여 여러 구성 요소 간의 중요한 움직임을 캡처하여 사람의 개입 없이 전체적인 모션 효과를 생성할 수 있습니다. 그런 다음 우리의 방법은 캡처된 움직임을 단일 지점 모션으로 압축하여 장면 구성 요소의 움직임을 나타내는 모션 프록시를 계산합니다. 모션 프록시는 모션 큐잉 알고리즘을 통해 모션 명령으로 변환되어 사용자에게 전달됩니다. 사용자 연구 결과에 따르면 우리의 알고리즘은 반수동으로 작성된 효과보다 4D 경험을 더 향상시키는 강력한 모션 효과를 생성할 수 있습니다. 우리의 접근 방식은 다양한 응용 분야에서 매력적인 모션 효과를 쉽게 생성할 수 있습니다.",https://doi.org/10.1109/ISMAR62088.2024.00019,Interaction & Input,Computer Vision,User Study,Algorithm / Method
216,2024,GlanXR: A Hands-Free Fast Switching System for Virtual Screens,GlanXR: 가상 화면을 위한 핸즈프리 고속 전환 시스템,"To date, virtual and augmented reality technologies enable users to view multiple, large virtual screens in their workspaces. However, users must frequently rotate their heads to shift focus among these screens. This paper presents GlanXR, a fast and robust handsfree approach for screen switching in virtual reality. GlanXR incorporates a peripheral interface that remains fixed within the user’s view, in which screens can be dynamically selected based on the user’s eye-head position beyond an adaptive range. Additionally, the user triggers the switch to the screen chosen by making an opposing head rotation in the direction of the eye-head position to minimize false triggers. We conducted an experiment including a fast-switching scenario and a working simulation scenario with 24 participants to assess the effectiveness of GlanXR as compared to a baseline (taskbar), an expansive multi-screen setup, and a gazebased screen selection method. The results indicate that GlanXR facilitates precise screen-switching, minimizes the necessity for head rotation, and allows users to maintain a neutral head position.","현재까지 가상 및 증강 현실 기술을 통해 사용자는 작업 공간에서 여러 개의 대형 가상 화면을 볼 수 있습니다. 그러나 사용자는 이러한 화면 간에 초점을 이동하기 위해 머리를 자주 회전해야 합니다. 이 문서에서는 가상 현실에서 화면 전환을 위한 빠르고 강력한 핸즈프리 접근 방식인 GlanXR을 소개합니다. GlanXR은 사용자의 시야 내에 고정된 상태로 유지되는 주변 인터페이스를 통합하며, 적응 범위를 넘어 사용자의 눈-머리 위치에 따라 화면을 동적으로 선택할 수 있습니다. 또한 사용자는 잘못된 트리거를 최소화하기 위해 눈 머리 위치 방향으로 머리를 반대 방향으로 회전시켜 선택한 화면으로 전환을 트리거합니다. 우리는 기준(작업 표시줄), 광범위한 멀티 스크린 설정 및 시선 기반 화면 선택 방법과 비교하여 GlanXR의 효율성을 평가하기 위해 24명의 참가자를 대상으로 빠른 전환 시나리오와 작업 시뮬레이션 시나리오를 포함한 실험을 수행했습니다. 결과는 GlanXR이 정확한 화면 전환을 촉진하고 머리 회전의 필요성을 최소화하며 사용자가 중립 머리 위치를 유지할 수 있음을 나타냅니다.",https://doi.org/10.1109/ISMAR62088.2024.00025,Interaction & Input,Sensor Fusion,User Study,Algorithm / Method
217,2024,HumanCoser: Layered 3D Human Generation via Semantic-Aware Diffusion Model,HumanCoser: 의미 인식 확산 모델을 통한 계층화된 3D 인간 생성,"This paper aims to generate physically-layered 3D humans from text prompts. Existing methods either generate 3D clothed humans as a whole or support only tight and simple clothing generation, which limits their applications to virtual try-on and partlevel editing. To achieve physically-layered 3D human generation with reusable and complex clothing, we propose a novel layer-wise dressed human representation based on a physically-decoupled diffusion model. Specifically, to achieve layer-wise clothing generation, we propose a dual-representation decoupling framework for generating clothing decoupled from the human body, in conjunction with an innovative multi-layer fusion volume rendering method. To match the clothing with different body shapes, we propose an SMPL-driven implicit field deformation network that enables the free transfer and reuse of clothing. Extensive experiments demonstrate that our approach not only achieves state-of-the-art layered 3D human generation with complex clothing but also supports virtual try-on and layered human animation. More results and the code can be found on our project page at https: //cic.tju.edu.cn/faculty/likun/projects/HumanCoser","이 논문은 텍스트 프롬프트로부터 물리적 계층의 3D 인간을 생성하는 것을 목표로 합니다. 기존 방법은 3D 옷을 입은 인간을 전체적으로 생성하거나 단단하고 단순한 의복 생성만 지원하므로 응용 프로그램이 가상 시착 및 부분 수준 편집으로 제한됩니다. 재사용 가능하고 복잡한 의류로 물리적으로 계층화된 3D 인간 생성을 달성하기 위해 우리는 물리적으로 분리된 확산 모델을 기반으로 하는 새로운 계층별 옷을 입은 인간 표현을 제안합니다. 구체적으로, 레이어별 의류 생성을 달성하기 위해 혁신적인 다층 융합 볼륨 렌더링 방법과 결합하여 인체에서 분리된 의류를 생성하기 위한 이중 표현 디커플링 프레임워크를 제안합니다. 다양한 체형에 맞는 옷을 맞추기 위해 옷의 자유로운 이동과 재사용을 가능하게 하는 SMPL 기반 암시적 장 변형 네트워크를 제안합니다. 광범위한 실험을 통해 우리의 접근 방식이 복잡한 의상으로 최첨단 계층형 3D 인간 생성을 달성할 뿐만 아니라 가상 체험 및 계층형 인간 애니메이션도 지원한다는 사실이 입증되었습니다. 더 많은 결과와 코드는 프로젝트 페이지 https://cic.tju.edu.cn/faculty/likun/projects/HumanCoser에서 찾을 수 있습니다.",https://doi.org/10.1109/ISMAR62088.2024.00058,Rendering & Visualization,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method
218,2024,I Feel You: Impact of Shared Body Sensations on Social Interactions in Virtual Reality,나는 당신을 느낍니다: 신체 공유 감각이 가상 현실의 사회적 상호 작용에 미치는 영향,"While one’s facial expression and voice can be easily broadcasted from one to many via digital media, the sense of touch is limited to direct interactions. What happens if such body sensations can be shared across individuals, in which one feels a touch while watching someone else being touched? In this work, we investigated the impact of such shared body sensations on social interactions in virtual reality (VR). Building upon previous research that used psychophysics methods, our work explores the practical implications of shared body sensations in Social VR, which enables interactions beyond what’s physically possible. We conducted a withingroup user study ($\mathrm{n}=32$) in which participants observed conversations between two virtual agents and shared touch with one of the agents, as shown in Figure 1. Our results showed that even experiencing shared touch sensations several times during a conversation can affect social perception and behavior. Participants reported a stronger body illusion and empathy towards the virtual agent they shared touch with and stood closer to them. These results occurred both with and without a virtual mirror that made participants’ selfavatars more salient. The findings from this study introduce a new technique to enhance social connectedness in VR, and we discuss its applications in various contexts, such as asynchronous communication and collaboration.","사람의 얼굴 표정과 목소리는 디지털 미디어를 통해 한 사람에게서 여러 사람에게 쉽게 전달될 수 있지만, 촉각은 직접적인 상호 작용으로 제한됩니다. 누군가가 만지는 것을 보면서 누군가가 만지는 것을 느끼는, 이러한 신체 감각을 개인 간에 공유할 수 있다면 어떻게 될까요? 본 연구에서는 이러한 신체 공유 감각이 가상 현실(VR)의 사회적 상호 작용에 미치는 영향을 조사했습니다. 정신물리학 방법을 사용한 이전 연구를 바탕으로 우리 연구에서는 물리적으로 가능한 것 이상의 상호 작용을 가능하게 하는 소셜 VR에서 공유되는 신체 감각의 실질적인 의미를 탐구합니다. 우리는 그림 1과 같이 참가자들이 두 가상 에이전트 간의 대화를 관찰하고 에이전트 중 하나와 터치를 공유하는 그룹 내 사용자 연구($\mathrm{n}=32$)를 수행했습니다. 우리의 결과는 대화 중에 공유 터치 감각을 여러 번 경험하는 것조차도 사회적 인식과 행동에 영향을 미칠 수 있음을 보여주었습니다. 참가자들은 접촉을 공유하고 더 가까이 서있는 가상 에이전트에 대해 더 강한 신체 환상과 공감을 보고했습니다. 이러한 결과는 참가자의 자기 아바타를 더욱 두드러지게 만드는 가상 거울의 유무에 관계없이 발생했습니다. 본 연구의 결과는 VR의 사회적 연결성을 향상시키는 새로운 기술을 소개하고 비동기식 커뮤니케이션 및 협업과 같은 다양한 맥락에서 그 적용을 논의합니다.",https://doi.org/10.1109/ISMAR62088.2024.00126,Collaboration & Social; Interaction & Input,Natural Language Processing,User Study,User Study / Empirical Findings; Algorithm / Method
219,2024,Immersive Authoring by Demonstration of Industrial Procedures,산업 절차 시연을 통한 몰입형 저작,"This work presents an authoring tool for supporting the creation of immersive instructions for industrial processes. Our system simplifies the creation of instructional content by providing an immersive virtual reality environment that enables expert operators to interact directly with virtual replicas of industrial devices. Hand movements, tool usage, gaze, spoken comments, and machine part movement are recorded using a head-mounted display. Editing of instructions in virtual reality is aided by automatic segmentation of recorded data into individual steps and visualizations of regions with intensive activity. A qualitative evaluation of our system by industrial experts shows that it is a viable alternative to current practices in authoring instructions for assembly and maintenance.","이 작업은 산업 프로세스에 대한 몰입형 지침 생성을 지원하기 위한 저작 도구를 제공합니다. 우리 시스템은 전문 운영자가 산업용 장치의 가상 복제본과 직접 상호 작용할 수 있는 몰입형 가상 현실 환경을 제공하여 교육 콘텐츠 생성을 단순화합니다. 손 움직임, 도구 사용, 시선, 음성 설명, 기계 부품 움직임은 헤드 마운트 디스플레이를 사용하여 기록됩니다. 가상 현실에서의 지침 편집은 기록된 데이터를 개별 단계로 자동 분할하고 집중적인 활동이 있는 영역을 시각화함으로써 지원됩니다. 산업 전문가가 우리 시스템을 정성적으로 평가한 결과, 이 시스템이 조립 및 유지 관리 지침 작성에 있어 현재 관행에 대한 실행 가능한 대안임을 보여줍니다.",https://doi.org/10.1109/ISMAR62088.2024.00146,Education & Training; Display & Optics,Computer Vision,Qualitative Analysis,System / Framework
220,2024,Immersive Focus+Context Techniques to Assist in Interpretation of High Density Forest Point Clouds,고밀도 산림 포인트 클라우드 해석을 지원하는 몰입형 초점+맥락 기술,"Virtual Reality (VR) is changing how we interact with complex spatial datasets. VR has the potential within forestry to help analyse remotely sensed spatial data for inventory management. However, challenges of scale and legibility of dense forest point clouds must be addressed. To do so, we present three VR Focus+Context (F+C) techniques: Wedgelight, Remote Lantern, and Ray Lantern. These techniques present point cloud data using a constrained highdetail focus within a broader abstracted context representation, manipulated using egocentric and exocentric interactions. We conducted two user studies. The first informed the design of the F+C techniques. The results from the first study indicate that VR significantly outperforms desktop displays in basic analysis of forestry point clouds, with improvements to both time and accuracy. The second study evaluated the performance of the resulting F+C techniques in a task analogous to real-world forestry operations. Our F+C techniques significantly outperformed current approaches, confirming their viability for facilitating detailed and effective forest assessments. These findings indicate that VR has significant advantages for spatial data analysis tasks in forestry, making a strong argument for its adoption in future analytical frameworks. The FF+CC visualisation techniques we introduced show considerable promise for analysis of high density point clouds, opening new avenues for development in immersive analytics. Supplemental materials (code, scans, results) are available at https://osf.io/hqc9u.","가상 현실(VR)은 우리가 복잡한 공간 데이터 세트와 상호 작용하는 방식을 변화시키고 있습니다. VR은 산림 내에서 재고 관리를 위해 원격으로 감지된 공간 데이터를 분석하는 데 도움이 되는 잠재력을 가지고 있습니다. 그러나 울창한 숲 포인트 클라우드의 규모와 가독성 문제를 해결해야 합니다. 이를 위해 우리는 세 가지 VR Focus+Context(F+C) 기술인 Wedgelight, Remote Lantern 및 Ray Lantern을 제시합니다. 이러한 기술은 자기중심적 및 외중심적 상호작용을 사용하여 조작된 더 넓은 추상화된 컨텍스트 표현 내에서 제한된 높은 세부 초점을 사용하여 포인트 클라우드 데이터를 제시합니다. 우리는 두 가지 사용자 연구를 수행했습니다. 첫 번째는 F+C 기술의 설계에 대한 정보를 제공했습니다. 첫 번째 연구 결과에 따르면 VR은 산림 포인트 클라우드의 기본 분석에서 데스크톱 디스플레이보다 시간과 정확성이 모두 향상되어 데스크톱 디스플레이보다 훨씬 뛰어난 것으로 나타났습니다. 두 번째 연구에서는 실제 산림 운영과 유사한 작업에서 결과적인 F+C 기술의 성능을 평가했습니다. 우리의 F+C 기술은 현재 접근 방식보다 훨씬 뛰어난 성능을 발휘하여 상세하고 효과적인 산림 평가를 촉진하는 실행 가능성을 확인했습니다. 이러한 결과는 VR이 산림 분야의 공간 데이터 분석 작업에 상당한 이점을 가지고 있음을 나타내며 향후 분석 프레임워크에서 VR을 채택해야 한다는 강력한 주장을 제시합니다. 우리가 도입한 FF+CC 시각화 기술은 고밀도 포인트 클라우드 분석에 대한 상당한 가능성을 보여주며 몰입형 분석 개발을 위한 새로운 길을 열어줍니다. 보충 자료(코드, 스캔, 결과)는 https://osf.io/hqc9u에서 확인할 수 있습니다.",https://doi.org/10.1109/ISMAR62088.2024.00112,Rendering & Visualization,Optical / Display Technology,Technical Evaluation,Algorithm / Method
221,2024,Impact of Auditory and Audiovisual Distractors on Task Performance in a VR-based Auditory Attention Task,VR 기반 청각 주의 작업에서 청각 및 시청각 방해 요소가 작업 성능에 미치는 영향,"Auditory attention is a fundamental cognitive process essential for effective communication and interaction. Auditory stimuli are often accompanied by distractors that can significantly impact task performance by means of reducing attention. This study investigates the influence of auditory and audiovisual distractors on an auditory attention task within a Virtual Reality classroom. As part of the user evaluation, participants had to listen to two short stories. They performed two tasks: (i) “DISTRACTORS”, where participants had to identify an auditory or an audiovisual stimulus and (ii) “KEYWORDS,” where participants had to identify a specific keyword in the story by pressing a button on the controller. During the experiment, the participants’ physiological (e.g. skin conductance levels, gaze data, etc.) and subjective data (i.e. questionnaires) were collected. The results revealed that the interval between the presentation of the keyword and the presentation of the distractors impacted task performance by negatively affecting auditory attention. Also, it was observed that the time spent looking at the speaker telling the story positively correlated with task performance, whereas the time spent looking at distractors was found to be negatively correlated with task performance. Finally, this study gives insights into how physiological metrics can be used to infer auditory attention in VR experiences.","청각적 주의는 효과적인 의사소통과 상호작용에 필수적인 기본적인 인지 과정입니다. 청각 자극은 종종 주의력을 감소시켜 작업 수행에 큰 영향을 미칠 수 있는 산만함을 동반합니다. 이 연구는 가상 현실 교실 내 청각 주의 작업에 대한 청각 및 시청각 방해 요소의 영향을 조사합니다. 사용자 평가의 일환으로 참가자들은 두 편의 단편 소설을 들어야 했습니다. 그들은 두 가지 작업을 수행했습니다. (i) 참가자가 청각 또는 시청각 자극을 식별해야 하는 ""DISTRACTORS""와 (ii) 참가자가 컨트롤러의 버튼을 눌러 스토리에서 특정 키워드를 식별해야 하는 ""키워드""입니다. 실험 동안 참가자의 생리학적 데이터(예: 피부 전도도 수준, 시선 데이터 등)와 주관적 데이터(예: 설문지)가 수집되었습니다. 결과는 키워드 제시와 방해 요소 제시 사이의 간격이 청각 주의에 부정적인 영향을 미쳐 작업 수행에 영향을 미치는 것으로 나타났습니다. 또한, 이야기를 하는 화자를 보는 시간은 과제 수행과 긍정적인 상관관계가 있는 반면, 방해 요인을 보는 시간은 과제 수행과 음의 상관관계가 있는 것으로 관찰되었습니다. 마지막으로, 이 연구는 VR 경험에서 청각적 주의를 추론하기 위해 생리학적 지표를 어떻게 사용할 수 있는지에 대한 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR62088.2024.00130,Interaction & Input; Perception & Cognition,Sensor Fusion,User Study,User Study / Empirical Findings
222,2024,Impact of Role Assignment through Complementary Design of Self and Other Avatars on Self-Image and Behavior Change,자신과 다른 아바타의 보완적인 디자인을 통한 역할 할당이 자아 이미지와 행동 변화에 미치는 영향,"This study investigates the impact of both self-and other’s avatars (virtual bodies) with complementary traits on self-image and behavior change, drawing on role theory: individuals adopt and internalize their social roles as expected by others. In our experiment, participants and a non-player character played a cooperative virtual reality action game together, embodying a “warrior” avatar and a “witch” avatar with complementary appearances and in-game abilities. Results revealed that role assignments based on the use of complementary avatars has significant interaction effects with participants’ personal characteristics on influencing individuals’ behavior and self-image. Furthermore, a risk of unpredicted change resulting from failed role assignments was discovered, suggesting the importance of achieving role assignments that match avatar complementarity. By providing a new perspective on the impact of interactions between multiple users with diverse avatars, these findings contribute to our understanding of the mechanisms of cognitive augmentation with avatars and have implications for the design of avatar-related experiences in the metaverse.","본 연구는 역할 이론을 바탕으로 상호 보완적인 특성을 지닌 자신과 타인의 아바타(가상 신체)가 자아 이미지와 행동 변화에 미치는 영향을 조사합니다. 개인은 타인이 기대하는 대로 자신의 사회적 역할을 채택하고 내면화합니다. 실험에서 참가자와 비플레이어 캐릭터는 상호보완적인 외모와 게임 내 능력을 갖춘 ""전사"" 아바타와 ""마녀"" 아바타를 구현한 협동 가상 현실 액션 게임을 함께 플레이했습니다. 결과는 보완적인 아바타의 사용을 기반으로 한 역할 할당이 개인의 행동과 자아상에 영향을 미치는 참가자의 개인적 특성과 중요한 상호 작용 효과를 갖는 것으로 나타났습니다. 또한, 역할 할당 실패로 인해 예상치 못한 변화가 발생할 위험이 발견되었으며, 이는 아바타 보완성과 일치하는 역할 할당을 달성하는 것이 중요함을 시사합니다. 다양한 아바타를 사용하는 여러 사용자 간의 상호 작용이 미치는 영향에 대한 새로운 관점을 제공함으로써 이러한 발견은 아바타를 통한 인지 강화 메커니즘에 대한 이해에 기여하고 메타버스에서 아바타 관련 경험의 디자인에 영향을 미칩니다.",https://doi.org/10.1109/ISMAR62088.2024.00103,Perception & Cognition,Optical / Display Technology,User Study,User Study / Empirical Findings
223,2024,Intelligent Pre-Reset for Redirected Walking through Reinforcement Learning,강화 학습을 통한 방향 전환을 위한 지능형 사전 재설정,"Redirected walking (RDW) algorithms subtly manipulates users’ movements in virtual environments through three gains, distorting their trajectories in the physical space, thus allowing them to explore infinite virtual environments within limited physical spaces. However, collisions with physical boundaries or obstacles in physical spaces are inevitable. At this point, explicit redirected techniques, such as reset, are needed to pause users’ roaming experience and redirect them to open areas. Currently, most reset strategies wait until a collision occurs before resetting the user, while these reset algorithms can achieve certain effectiveness in simple physical environments. However, in complex physical environments, users may walk into narrow areas before collisions occur. Therefore, no matter which direction they turn, further collisions are likely to happen over short distances. To enhance obstacle avoidance capability while adapting to various complex physical layouts, this paper adopts the concept of pre-reset and presents a novel method of intelligent pre-reset IPR for redirected walking through reinforcement learning, which determines whether an early reset should be performed and the optimal reset direction based on the current physical environment, as well as the user’s position and walking trajectory. Through simulation and human experiments compared with other reset algorithms in several environments, we demonstrate the superiority of our approach, achieving higher average walking distances between resets while adapting to various nuances of reset techniques. At the same time, IPR is applicable to various RDW algorithms and environments of different complexities.","RDW(Redirected Walking) 알고리즘은 세 가지 이득을 통해 가상 환경에서 사용자의 움직임을 미묘하게 조작하여 물리적 공간에서의 궤적을 왜곡함으로써 제한된 물리적 공간 내에서 무한한 가상 환경을 탐색할 수 있도록 합니다. 그러나 물리적 경계나 물리적 공간 내 장애물과의 충돌은 불가피하다. 이 시점에서 사용자의 로밍 경험을 일시 중지하고 열린 영역으로 리디렉션하려면 재설정과 같은 명시적인 리디렉션 기술이 필요합니다. 현재 대부분의 재설정 전략은 사용자를 재설정하기 전에 충돌이 발생할 때까지 기다리는 반면, 이러한 재설정 알고리즘은 단순한 물리적 환경에서 특정 효율성을 달성할 수 있습니다. 그러나 복잡한 물리적 환경에서는 충돌이 발생하기 전에 사용자가 좁은 영역으로 걸어갈 수 있습니다. 따라서 어느 방향으로 회전하더라도 짧은 거리에서는 추가 충돌이 발생할 가능성이 높습니다. 다양하고 복잡한 물리적 레이아웃에 적응하면서 장애물 회피 능력을 향상시키기 위해 본 논문에서는 사전 재설정 개념을 채택하고 강화 학습을 통해 방향 전환된 보행을 위한 지능형 사전 재설정 IPR의 새로운 방법을 제시합니다. 강화 학습은 현재 물리적 환경과 사용자의 위치 및 보행 궤적을 기반으로 조기 재설정 수행 여부와 최적의 재설정 방향을 결정합니다. 여러 환경에서 다른 재설정 알고리즘과 비교한 시뮬레이션 및 인간 실험을 통해 우리는 재설정 기술의 다양한 뉘앙스에 적응하면서 재설정 간 평균 도보 거리를 더 높게 달성하는 접근 방식의 우수성을 보여줍니다. 동시에 IPR은 다양한 RDW 알고리즘과 다양한 복잡성의 환경에 적용 가능합니다.",https://doi.org/10.1109/ISMAR62088.2024.00048,Education & Training,Sensor Fusion,Simulation,Algorithm / Method
224,2024,Inter Brain Synchrony in Remote AR Education: Can Warming up Activities Positively Impact Educational Quality?,원격 AR 교육의 두뇌 간 동기화: 워밍업 활동이 교육 품질에 긍정적인 영향을 미칠 수 있습니까?,"Previous education studies suggested that “warming up” activities featuring interpersonal synchrony components help improve educational quality and social closeness between teachers and students. While educators are eager to incorporate augmented reality (AR) into remote educational settings, prior research has not investigated the beneficial impacts of interpersonal synchrony on remote AR education. This study investigates the effects of prior physical synchrony (PS) on learning outcomes, inter brain synchrony (IBS), and social closeness in remote AR education. 41 participants were recruited for learning English vocabulary from a teacher. They conducted a movement learning task using prior PS and physical asynchrony (PA). The results showed that the positive effects of warming up activity on quiz score, IBS, and social closeness were observed in the PS section, but not in the PA section. Moreover, we found a high correlation between quiz score and IBS but not with social closeness. These results suggest that prior PS is a beneficial component of remote AR education and that IBS could be used as an index for evaluating educational quality. The implications of these results for remote AR education are discussed.","이전의 교육 연구에서는 대인 동시성 구성 요소를 특징으로 하는 ""워밍업"" 활동이 교사와 학생 간의 교육 품질과 사회적 친밀감을 향상시키는 데 도움이 된다고 제안했습니다. 교육자들은 증강 현실(AR)을 원격 교육 환경에 통합하기를 열망하고 있지만, 이전 연구에서는 대인 관계 동기화가 원격 AR 교육에 미치는 유익한 영향을 조사하지 않았습니다. 이 연구는 원격 AR 교육에서 학습 결과, 뇌 간 동기화(IBS) 및 사회적 친밀감에 대한 이전 물리적 동기화(PS)의 영향을 조사합니다. 교사로부터 영어 단어를 배우기 위해 41명의 참가자를 모집했습니다. 그들은 이전 PS와 물리적 비동기(PA)를 사용하여 움직임 학습 작업을 수행했습니다. 그 결과 워밍업 활동이 퀴즈점수, IBS, 사회적 친밀감에 긍정적인 영향을 미치는 것으로 나타났으며 PS 영역에서는 나타났으나 PA 영역에서는 그렇지 않은 것으로 나타났다. 또한 퀴즈 점수와 IBS 사이에는 높은 상관관계가 있었지만 사회적 친밀감과는 상관관계가 없는 것으로 나타났습니다. 이러한 결과는 사전 PS가 원격 AR 교육의 유익한 구성 요소이며 IBS가 교육 품질을 평가하는 지표로 사용될 수 있음을 시사합니다. 원격 AR 교육에 대한 이러한 결과의 의미가 논의됩니다.",https://doi.org/10.1109/ISMAR62088.2024.00073,Education & Training; Collaboration & Social,Other,User Study,User Study / Empirical Findings
225,2024,Investigating eXtended Reality-powered Digital Twins for Sequential Instruction Learning: the Case of the Rubik's Cube,순차적 교육 학습을 위한 확장된 현실 기반 디지털 트윈 조사: 루빅스 큐브 사례,"Educational practices are increasingly experimenting with eXtended Reality (XR) paradigms to offer novel opportunities for boundaryless learning experiences with real-time interactions in immersive environments. Digital Twins (DT) are also gaining traction in this field to facilitate personalized learning experiences. However, a still unexplored space in learning frameworks amounts to the one where XR intersects with DTs. This work wants to move a step in such a direction with the design, implementation, and test of a DT-driven XR framework to learn procedural tasks. The framework offers three distinct learning modalities where virtual and physical interactions enhance learning retention by engaging users actively in digital and real-world environments. We contextualize such a framework for procedural task learning through one of its pivotal use cases: learning Rubik’s Cube notations. To evaluate and compare the effectiveness of these modalities, we perform an experimental user campaign evaluating short-term skill retention, performance accuracy, usability, and cognitive load of each of them. We then provide an extensive statistical analysis to compare each kind of guidance while analyzing correlations between the examined variables, offering insights into optimizing instructional methodologies within XR-based educational frameworks.","교육 현장에서는 몰입형 환경에서 실시간 상호 작용을 통해 경계 없는 학습 경험을 위한 새로운 기회를 제공하기 위해 확장 현실(XR) 패러다임을 점점 더 실험하고 있습니다. 디지털 트윈(DT)도 개인화된 학습 경험을 촉진하기 위해 이 분야에서 주목을 받고 있습니다. 그러나 학습 프레임워크에서 아직 탐색되지 않은 공간은 XR이 DT와 교차하는 공간입니다. 본 작업에서는 절차적 작업을 학습하기 위한 DT 기반 XR 프레임워크의 설계, 구현 및 테스트를 통해 그러한 방향으로 한 단계 더 나아가고자 합니다. 프레임워크는 가상 및 물리적 상호 작용이 디지털 및 실제 환경에서 사용자를 적극적으로 참여시켜 학습 유지를 강화하는 세 가지 학습 방식을 제공합니다. 우리는 중요한 사용 사례 중 하나인 루빅스 큐브 표기법 학습을 통해 절차적 작업 학습을 위한 프레임워크를 맥락화합니다. 이러한 양식의 효과를 평가하고 비교하기 위해 우리는 각각의 단기 기술 유지, 성능 정확성, 유용성 및 인지 부하를 평가하는 실험적인 사용자 캠페인을 수행합니다. 그런 다음 조사된 변수 간의 상관관계를 분석하는 동시에 각 종류의 지침을 비교하기 위한 광범위한 통계 분석을 제공하여 XR 기반 교육 프레임워크 내에서 교육 방법론을 최적화하는 데 대한 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR62088.2024.00040,Education & Training; Interaction & Input,Other,Quantitative Experiment,System / Framework
226,2024,Investigating the Carryover Effects of Calibration of Size Perception in Augmented Reality to the Real World,증강 현실에서 크기 인식 보정이 현실 세계로 이월되는 효과 조사,"Many AR applications require users to perceive, estimate and calibrate to the size of objects presented in the scene. Distortions in size perception in AR could potentially influence the effectiveness of skills transferred from the AR to the real world. We investigated the after-effects or carry-over effects of calibration of size perception in AR to the real world (RW), by providing feedback and an opportunity for participants to correct their judgments in AR. In an empirical evaluation, we employed a three-phase experiment design. In the pretest phase, participants made size estimations to target objects concurrently using both verbal reports and physical judgment in RW as a baseline. Then, they estimated the size of targets, and then were provided with feedback and subsequently corrected their judgments in a calibration phase. Followed by which, participants made size estimates to target objects in the real world. Our findings revealed that the carryover effects of calibration successfully transferred from AR to RW in both verbal reports and physical judgment methods.",많은 AR 애플리케이션에서는 사용자가 장면에 나타나는 물체의 크기를 인식하고 추정하고 보정해야 합니다. AR의 크기 인식 왜곡은 AR에서 실제 세계로 이전된 기술의 효율성에 잠재적으로 영향을 미칠 수 있습니다. 우리는 참가자들이 AR에서 자신의 판단을 수정할 수 있는 기회와 피드백을 제공함으로써 AR에서 크기 인식 보정이 현실 세계(RW)로 미치는 후유증 또는 이월 효과를 조사했습니다. 실증적 평가에서는 3단계 실험 설계를 사용했습니다. 사전 테스트 단계에서 참가자들은 RW의 구두 보고와 신체적 판단을 기준으로 동시에 대상 물체의 크기를 추정했습니다. 그런 다음 목표의 크기를 추정하고 피드백을 제공한 후 보정 단계에서 판단을 수정했습니다. 그런 다음 참가자들은 실제 세계의 대상 물체에 대한 크기 추정을 했습니다. 우리의 연구 결과에 따르면 교정의 이월 효과는 구두 보고와 신체적 판단 방법 모두에서 AR에서 RW로 성공적으로 이전된 것으로 나타났습니다.,https://doi.org/10.1109/ISMAR62088.2024.00096,Interaction & Input,Optical / Display Technology,User Study,User Study / Empirical Findings; Algorithm / Method
227,2024,Investigating the Effects of Physical Landmarks on Spatial Memory for Information Visualisation in Augmented Reality,증강 현실의 정보 시각화를 위한 공간 기억에 대한 물리적 랜드마크의 효과 조사,"Augmented Reality (AR) is touted to be beneficial in supporting situated information display, allowing virtual information panels to be overlaid on real-world scenes. People must then use their spatial memory to navigate among these virtual panels effectively. While spatial memory has been studied in physical environments (wall displays) and virtual reality environments, there has been little research on how physical surroundings might affect memorisation of virtual content in a mixed environment like AR. Therefore, we provide the first AR study of spatial memory, comparing two different room settings with two different situated layouts of virtual targets on an abstract spatial memory task. We find that participants recall spatial patterns with greater accuracy and higher subjective ratings in a room with furniture compared to an empty room. Our findings lead to important design implications for mixed-reality user interfaces, particularly in information-rich applications like situated analytics and small-multiples information visualisation.","증강 현실(AR)은 가상 정보 패널을 실제 장면에 오버레이할 수 있도록 하여 상황 정보 표시를 지원하는 데 도움이 되는 것으로 알려져 있습니다. 그런 다음 사람들은 이러한 가상 패널을 효과적으로 탐색하기 위해 공간 기억을 사용해야 합니다. 물리적 환경(벽면 디스플레이)과 가상 현실 환경에서 공간 기억이 연구된 반면, AR과 같은 혼합 환경에서 물리적 환경이 가상 콘텐츠 기억에 어떤 영향을 미칠 수 있는지에 대한 연구는 거의 없었습니다. 따라서 우리는 추상 공간 기억 작업에 대한 두 가지 서로 다른 공간 설정과 두 가지 서로 다른 위치의 가상 대상 레이아웃을 비교하여 공간 기억에 대한 최초의 AR 연구를 제공합니다. 우리는 참가자들이 빈 방에 비해 가구가 있는 방에서 더 높은 정확도와 더 높은 주관적 평가로 공간 패턴을 기억한다는 것을 발견했습니다. 우리의 연구 결과는 혼합 현실 사용자 인터페이스, 특히 위치 분석 및 소배수 정보 시각화와 같은 정보가 풍부한 응용 프로그램에 대한 중요한 디자인 영향으로 이어집니다.",https://doi.org/10.1109/ISMAR62088.2024.00043,Content Authoring,Other,User Study,User Study / Empirical Findings
228,2024,Is 3DGS Useful?: Comparing the Effectiveness of Recent Reconstruction Methods in VR,3DGS가 유용한가요?: VR에서 최근 재구성 방법의 효과 비교,"Recent advances in 3D object reconstruction have been remarkable, and 3D object reconstruction methods capable of real-time rendering are crucial for the creation of real-world content. Most reconstruction research has focused on improving algorithmic performance (e.g., rendering time, and visual quality). However, we need to know which reconstruction method can improve the user experience when used in real-world content, and whether the experience differs across platforms. In this study, we investigate the effects of three different visualization methods, including two real-time reconstruction methods (3DGS and Image-to-3D) and video playback, on user recognition memory and experience on two different platforms (VR and PC). The results show that different visualization methods improve recognition memory and user experience differently and that there are differences in the effects across platforms. In addition, we investigate designers’ views on 3D object visualization techniques and discuss how they can be used in actual content creation and their scalability. The results of this study suggest that it is possible to improve user experience and recognition memory by recommending different methods depending on the visualization perspective and platform used.","최근 3D 객체 재구성의 발전은 눈부시게 발전했으며, 실시간 렌더링이 가능한 3D 객체 재구성 방법은 실제 콘텐츠를 생성하는 데 매우 중요합니다. 대부분의 재구성 연구는 알고리즘 성능(예: 렌더링 시간 및 시각적 품질) 개선에 중점을 두었습니다. 그러나 실제 콘텐츠에 사용될 때 어떤 재구성 방법이 사용자 경험을 향상시킬 수 있는지, 플랫폼에 따라 경험이 다른지 여부를 알아야 합니다. 본 연구에서는 두 가지 실시간 재구성 방법(3DGS 및 Image-to-3D)과 비디오 재생을 포함한 세 가지 시각화 방법이 두 가지 플랫폼(VR 및 PC)에서 사용자 인식 메모리 및 경험에 미치는 영향을 조사합니다. 결과는 다양한 시각화 방법이 인식 기억과 사용자 경험을 다르게 개선하고 플랫폼에 따라 효과에 차이가 있음을 보여줍니다. 또한 3D 객체 시각화 기법에 대한 디자이너의 견해를 조사하고, 이를 실제 콘텐츠 제작에 어떻게 활용할 수 있는지와 확장성에 대해 논의한다. 본 연구 결과는 시각화 관점과 사용하는 플랫폼에 따라 다양한 방법을 추천함으로써 사용자 경험과 인식기억을 향상시킬 수 있음을 시사한다.",https://doi.org/10.1109/ISMAR62088.2024.00021,Rendering & Visualization,3D Reconstruction,Other,Algorithm / Method
229,2024,Leveraging Augmented Reality for Understanding Schizophrenia - Design and Evaluation of a Dedicated Educational Tool,정신분열증 이해를 위한 증강 현실 활용 - 전용 교육 도구의 설계 및 평가,"Schizophrenia is a serious mental health disorder which may include symptoms such as hallucinations, delusions, and disorganized behavior. In addition to experiencing a diverse symptomatology, individuals with schizophrenia suffer from significant stigmatization which can interfere with effective treatment of the disorder among other issues. As a primary source of stigmatization comes from healthcare professionals, we were motivated to explore new ways in which to educate healthcare students about the symptoms of schizophrenia. Despite its potential to immerse users in new experiences within a real environment, little research concerning the use of augmented reality (AR) to simulate schizophrenia exists. In this paper, we present Live-It, a tool designed using recommendations from prior work as well as inspiration from lived experiences to educate mental health students about schizophrenia. The simulation uses the video passthrough capabilities of the Meta Quest 3 headset to simulate delusions, auditory hallucinations and additional symptomatology. Using thematic analysis, we evaluated our simulation with nineteen students and eighteen experts in healthcare to understand its ability to engage users and reliably represent symptoms of the disorder, as well as to determine how best to improve upon the design before the tool is widely deployed in mental health curricula.Our findings suggest that participants better understood symptoms of schizophrenia after experiencing the simulation, highlighting the potential of Live-It to be used as an educational tool. We present our design, provide a detailed analysis of our findings, and underline next steps in the development of our tool.","정신분열증은 환각, 망상, 혼란스러운 행동과 같은 증상을 포함할 수 있는 심각한 정신 건강 장애입니다. 다양한 증상을 경험하는 것 외에도, 정신분열병 환자는 심각한 낙인을 겪으며 이는 다른 문제 중에서도 장애의 효과적인 치료를 방해할 수 있습니다. 낙인의 주요 원인은 의료 전문가로부터 나오므로, 우리는 정신분열증 증상에 대해 의료계 학생들을 교육하는 새로운 방법을 모색하려는 동기를 부여받았습니다. 실제 환경 내에서 새로운 경험에 사용자를 몰입시킬 수 있는 잠재력에도 불구하고 정신분열증을 시뮬레이션하기 위해 증강 현실(AR)을 사용하는 것에 관한 연구는 거의 없습니다. 이 논문에서는 이전 작업의 권장 사항과 실제 경험에서 얻은 영감을 사용하여 정신 건강 학생들에게 정신 분열증에 대해 교육하기 위해 설계된 도구인 Live-It을 제시합니다. 시뮬레이션에서는 Meta Quest 3 헤드셋의 비디오 패스스루 기능을 사용하여 망상, 청각 환각 및 추가 증상을 시뮬레이션합니다. 주제별 분석을 사용하여 우리는 19명의 학생과 18명의 의료 전문가와 함께 시뮬레이션을 평가하여 사용자의 참여를 유도하고 장애의 증상을 안정적으로 나타내는 능력을 이해하고 도구가 정신 건강 커리큘럼에 널리 배포되기 전에 디자인을 가장 잘 개선할 수 있는 방법을 결정했습니다. 우리의 연구 결과는 참가자들이 시뮬레이션을 경험한 후 정신분열증의 증상을 더 잘 이해했음을 시사하며 Live-It이 교육 도구로 사용될 가능성을 강조합니다. 우리는 설계를 제시하고, 연구 결과에 대한 자세한 분석을 제공하며, 도구 개발의 다음 단계를 강조합니다.",https://doi.org/10.1109/ISMAR62088.2024.00110,Education & Training,Deep Learning / Neural Networks,User Study,User Study / Empirical Findings
230,2024,LookUP: Command Search Using Dwell-free Eye Typing in Mixed Reality,LookUP: 혼합 현실에서 Dwell-Free Eye Typing을 사용한 명령 검색,"We introduce LookUP, a novel general purpose command search system for mixed reality headsets, offering a hands-free experience through dwell-free eye typing. With LookUP, users can trigger the display of a virtual keyboard with a simple upward head motion. The keyboard then uses a statistical decoder to interpret users’ intended text based on their eye movements. This approach diverges from traditional dwell-time methods, significantly enhancing typing speed and efficiency. Our research involved deploying LookUP on a HoloLens 2, and benchmarking it against a dwell-based command search baseline and the native HoloLens system menu. Our user study indicated that participants spent a significantly shorter time using LookUP with dwell-free eye typing in command search and entry, demonstrating LookUP’s potential to be a complementary command input for mixed reality headsets.","혼합 현실 헤드셋을 위한 새로운 범용 명령 검색 시스템인 LookUP을 소개합니다. 이 시스템은 체류하지 않는 눈 입력을 통해 핸즈프리 경험을 제공합니다. LookUP을 사용하면 사용자는 간단한 머리 위쪽 동작으로 가상 키보드 표시를 실행할 수 있습니다. 그런 다음 키보드는 통계 디코더를 사용하여 눈 움직임을 기반으로 사용자가 의도한 텍스트를 해석합니다. 이 접근 방식은 기존의 체류 시간 방식과 다르므로 타이핑 속도와 효율성이 크게 향상됩니다. 우리의 연구에는 HoloLens 2에 LookUP을 배포하고 거주 기반 명령 검색 기준선 및 기본 HoloLens 시스템 메뉴에 대한 벤치마킹이 포함되었습니다. 우리의 사용자 연구에 따르면 참가자들은 명령 검색 및 입력 시 눈을 떼지 않고 입력하면서 LookUP을 사용하는 데 훨씬 짧은 시간을 보냈으며, 이는 혼합 현실 헤드셋을 위한 보완적인 명령 입력이 될 수 있는 LookUP의 잠재력을 입증했습니다.",https://doi.org/10.1109/ISMAR62088.2024.00114,Interaction & Input; Display & Optics,Deep Learning / Neural Networks,User Study; Technical Evaluation,Algorithm / Method; Hardware / Device
231,2024,Manipulating Immersion: The Impact of Perceptual Incongruence on Perceived Plausibility in VR,몰입 조작: VR에서 인지된 타당성에 대한 지각 불일치의 영향,"This work presents a study where we used incongruencies on the cognitive and the perceptual layer to investigate their effects on perceived plausibility and, thereby, presence and spatial presence. We used a $2 \times 3$ within-subject design with the factors familiar size (cognitive manipulation) and immersion (perceptual manipulation). For the different levels of immersion, we implemented three different tracking qualities: rotation-and-translation tracking, rotation-only tracking, and stereoscopic-view-only tracking. Participants scanned products in a virtual supermarket where the familiar size of these objects was manipulated. Simultaneously, they could either move their head normally or need to use the thumbsticks to navigate their view of the environment. Results show that both manipulations had a negative effect on perceived plausibility and, thereby, presence. In addition, the tracking manipulation also had a negative effect on spatial presence. These results are especially interesting in light of the ongoing discussion about the role of plausibility and congruence in evaluating XR environments. The results can hardly be explained by traditional presence models, where immersion should not be an influencing factor for perceived plausibility. However, they are in agreement with the recently introduced Congruence and Plausibility (CaP) model and provide empirical evidence for the model’s predicted pathways.","이 작업은 인지된 타당성과 이에 따른 존재감 및 공간적 존재감에 대한 영향을 조사하기 위해 인지 및 지각 계층의 불일치를 사용한 연구를 제시합니다. 우리는 친숙한 크기(인지 조작)와 몰입(지각 조작) 요소를 갖춘 $2 \times 3$ 주제 내 디자인을 사용했습니다. 다양한 몰입 수준을 위해 회전 및 이동 추적, 회전 전용 추적, 입체 보기 전용 추적이라는 세 가지 서로 다른 추적 품질을 구현했습니다. 참가자들은 이러한 물체의 친숙한 크기가 조작된 가상 슈퍼마켓에서 제품을 스캔했습니다. 동시에 그들은 머리를 정상적으로 움직일 수도 있고, 환경에 대한 시야를 탐색하기 위해 엄지스틱을 사용해야 할 수도 있습니다. 결과는 두 조작 모두 인지된 타당성과 그에 따른 존재감에 부정적인 영향을 미쳤다는 것을 보여줍니다. 또한, 추적 조작 역시 공간 존재감에 부정적인 영향을 미쳤다. 이러한 결과는 XR 환경을 평가할 때 타당성과 합치성의 역할에 대한 지속적인 논의를 고려할 때 특히 흥미롭습니다. 결과는 몰입도가 인지된 타당성에 영향을 미치는 요소가 되어서는 안 되는 전통적인 존재감 모델로는 거의 설명할 수 없습니다. 그러나 이는 최근 도입된 합동 및 타당성(CaP) 모델과 일치하며 모델의 예측 경로에 대한 경험적 증거를 제공합니다.",https://doi.org/10.1109/ISMAR62088.2024.00124,Perception & Cognition,Sensor Fusion,User Study,User Study / Empirical Findings
232,2024,May the Force Be with You: Dexterous Finger Force-Aware VR Interface,포스가 당신과 함께하길: 손재주가 뛰어난 손가락의 힘 인식 VR 인터페이스,"Advances in virtual reality (VR) have reduced experience differentials for users. However, gaps between reality and virtuality persist in tasks that require coupling users’ multimodal physical skills with virtual environments in delicate ways. User embodiment in VR easily breaks when physicality feels inauthentic, especially when users invoke their innate predilection to touch and manipulate things that they encounter. In this research, we examine the potential of forceaware VR interfaces for enabling natural connections to user physicality and evaluate them in high-finesse cases of touch. Combining surface electromyography (SEMG) with visual tracking, we develop an end-to-end learning-based system, ForceSense, to decode users’ dexterous finger forces from their forearm sEMG signals for direct usage in standard VR pipelines. This approach eliminates the need for hand-held tactile equipment, thereby promoting natural embodiment. A series of user studies on manipulation tasks in VR validate that ForceSense is more accurate, robust, and intuitive than alternative solutions. Two proofs-of-concept VR applications, calligraphy and piano playing, demonstrate that the good synergy between visual, auditory, and tactile modalities, as ForceSense affords, has the potential of enhancing users’ task learning performance in VR. Our source code and trained models are released at https://github.com/NYU-ICL/vr-force-aware-multimodal-interface.","가상현실(VR)의 발전으로 사용자 경험의 차이가 줄어들었습니다. 그러나 사용자의 다중 모드 물리적 기술과 가상 환경을 섬세한 방식으로 결합해야 하는 작업에서는 현실과 가상 사이의 격차가 지속됩니다. VR의 사용자 구현은 물리적인 것이 진짜처럼 느껴지면 쉽게 깨집니다. 특히 사용자가 자신이 접하는 사물을 만지고 조작하려는 타고난 성향을 불러일으킬 때 더욱 그렇습니다. 본 연구에서 우리는 사용자의 신체적 특성에 자연스러운 연결을 가능하게 하는 forceaware VR 인터페이스의 잠재력을 조사하고 정교한 터치 사례에서 이를 평가합니다. 표면 근전도검사(SEMG)와 시각적 추적을 결합하여 우리는 표준 VR 파이프라인에서 직접 사용할 수 있도록 팔뚝 sEMG 신호에서 사용자의 손재주 힘을 디코딩하는 엔드투엔드 학습 기반 시스템인 ForceSense를 개발합니다. 이 접근 방식은 손에 쥐는 촉각 장비의 필요성을 제거하여 자연스러운 구현을 촉진합니다. VR의 조작 작업에 대한 일련의 사용자 연구를 통해 ForceSense가 대체 솔루션보다 더 정확하고 강력하며 직관적이라는 것이 입증되었습니다. 두 가지 개념 증명 VR 애플리케이션인 서예와 피아노 연주는 ForceSense가 제공하는 시각적, 청각적, 촉각적 양식 간의 좋은 시너지 효과가 VR에서 사용자의 작업 학습 성능을 향상시킬 수 있는 잠재력을 가지고 있음을 보여줍니다. 우리의 소스 코드와 훈련된 모델은 https://github.com/NYU-ICL/vr-force-aware-multimodal-interface에서 공개됩니다.",https://doi.org/10.1109/ISMAR62088.2024.00060,Interaction & Input; Education & Training,Haptic / Tactile Feedback,Technical Evaluation,System / Framework; Algorithm / Method
233,2024,"Mazed and Confused: A Dataset of Cybersickness, Working Memory, Mental Load, Physical Load, and Attention During a Real Walking Task in VR","어리둥절하고 혼란스럽습니다: VR에서 실제 걷는 작업 중 사이버 멀미, 작업 기억, 정신적 부하, 신체적 부하 및 주의력에 대한 데이터 세트","Virtual Reality (VR) is quickly establishing itself in various industries, including training, education, medicine, and entertainment, in which users are frequently required to carry out multiple complex cognitive and physical activities. However, the relationship between cognitive activities, physical activities, and familiar feelings of cybersickness is not well understood and thus can be unpredictable for developers. Researchers have previously provided labeled datasets for predicting cybersickness while users are stationary, but there have been few labeled datasets on cybersickness while users are physically walking. Moreover, it is unclear how walking while cybersick will affect cognitive load, even though room-scale interaction is typical in many VR games. Thus, from 39 participants, we collected head orientation, head position, eye tracking, images, physiological readings from external sensors, and the self-reported cybersickness severity, physical load, and mental load in VR. Throughout the data collection, participants navigated mazes via real walking and performed tasks challenging their attention and working memory. To demonstrate the dataset’s utility, we conducted a case study of training classifiers in which we achieved 95% accuracy for cybersickness severity classification. The noteworthy performance of the straightforward classifiers makes this dataset ideal for future researchers to develop cybersickness detection and reduction models. To better understand the features that helped with classification, we performed SHAP(SHapley Additive exPlanations) analysis, highlighting the importance of eye tracking and physiological measures for cybersickness prediction while walking. This open dataset can allow future researchers to study the connection between cybersickness and cognitive loads and develop prediction models. This dataset will empower future VR developers to design efficient and effective Virtual Environments by improving cognitive load management and minimizing cybersickness.","가상 현실(VR)은 사용자가 여러 복잡한 인지 및 신체 활동을 자주 수행해야 하는 훈련, 교육, 의료, 엔터테인먼트 등 다양한 산업 분야에서 빠르게 자리잡고 있습니다. 그러나 인지 활동, 신체 활동 및 친숙한 사이버 멀미 사이의 관계는 잘 이해되지 않아 개발자가 예측할 수 없습니다. 연구원들은 이전에 사용자가 정지해 있는 동안 사이버 멀미를 예측하기 위해 레이블이 지정된 데이터 세트를 제공했지만, 사용자가 실제로 걷고 있는 동안 사이버 멀미에 대해 레이블이 지정된 데이터 세트는 거의 없었습니다. 더욱이, 많은 VR 게임에서 룸 규모 상호 작용이 일반적임에도 불구하고 사이버 멀미 중 걷기가 인지 부하에 어떤 영향을 미치는지는 불분명합니다. 따라서 39명의 참가자로부터 머리 방향, 머리 위치, 시선 추적, 이미지, 외부 센서의 생리적 판독값, VR에서 자체 보고된 사이버 멀미 심각도, 신체적 부하 및 정신적 부하를 수집했습니다. 데이터 수집 전반에 걸쳐 참가자들은 실제 걷기를 통해 미로를 탐색하고 주의력과 작업 기억력에 도전하는 작업을 수행했습니다. 데이터 세트의 유용성을 입증하기 위해 우리는 사이버 멀미 심각도 분류에 대해 95%의 정확도를 달성한 훈련 분류자에 대한 사례 연구를 수행했습니다. 간단한 분류기의 주목할만한 성능으로 인해 이 데이터 세트는 미래 연구자가 사이버 멀미 감지 및 감소 모델을 개발하는 데 이상적입니다. 분류에 도움이 되는 기능을 더 잘 이해하기 위해 SHAP(SHapley Additive exPlanations) 분석을 수행하여 걷는 동안 사이버 멀미 예측을 위한 시선 추적 및 생리적 측정의 중요성을 강조했습니다. 이 공개 데이터 세트를 통해 미래의 연구자는 사이버 멀미와 인지 부하 사이의 연관성을 연구하고 예측 모델을 개발할 수 있습니다. 이 데이터 세트는 미래의 VR 개발자가 인지 부하 관리를 개선하고 사이버 멀미를 최소화하여 효율적이고 효과적인 가상 환경을 설계할 수 있도록 지원합니다.",https://doi.org/10.1109/ISMAR62088.2024.00121,Interaction & Input; Perception & Cognition,Eye / Gaze Tracking,Technical Evaluation,Algorithm / Method; Dataset / Benchmark
234,2024,Mind the Visual Discomfort: Assessing Event-Related Potentials as Indicators for Visual Strain in Head-Mounted Displays,시각적 불편함을 염두에 두세요: 헤드 마운트 디스플레이의 시각적 긴장에 대한 지표로 이벤트 관련 가능성 평가,"When using Head-Mounted Displays (HMDs), users may not always notice or report visual discomfort by blurred vision through unadjusted lenses, motion sickness, and increased eye strain. Current measures for visual discomfort rely on users’ self-reports those susceptible to subjective differences and lack of real-time insights. In this work, we investigate if Electroencephalography (EEG) can objectively measure visual discomfort by sensing Event-Related Potentials (ERPs). In a user study ($\mathrm{N}=20$), we compare four different levels of Gaussian blur in a user study while measuring ERPs at occipito-parietal EEG electrodes. The findings reveal that specific ERP components (i.e., P1, N2, and P3) discriminated discomfort-related visual stimuli and indexed increased load on visual processing and fatigue. We conclude that time-locked brain activity can be used to evaluate visual discomfort and propose EEG-based automatic discomfort detection and prevention tools.","헤드 마운트 디스플레이(HMD)를 사용할 때 사용자는 조정되지 않은 렌즈, 멀미 및 눈의 피로 증가로 인한 시야 흐림으로 인한 시각적 불편함을 항상 느끼거나 보고하지 않을 수 있습니다. 시각적 불편함에 대한 현재 조치는 주관적인 차이에 취약하고 실시간 통찰력이 부족한 사용자의 자체 보고에 의존합니다. 이 연구에서는 뇌파검사(EEG)가 ERP(사건 관련 전위)를 감지하여 시각적 불편함을 객관적으로 측정할 수 있는지 조사합니다. 사용자 연구($\mathrm{N}=20$)에서는 후두-두정엽 EEG 전극에서 ERP를 측정하는 동안 사용자 연구에서 4가지 다른 수준의 가우시안 블러를 비교합니다. 연구 결과에 따르면 특정 ERP 구성 요소(예: P1, N2 및 P3)는 불편함과 관련된 시각적 자극을 식별하고 시각적 처리 및 피로에 대한 부하 증가를 색인화했습니다. 우리는 시간 고정 뇌 활동이 시각적 불편함을 평가하는 데 사용될 수 있다고 결론을 내리고 EEG 기반 자동 불편 감지 및 예방 도구를 제안합니다.",https://doi.org/10.1109/ISMAR62088.2024.00029,Display & Optics,Sensor Fusion,Questionnaire / Survey,User Study / Empirical Findings
235,2024,Mitigating Latency Effects on Subjective Experience in Robot Teleoperation Using a VR-Enabled Virtual Spring,VR 지원 가상 스프링을 사용하여 로봇 원격 조작의 주관적 경험에 대한 지연 효과 완화,"Remote robot teleoperation is crucial for tasks that require human oversight, yet latency can significantly impair operator performance and result in discomfort, break inpresence and increased workload. In this paper, we propose a new robot teleoperation technique based on Virtual Reality that is expected to alleviate the negative impactson subjective experience caused by latency. The technique allows users to manipulate a virtual robot synchronized with a real one by ‘grabbing’ a virtual spring attached to it. Controller vibration is also used to simulate spring force feedback, together creating an illusion that the position discrepancy is caused by the dynamics of the spring instead of latency. We hypothesize that this approach can mitigate the negative effects of latency by making the robot’s movement appear less strange and more natural. A user study was conducted to evaluate the effectiveness of virtual spring and controller vibration separately using a $2 \times 2$ factorial design. The results suggest that the virtual spring enhanced user comfort level and the sense of presence, but the controller vibration showed no clear benefits.","원격 로봇 원격 조작은 사람의 감독이 필요한 작업에 매우 중요하지만 대기 시간은 작업자 성능을 크게 저하시키고 불편함, 존재 중단 및 작업량 증가를 초래할 수 있습니다. 본 논문에서는 대기시간으로 인한 주관적 경험에 대한 부정적인 영향을 완화할 수 있을 것으로 기대되는 가상현실 기반의 새로운 로봇 원격조종 기술을 제안한다. 이 기술을 통해 사용자는 가상 로봇에 부착된 가상 스프링을 '잡아' 실제 로봇과 동기화된 가상 로봇을 조작할 수 있습니다. 컨트롤러 진동은 스프링 힘 피드백을 시뮬레이션하는 데에도 사용되며, 함께 위치 불일치가 대기 시간이 아닌 스프링의 역학으로 인해 발생한다는 환상을 만듭니다. 우리는 이 접근 방식이 로봇의 움직임을 덜 이상하고 더 자연스럽게 보이게 만들어 대기 시간의 부정적인 영향을 완화할 수 있다고 가정합니다. $2 \times 2$ 요인 설계를 사용하여 가상 스프링과 컨트롤러 진동의 효과를 개별적으로 평가하기 위한 사용자 연구가 수행되었습니다. 결과는 가상 스프링이 사용자의 편안함과 현장감을 향상시켰으나 컨트롤러 진동은 뚜렷한 이점을 나타내지 않았음을 시사합니다.",https://doi.org/10.1109/ISMAR62088.2024.00144,Interaction & Input,Sensor Fusion,User Study,Algorithm / Method; User Study / Empirical Findings
236,2024,Multimodal 3D Fusion and In-Situ Learning for Spatially Aware AI,공간 인식 AI를 위한 다중 모드 3D 융합 및 현장 학습,"Seamless integration of virtual and physical worlds in augmented reality benefits from the system semantically “understanding” the physical environment. AR research has long focused on the potential of context awareness, demonstrating novel capabilities that leverage the semantics in the 3D environment for various object-level interactions. Meanwhile, the computer vision community has made leaps in neural vision-language understanding to enhance environment perception for autonomous tasks. In this work, we introduce a multimodal 3D object representation that unifies both semantic and linguistic knowledge with the geometric representation, enabling user-guided machine learning involving physical objects. We first present a fast multimodal 3D reconstruction pipeline that brings linguistic understanding to AR by fusing CLIP vision-language features into the environment and object models. We then propose “in-situ” machine learning, which, in conjunction with the multimodal representation, enables new tools and interfaces for users to interact with physical spaces and objects in a spatially and linguistically meaningful manner. We demonstrate the usefulness of the proposed system through two real-world AR applications on Magic Leap 2: a) spatial search in physical environments with natural language and b) an intelligent inventory system that tracks object changes over time. We also make our full implementation and demo data available at (https://github.com/cy-xu/spatially_aware_AI) to encourage further exploration and research in spatially aware AI.","증강 현실에서 가상 세계와 실제 세계의 원활한 통합은 물리적 환경을 의미론적으로 ""이해""하는 시스템의 이점을 활용합니다. AR 연구는 오랫동안 상황 인식의 잠재력에 초점을 맞춰 왔으며 다양한 객체 수준 상호 작용을 위해 3D 환경의 의미를 활용하는 새로운 기능을 보여주었습니다. 한편, 컴퓨터 비전 커뮤니티는 자율 작업에 대한 환경 인식을 향상시키기 위해 신경 비전-언어 이해 분야에서 도약을 이루었습니다. 이 연구에서는 의미론적, 언어적 지식을 기하학적 표현과 통합하여 물리적 객체와 관련된 사용자 안내 기계 학습을 가능하게 하는 다중 모드 3D 객체 표현을 소개합니다. 먼저 CLIP 비전 언어 기능을 환경과 개체 모델에 융합하여 AR에 언어적 이해를 제공하는 빠른 다중 모드 3D 재구성 파이프라인을 제시합니다. 그런 다음 다중 모드 표현과 함께 사용자가 공간적, 언어적으로 의미 있는 방식으로 물리적 공간 및 객체와 상호 작용할 수 있는 새로운 도구 및 인터페이스를 가능하게 하는 ""현장"" 기계 학습을 제안합니다. 우리는 Magic Leap 2의 두 가지 실제 AR 애플리케이션을 통해 제안된 시스템의 유용성을 입증합니다. a) 자연어를 사용하는 물리적 환경에서의 공간 검색 및 b) 시간에 따른 객체 변화를 추적하는 지능형 인벤토리 시스템. 또한 공간 인식 AI에 대한 추가 탐색 및 연구를 장려하기 위해 (https://github.com/cy-xu/spatially_aware_AI)에서 전체 구현 및 데모 데이터를 제공합니다.",https://doi.org/10.1109/ISMAR62088.2024.00063,Perception & Cognition; Interaction & Input,Computer Vision,Other,System / Framework
237,2024,Neural Panoramic Representation for Spatially and Temporally Consistent 360° Video Editing,"공간적, 시간적으로 일관된 360° 비디오 편집을 위한 신경 파노라마 표현","Content-based 360° video editing allows users to manipulate panoramic content for interaction in a dynamic visual world. However, the current related methods (2D neural representation and optical flow) show limitations in producing high-quality panoramic content from 360° videos due to their lack of capacity to model the inherent spatiotemporal relationships among pixels in the true panoramic space. To address this issue, we propose a Neural Panoramic Representation (NPR) method to model the global inter-pixel relationships, facilitating immersive video editing. Specifically, our method utilizes MLP-based networks to learn spherical implicit content layers, by encoding the spherical spatiotemporal positions and appearance details within the panoramic video, and bi-directional mapping between the original video frames and the learned content layers, to capture the interpretable and global omnidirectional visual characteristics of individual dynamic scenes. Additionally, we introduce innovative loss functions (spherical neighborhood consistency and unit spherical regularization) to ensure the creation of appropriate implicit spherical content layers. We further provide an interactive layer neural panoramic editing approach based on the proposed NPR, in the head-mounted display device. We evaluate this framework on diverse real-world 360° videos, showing superior performance on both reconstruction and consistent editing compared to existing state-of-the-art (SOTA) neural representation techniques.","콘텐츠 기반 360° 비디오 편집을 통해 사용자는 역동적인 시각적 세계에서 상호 작용할 수 있도록 파노라마 콘텐츠를 조작할 수 있습니다. 그러나 현재의 관련 방법(2D 신경 표현 및 광학 흐름)은 실제 파노라마 공간에서 픽셀 간의 고유한 시공간 관계를 모델링할 수 있는 용량이 부족하여 360° 비디오에서 고품질 파노라마 콘텐츠를 생성하는 데 한계가 있습니다. 이 문제를 해결하기 위해 우리는 몰입형 비디오 편집을 용이하게 하는 전역 픽셀 간 관계를 모델링하는 NPR(Neural Panoramic Representation) 방법을 제안합니다. 구체적으로, 우리의 방법은 MLP 기반 네트워크를 활용하여 파노라마 비디오 내의 구형 시공간 위치 및 모양 세부 정보를 인코딩하고 원본 비디오 프레임과 학습된 콘텐츠 레이어 간의 양방향 매핑을 통해 구형 암시적 콘텐츠 레이어를 학습하여 개별 동적 장면의 해석 가능한 글로벌 전방향 시각적 특성을 캡처합니다. 또한 적절한 암시적 구형 콘텐츠 레이어 생성을 보장하기 위해 혁신적인 손실 함수(구형 이웃 일관성 및 단위 구형 정규화)를 도입합니다. 우리는 머리 장착형 디스플레이 장치에서 제안된 NPR을 기반으로 하는 대화형 레이어 신경 파노라마 편집 접근 방식을 추가로 제공합니다. 우리는 다양한 실제 360° 비디오에서 이 프레임워크를 평가하여 기존 최첨단(SOTA) 신경 표현 기술에 비해 재구성 및 일관된 편집 모두에서 우수한 성능을 보여줍니다.",https://doi.org/10.1109/ISMAR62088.2024.00034,Display & Optics,Computer Vision,Technical Evaluation,Algorithm / Method; Hardware / Device
238,2024,New Ears: An Exploratory Study of Audio Interaction Techniques for Performing Search in a Virtual Reality Environment,새로운 귀: 가상 현실 환경에서 검색을 수행하기 위한 오디오 상호 작용 기술에 대한 탐색적 연구,"Efficiently searching and navigating virtual scenes is essential for performing various downstream tasks and ensuring a positive user experience in VR. Prior VR interaction techniques for such scenarios predominantly rely on users’ visual perception, which contrasts with physical reality, where people typically rely on multimodal information, especially auditory cues, to guide their spatial awareness. In this work, we explore the potential of leveraging auditory interaction techniques to enhance spatial navigation in virtual environments. We drew inspiration from prior distant interaction techniques and developed four approaches to augmenting how users hear in the virtual environment: Audio Teleportation, Audio Cone, Ninja Ears, and Boom Mic. In a comparative user study (N = 25), we evaluated these approaches against a baseline Teleportation technique in a search task, where participants traversed a virtual environment to locate target items. Our results suggest that several of our audio interaction techniques may enable more efficient search behaviors while enhancing overall user experience. However, not all techniques were appreciated equally, suggesting that careful attention to their design is critical for ensuring their effectiveness. We conclude by discussing the potential implications of our results for future audio interaction technique designs.","다양한 다운스트림 작업을 수행하고 VR에서 긍정적인 사용자 경험을 보장하려면 가상 장면을 효율적으로 검색하고 탐색하는 것이 필수적입니다. 이러한 시나리오에 대한 이전 VR 상호 작용 기술은 주로 사용자의 시각적 인식에 의존합니다. 이는 사람들이 일반적으로 공간 인식을 안내하기 위해 다중 모드 정보, 특히 청각 신호에 의존하는 물리적 현실과 대조됩니다. 이 작업에서 우리는 가상 환경에서 공간 탐색을 향상시키기 위해 청각 상호 작용 기술을 활용하는 잠재력을 탐구합니다. 우리는 이전의 원거리 상호 작용 기술에서 영감을 얻어 가상 환경에서 사용자가 듣는 방식을 강화하는 네 가지 접근 방식인 오디오 순간 이동, 오디오 콘, 닌자 귀 및 붐 마이크를 개발했습니다. 비교 사용자 연구(N = 25)에서 우리는 참가자가 대상 항목을 찾기 위해 가상 환경을 탐색하는 검색 작업의 기본 순간 이동 기술에 대해 이러한 접근 방식을 평가했습니다. 우리의 결과는 우리의 여러 오디오 상호 작용 기술이 전반적인 사용자 경험을 향상시키면서 보다 효율적인 검색 행동을 가능하게 할 수 있음을 시사합니다. 그러나 모든 기술이 동등하게 평가되는 것은 아니므로 효율성을 보장하려면 설계에 세심한 주의를 기울이는 것이 중요합니다. 우리는 미래의 오디오 상호 작용 기술 설계에 대한 결과의 잠재적인 영향을 논의함으로써 결론을 내립니다.",https://doi.org/10.1109/ISMAR62088.2024.00053,Perception & Cognition; Interaction & Input,Redirected Walking / Locomotion,User Study,Algorithm / Method
239,2024,Object Speed Control with a Signed Distance Field for Distant Mid-Air Object Manipulation in Virtual Reality,가상 현실에서 원거리 공중 객체 조작을 위한 서명된 거리장을 사용한 객체 속도 제어,"In Virtual Reality (VR) applications, interacting with distant objects relies heavily on mid-air object manipulation. Yet, the inherent distance between the user and the object often restricts movement precision. This paper introduces the Signed Distance Field (SDF) method for mid-air object manipulation and combines it with the ray casting interaction technique to investigate its effect on user performance and user experience. To increase movement accuracy, we leverage the speed-accuracy trade-off to dynamically adjust object manipulation speed based on the SDF algorithm’s output. Our study with 18 participants examines the effects of SDF across three different tasks with different complexity. Our results showed that ray casting with SDF reduces the number of errors in complex tasks without slowing down the participants and improves the user experience. We hope that our proposed assistive system, designed for tasks and applications, can be used as an interaction technique to enable more accurate manipulation of distant objects in fields like surgical planning, architecture, and games.","가상 현실(VR) 애플리케이션에서 멀리 있는 객체와 상호 작용하는 것은 공중 객체 조작에 크게 의존합니다. 그러나 사용자와 물체 사이의 고유한 거리로 인해 움직임의 정밀도가 제한되는 경우가 많습니다. 본 논문에서는 공중 객체 조작을 위한 SDF(Signed Distance Field) 방법을 소개하고 이를 레이 캐스팅 상호 작용 기술과 결합하여 사용자 성능과 사용자 경험에 미치는 영향을 조사합니다. 이동 정확도를 높이기 위해 속도-정확도 균형을 활용하여 SDF 알고리즘의 출력을 기반으로 객체 조작 속도를 동적으로 조정합니다. 18명의 참가자를 대상으로 한 연구에서는 복잡성이 서로 다른 세 가지 작업에 대한 SDF의 효과를 조사했습니다. 우리의 결과는 SDF를 사용한 레이 캐스팅이 참가자의 속도를 늦추지 않고 복잡한 작업에서 오류 수를 줄이고 사용자 경험을 향상시키는 것으로 나타났습니다. 우리는 작업 및 응용을 위해 설계된 우리가 제안한 보조 시스템이 수술 계획, 건축 및 게임과 같은 분야에서 멀리 있는 물체를 보다 정확하게 조작할 수 있는 상호 작용 기술로 사용될 수 있기를 바랍니다.",https://doi.org/10.1109/ISMAR62088.2024.00061,Interaction & Input,Other,Quantitative Experiment,Algorithm / Method; System / Framework
240,2024,Optimizing In-Contact Force Planning in Robotic Ultrasound with Augmented Reality Visualization Techniques,증강 현실 시각화 기술을 사용하여 로봇 초음파에서 접촉 중 힘 계획 최적화,"The utilization of augmented reality (AR) in medical robotics offers significant advancements in enhancing procedural accuracy and patient safety. This paper investigates novel AR visualization techniques designed to depict in-contact force applied by a robotic ultrasound probe, aiming to optimize the control practitioners have over probe force for ultrasound procedures, thereby enhancing both image quality and patient comfort. We developed and evaluated four distinct AR visualization techniques through a comprehensive user study conducted in a clinical setting. The study assessed the efficiency and user experience associated with each technique. The findings revealed notable differences in user performance and preferences, indicating that specific visualizations significantly improve the precision of force application and could lead to better procedural outcomes. The results underscore the potential of AR visualizations to transform robotic-assisted medical procedures by improving the interface between clinicians and robotic systems. Moreover, these advancements foster a deeper trust and acceptance of robotic technologies among healthcare professionals and patients. This study not only highlights the immediate benefits of AR in enhancing robotic ultrasound but also sets the stage for further research into AR’s expansive role in complex medical robotics scenarios.","의료 로봇 공학에 증강 현실(AR)을 활용하면 절차의 정확성과 환자 안전을 향상시키는 데 상당한 발전이 가능합니다. 이 논문에서는 로봇 초음파 프로브에 의해 적용되는 접촉력을 묘사하도록 설계된 새로운 AR 시각화 기술을 조사하고, 초음파 시술에 대한 제어 실무자가 갖는 프로브 힘을 최적화하여 영상 품질과 환자의 편안함을 모두 향상시키는 것을 목표로 합니다. 우리는 임상 환경에서 수행된 포괄적인 사용자 연구를 통해 네 가지 AR 시각화 기술을 개발하고 평가했습니다. 이 연구에서는 각 기술과 관련된 효율성과 사용자 경험을 평가했습니다. 연구 결과에 따르면 사용자 성능과 선호도에서 눈에 띄는 차이가 나타났으며, 이는 특정 시각화가 힘 적용의 정확성을 크게 향상시키고 더 나은 절차적 결과를 가져올 수 있음을 나타냅니다. 결과는 임상의와 로봇 시스템 간의 인터페이스를 개선하여 로봇 지원 의료 절차를 변화시키는 AR 시각화의 잠재력을 강조합니다. 또한, 이러한 발전은 의료 전문가와 환자 사이에서 로봇 기술에 대한 더 깊은 신뢰와 수용을 촉진합니다. 이 연구는 로봇 초음파를 향상시키는 데 있어 AR의 즉각적인 이점을 강조할 뿐만 아니라 복잡한 의료 로봇 시나리오에서 AR의 광범위한 역할에 대한 추가 연구를 위한 발판을 마련합니다.",https://doi.org/10.1109/ISMAR62088.2024.00070,Medical & Healthcare,Optical / Display Technology,Quantitative Experiment,User Study / Empirical Findings
241,2024,PaletteGaussian: 3D Photorealistic Color Editing with Gaussian Splatting,PaletteGaussian: 가우스 스플래팅을 사용한 3D 사실적 색상 편집,"3D editing, particularly involving realistic color editing, plays a crucial role in various multimedia domains, such as augmented reality and filmmaking. Traditional 3D reconstruction methods encounter challenges in achieving high-fidelity reconstruction for complex scenes. In recent years, methods based on implicit 3D representations, like Neural Radiance Fields (NeRF), have demonstrated effectiveness in rendering complex scenes. However, these methods face difficulties in interactively editing scene colors and often exhibit slow processing speeds. Addressing these challenges, we propose the PaletteGaussian framework for interactive color editing and real-time rendering based on a palette and 3D Gaussian Splatting (3DGS). First, we introduce a two-stage training strategy to ensure rendering quality and enhance the accuracy of object extraction in the scene. Next, we present an image-driven learning-based approach, I-learning, for convenient interactive color editing driven by both images and text. Finally, we perform parameter baking to achieve real-time rendering. In summary, PaletteGaussian supports two editing levels, scene-level and object-level, offering three interaction modes: manual, image-driven, and text-driven editing. It enables high-resolution real-time rendering. Our comprehensive experiments demonstrate that PaletteGaussian exhibits efficient performance, diverse interaction modes, and realistic color editing.","특히 사실적인 색상 편집과 관련된 3D 편집은 증강 현실 및 영화 제작과 같은 다양한 멀티미디어 영역에서 중요한 역할을 합니다. 전통적인 3D 재구성 방법은 복잡한 장면에 대한 충실도 높은 재구성을 달성하는 데 어려움을 겪습니다. 최근에는 NeRF(Neural Radiance Fields)와 같은 암시적 3D 표현을 기반으로 하는 방법이 복잡한 장면을 렌더링하는 데 효율성이 입증되었습니다. 그러나 이러한 방법은 장면 색상을 대화식으로 편집하는 데 어려움이 있으며 처리 속도가 느린 경우가 많습니다. 이러한 과제를 해결하기 위해 우리는 팔레트 및 3DGS(3D Gaussian Splatting)를 기반으로 하는 대화형 색상 편집 및 실시간 렌더링을 위한 PaletteGaussian 프레임워크를 제안합니다. 먼저, 렌더링 품질을 보장하고 장면에서 객체 추출의 정확성을 높이기 위해 2단계 학습 전략을 도입합니다. 다음으로 이미지와 텍스트를 기반으로 한 편리한 대화형 색상 편집을 위한 이미지 기반 학습 기반 접근 방식인 I-learning을 제시합니다. 마지막으로 실시간 렌더링을 달성하기 위해 매개변수 베이킹을 수행합니다. 요약하면 PaletteGaussian은 장면 수준과 객체 수준의 두 가지 편집 수준을 지원하며 수동, 이미지 기반 및 텍스트 기반 편집의 세 가지 상호 작용 모드를 제공합니다. 고해상도 실시간 렌더링이 가능합니다. 우리의 포괄적인 실험은 PaletteGaussian이 효율적인 성능, 다양한 상호 작용 모드 및 사실적인 색상 편집을 나타냄을 보여줍니다.",https://doi.org/10.1109/ISMAR62088.2024.00137,Rendering & Visualization; Education & Training,3D Reconstruction,Quantitative Experiment,Algorithm / Method
242,2024,Perceived Empathy in Mixed Reality: Assessing the Impact of Empathic Agents' Awareness of User Physiological States,혼합 현실에서 인지된 공감: 공감 에이전트의 사용자 생리적 상태 인식의 영향 평가,"In human-agent interaction, establishing trust and a social bond with the agent is crucial to improving communication quality and performance in collaborative tasks. This paper investigates how a Mixed Reality Agent’s (MiRA) ability to acknowledge a user’s physiological state affects perceptions such as empathy, social connectedness, presence, and trust. In a within-subject study with 24 subjects, we varied the companion agent’s awareness during a mixed-reality first-person shooting game. Three agents provided feedback based on the users’ physiological states: (1) No Awareness Agent (NAA), which did not acknowledge the user’s physiological state; (2) Random Awareness Agent (RAA), offering feedback with varying accuracy; and (3) Accurate Awareness Agent (AAA), which provided consistently accurate feedback. Subjects reported higher scores on perceived empathy, social connectedness, presence, and trust with AAA compared to RAA and NAA. Interestingly, despite exceeding NAA in perception scores, RAA was the least favored as a companion. The findings and implications for the design of MiRA interfaces are discussed, along with the limitations of the study and directions for future work.","인간-에이전트 상호 작용에서 에이전트와의 신뢰와 사회적 유대를 구축하는 것은 협업 작업에서 커뮤니케이션 품질과 성과를 향상시키는 데 매우 중요합니다. 본 논문에서는 사용자의 생리적 상태를 인식하는 혼합 현실 에이전트(MiRA)의 능력이 공감, 사회적 연결성, 존재감, 신뢰와 같은 인식에 어떤 영향을 미치는지 조사합니다. 24명의 피험자를 대상으로 한 피험자 내 연구에서 우리는 혼합 현실 1인칭 슈팅 게임 중에 동반 에이전트의 인식을 다양화했습니다. 세 가지 에이전트는 사용자의 생리적 상태에 따라 피드백을 제공했습니다. (1) 사용자의 생리적 상태를 인식하지 않은 NAA(No Awareness Agent); (2) RAA(Random Awareness Agent)는 다양한 정확도로 피드백을 제공합니다. (3) 지속적으로 정확한 피드백을 제공하는 AAA(Accurate Awareness Agent). 피험자들은 RAA 및 NAA에 비해 AAA에 대해 인지된 공감, 사회적 연결성, 존재감 및 신뢰에 대해 더 높은 점수를 보고했습니다. 흥미롭게도 인식 점수에서는 NAA를 초과했음에도 불구하고 RAA는 동반자로서 가장 선호도가 낮았습니다. MiRA 인터페이스 설계에 대한 연구 결과와 시사점, 연구의 한계 및 향후 작업 방향에 대해 논의합니다.",https://doi.org/10.1109/ISMAR62088.2024.00055,Perception & Cognition; Collaboration & Social,Optical / Display Technology,User Study,User Study / Empirical Findings
243,2024,Ping! Your Food is Ready: Comparing Different Notification Techniques in 3D AR Cooking Environment,핑! 음식이 준비되었습니다: 3D AR 요리 환경의 다양한 알림 기술 비교,"Implementing visual and audio notifications on augmented reality devices is a crucial element of intuitive and easy-to-use interfaces. In this paper, we explored creating intuitive interfaces through visual and audio notifications. The study evaluated user performance and preference across three conditions: visual notifications in fixed positions, visual notifications above objects, and no visual notifications with monaural sounds. The users were tasked with cooking and serving customers in an open-source Augmented-Reality sandbox environment called ARtisan Bistro. The results indicated that visual notifications above objects combined with localized audio feedback were the most effective and preferred method by participants. The findings highlight the importance of strategic placement of visual and audio notifications in AR, providing insights for engineers and developers to design intuitive 3D user interfaces.","증강 현실 장치에 시각적 및 오디오 알림을 구현하는 것은 직관적이고 사용하기 쉬운 인터페이스의 중요한 요소입니다. 본 논문에서는 시각적 및 오디오 알림을 통해 직관적인 인터페이스를 만드는 방법을 살펴보았습니다. 이 연구에서는 고정된 위치에서의 시각적 알림, 객체 위의 시각적 알림, 모노럴 사운드가 있는 시각적 알림 없음이라는 세 가지 조건에 대해 사용자 성능과 선호도를 평가했습니다. 사용자는 ARtisan Bistro라는 오픈 소스 증강 현실 샌드박스 환경에서 요리하고 고객에게 서비스를 제공하는 임무를 맡았습니다. 결과는 현지화된 오디오 피드백과 결합된 객체 위의 시각적 알림이 참가자가 가장 효과적이고 선호하는 방법임을 나타냅니다. 이번 조사 결과는 AR에서 시각 및 청각 알림을 전략적으로 배치하는 것이 중요하다는 점을 강조하여 엔지니어와 개발자가 직관적인 3D 사용자 인터페이스를 설계할 수 있는 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR62088.2024.00132,Interaction & Input; Audio & Sound,Optical / Display Technology,User Study,Algorithm / Method
244,2024,Pinging Between Worlds: Training Table Tennis Novice Players in Real Environment for Virtual Reality Competitions,세계 간 핑: 가상 현실 대회를 위해 실제 환경에서 탁구 초심자 훈련,"Modern Virtual Reality (VR) technology has enabled users to experience Real Environment (RE) sports in their homes. For VR table tennis, one of the most popular VR sports, the players have rankings and tournaments and compete for RE awards. Based on this phenomenon, this paper aims to understand the benefits of RE training in improving VR table tennis skills. In a user study with 12 novice table tennis players, we measured their performance in 16 basic skills via a pre- and post-study design using a novel training protocol designed for both RE and VR players. We also asked participants for their insights into the training and to evaluate their experience. Our results show a significant improvement in all measured skills. However, participants identified issues with the technology that caused discomfort. Our findings provide valuable insights for software developers working on VR sports applications, enabling them to create better experiences for VR table tennis players. They can also help developers of VR training applications identify areas for improvement with the current technology.",현대 가상 현실(VR) 기술을 통해 사용자는 집에서 RE(실제 환경) 스포츠를 경험할 수 있습니다. VR 스포츠의 가장 인기 있는 스포츠 중 하나인 VR 탁구에서는 선수들이 랭킹과 토너먼트를 통해 RE 상을 놓고 경쟁합니다. 이러한 현상을 바탕으로 본 논문은 VR 탁구 기술 향상에 있어 RE 훈련의 이점을 이해하는 것을 목표로 합니다. 12명의 초보 탁구 선수를 대상으로 한 사용자 연구에서 우리는 RE 및 VR 선수 모두를 위해 설계된 새로운 훈련 프로토콜을 사용하여 사전 및 사후 연구 설계를 통해 16가지 기본 기술의 성능을 측정했습니다. 우리는 또한 참가자들에게 교육에 대한 통찰력과 경험을 평가하도록 요청했습니다. 우리의 결과는 측정된 모든 기술이 크게 향상되었음을 보여줍니다. 그러나 참가자들은 불편함을 야기하는 기술 문제를 확인했습니다. 우리의 연구 결과는 VR 스포츠 애플리케이션을 작업하는 소프트웨어 개발자에게 귀중한 통찰력을 제공하여 VR 탁구 선수를 위한 더 나은 경험을 만들 수 있도록 해줍니다. 또한 VR 교육 애플리케이션 개발자가 현재 기술로 개선할 영역을 식별하는 데 도움을 줄 수도 있습니다.,https://doi.org/10.1109/ISMAR62088.2024.00080,Education & Training,Optical / Display Technology,User Study,User Study / Empirical Findings
245,2024,Predicting Locomotion Intention using Eye Movements and EEG with LSTM and Transformers,LSTM 및 Transformer를 사용하여 눈 움직임과 EEG를 사용하여 운동 의도 예측,"Predicting future locomotion based on intrinsic data serves many purposes, including optimizing the utilization of physical space in virtual reality environments and enhancing the control of electronic aids for patients with motor impairments. However, predicting human locomotion intentions proves challenging due to the inherent difficulty arising from the highly complex and nonlinear interactions among the relevant parameters. Deep neural networks offer a significant advantage over conventional approaches in addressing this challenge. We treat this task as a time series prediction problem and compare LSTM networks to transformer models. A distinctive aspect of our work is our approach’s emphasis on eye movements as a central feature, contributing to its novel predictive capabilities. Besides gaze data, we evaluate the addition of EEG as a data source for this prediction task to be used in brain-computer interfaces. To achieve this, we conducted two data collection experiments in custom virtual environments that feature different tasks utilizing joystick control. We present these novel datasets in conjunction with this work. The results demonstrate that gaze data proves to be a valuable tool for locomotion prediction in different contexts, even when there is not a strong and direct connection between gaze and future waypoints. Transformer models were able to achieve better performance than LSTM networks, and we conclude that successful prediction across diverse situations requires datasets containing a wide range of movement scenarios.","본질적인 데이터를 기반으로 미래의 운동을 예측하는 것은 가상 현실 환경에서 물리적 공간 활용을 최적화하고 운동 장애가 있는 환자를 위한 전자 보조 장치의 제어를 강화하는 등 다양한 목적에 사용됩니다. 그러나 인간의 운동 의도를 예측하는 것은 관련 매개 변수 간의 매우 복잡하고 비선형적인 상호 작용으로 인해 발생하는 본질적인 어려움으로 인해 어려운 것으로 입증되었습니다. 심층 신경망은 이 문제를 해결하는 데 있어 기존 접근 방식에 비해 상당한 이점을 제공합니다. 우리는 이 작업을 시계열 예측 문제로 처리하고 LSTM 네트워크를 변환기 모델과 비교합니다. 우리 작업의 독특한 측면은 안구 운동을 중심 특징으로 강조하는 접근 방식으로, 새로운 예측 능력에 기여합니다. 시선 데이터 외에도 뇌-컴퓨터 인터페이스에 사용되는 예측 작업의 데이터 소스로 EEG를 추가하는 것을 평가합니다. 이를 달성하기 위해 조이스틱 제어를 활용하여 다양한 작업을 수행하는 맞춤형 가상 환경에서 두 가지 데이터 수집 실험을 수행했습니다. 우리는 이 작업과 함께 이러한 새로운 데이터 세트를 제시합니다. 결과는 시선 데이터가 시선과 미래 웨이포인트 사이에 강력하고 직접적인 연결이 없는 경우에도 다양한 상황에서 이동 예측을 위한 귀중한 도구임이 입증되었음을 보여줍니다. Transformer 모델은 LSTM 네트워크보다 더 나은 성능을 달성할 수 있었으며, 다양한 상황에 대한 성공적인 예측을 위해서는 광범위한 이동 시나리오를 포함하는 데이터 세트가 필요하다는 결론을 내렸습니다.",https://doi.org/10.1109/ISMAR62088.2024.00016,Interaction & Input,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method
246,2024,Preserving Personal Space: Differentially Private Cybersickness Detection in Immersive Virtual Reality Environments,개인 공간 보존: 몰입형 가상 현실 환경에서 차등적으로 개인정보 보호를 받는 사이버 질병 감지,"Cybersickness is a common problem that users often encounter during virtual reality (VR) experiences. Several automated methods exist based on machine learning (ML)/deep learning (DL) to detect cybersickness. However, the sensitive nature of data used by these ML/DL models (e.g., eye-tracking, head-tracking, etc.) introduces significant privacy risks since adversaries could exploit this data to infer and leak sensitive personal information, track individuals, or manipulate user experiences. Our research seeks to address this gap, underscoring the necessity for a private approach to cybersickness detection to protect user privacy and ensure a better VR experience. Thus, this paper proposes a privacy-preserving mechanism for DL-enabled cybersickness detection modeled. Specifically, we employ differential privacy (DP) to develop four private DL cybersickness detection models: long short-term memory (LSTM), grated recurrent unit (GRU), convolutional neural network (CNN), and multilayer perceptron (MLP) using Simulations 2021 and Gameplay, two open-source datasets. Our proposed models show high cybersickness detection accuracy for the proposed private cybersickness models. For instance, the private LSTM model shows the cybersickness detection accuracy of up to 92% and 91% for the Simulations 2021 and Gameplay datasets, respectively. Our experimental results also exhibit the privacy-preserving nature of private cybersickness detection. For instance, the private LSTM model reduces the membership inference attack’s success rate by up to 32% and 45% for the Simulations 2021 and Gameplay datasets compared to the baseline/non-private LSTM model for the same datasets.","사이버 멀미는 가상 현실(VR) 경험 중에 사용자가 자주 직면하는 일반적인 문제입니다. 머신 러닝(ML)/딥 러닝(DL)을 기반으로 사이버 멀미를 감지하는 몇 가지 자동화된 방법이 있습니다. 그러나 이러한 ML/DL 모델(예: 시선 추적, 머리 추적 등)에서 사용되는 데이터의 민감한 특성으로 인해 공격자가 이 데이터를 악용하여 민감한 개인 정보를 추론 및 유출하거나, 개인을 추적하거나, 사용자 경험을 조작할 수 있으므로 심각한 개인 정보 보호 위험이 발생합니다. 우리의 연구에서는 사용자 개인 정보를 보호하고 더 나은 VR 경험을 보장하기 위해 사이버 질병 감지에 대한 비공개 접근 방식의 필요성을 강조하면서 이러한 격차를 해결하려고 합니다. 따라서 본 논문에서는 모델링된 DL 기반 사이버 질병 탐지를 위한 개인 정보 보호 메커니즘을 제안합니다. 구체적으로 우리는 DP(차등 개인 정보 보호)를 사용하여 두 가지 오픈 소스 데이터 세트인 Simulations 2021 및 Gameplay를 사용하여 LSTM(장단기 기억), GRU(Grated Recurrent Unit), CNN(컨볼루션 신경망), MLP(다층 퍼셉트론) 등 4가지 비공개 DL 사이버 멀미 감지 모델을 개발합니다. 우리가 제안한 모델은 제안된 개인 사이버 질병 모델에 대해 높은 사이버 질병 탐지 정확도를 보여줍니다. 예를 들어, 프라이빗 LSTM 모델은 Simulations 2021 및 Gameplay 데이터 세트에 대해 각각 최대 92% 및 91%의 사이버 멀미 감지 정확도를 보여줍니다. 우리의 실험 결과는 또한 개인 사이버 멀미 탐지의 개인 정보 보호 특성을 보여줍니다. 예를 들어, 프라이빗 LSTM 모델은 동일한 데이터세트에 대한 기준/비프라이빗 LSTM 모델과 비교하여 시뮬레이션 2021 및 게임플레이 데이터세트의 멤버십 추론 공격 성공률을 최대 32% 및 45% 줄입니다.",https://doi.org/10.1109/ISMAR62088.2024.00015,Education & Training,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method
247,2024,Push2AR: Enhancing Mobile List Interactions Using Augmented Reality,Push2AR: 증강 현실을 사용하여 모바일 목록 상호 작용 향상,"Smartphones provide convenient access to vast data collections (e.g., online shops, social media) within a compact, portable form factor. While the prevalent infinite scroll lists address the inherently restricted screen space, they also introduce navigation and orientation challenges. Users often lose track of their position within these lists and find it difficult to efficiently access, compare, and filter items of interest. To address this challenge, we introduce Push2AR, a novel interaction concept that extends the phone’s high-resolution display and familiar touch interaction with the virtual display space offered by Augmented Reality (AR) headsets. Push2AR enables users to transfer individual list items from their phone to its surrounding AR space, facilitating bookmarking, filtering, and side-by-side comparisons while maintaining orientation through visual links to the original scroll position. Our evaluation shows that our approach enhances user experience and reduces subjective workload involved in locating and comparing list items in contrast to conventional phone-only lists.","스마트폰은 컴팩트하고 휴대 가능한 폼 팩터 내에서 방대한 데이터 컬렉션(예: 온라인 상점, 소셜 미디어)에 편리하게 액세스할 수 있는 기능을 제공합니다. 널리 사용되는 무한 스크롤 목록은 본질적으로 제한된 화면 공간을 해결하지만 탐색 및 방향 문제도 발생시킵니다. 사용자는 종종 이러한 목록 내에서 자신의 위치를 ​​추적하지 못하고 관심 있는 항목에 효율적으로 액세스, 비교 및 ​​필터링하는 데 어려움을 겪습니다. 이러한 문제를 해결하기 위해 우리는 전화기의 고해상도 디스플레이와 친숙한 터치 상호 작용을 증강 현실(AR) 헤드셋이 제공하는 가상 디스플레이 공간으로 확장하는 새로운 상호 작용 개념인 Push2AR을 소개합니다. Push2AR을 사용하면 사용자는 개별 목록 항목을 휴대폰에서 주변 AR 공간으로 전송할 수 있으며, 북마크 설정, 필터링 및 병렬 비교를 촉진하는 동시에 원래 스크롤 위치에 대한 시각적 링크를 통해 방향을 유지할 수 있습니다. 우리의 평가에 따르면 우리의 접근 방식은 사용자 경험을 향상시키고 기존의 전화 전용 목록과 달리 목록 항목을 찾고 비교하는 데 관련된 주관적인 작업량을 줄이는 것으로 나타났습니다.",https://doi.org/10.1109/ISMAR62088.2024.00082,Interaction & Input,Other,Technical Evaluation,Algorithm / Method
248,2024,R2Human: Real-Time 3D Human Appearance Rendering from a Single Image,R2Human: 단일 이미지에서 실시간 3D 인간 모습 렌더링,"Rendering 3D human appearance from a single image in real-time is crucial for achieving holographic communication and immersive VR/AR. Existing methods either rely on multi-camera setups or are constrained to offline operations. In this paper, we propose R2Human, the first approach for real-time inference and rendering of photorealistic 3D human appearance from a single image. The core of our approach is to combine the strengths of implicit texture fields and explicit neural rendering with our novel representation, namely Z-map. Based on this, we present an end-to-end network that performs high-fidelity color reconstruction of visible areas and provides reliable color inference for occluded regions. To further enhance the 3D perception ability of our network, we leverage the Fourier occupancy field as a prior for generating the texture field and providing a sampling surface in the rendering stage. We also propose a consistency loss and a spatial fusion strategy to ensure the multi-view coherence. Experimental results show that our method outperforms the state-of-the-art methods on both synthetic data and challenging real-world images, in real-time. The project page can be found at http://cic.tju. edu.cn/faculty/likun/projects/R2Human.",단일 이미지에서 실시간으로 3D 인간 모습을 렌더링하는 것은 홀로그램 커뮤니케이션과 몰입형 VR/AR을 달성하는 데 중요합니다. 기존 방법은 다중 카메라 설정에 의존하거나 오프라인 작업으로 제한됩니다. 본 논문에서는 단일 이미지로부터 사실적인 3D 인간 모습을 실시간으로 추론하고 렌더링하는 최초의 접근 방식인 R2Human을 제안합니다. 우리 접근 방식의 핵심은 암시적 텍스처 필드와 명시적 신경 렌더링의 장점을 새로운 표현인 Z-맵과 결합하는 것입니다. 이를 기반으로 가시 영역의 충실도 높은 색상 재구성을 수행하고 가려진 영역에 대해 신뢰할 수 있는 색상 추론을 제공하는 엔드 투 엔드 네트워크를 제시합니다. 네트워크의 3D 인식 능력을 더욱 향상시키기 위해 텍스처 필드를 생성하고 렌더링 단계에서 샘플링 표면을 제공하기 위한 사전 요소로 푸리에 점유 필드를 활용합니다. 또한 다중 시점 일관성을 보장하기 위해 일관성 손실과 공간 융합 전략을 제안합니다. 실험 결과는 우리의 방법이 실시간으로 합성 데이터와 까다로운 실제 이미지 모두에 대해 최첨단 방법보다 성능이 우수하다는 것을 보여줍니다. 프로젝트 페이지는 http://cic.tju에서 확인할 수 있습니다. edu.cn/faculty/likun/projects/R2Human.,https://doi.org/10.1109/ISMAR62088.2024.00135,Rendering & Visualization; Perception & Cognition,3D Reconstruction,Technical Evaluation,Algorithm / Method
249,2024,ReaWristic: Remote Touch Sensation to Fingers from a Wristband via Visually Augmented Electro-Tactile Feedback,ReaWristic: 시각적으로 증강된 전기 촉각 피드백을 통해 손목 밴드의 손가락에 대한 원격 터치 감지,"We present a technique for providing remote tactile feedback to the thumb and index finger via a wristband device. This enables haptics for touch and pinch interactions in mixed reality (MR) while keeping the user’s hand entirely free. We achieve this through a novel cross-modal stimulation, which we term visually augmented electro-tactile feedback. This consists of (1) electrically stimulating the nerves that innervate the targeted fingers using our wristband device; and (2) concurrently, visually augmenting the targeted finger in MR to steer the perceived sensation to the desired location. In our psychophysics study, we found that our approach provides tactile perception akin to tapping and, even from the wrist, it is capable of delivering the sensation to the targeted fingers with $\sim$50% of sensation occurring in the thumb and $\sim$40% of sensation occurring in the index finger. These results on localizability are unprecedented compared to electro-tactile feedback alone or any prior work for creating sensations in the hand with devices worn on the wrist/arm. Moreover, unlike conventional electro-tactile techniques, our wristband dispenses with gel electrodes. Instead, it incorporates custommade elastomer-based dry electrodes and a stimulation waveform designed for the electrodes, ensuring the practicality of the device beyond laboratory settings. Lastly, we evaluated the haptic realism of our approach in mixed reality and elicited qualitative feedback from users. Participants preferred our approach to a baseline vibrotactile wrist-worn device.",손목밴드 장치를 통해 엄지와 검지에 원격 촉각 피드백을 제공하는 기술을 제시합니다. 이를 통해 사용자의 손을 완전히 자유롭게 유지하면서 혼합 현실(MR)에서 터치 및 핀치 상호 작용을 위한 햅틱을 사용할 수 있습니다. 우리는 시각적으로 증강된 전기 촉각 피드백이라는 새로운 교차 모달 자극을 통해 이를 달성합니다. 이는 (1) 손목 밴드 장치를 사용하여 대상 손가락에 신경을 분포시키는 신경을 전기적으로 자극하는 것; (2) 동시에 MR에서 목표 손가락을 시각적으로 확대하여 인지된 감각을 원하는 위치로 조정합니다. 우리의 정신물리학 연구에서 우리는 우리의 접근 방식이 두드리는 것과 유사한 촉각 인식을 제공하고 심지어 손목에서도 엄지손가락에서 $\sim$50%의 감각이 발생하고 검지에서 $\sim$40%의 감각이 발생하여 대상 손가락에 감각을 전달할 수 있음을 발견했습니다. 위치 파악 가능성에 대한 이러한 결과는 전기 촉각 피드백 단독이나 손목/팔에 착용한 장치를 사용하여 손에 감각을 생성하는 이전 작업과 비교할 때 전례가 없습니다. 또한 기존의 전기 촉각 기술과 달리 손목 밴드에는 젤 전극이 필요하지 않습니다. 대신 맞춤형 엘라스토머 기반 건식 전극과 전극용으로 설계된 자극 파형을 통합하여 실험실 설정을 넘어서는 장치의 실용성을 보장합니다. 마지막으로 혼합 현실에서 우리 접근 방식의 햅틱 현실성을 평가하고 사용자로부터 정성적인 피드백을 이끌어냈습니다. 참가자들은 기본 진동촉각 손목 착용 장치에 대한 우리의 접근 방식을 선호했습니다.,https://doi.org/10.1109/ISMAR62088.2024.00111,Interaction & Input; Perception & Cognition,Haptic / Tactile Feedback,User Study,Algorithm / Method
250,2024,Room Size Perception in Virtual Reality by Means of Sound and Vision: The Role of Perception-Action Calibration,소리와 시각을 통한 가상 현실의 방 크기 인식: 인식-동작 보정의 역할,"Spatial perception in virtual reality (VR) has been a hot research topic for years. Most of the studies on this topic have focused on visual perception and distance perception. Fewer have examined auditory perception and room size perception, although these aspects are important for improving VR experiences. Recently, a number of studies have shown that perception can be calibrated to information that is relevant to the successful completion of everyday tasks in VR (such as distance estimation and spatial perception). Also, some recent studies have examined calibration of auditory perception as a way to compensate for the classic distance compression problem in VR. In this paper, we present a calibration method for both visual and auditory room size perception. We conducted experiments to investigate how people perceive the size of a virtual room and how the accuracy of their size perception can be calibrated by manipulating perceptible auditory and visual information in VR. The results show that people were more accurate in perceiving room size by means of vision than in audition, but that they could still use audition to perceive room size. The results also show that during calibration, auditory room size perception exhibits learning effects and its accuracy was greatly improved after calibration.","가상 현실(VR)의 공간 인식은 수년간 뜨거운 연구 주제였습니다. 이 주제에 관한 대부분의 연구는 시각적 인식과 거리 인식에 중점을 두었습니다. 청각적 인식과 공간 크기 인식을 조사한 사람은 거의 없지만 이러한 측면은 VR 경험을 개선하는 데 중요합니다. 최근 많은 연구에 따르면 VR에서 일상적인 작업(예: 거리 추정 및 공간 인식)을 성공적으로 완료하는 데 관련된 정보로 인식을 보정할 수 있는 것으로 나타났습니다. 또한 최근 일부 연구에서는 VR의 고전적인 거리 압축 문제를 보상하는 방법으로 청각 인식 보정을 조사했습니다. 본 논문에서는 시각 및 청각 공간 크기 인식을 위한 교정 방법을 제시합니다. 우리는 사람들이 가상 공간의 크기를 어떻게 인식하는지, VR에서 인지 가능한 청각 및 시각 정보를 조작하여 크기 인식의 정확성을 어떻게 보정할 수 있는지 조사하기 위한 실험을 수행했습니다. 결과는 사람들이 오디션보다 시각을 통해 방 크기를 더 정확하게 인식했지만 여전히 방 크기를 인식하기 위해 오디션을 사용할 수 있음을 보여줍니다. 결과는 또한 교정 중 청각 공간 크기 인식이 학습 효과를 나타내며 교정 후 정확도가 크게 향상되었음을 보여줍니다.",https://doi.org/10.1109/ISMAR62088.2024.00057,Perception & Cognition; Audio & Sound,Other,Quantitative Experiment,Algorithm / Method
251,2024,RoomRecon: High-Quality Textured Room Layout Reconstruction on Mobile Devices,RoomRecon: 모바일 장치에서 고품질 질감의 방 레이아웃 재구성,"Widespread RGB-Depth (RGB-D) sensors and advanced 3D reconstruction technologies facilitate the capture of indoor spaces, improving the fields of augmented reality (AR), virtual reality (VR), and extended reality (XR). Nevertheless, current technologies still face limitations, such as the inability to reflect minor scene changes without a complete recapture, the lack of semantic scene understanding, and various texturing challenges that affect the 3D model’s visual quality. These issues affect the realism required for VR experiences and other applications such as in interior design and real estate. To address these challenges, we introduce RoomRecon, an interactive, real-time scanning and texturing pipeline for 3D room models. We propose a two-phase texturing pipeline that integrates AR-guided image capturing for texturing and generative AI models to improve texturing quality and provide better replicas of indoor spaces. Moreover, we suggest to focus only on permanent room elements such as walls, floors, and ceilings, to allow for easily customizable 3D models. We conduct experiments in a variety of indoor spaces to assess the texturing quality and speed of our method. The quantitative results and user study demonstrate that RoomRecon surpasses state-of-the-art methods in terms of texturing quality and on-device computation time.","광범위한 RGB 깊이(RGB-D) 센서와 고급 3D 재구성 기술은 실내 공간 캡처를 촉진하여 증강 현실(AR), 가상 현실(VR) 및 확장 현실(XR) 분야를 개선합니다. 그럼에도 불구하고, 현재 기술은 완전한 재현 없이는 사소한 장면 변화를 반영할 수 없고, 의미론적 장면 이해가 부족하며, 3D 모델의 시각적 품질에 영향을 미치는 다양한 텍스처링 문제 등 여전히 한계에 직면해 있습니다. 이러한 문제는 VR 경험과 인테리어 디자인, 부동산 등 기타 애플리케이션에 필요한 현실성에 영향을 미칩니다. 이러한 문제를 해결하기 위해 우리는 3D 공간 모델을 위한 대화형 실시간 스캐닝 및 텍스처링 파이프라인인 RoomRecon을 소개합니다. 텍스처링 품질을 향상하고 실내 공간의 더 나은 복제본을 제공하기 위해 텍스처링을 위한 AR 기반 이미지 캡처와 생성 AI 모델을 통합하는 2단계 텍스처링 파이프라인을 제안합니다. 또한 쉽게 사용자 정의할 수 있는 3D 모델을 허용하려면 벽, 바닥, 천장과 같은 영구적인 공간 요소에만 집중하는 것이 좋습니다. 우리는 우리 방법의 텍스처링 품질과 속도를 평가하기 위해 다양한 실내 공간에서 실험을 수행합니다. 정량적 결과와 사용자 연구는 RoomRecon이 텍스처링 품질과 기기 내 계산 시간 측면에서 최첨단 방법을 능가한다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR62088.2024.00069,Content Authoring; Rendering & Visualization,Sensor Fusion,User Study,Algorithm / Method; User Study / Empirical Findings
252,2024,Self-Avatar Recognition in Virtual Reality: the Self Advantage Phenomenon and the Relative Importance of Motion and Visual Congruence,가상 현실에서의 자기 아바타 인식: 자기 이익 현상과 동작 및 시각적 일치의 상대적 중요성,"In virtual reality, avatars represent ourselves and serve as a means to interact with others and the environment. Thus, understanding the self-recognition process in VR and designing self-avatars that have strong connections with the self is critical in enhancing immersion and efficiency of interaction in VR. In this paper, we investigated the characteristics of the self-recognition process in VR and the crucial factors that enhance the bond between the self and the avatar. In Study 1, we tested whether the advantage in cognitive processing of self-related information, often observed in reality, is also replicated for briefly embodied self-avatars in VR and how it is modulated by way of avatar selection and the degree of embodiment. The results showed that the self-avatar was processed more efficiently than other avatars, despite the constant changes in the appearance and a brief embodiment period. Moreover, the self-advantage effect was more pronounced for personally selected avatars, rather than those assigned by the experimenter. In Study 2, we compared the relative importance of motion and visual congruence in the process of identifying an entity as the self-avatar. The results indicated that motion synchrony between the user and the avatar is relatively more emphasized than the match in visual appearance when identifying oneself in VR. These findings highlight the underlying mechanisms and crucial factors for self-recognition in VR, and provide valuable insights for designing more immersive virtual experiences in various social VR applications.","가상 현실에서 아바타는 자신을 표현하고 다른 사람 및 환경과 상호 작용하는 수단 역할을 합니다. 따라서 VR에서의 자기인식과정을 이해하고 자신과 강한 연결성을 갖는 자기아바타를 디자인하는 것은 VR에서의 몰입도와 상호작용의 효율성을 높이는 데 매우 중요하다. 본 논문에서는 VR의 자기인식 과정의 특징과 자아와 아바타의 유대감을 강화하는 중요한 요소에 대해 알아보았다. 연구 1에서는 현실에서 자주 관찰되는 자기 관련 정보의 인지 처리 이점이 VR에서 간략하게 구현된 자기 아바타에도 재현되는지 여부와 그것이 아바타 선택 및 구현 정도를 통해 어떻게 변조되는지 테스트했습니다. 그 결과, 자기 아바타는 지속적인 외형 변화와 짧은 구현 기간에도 불구하고 다른 아바타에 비해 효율적으로 처리되는 것으로 나타났다. 또한, 실험자가 할당한 아바타보다 개인적으로 선택한 아바타의 경우 자기이득 효과가 더 두드러졌습니다. 연구 2에서는 개체를 자기 아바타로 식별하는 과정에서 동작과 시각적 일치의 상대적 중요성을 비교했습니다. 그 결과, VR에서 자신을 식별할 때 시각적 외관의 일치보다 사용자와 아바타 간의 모션 동기화가 상대적으로 더 강조되는 것으로 나타났습니다. 이러한 연구 결과는 VR의 자기 인식에 대한 기본 메커니즘과 중요한 요소를 강조하고 다양한 소셜 VR 애플리케이션에서 보다 몰입감 있는 가상 경험을 디자인하기 위한 귀중한 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR62088.2024.00079,Perception & Cognition; Interaction & Input,Other,Other,User Study / Empirical Findings
253,2024,Spatial Affordance-aware Interactable Subspace Allocation for Mixed Reality Telepresence,혼합 현실 텔레프레즌스를 위한 공간적 여유 인식 상호 작용 가능한 하위 공간 할당,"To enable remote Virtual Reality (VR) and Augmented Reality (AR) clients to collaborate as if they were in the same space during Mixed Reality (MR) telepresence, it is essential to overcome spatial heterogeneity and generate a unified shared collaborative environment by integrating remote spaces into a target host space. Especially when multiple remote users connect, a large shared space is necessary for people to maintain their personal space while collaborating, but the existing simple intersection method leads to the creation of narrow shared spaces as the number of remote spaces increases. To robustly align to the host space even as the number of remote spaces increases, we propose a spatial affordance-aware interactable subspace allocation algorithm. The key concept of our approach is to consider the perceivable and interactable areas separately, where every user views the same mutual space, but each remote user has a different interactable subspace, considering their location and spatial affordance. We conducted an evaluation with 900 space combinations, varying the number of remote spaces as two, four, and six, and results show our method outperformed in securing wide interactable mutual space and instantiating users compared to the other spatial matching methods. Our work enables multiple clients from diverse remote locations to access the AR host’s space, allowing them to interact directly with the table, wall, or floor by aligning their physical subspaces within a connected mutual space.","혼합현실(MR) 텔레프레즌스 동안 원격 가상현실(VR)과 증강현실(AR) 클라이언트가 같은 공간에 있는 것처럼 협업할 수 있도록 하려면 원격 공간을 대상 호스트 공간에 통합해 공간적 이질성을 극복하고 통일된 공유 협업 환경을 생성하는 것이 필수적이다. 특히 다수의 원격 사용자가 접속하는 경우 협업을 하면서 개인 공간을 유지하기 위해서는 넓은 공유 공간이 필요하지만, 기존의 단순 교차 방식은 원격 공간이 늘어날수록 좁은 공유 공간이 생성되는 결과를 낳는다. 원격 공간의 수가 증가하더라도 호스트 공간에 견고하게 정렬하기 위해 공간적 여유를 인식하는 상호 작용 가능한 하위 공간 할당 알고리즘을 제안합니다. 우리 접근 방식의 핵심 개념은 인식 가능한 영역과 상호 작용 가능한 영역을 개별적으로 고려하는 것입니다. 여기서 모든 사용자는 동일한 상호 공간을 보지만 각 원격 사용자는 위치와 공간적 여유를 고려하여 서로 다른 상호 작용 가능한 하위 공간을 가집니다. 원격 공간의 수를 2, 4, 6으로 다양하게 변경하여 900개의 공간 조합으로 평가를 수행한 결과, 우리의 방법이 다른 공간 매칭 방법에 비해 상호 작용할 수 있는 넓은 상호 공간 확보 및 사용자 인스턴스화 측면에서 우수한 성능을 보였습니다. 우리의 작업을 통해 다양한 원격 위치의 여러 클라이언트가 AR 호스트의 공간에 액세스할 수 있으며 연결된 상호 공간 내에서 물리적 하위 공간을 정렬하여 테이블, 벽 또는 바닥과 직접 상호 작용할 수 있습니다.",https://doi.org/10.1109/ISMAR62088.2024.00142,Collaboration & Social; Perception & Cognition,Other,Other,Algorithm / Method
254,2024,Speaking with Objects: Conversational Agents' Embodiment in Virtual Museums,사물과의 대화: 가상 박물관에서의 대화 주체의 구현,"Conversational agents in virtual environments are an established approach for immersively conveying the information and narratives of museums and cultural heritage while expanding their accessibility to a wider and remote audience. The rapid development of large language models and text-to-speech technologies has raised the agents’ conversational level significantly, which allows their use for proactive guidance of visitors. This raises the vital question of how such agents should be visually represented to promote Knowledge transfer in immersive virtual environments. In this paper, we compared two representation concepts for agent embodiments in the context of a virtual museum by examining a stylized humanoid guide and a novel animism-based approach that enables users to talk to exhibited objects. Our work addresses the challenge of naturally introducing a virtual educational environment to users and encouraging their interest and engagement with the content. A user study $(N=29)$ revealed high usability and similar presence scores for the experience with each of the embodiments. A majority of participants showed a preference for the animated objects. In terms of user experience, they evoked significant stimulation and high levels of engagement. Our results suggest that agents that show emotions through appropriate word choice influence engagement levels. Based on our findings, we recommend humanoid guides for delivering general background information, while animated objects promote detailed questions about their own stories and a more stimulating exchange.","가상 환경의 대화형 에이전트는 박물관과 문화유산의 정보와 내러티브를 몰입적으로 전달하는 동시에 더 넓고 멀리 있는 청중에게 접근성을 확장하기 위한 확립된 접근 방식입니다. 대규모 언어 모델과 텍스트 음성 변환 기술의 급속한 발전으로 상담원의 대화 수준이 크게 향상되어 방문자를 적극적으로 안내하는 데 사용할 수 있게 되었습니다. 이는 몰입형 가상 환경에서 지식 전달을 촉진하기 위해 그러한 에이전트를 시각적으로 어떻게 표현해야 하는지에 대한 중요한 질문을 제기합니다. 본 논문에서는 양식화된 휴머노이드 가이드와 사용자가 전시된 개체와 대화할 수 있는 새로운 애니미즘 기반 접근 방식을 검토하여 가상 박물관의 맥락에서 에이전트 구현에 대한 두 가지 표현 개념을 비교했습니다. 우리의 작업은 사용자에게 가상 교육 환경을 자연스럽게 소개하고 콘텐츠에 대한 관심과 참여를 장려하는 과제를 해결합니다. 사용자 연구 $(N=29)$는 각 실시예에 대한 경험에 대해 높은 유용성과 유사한 존재 점수를 보여주었습니다. 대다수의 참가자는 애니메이션 개체를 선호하는 것으로 나타났습니다. 사용자 경험 측면에서 이는 상당한 자극과 높은 수준의 참여를 불러일으켰습니다. 우리의 결과는 적절한 단어 선택을 통해 감정을 나타내는 에이전트가 참여 수준에 영향을 미친다는 것을 시사합니다. 우리의 연구 결과를 바탕으로 일반적인 배경 정보를 전달하기 위해서는 휴머노이드 가이드를 추천하고, 애니메이션 개체는 자신의 이야기에 대한 자세한 질문을 유도하고 더욱 자극적인 교류를 촉진합니다.",https://doi.org/10.1109/ISMAR62088.2024.00042,Perception & Cognition,Deep Learning / Neural Networks,User Study,Algorithm / Method; User Study / Empirical Findings
255,2024,Study of Interfaces for Time-Continuous Emotion Reporting and the Relationship Between Interface and Reported Emotion,시간 연속적 감정 보고를 위한 인터페이스 연구 및 인터페이스와 보고된 감정의 관계,"This paper presents interfaces for reporting emotion in real-time during VR stimuli. Self-reported emotional responses are critical for developing emotion recognition systems. Such responses can vary throughout a stimulus such as 360° video, but most interfaces for reporting emotion are designed to be used after the experience. This reduces the entire experience to a single data point and raises concerns about validity when multiple emotions can be elicited across the stimulus. We introduce and compare user interfaces that allow for real-time emotion reporting throughout the length of the stimulus. Each interface varies on how emotion is physically input by the user and displayed back to them for confirmation. A preliminary study compared five such interfaces, gathering initial impressions, comparing control schemes, and rating intuitiveness. A primary study considered four refined interface designs and compared reporting precision and subjective opinions. Results suggest that a single interface face icon responding to arousal and valence reports and a gradiating color wheel are intuitive, precise, and unobtrusive. More broadly, results indicate the type of rating interface has a significant effect on the given ratings.","본 논문은 VR 자극 중에 실시간으로 감정을 보고하기 위한 인터페이스를 제시합니다. 자기 보고된 감정적 반응은 감정 인식 시스템을 개발하는 데 중요합니다. 이러한 반응은 360° 비디오와 같은 자극에 따라 다양할 수 있지만 감정 보고를 위한 대부분의 인터페이스는 경험 후에 사용하도록 설계되었습니다. 이는 전체 경험을 단일 데이터 포인트로 줄이고 자극 전반에 걸쳐 여러 감정이 도출될 수 있을 때 타당성에 대한 우려를 불러일으킵니다. 자극이 지속되는 동안 실시간 감정 보고가 가능한 사용자 인터페이스를 소개하고 비교합니다. 각 인터페이스는 사용자가 감정을 물리적으로 입력하고 확인을 위해 다시 표시하는 방식에 따라 다릅니다. 예비 연구에서는 이러한 5가지 인터페이스를 비교하고 초기 인상을 수집하고 제어 방식을 비교하고 직관성을 평가했습니다. 1차 연구에서는 네 가지 세련된 인터페이스 디자인을 고려하고 보고의 정확성과 주관적인 의견을 비교했습니다. 결과는 각성 및 원자가 보고에 반응하는 단일 인터페이스 얼굴 아이콘과 그라데이션 색상환이 직관적이고 정확하며 눈에 띄지 않는다는 것을 시사합니다. More broadly, results indicate the type of rating interface has a significant effect on the given ratings.",https://doi.org/10.1109/ISMAR62088.2024.00104,Interaction & Input,Sensor Fusion,Quantitative Experiment,System / Framework
256,2024,"Studying the Perception of Vibrotactile Haptic Cues on the Finger, Hand and Forearm for Representing Microgestures","마이크로 제스처를 표현하기 위한 손가락, 손, 팔뚝의 진동촉각 촉각 신호에 대한 인식 연구","We explore the use of vibrotactile haptic cues for representing microgestures. We built a four-axes haptic device for providing vibrotactile cues mapped to all four fingers. We also designed six patterns, inspired by six most commonly studied microgestures. The patterns can be played independently on each axis of the device. We ran an experiment with 36 participants testing three different device locations (fingers, back of the hand, and forearm) for pattern and axis recognition. For all three device locations, participants interpreted the patterns with similar accuracy. We also found that they were better at distinguishing the axes when the device is placed on the fingers. Hand and Forearm device locations remain suitable alternatives but involve a greater trade-off between recognition rate and expressiveness. We report the recognition rates obtained for the different patterns, axes and their combinations per device location. These results per device location are important, as constraints of various kinds, such as hardware, context of use and user activities, influence device location. We discuss this choice of device location by improving literature microgesture-based scenarios with haptic feedback or feedforward.","우리는 마이크로 제스처를 표현하기 위해 진동촉각 촉각 신호의 사용을 탐구합니다. 우리는 네 손가락 모두에 매핑된 진동촉각 신호를 제공하기 위해 4축 촉각 장치를 만들었습니다. 또한 우리는 가장 일반적으로 연구되는 6가지 마이크로 제스처에서 영감을 받아 6가지 패턴을 디자인했습니다. 패턴은 장치의 각 축에서 독립적으로 재생할 수 있습니다. 패턴 및 축 인식을 위해 36명의 참가자를 대상으로 세 가지 장치 위치(손가락, 손등, 팔뚝)를 테스트하는 실험을 진행했습니다. 세 가지 장치 위치 모두에서 참가자들은 비슷한 정확도로 패턴을 해석했습니다. 또한 장치를 손가락에 착용했을 때 축을 더 잘 구별한다는 사실도 발견했습니다. 손과 팔뚝 장치 위치는 여전히 적합한 대안이지만 인식률과 표현력 사이에 더 큰 균형이 필요합니다. 우리는 장치 위치별로 다양한 패턴, 축 및 그 조합에 대해 얻은 인식률을 보고합니다. 하드웨어, 사용 상황, 사용자 활동 등 다양한 종류의 제약 조건이 장치 위치에 영향을 미치기 때문에 장치 위치별 결과는 중요합니다. 우리는 햅틱 피드백이나 피드포워드를 통해 문헌 마이크로 제스처 기반 시나리오를 개선함으로써 이러한 장치 위치 선택에 대해 논의합니다.",https://doi.org/10.1109/ISMAR62088.2024.00092,Interaction & Input,Haptic / Tactile Feedback,User Study,Hardware / Device
257,2024,Stuet: Dual Stewart Platforms for Pinch Grasping Objects in VR,Stuet: VR에서 물체를 핀치로 잡는 듀얼 Stewart 플랫폼,"Complex 3D shapes’ surfaces can be characterised using three shape descriptors: zeroth-order for rendering width; first-order to convey slope; and second-order for curvature. These shapes can be symmetric or asymmetric. To date, controllers in VR have been unable to render these properties in 3D. We present Stuet - a handheld VR controller that can render complex asymmetrical 3D objects for two-finger grasping and shape exploration. Stuet leverages dual 3 degrees of freedom (3-DOF) Stewart Platforms. This enables the contact plates for the fingers to be controlled individually, rendering object widths up to 75 mm and individual plate angles up to 30° in any tilt direction with respect to the vertical plane. We present the design and implementation of Stuet. We explain and benchmark its mechanical capabilities, present the inverse kinematics model required for its use, and report on a feasibility demonstration. Our results reveal that dual Stewart platforms offer new capabilities for asymmetric, advanced haptic interactions in VR.","복잡한 3D 모양의 표면은 세 가지 모양 설명자를 사용하여 특성화할 수 있습니다. 경사를 전달하기 위한 1차; 곡률에 대한 2차. 이러한 모양은 대칭이거나 비대칭일 수 있습니다. 현재까지 VR의 컨트롤러는 이러한 속성을 3D로 렌더링할 수 없었습니다. 두 손가락으로 잡고 모양을 탐색할 수 있도록 복잡한 비대칭 3D 개체를 렌더링할 수 있는 휴대용 VR 컨트롤러인 Stuet을 소개합니다. Stuet은 이중 3자유도(3-DOF) Stewart 플랫폼을 활용합니다. 이를 통해 손가락의 접촉 플레이트를 개별적으로 제어할 수 있으며, 수직면에 대해 모든 기울기 방향에서 최대 75mm의 물체 너비와 최대 30°의 개별 플레이트 각도를 렌더링할 수 있습니다. Stuet의 디자인과 구현을 소개합니다. 우리는 기계적 성능을 설명 및 벤치마킹하고, 사용에 필요한 역운동학 모델을 제시하고, 타당성 입증에 대해 보고합니다. 우리의 결과는 듀얼 Stewart 플랫폼이 VR에서 비대칭 고급 햅틱 상호 작용을 위한 새로운 기능을 제공한다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR62088.2024.00045,Interaction & Input,Computer Vision,Technical Evaluation,System / Framework
258,2024,Subtle Cueing For Improving Depth Perception in Virtual Reality,가상 현실에서 깊이 인식을 개선하기 위한 미묘한 신호,"Understanding an environment relies on human sensory systems, with visual perception as the primary source for defining spatial relationships and estimating distances. The visual system uses natural cues, each offering partial information that can lead to bias and conflicts, especially when ambiguous. Virtual Reality (VR) environments challenge these natural depth cues with discrepancies in perspective and variations in light and shadow depiction, leading to potential confusion in depth perception. However, VR also allows for the isolation and study of these cues. This paper introduces artificial subtle cues (texture blur) to enhance natural depth information in VR. Our results show that augmenting natural depth cues with artificial ones improves depth prediction accuracy and spatial relationship awareness. Subtle blur cues enhance depth estimation without participants’ subjective awareness of the augmentation, suggesting that such subtle cueing can effectively enhance depth perception.","환경을 이해하는 것은 인간의 감각 시스템에 의존하며, 시각적 인식은 공간 관계를 정의하고 거리를 추정하기 위한 주요 소스입니다. 시각 시스템은 자연스러운 신호를 사용하며 각 신호는 특히 모호한 경우 편견과 갈등을 일으킬 수 있는 부분적인 정보를 제공합니다. 가상 현실(VR) 환경은 관점의 불일치, 빛과 그림자 묘사의 변화로 인해 이러한 자연스러운 깊이 단서에 도전하여 깊이 인식에 혼란을 초래할 수 있습니다. 그러나 VR을 사용하면 이러한 단서를 분리하고 연구할 수도 있습니다. 본 논문에서는 VR에서 자연스러운 깊이 정보를 향상시키기 위해 인공적인 미묘한 단서(텍스처 블러)를 소개합니다. 우리의 결과는 자연적인 깊이 단서를 인공적인 단서로 강화하면 깊이 예측 정확도와 공간 관계 인식이 향상된다는 것을 보여줍니다. 미묘한 흐림 신호는 참가자의 증강에 대한 주관적인 인식 없이 깊이 추정을 향상시키며, 이러한 미묘한 신호가 깊이 인식을 효과적으로 향상시킬 수 있음을 시사합니다.",https://doi.org/10.1109/ISMAR62088.2024.00088,Perception & Cognition,Computer Vision,User Study,Algorithm / Method
259,2024,Superpowering Emotion Through Multimodal Cues in Collaborative VR,협업 VR의 다양한 신호를 통해 감정을 강화하세요,"Representing emotion in collaborative Virtual Reality (VR) environments is an emerging topic, as VR can enable humans to express augmented emotions beyond their normal abilities. This research explores how emotion can be represented in collaborative VR beyond facial expressions. We developed a virtual system that communicates emotion using three sensory modalities (textual, auditory and visual) through two spatiotemporal representations (human-form avatar and superpower). We show real-time emotion through a natural avatar and objectify emotion states into superpower phenomena in in-situ environments for time periods. We incorporated subjective, physiological and behavioural measures to evaluate emotion in an asynchronous VR collaboration scenario. The results suggested that showing emotions through the avatar and superpower augmentations (audio-visual) provided the best immersive VR experience, where users were more aroused, and the positive emotion felt more dominating. We also found that understanding emotion requires easy and relatable visuals that people commonly acknowledge, whereas arousing emotion requires a change of environmental contexts to indicate different states. We provide design insights for using multisensory modalities in empathic VR systems to address the lack of a standardised representation of emotion in collaborative VR.","협업적인 가상 현실(VR) 환경에서 감정을 표현하는 것은 최근 떠오르는 주제입니다. VR을 통해 인간은 정상적인 능력 이상으로 증강된 감정을 표현할 수 있기 때문입니다. 본 연구에서는 협업 VR에서 얼굴 표정을 넘어 감정이 어떻게 표현될 수 있는지 탐구합니다. 우리는 두 가지 시공간 표현(인간 형태의 아바타 및 초능력)을 통해 세 가지 감각 양식(텍스트, 청각 및 시각)을 사용하여 감정을 전달하는 가상 시스템을 개발했습니다. 자연스러운 아바타를 통해 실시간 감정을 보여주고 감정 상태를 일정 기간 동안 현장 환경에서 초능력 현상으로 객관화합니다. 우리는 비동기식 VR 협업 시나리오에서 감정을 평가하기 위해 주관적, 생리적, 행동적 측정을 통합했습니다. 결과는 아바타와 초능력 증강(시청각)을 통해 감정을 보여주는 것이 최고의 몰입형 VR 경험을 제공하여 사용자가 더 자극을 받고 긍정적인 감정이 더 지배적으로 느껴지는 것으로 나타났습니다. 우리는 또한 감정을 이해하려면 사람들이 일반적으로 인정하는 쉽고 공감할 수 있는 시각적 요소가 필요한 반면, 감정을 불러일으키려면 다양한 상태를 나타내는 환경적 맥락의 변화가 필요하다는 것을 발견했습니다. 우리는 협업 VR에서 표준화된 감정 표현의 부족을 해결하기 위해 공감 VR 시스템에서 다감각 양식을 사용하기 위한 디자인 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR62088.2024.00030,Collaboration & Social; Audio & Sound,Optical / Display Technology,Other,System / Framework
260,2024,System Usability and Technology Acceptance of a Geriatric Embodied Virtual Human Simulation in Augmented Reality,증강 현실에서 노인 체현 가상 인간 시뮬레이션의 시스템 유용성 및 기술 수용,"With the rise of the aging population, the demand of care increases yet the turnover rate of caregivers for geriatric patients has increased over the past few decades. While many factors contribute to the caregivers’ turnover rate, inadequate training and lack of communication skills and relationship-building skills are vital issues. Technology can help in providing training for soft skills. Embodied Virtual Human (VH) assistants and Augmented Reality (AR) can be a key method to train caregivers and to improve their experience in interacting with older adults, and their perception of the interaction. We designed and developed an immersive simulation in AR where caregivers interact with an embodied VH in two different conditions, i.e., first with an unaware VH and then with a VH having simulated awareness achieved through human-in-the-loop. We conducted a study with caregivers of older adults to evaluate the usability and technology acceptance of our system. The findings suggest that the majority of the participants found the system acceptable, and rated the system as user-friendly and easy to use.","노인인구의 증가로 인해 간병에 대한 수요가 증가하고 있지만 지난 수십 년 동안 노인 환자 간병인의 이직률은 증가했습니다. 간병인의 이직률에 영향을 미치는 요인은 많지만, 부적절한 교육, 의사소통 기술 및 관계 구축 기술의 부족도 중요한 문제입니다. 기술은 소프트 스킬 훈련을 제공하는 데 도움이 될 수 있습니다. 구현된 가상 인간(VH) 보조자와 증강 현실(AR)은 간병인을 교육하고 노인과의 상호 작용 경험과 상호 작용에 대한 인식을 향상시키는 핵심 방법이 될 수 있습니다. 우리는 간병인이 두 가지 다른 조건에서 구현된 VH와 상호 작용하는 AR의 몰입형 시뮬레이션을 설계하고 개발했습니다. 즉, 먼저 인식하지 못하는 VH와 인간 참여를 통해 달성된 시뮬레이션된 인식을 갖는 VH를 사용합니다. 우리는 시스템의 유용성과 기술 수용성을 평가하기 위해 노인 간병인을 대상으로 연구를 수행했습니다. 연구 결과에 따르면 대다수의 참가자는 시스템이 수용 가능하다고 생각하고 시스템이 사용자 친화적이고 사용하기 쉽다고 평가했습니다.",https://doi.org/10.1109/ISMAR62088.2024.00074,Education & Training; Interaction & Input,Sensor Fusion,Simulation,System / Framework
261,2024,Teaching Dance with Mixed Reality Mirrors,혼합 현실 거울을 이용한 댄스 교육,"Regardless of the style, most western dance is taught in a similar manner. A student comes to a studio, the instructor demonstrates a series of movements that the student attempts to replicate - often in front of a mirror - and the instructor provides corrective feedback. Unfortunately, this approach means that beginners are only able to practice and receive corrective feedback when the instructor is available. This paper reports on a study that assessed whether a Mixed Reality (MR) mirror displaying a virtual instructor overlaid with visual feedback can be used to teach a beginner a simple dance routine, replacing the traditional instructor and mirror method, and making beginners dance training more accessible. Three visual feedback modes to indicate how to achieve a correct pose in a dance sequence were designed, based on findings from a literature review, input from expert interviews and an online survey. These feedback modes, titled Spheres, Rubber Bands, and Arrows, were implemented and used as randomized conditions in a user study where participants learned three simple dance sequences. The user study showed that participants performed best with the Arrows feedback mode, though the preference rank for this feedback mode was the lowest. In contrast, the participants’ most preferred feedback mode was Spheres, though participants performed poorest with this mode. These findings suggest that user preference and performance in MR mirror dance training systems need to be balanced to create a system that is effective and enjoyable.","스타일에 관계없이 대부분의 서양 무용은 비슷한 방식으로 진행됩니다. 학생이 스튜디오에 오면 강사는 학생이 복제하려고 시도하는 일련의 동작(종종 거울 앞에서)을 보여주고 강사는 교정 피드백을 제공합니다. 불행하게도 이 접근 방식은 초보자가 강사가 있을 때만 연습하고 교정 피드백을 받을 수 있음을 의미합니다. 이 논문은 시각적 피드백과 중첩된 가상 강사를 표시하는 혼합 현실(MR) 거울을 사용하여 초보자에게 간단한 댄스 루틴을 가르치고, 전통적인 강사 및 거울 방법을 대체하고, 초보자 댄스 훈련을 더 쉽게 만들 수 있는지 여부를 평가한 연구에 대해 보고합니다. 댄스 시퀀스에서 올바른 포즈를 취하는 방법을 나타내는 세 가지 시각적 피드백 모드는 문헌 검토 결과, 전문가 인터뷰 및 온라인 설문조사를 바탕으로 설계되었습니다. 구체, 고무줄 및 화살표라는 제목의 이러한 피드백 모드는 참가자가 세 가지 간단한 댄스 시퀀스를 학습한 사용자 연구에서 무작위 조건으로 구현 및 사용되었습니다. 사용자 연구에 따르면 참가자는 Arrows 피드백 모드에서 가장 좋은 성과를 거두었지만 이 피드백 모드에 대한 선호 순위는 가장 낮았습니다. 이와 대조적으로, 참가자들이 가장 선호하는 피드백 모드는 Spheres였지만 참가자들은 이 모드에서 가장 낮은 성과를 거두었습니다. 이러한 결과는 효과적이고 즐거운 시스템을 만들기 위해서는 MR 미러 댄스 훈련 시스템의 사용자 선호도와 성능의 균형이 필요함을 시사합니다.",https://doi.org/10.1109/ISMAR62088.2024.00113,Education & Training,Optical / Display Technology,User Study,Algorithm / Method; User Study / Empirical Findings
262,2024,"Textual Information Presentation in Virtual Museums: Exploring Environment-, Object-, and User-based Approaches","가상 박물관의 텍스트 정보 표현: 환경, 객체 및 사용자 기반 접근 방식 탐색","In a physical museum, text descriptions are typically displayed on placards or signage next to exhibits. Within a virtual museum environment, these text descriptions can be presented in various ways, such as fixed in the virtual environment, attached to the exhibits, or held in users’ hands. By seamlessly integrating text descriptions into the virtual environment and allowing users to engage with the content, the information presentation can be highly interactive. However, the design space of artifact information presentation in virtual museums was under-explored. In this paper, we investigated appropriate ways to present text descriptions of artifacts in immersive virtual museums. Specifically, we studied (1) users’ perceived importance of various information dimensions (observable, non-observable, and interpretation), (2) users’ expected display of text panels (shown or hidden), and (3) the relationship between the artifact information dimensions and layout types (environment-, object-, and user-based). Our results showed that participants rated significantly higher importance for non-observable information than observable and interpretation information. In addition, we summarize a design space for artifact information presentation using different layout types with prioritized options. Our work provides insights for the interaction design of artifact information in virtual museums and the presentation of text in virtual reality.","실제 박물관에서는 일반적으로 텍스트 설명이 전시물 옆의 플래카드나 간판에 표시됩니다. 가상 박물관 환경 내에서 이러한 텍스트 설명은 가상 환경에 고정되거나, 전시물에 부착되거나, 사용자의 손에 쥐어지는 등 다양한 방식으로 제시될 수 있습니다. 텍스트 설명을 가상 환경에 완벽하게 통합하고 사용자가 콘텐츠에 참여할 수 있도록 함으로써 정보 표시가 고도로 대화형으로 이루어질 수 있습니다. 그러나 가상 박물관에서 유물 정보를 제시하는 디자인 공간은 충분히 탐구되지 않았습니다. 본 논문에서는 몰입형 가상 박물관에서 유물에 대한 텍스트 설명을 제시하는 적절한 방법을 조사했습니다. 구체적으로 (1) 사용자가 인식한 다양한 정보 차원(관찰 가능, 비관찰 가능 및 해석)의 중요성, (2) 사용자가 기대하는 텍스트 패널 표시(표시 또는 숨김), (3) 아티팩트 정보 차원과 레이아웃 유형(환경, 객체, 사용자 기반) 간의 관계를 연구했습니다. 우리의 결과는 참가자들이 관찰 가능한 정보와 해석 가능한 정보보다 관찰 불가능한 정보에 대해 훨씬 더 높은 중요성을 평가했음을 보여주었습니다. 또한 우선순위가 지정된 옵션과 함께 다양한 레이아웃 유형을 사용하여 유물 정보 표시를 위한 디자인 공간을 요약합니다. 우리의 작업은 가상 박물관에서 유물 정보의 상호 작용 디자인과 가상 현실에서 텍스트 표시에 대한 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR62088.2024.00067,Interaction & Input,Other,User Study,Design Guidelines
263,2024,The Art of Timing: Effects of AR Guidance Timing on Speed Control,타이밍의 기술: AR 안내 타이밍이 속도 제어에 미치는 영향,"Augmented Reality (AR) holds significant potential to facilitate users in executing manual tasks. For effective support, however, we need to understand how showing movement instructions in AR affects how well people can follow those movements in real life. In this paper, we examine the degree to which users can synchronize the speed of their movements with speed cues presented through an AR environment. Specifically, we investigate the effects of timing in AR visual guidance. We assess performance using a highly realistic Mixed Reality (MR) welding simulation. Welding is a task that requires very precise timing and control over hand and arm motion. Our results show that upfront visual guidance (before manual task execution) alone often fails to transfer the knowledge of intended speeds, especially at higher target speeds. Live guidance (during manual task execution) during the activity provides more accurate speed results but typically requires a higher overshoot at the start. Optimal outcomes occur when visual guidance appears upfront and continues during the activity for users to follow through.",증강 현실(AR)은 사용자가 수동 작업을 수행하는 데 도움을 줄 수 있는 상당한 잠재력을 가지고 있습니다. 그러나 효과적인 지원을 위해서는 AR에서 동작 지침을 표시하는 것이 사람들이 실제 생활에서 이러한 동작을 얼마나 잘 따라갈 수 있는지에 어떤 영향을 미치는지 이해해야 합니다. 본 논문에서는 AR 환경을 통해 제공되는 속도 단서와 사용자의 이동 속도를 어느 정도 동기화할 수 있는지 살펴봅니다. 구체적으로 우리는 AR 시각적 안내에서 타이밍의 효과를 조사합니다. 우리는 매우 현실적인 혼합 현실(MR) 용접 시뮬레이션을 사용하여 성능을 평가합니다. 용접은 손과 팔의 움직임에 대한 매우 정확한 타이밍과 제어가 필요한 작업입니다. 우리의 결과는 수동 작업 실행 전의 시각적 안내만으로는 특히 더 높은 목표 속도에서 의도된 속도에 대한 지식을 전달하지 못하는 경우가 많다는 것을 보여줍니다. 활동 중 실시간 안내(수동 작업 실행 중)는 더 정확한 속도 결과를 제공하지만 일반적으로 시작 시 더 높은 오버슈트가 필요합니다. 최적의 결과는 시각적 안내가 미리 나타나고 사용자가 따라갈 수 있도록 활동 중에 계속될 때 발생합니다.,https://doi.org/10.1109/ISMAR62088.2024.00017,Education & Training,Sensor Fusion,Simulation,User Study / Empirical Findings
264,2024,The Effect of Interface Types and Immersive Environments on Drawing Accuracy and User Comfort,인터페이스 유형과 몰입형 환경이 도면 정확도와 사용자 편의성에 미치는 영향,"In this research, we investigate the effectiveness of asymmetric interactions (HandStylus, HandController, and TwoHands) in Augmented Reality (AR), Virtual Reality (VR), and Extended Reality (XR) for 3D digital drawing overlaying on physical and virtual objects. We evaluate the input accuracy and fatigue of these object-based 3D drawing experiences using quantitative measurements and further explore the correlation between these outcomes with subjective questionnaires. We found significant independence between environments and interface types, which considerably influence the performance and usability of 3D immersive drawing. We noted discrepancies between users’ subjective experiences and objective performance. Specifically, although AR drawing on physical objects provides superior accuracy and minimal muscle fatigue due to tangible feedback, and the TwoHands interaction offers the highest precision, the subjective results show the reverse outcome. Based on these findings, we propose design recommendations and discuss directions for future research in immersive drawing environments.","본 연구에서는 실제 개체와 가상 개체에 오버레이되는 3D 디지털 드로잉을 위한 증강 현실(AR), 가상 현실(VR) 및 확장 현실(XR)에서 비대칭 상호 작용(HandStylus, HandController 및 TwoHands)의 효과를 조사합니다. 우리는 정량적 측정을 사용하여 이러한 객체 기반 3D 드로잉 경험의 입력 정확도와 피로도를 평가하고 이러한 결과와 주관적 설문지 간의 상관 관계를 추가로 탐색합니다. 우리는 3D 몰입형 도면의 성능과 유용성에 상당한 영향을 미치는 환경과 인터페이스 유형 간의 상당한 독립성을 발견했습니다. 우리는 사용자의 주관적인 경험과 객관적인 성능 사이의 불일치를 확인했습니다. 구체적으로, 실제 물체에 대한 AR 드로잉은 가시적 피드백으로 인해 정확도가 뛰어나고 근육 피로가 최소화되었으며, TwoHands 상호 작용이 가장 높은 정밀도를 제공하지만 주관적인 결과는 반대의 결과를 나타냅니다. 이러한 결과를 바탕으로 디자인 권장 사항을 제안하고 몰입형 드로잉 환경에 대한 향후 연구 방향을 논의합니다.",https://doi.org/10.1109/ISMAR62088.2024.00099,Interaction & Input,Other,Quantitative Experiment,User Study / Empirical Findings
265,2024,The Effects of Immersion and Dimensionality in Virtual Reality Science Simulations: The Case of Charged Particles,가상 현실 과학 시뮬레이션에서 몰입감과 차원성의 효과: 하전 입자의 사례,"Researchers have provided insights into using virtual reality (VR) for visualization and interaction with 3D models and simulations. The interaction allows users to manipulate the 3D elements and visualize changes based on their inputs from movement with controllers or spatial actions. However, some users may find this interaction overwhelming, especially when immersed in a virtual environment. Additionally, the choice of dimensionality for visualizations influences user interaction, with potential implications for immersive experiences. Thus, we conducted a 2 (Immersion: Desktop vs. HMDVR) $\times 2$ (Dimensionality: 2 D vs. 3 D) within-group study $(N=32)$ to explore the impact of the utilized immersive degree and the dimensionality representation of the content on participants’ experience in terms of engagement, task load, usability, skill, and emotions when interacting with a science simulation. We designed and developed an application to simulate charged particles and electric field lines. We asked participants to complete a task of changing particles by matching them to a given simulation output. Our results indicated higher workload rates for HMDVR conditions, particularly with 3D representation, compared to Desktop. However, HMDVR conditions also showed greater engagement, emotional response, and presence. Based on our findings, we argue that participants prefer HMDVR over Desktop environments regardless of dimensionality.","연구원들은 3D 모델 및 시뮬레이션과의 시각화 및 상호 작용을 위해 가상 현실(VR)을 사용하는 방법에 대한 통찰력을 제공했습니다. 상호 작용을 통해 사용자는 3D 요소를 조작하고 컨트롤러 또는 공간 동작을 사용한 움직임의 입력을 기반으로 변경 사항을 시각화할 수 있습니다. 그러나 일부 사용자는 특히 가상 환경에 몰입할 때 이러한 상호 작용이 압도적이라고 느낄 수 있습니다. 또한 시각화를 위한 차원 선택은 사용자 상호 작용에 영향을 미치며 몰입형 경험에 대한 잠재적인 영향을 미칩니다. 따라서 우리는 과학 시뮬레이션과 상호 작용할 때 참여도, 작업 부하, 유용성, 기술 및 감정 측면에서 활용된 몰입도와 콘텐츠의 차원 표현이 참가자의 경험에 미치는 영향을 탐색하기 위해 2(Immersion: Desktop vs. HMDVR) $\times 2$(Dimensionality: 2 D vs. 3 D) 그룹 내 연구 $(N=32)$를 수행했습니다. 우리는 하전입자와 전기장선을 시뮬레이션하는 애플리케이션을 설계하고 개발했습니다. 우리는 참가자들에게 입자를 주어진 시뮬레이션 출력과 일치시켜 입자 변경 작업을 완료하도록 요청했습니다. 우리의 결과는 데스크톱에 비해 HMDVR 조건, 특히 3D 표현의 작업 부하율이 더 높은 것으로 나타났습니다. 그러나 HMDVR 조건은 또한 더 큰 참여도, 감정적 반응 및 존재감을 보여주었습니다. 우리의 연구 결과에 따르면 참가자들은 차원에 관계없이 데스크톱 환경보다 HMDVR을 선호한다고 주장합니다.",https://doi.org/10.1109/ISMAR62088.2024.00031,Interaction & Input; Perception & Cognition,Sensor Fusion,Simulation,User Study / Empirical Findings
266,2024,The Influence of Emotion-based Prioritized Facial Expressions on Social Presence in Avatar-mediated Remote Communication,아바타를 통한 원격 커뮤니케이션에서 감정 기반 우선순위 얼굴 표정이 사회적 존재감에 미치는 영향,"In avatar-mediated remote communication, avatars’ facial expressions can be dynamically adjusted according to each user’s computational and device constraints, highlighting the importance of varied expressions and their impact on user perception. However, there is a lack of research on how variations in avatar facial expressions, especially when simplified, influence user perception, particularly in terms of social presence. To address this, we examine the impact of various facial expression combinations on social presence in avatar-mediated communication scenarios, ranging from informative speeches to emotional conversations. Our approach involves prioritizing avatar facial blendshape combinations using two main approaches: (1) commonly activated expressions that reflect the active facial movements observed during casual conversations, and (2) emotion-based expressions derived from Facial Action Coding System (FACS). These combinations were compared against minimal baseline and full blendshape conditions through a comprehensive study involving 32 participants. Our findings reveal that emotion-based condition achieves comparable levels of social presence and communication quality to the full condition, in both informative speeches and emotional conversations. This highlights the effectiveness of prioritizing emotion-based expressions and adopting a streamlined approach to avatar facial control. By focusing on emotional expressions while optimizing resources, this approach shows potential for enhancing the avatar-mediated communication experience, accommodating the diverse users’ contexts.",아바타를 통한 원격 통신에서는 아바타의 얼굴 표정이 각 사용자의 컴퓨팅 및 장치 제약에 따라 동적으로 조정될 수 있으므로 다양한 표현의 중요성과 사용자 인식에 미치는 영향이 강조됩니다. 그러나 아바타 얼굴 표정의 변화(특히 단순화된 경우)가 특히 사회적 존재 측면에서 사용자 인식에 어떻게 영향을 미치는지에 대한 연구는 부족합니다. 이 문제를 해결하기 위해 우리는 유익한 연설부터 감정적인 대화에 이르기까지 아바타를 통한 의사소통 시나리오에서 다양한 얼굴 표정 조합이 사회적 존재에 미치는 영향을 조사합니다. 우리의 접근 방식에는 두 가지 주요 접근 방식을 사용하여 아바타 얼굴 블렌드 셰이프 조합의 우선 순위를 지정하는 것이 포함됩니다. (1) 일상적인 대화 중에 관찰되는 활동적인 얼굴 움직임을 반영하는 일반적으로 활성화되는 표현과 (2) 얼굴 동작 코딩 시스템(FACS)에서 파생된 감정 기반 표현입니다. 이러한 조합은 32명의 참가자가 참여한 포괄적인 연구를 통해 최소 기준선 및 전체 블렌드셰이프 조건과 비교되었습니다. 우리의 연구 결과는 감정 기반 상태가 유익한 연설과 감정적 대화 모두에서 전체 상태와 비슷한 수준의 사회적 존재감과 의사소통 품질을 달성한다는 것을 보여줍니다. 이는 감정 기반 표현을 우선시하고 아바타 얼굴 제어에 대한 간소화된 접근 방식을 채택하는 것의 효율성을 강조합니다. 리소스를 최적화하면서 감정 표현에 중점을 두는 이 접근 방식은 아바타를 통한 커뮤니케이션 경험을 향상하고 다양한 사용자의 상황을 수용할 수 있는 가능성을 보여줍니다.,https://doi.org/10.1109/ISMAR62088.2024.00131,Perception & Cognition; Collaboration & Social,Other,Technical Evaluation,User Study / Empirical Findings
267,2024,ThermicVib: Enabling Dynamic Thermal Sensation with Multimodal Haptic Glove for Thermal-Responsive Interaction,ThermicVib: 열 반응 상호 작용을 위한 다중 모드 햅틱 장갑으로 동적 열 감각 구현,"We propose ThermicVib, a wearable multimodal haptic glove that enhances the active perception of thermo-tactile interactions with virtual objects by integrating thermal referrals and vibrotactile phantom sensations. By fusing multimodal sensory illusions through flexible thermoelectric devices (FTED) and linear resonant actuators (LRAs), we aim to support dynamic thermal sensation adaptive to the user’s action in virtual reality (VR). Here, we developed an algorithm to render a whole-hand thermal sensation while accommodating contact and noncontact heat conditions. Based on the computed heat, we propose a simultaneous thermal and tactile rendering approach to enable dynamic thermal sensation. The user study validated the capability of our interface to support various whole-hand thermal sensations.",우리는 열 참조와 진동촉각 팬텀 감각을 통합하여 가상 물체와의 열촉각 상호 작용에 대한 능동적 인식을 향상시키는 착용 가능한 다중 모드 햅틱 장갑인 ThermicVib를 제안합니다. FTED(Flexible Thermoelectric Device)와 LRA(Linear Resonant Actuator)를 통해 다중 모드 감각 환상을 융합함으로써 가상 현실(VR)에서 사용자의 행동에 적응하는 동적 열 감각을 지원하는 것을 목표로 합니다. 여기에서는 접촉 및 비접촉 열 조건을 수용하면서 손 전체에 열 감각을 렌더링하는 알고리즘을 개발했습니다. 계산된 열을 기반으로 동적 열 감각을 가능하게 하는 열 및 촉각 렌더링 동시 접근 방식을 제안합니다. 사용자 연구를 통해 손 전체에 닿는 다양한 열 감각을 지원하는 인터페이스의 기능이 검증되었습니다.,https://doi.org/10.1109/ISMAR62088.2024.00076,Interaction & Input,Haptic / Tactile Feedback,User Study,Hardware / Device; Algorithm / Method
268,2024,Touch It Like It's Hot: A Thermal Feedback Enabled Encountered-type Haptic Display for Virtual Reality,뜨거운 것처럼 터치하세요: 가상 현실을 위한 열 피드백 지원 조우형 햅틱 디스플레이,"In recent years, the community has presented various novel solutions to address the lack of haptic feedback in virtual reality experiences. Yet, it remains a major challenge for Virtual Reality applications. Encountered-type Haptic Displays (ETHDs) have emerged as a promising alternative to enable haptic feedback in VR without requiring the user to wear any device while allowing for sensorily rich experiences such as texture, kinaesthetic feedback, and even ultrasonic tactile feedback. Nevertheless, as important as thermal feedback is for daily life interactions, such as assessing the temperature of a mug or knowing if the microwave is on, thermal feedback in ETHD has remained largely unexplored. In this paper, we present a novel ETHD that provides thermal feedback and explore its potential in VR. We describe the design of our ETHD, and we report the results of a user study that compares different thermal feedback settings in VR. Our results show that thermal feedback can significantly enhance the user immersion and haptic experience in VR, and we discuss the implications of our findings for the design of ETHD and VR experiences.","최근 몇 년 동안 커뮤니티에서는 가상 현실 경험에서 햅틱 피드백 부족을 해결하기 위한 다양하고 새로운 솔루션을 제시했습니다. 그러나 이는 가상 현실 애플리케이션의 주요 과제로 남아 있습니다. 만남형 햅틱 디스플레이(ETHD)는 사용자가 어떤 장치도 착용하지 않고도 VR에서 질감, 운동 감각 피드백, 심지어 초음파 촉각 피드백과 같은 감각적으로 풍부한 경험을 허용하면서 햅틱 피드백을 가능하게 하는 유망한 대안으로 떠올랐습니다. 그럼에도 불구하고 머그잔의 온도를 평가하거나 전자레인지가 켜져 있는지 확인하는 등 일상 생활 상호 작용에 열 피드백이 중요한 만큼 ETHD의 열 피드백은 아직까지 거의 탐구되지 않은 상태로 남아 있습니다. 본 논문에서는 열 피드백을 제공하고 VR에서 그 잠재력을 탐색하는 새로운 ETHD를 제시합니다. 우리는 ETHD의 설계를 설명하고 VR의 다양한 열 피드백 설정을 비교하는 사용자 연구 결과를 보고합니다. 우리의 결과는 열 피드백이 VR에서 사용자 몰입도와 햅틱 경험을 크게 향상시킬 수 있음을 보여주고, ETHD 및 VR 경험 설계에 대한 우리의 연구 결과가 갖는 의미를 논의합니다.",https://doi.org/10.1109/ISMAR62088.2024.00085,Interaction & Input,Haptic / Tactile Feedback,User Study,User Study / Empirical Findings; Hardware / Device
269,2024,Toward User-Aware Interactive Virtual Agents: Generative Multi-Modal Agent Behaviors in VR,사용자 인식 대화형 가상 에이전트를 향하여: VR의 생성적 다중 모달 에이전트 동작,"Virtual agents serve as a vital interface within XR platforms. However, generating virtual agent behaviors typically rely on pre-coded actions or physics-based reactions. In this paper we present a learning-based multimodal agent behavior generation framework that adapts to users’ in-situ behaviors, similar to how humans interact with each other in the real world. By leveraging an in-house collected, dyadic conversational behavior dataset, we trained a conditional variational autoencoder (CVAE) model to achieve user-conditioned generation of virtual agents’ behaviors. Together with large language models (LLM), our approach can generate both the verbal and non-verbal reactive behaviors of virtual agents. Our comparative user study confirmed our method’s superiority over conventional animation graph-based baseline techniques, particularly regarding user-centric criteria. Thorough analyses of our results underscored the authentic nature of our virtual agents’ interactions and the heightened user engagement during VR interaction.",가상 에이전트는 XR 플랫폼 내에서 중요한 인터페이스 역할을 합니다. 그러나 가상 에이전트 동작 생성은 일반적으로 사전 코딩된 작업이나 물리 기반 반응에 의존합니다. 본 논문에서는 현실 세계에서 인간이 서로 상호 작용하는 방식과 유사하게 사용자의 현장 행동에 적응하는 학습 기반 다중 모드 에이전트 행동 생성 프레임워크를 제시합니다. 사내에서 수집된 이중 대화 행동 데이터 세트를 활용하여 조건부 변형 자동 인코더(CVAE) 모델을 훈련하여 가상 에이전트 행동의 사용자 조건 생성을 달성했습니다. LLM(대형 언어 모델)과 함께 우리의 접근 방식은 가상 에이전트의 언어적 및 비언어적 반응 동작을 모두 생성할 수 있습니다. 우리의 비교 사용자 연구는 특히 사용자 중심 기준과 관련하여 기존 애니메이션 그래프 기반 기준 기술에 비해 우리 방법의 우월성을 확인했습니다. 결과에 대한 철저한 분석을 통해 가상 에이전트 상호 작용의 진정한 특성과 VR 상호 작용 중 사용자 참여가 높아졌다는 사실이 강조되었습니다.,https://doi.org/10.1109/ISMAR62088.2024.00123,Interaction & Input,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method; System / Framework
270,2024,"Towards Energy-Efficiency by Navigating the Trilemma of Energy, Latency, and Accuracy","에너지, 지연 시간, 정확성의 트릴레마를 탐색하여 에너지 효율성을 향해","Extended Reality (XR) enables immersive experiences through untethered headsets but suffers from stringent battery and resource constraints. Energy-efficient design is crucial to ensure both longevity and high performance in XR devices. However, latency and accuracy are often prioritized over energy, leading to a gap in achieving energy efficiency. This paper examines scene reconstruction, a key building block for immersive XR experiences, and demonstrates how energy efficiency can be achieved by navigating the trilemma of energy, latency, and accuracy. We explore three classes of energy-oriented optimizations, covering the algorithm, execution, and data, that reveal a broad de-sign space through configurable parameters. Our resulting 72 designs expose a wide range of latency and energy trade-offs, with a smaller range of accuracy loss. We identify a Pareto-optimal curve and show that the designs on the curve are achievable only through synergistic co-optimization of all three optimization classes and by considering the latency and accuracy needs of downstream scene reconstruction consumers. Our analysis covering various use cases and measurements on an embedded class system shows that, relative to the baseline, our designs offer energy benefits of up to $60 \times$ with potential latency range of $4 \times$ slowdown to $2 \times$ speedup. Detailed exploration of a use case across representative data sequences from ScanNet showed about $25 \times$ energy savings with $1.5 \times$ latency reduction and negligible reconstruction quality loss.","확장 현실(XR)은 무선 헤드셋을 통해 몰입형 경험을 제공하지만 엄격한 배터리 및 리소스 제약이 있습니다. XR 장치의 수명과 고성능을 모두 보장하려면 에너지 효율적인 설계가 중요합니다. 그러나 대기 시간과 정확성이 에너지보다 우선시되는 경우가 많아 에너지 효율성 달성에 격차가 발생합니다. 본 백서에서는 몰입형 XR 경험의 핵심 구성 요소인 장면 재구성을 검토하고, 에너지, 지연 시간, 정확성의 삼중레마를 탐색하여 에너지 효율성을 달성할 수 있는 방법을 보여줍니다. 우리는 구성 가능한 매개변수를 통해 광범위한 설계 공간을 드러내는 알고리즘, 실행 및 데이터를 포괄하는 에너지 중심 최적화의 세 가지 클래스를 탐구합니다. 그 결과 72개의 디자인은 정확도 손실 범위가 더 작으면서도 광범위한 대기 시간 및 에너지 상충 관계를 노출했습니다. 우리는 파레토 최적 곡선을 식별하고 세 가지 최적화 클래스 모두의 시너지 공동 최적화를 통해서만 그리고 다운스트림 장면 재구성 소비자의 대기 시간 및 정확도 요구 사항을 고려함으로써 곡선의 설계를 달성할 수 있음을 보여줍니다. 임베디드 클래스 시스템에 대한 다양한 사용 사례와 측정을 다루는 우리의 분석에 따르면 우리의 설계는 기준선에 비해 최대 $60 \times$의 에너지 이점을 제공하며 잠재적 대기 시간 범위는 $4 \times$ 감속에서 $2 \times$ 속도 향상까지 가능합니다. ScanNet의 대표적인 데이터 시퀀스에 대한 사용 사례를 자세히 조사한 결과 $1.5 \times$의 대기 시간 감소와 미미한 재구성 품질 손실로 약 $25 \times$의 에너지 절감 효과가 나타났습니다.",https://doi.org/10.1109/ISMAR62088.2024.00107,Interaction & Input,Optical / Display Technology,Quantitative Experiment; Technical Evaluation,Algorithm / Method; Hardware / Device
271,2024,Towards Open-World Gesture Recognition,오픈 월드 제스처 인식을 향하여,"Providing users with accurate gestural interfaces, such as gesture recognition based on wrist-worn devices, is a key challenge in mixed reality. However, static machine learning processes in gesture recognition assume that training and test data come from the same underlying distribution. Unfortunately, in real-world applications involving gesture recognition, such as gesture recognition based on wrist-worn devices, the data distribution may change over time. We formulate this problem of adapting recognition models to new tasks, where new data patterns emerge, as open-world gesture recognition (OWGR). We propose the use of continual learning to enable machine learning models to be adaptive to new tasks without degrading performance on previously learned tasks. However, the process of exploring parameters for questions around when, and how, to train and deploy recognition models requires resource-intensive user studies may be impractical. To address this challenge, we propose a design engineering approach that enables offline analysis on a collected large-scale dataset by systematically examining various parameters and comparing different continual learning methods. Finally, we provide design guidelines to enhance the development of an open-world wrist-worn gesture recognition process.","손목에 착용하는 장치를 기반으로 한 제스처 인식과 같은 정확한 제스처 인터페이스를 사용자에게 제공하는 것은 혼합 현실의 핵심 과제입니다. 그러나 동작 인식의 정적 기계 학습 프로세스에서는 훈련 및 테스트 데이터가 동일한 기본 분포에서 나온다고 가정합니다. 불행하게도 손목에 착용하는 장치를 기반으로 한 동작 인식과 같은 동작 인식과 관련된 실제 애플리케이션에서는 시간이 지남에 따라 데이터 분포가 변경될 수 있습니다. 우리는 새로운 데이터 패턴이 나타나는 새로운 작업에 인식 모델을 적용하는 이 문제를 개방형 제스처 인식(OWGR)으로 공식화합니다. 우리는 기계 학습 모델이 이전에 학습한 작업의 성능을 저하시키지 않고 새로운 작업에 적응할 수 있도록 지속적인 학습의 사용을 제안합니다. 그러나 인식 모델을 훈련하고 배포하는 시기와 방법에 대한 질문에 대한 매개 변수를 탐색하는 프로세스에는 리소스 집약적인 사용자 연구가 필요하므로 비실용적일 수 있습니다. 이러한 문제를 해결하기 위해 우리는 다양한 매개변수를 체계적으로 조사하고 다양한 연속 학습 방법을 비교함으로써 수집된 대규모 데이터 세트에 대한 오프라인 분석을 가능하게 하는 설계 엔지니어링 접근 방식을 제안합니다. 마지막으로, 오픈 월드 손목 착용 제스처 인식 프로세스의 개발을 향상시키기 위한 설계 지침을 제공합니다.",https://doi.org/10.1109/ISMAR62088.2024.00140,Interaction & Input; Education & Training,Hand / Gesture Recognition,Technical Evaluation,Algorithm / Method
272,2024,Tracking Eye Position and Gaze Direction in Near-Eye Volumetric Displays,가까운 눈 체적 디스플레이에서 눈 위치 및 시선 방향 추적,"Near-eye volumetric displays, showing multiple focal planes, require knowledge of the accurate position of the nodal point of the eye to correctly render a 3D scene. This is because pixels seen through multiple planes must be accurately aligned with the eye’s visual axis to ensure consistency across focal planes. While most eye-tracking methods focus on determining a gaze position within a designated target space, this work aims to track both the eye position and the corresponding gaze direction expressed in coordinates relative to the physical location of the volumetric display planes. To achieve this, we rely on a near-infra-red (NIR) camera image of the pupil and corneal reflections (glints). The existing eye model is used to establish the relationship between the pupil and glint positions in a NIR image and the eye position and rotation in a 3D space. We address the key challenge of robust tracking of the glints in a system that introduces multiple reflections. We also demonstrate that the system reduces the need for recalibration on subsequent uses. Our experiments on a multiple-focal plane display demonstrate that the method can maintain an accurate projection point for volumetric displays.","여러 초점면을 보여주는 근안 체적 디스플레이는 3D 장면을 올바르게 렌더링하기 위해 눈의 노드 포인트의 정확한 위치에 대한 지식이 필요합니다. 이는 여러 평면을 통해 보이는 픽셀이 초점 평면 전체의 일관성을 보장하기 위해 눈의 시축과 정확하게 정렬되어야 하기 때문입니다. 대부분의 시선 추적 방법은 지정된 대상 공간 내에서 시선 위치를 결정하는 데 중점을 두지만, 이 작업은 눈 위치와 체적 디스플레이 평면의 물리적 위치에 대한 좌표로 표현되는 해당 시선 방향을 모두 추적하는 것을 목표로 합니다. 이를 달성하기 위해 우리는 동공의 근적외선(NIR) 카메라 이미지와 각막 반사(광선)에 의존합니다. 기존 눈 모델은 NIR 이미지의 동공 및 반짝임 위치와 3D 공간의 눈 위치 및 회전 간의 관계를 설정하는 데 사용됩니다. 우리는 다중 반사를 도입하는 시스템에서 글린트를 강력하게 추적하는 주요 과제를 해결합니다. 또한 시스템이 후속 사용 시 재보정의 필요성을 줄여준다는 사실도 입증했습니다. 다중 초점 평면 디스플레이에 대한 우리의 실험은 이 방법이 체적 디스플레이에 대한 정확한 투사점을 유지할 수 있음을 보여줍니다.",https://doi.org/10.1109/ISMAR62088.2024.00094,Display & Optics; Interaction & Input,Eye / Gaze Tracking,Other,Algorithm / Method
273,2024,"Usability, Acceptance, and Trust of Privacy Protection Mechanisms and Identity Management in Social Virtual Reality","소셜 가상 현실에서의 개인정보 보호 메커니즘과 신원 관리의 유용성, 수용 및 신뢰","In social virtual reality (social VR), users are threatened by potential cybercrimes, such as identity theft, sensitive data breaches, and embodied harassment. These concerns are heightened by the increasing interest in the metaverse, the advancements in photorealistic 3D user reconstructions, and the rising incidents of online privacy violations. Designing secure social VR applications that protect users while enhancing their experience, acceptance and trust remains a challenge. This article investigates potential identity management solutions in social VR, and their impacts on usability and user acceptance. We developed a social VR prototype with novel and established countermeasures, including motion biometric verification, and conducted a study with 52 participants. Our findings reveal diverse preferences for identity management and underscore the importance of authenticity, autonomy, and reciprocity. Key findings include: passive verification is favored for pragmatic user experience, while active verification is preferred for its hedonic quality; continuous or periodic verification strengthens users’ confidence in their privacy; and while user awareness promotes authentic engagement, it may also diminish the willingness to disclose personal information. This research not only offers foundational insights into the evaluated scenarios and countermeasures, but also sheds light on the designs of more trustworthy and inclusive social VR applications.","소셜 가상 현실(소셜 VR)에서 사용자는 신원 도용, 민감한 데이터 침해, 구체화된 괴롭힘 등 잠재적인 사이버 범죄로 위협을 받습니다. 이러한 우려는 메타버스에 대한 관심 증가, 사실적인 3D 사용자 재구성의 발전, 온라인 개인정보 침해 사건 증가로 인해 더욱 높아지고 있습니다. 사용자의 경험, 수용 및 신뢰를 향상시키면서 사용자를 보호하는 안전한 소셜 VR 애플리케이션을 설계하는 것은 여전히 ​​어려운 과제입니다. 이 기사에서는 소셜 VR의 잠재적인 ID 관리 솔루션과 이것이 유용성 및 사용자 수용에 미치는 영향을 조사합니다. 우리는 동작 생체 인식 검증을 포함한 새롭고 확립된 대응책을 갖춘 소셜 VR 프로토타입을 개발하고 52명의 참가자를 대상으로 연구를 수행했습니다. 우리의 연구 결과는 신원 관리에 대한 다양한 선호도를 보여주고 진정성, 자율성 및 상호주의의 중요성을 강조합니다. 주요 결과는 다음과 같습니다: 실용적인 사용자 경험에는 수동적 검증이 선호되는 반면, 쾌락적 품질에서는 능동적 검증이 선호됩니다. 지속적이고 주기적인 검증을 통해 개인 정보 보호에 대한 사용자의 신뢰를 강화합니다. 사용자 인식은 진정한 참여를 촉진하지만 개인 정보 공개 의지를 감소시킬 수도 있습니다. 이 연구는 평가된 시나리오와 대응책에 대한 근본적인 통찰력을 제공할 뿐만 아니라 보다 신뢰할 수 있고 포괄적인 소셜 VR 애플리케이션의 설계에 대한 정보를 제공합니다.",https://doi.org/10.1109/ISMAR62088.2024.00027,Privacy & Security,Other,User Study,Design Guidelines
274,2024,User-Defined Gesture Interactions for VR Museums: An Elicitation Study,VR 박물관을 위한 사용자 정의 제스처 상호 작용: 유도 연구,"Recognizing the potential of freehand gestures for interacting with virtual objects in virtual environments, our research introduces a user-defined freehand gesture set of typical referents in virtual reality (VR), focusing on a specific scenario: VR museums. We conducted a comprehensive elicitation study with two experiments to define and refine the gesture set. Meanwhile, we demonstrated an enhanced real-time Wizard of Oz approach that facilitated users’ understanding of referents in VR and their gesture design. Our findings revealed significant improvements in gesture consistency and user agreement through two experiments, with an average agreement score of firstchoice and second-choice advancing from 0.211 and 0.160 to 0.412 and 0.284, respectively. By offering a consistent user-centered gesture set, this work contributes to guiding museum curators toward creating more immersive user experiences in VR museums. The gestures can also be extended to other VR applications that necessitate travel, selection and manipulation, and system control tasks.","가상 환경에서 가상 개체와 상호 작용하기 위한 자유형 제스처의 잠재력을 인식한 우리 연구에서는 특정 시나리오인 VR 박물관에 초점을 맞춰 가상 현실(VR)의 일반적인 지시 대상에 대한 사용자 정의 자유형 제스처 세트를 소개합니다. 우리는 제스처 세트를 정의하고 개선하기 위해 두 가지 실험을 통해 포괄적인 도출 연구를 수행했습니다. Meanwhile, we demonstrated an enhanced real-time Wizard of Oz approach that facilitated users’ understanding of referents in VR and their gesture design. 우리의 연구 결과는 두 가지 실험을 통해 제스처 일관성과 사용자 동의가 크게 개선된 것으로 나타났습니다. 첫 번째 선택과 두 번째 선택의 평균 동의 점수는 각각 0.211과 0.160에서 0.412와 0.284로 향상되었습니다. 일관된 사용자 중심 제스처 세트를 제공함으로써 이 작업은 박물관 큐레이터가 VR 박물관에서 더욱 몰입감 있는 사용자 경험을 만들 수 있도록 안내하는 데 도움이 됩니다. 제스처는 이동, 선택 및 조작, 시스템 제어 작업이 필요한 다른 VR 애플리케이션으로 확장될 수도 있습니다.",https://doi.org/10.1109/ISMAR62088.2024.00078,Interaction & Input,Hand / Gesture Recognition,Simulation,User Study / Empirical Findings
275,2024,Validating Eyes-free Affordance of On-Finger Hand Proximate User Interfaces in In-situ Scenarios,현장 시나리오에서 손가락을 대고 있는 손에 근접한 사용자 인터페이스의 눈이 없는 어포던스 검증,"We explore the value of Hand Proximate User Interfaces (HPUIs) for in-situ interactive head-mounted systems, i.e. systems designed to support a user’s primary activity. HPUIs are unencumbered, single-handed, and have tactile and proprioceptive affordances. This allows novice and expert users to rely on the visual cues available in the UI, but to then gradually interact eyes-free such that they minimize interrupting the user’s core task. Prior work on HPUI falls short of validating this premise. We address this gap with comparative analysis in the context of a compound task, where one hand is involved in a primary task and the second interacts with a secondary but supporting task (e.g. selecting items from a menu). We compare HPUI with mid-air direct interactions, a common form of interacting with head-mounted displays. The results show that HPUI performs similarly to mid-air but with better eyes-free affordances and user preferences.","우리는 현장 대화형 헤드 장착 시스템, 즉 사용자의 주요 활동을 지원하도록 설계된 시스템을 위한 HPUI(Hand Proximate User Interface)의 가치를 탐구합니다. HPUI는 방해받지 않고 한 손으로 사용할 수 있으며 촉각적이고 고유한 감각을 제공합니다. 이를 통해 초보자와 숙련된 사용자는 UI에서 사용할 수 있는 시각적 단서에 의존할 수 있지만, 사용자의 핵심 작업을 방해하는 것을 최소화하도록 눈에 띄지 않고 점진적으로 상호 작용할 수 있습니다. HPUI에 대한 이전 작업은 이 전제를 검증하는 데 부족합니다. 우리는 한 손은 기본 작업에 관여하고 두 번째 손은 보조적이지만 지원 작업(예: 메뉴에서 항목 선택)과 상호 작용하는 복합 작업의 맥락에서 비교 분석을 통해 이러한 격차를 해결합니다. 우리는 HPUI를 머리 장착 디스플레이와 상호 작용하는 일반적인 형태인 공중 직접 상호 작용과 비교합니다. 결과는 HPUI가 공중과 유사하게 작동하지만 눈이 없는 어포던스와 사용자 선호도가 더 우수하다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR62088.2024.00145,Interaction & Input; Display & Optics,Haptic / Tactile Feedback,Other,System / Framework
276,2024,ViP-Fluid: Visual Perception Driven Method for VR Fluid Rendering,ViP-Fluid: VR 유체 렌더링을 위한 시각적 인식 기반 방법,"The demand for fluid simulation and rendering in virtual reality (VR) is increasing. However, achieving high visual quality while maintaining real-time efficiency remains a challenge. Traditional foveated rendering methods balance the simulation quality in the foveated region but neglect the physical realism in the peripheral areas, and fail to account for the perceptual degradation caused by frame rate fluctuations during adaptive updates. To address these challenges, we propose a novel visual perception driven fluid rendering method ViP-Fluid, which further enhances rendering quality while balancing efficiency. Our approach employs a spatiotemporal saliency model for multi-granularity simulation and rendering of Lagrangian fluid systems, and introduces a Perception Threshold for Physical Process Elapsing (PTPE) metric, which guides our temporal acceleration strategy. Through a series of objective experiments, we demonstrate the advantages of our method in rendering quality and performance efficiency. ViP-Fluid demonstrates superior metrics not only in the foveated region but also in the salient and overall regions, achieving up to 2.15 times speed-up compared to the high-resolution Position Based Fluids (PBF) benchmark. Subsequent user experiments further validate the visual perception advantages of ViP-Fluid over both traditional and state-of-the-art methods, confirming the spatiotemporal fidelity of our acceleration strategy as well as a user preference for our approach.","가상 현실(VR)에서의 유체 시뮬레이션 및 렌더링에 대한 수요가 증가하고 있습니다. 그러나 실시간 효율성을 유지하면서 높은 시각적 품질을 달성하는 것은 여전히 ​​어려운 과제로 남아 있습니다. 전통적인 포비티드 렌더링 방법은 포비티드 영역의 시뮬레이션 품질 균형을 유지하지만 주변 영역의 물리적 현실성을 무시하고 적응형 업데이트 중 프레임 속도 변동으로 인한 지각 저하를 설명하지 못합니다. 이러한 문제를 해결하기 위해 우리는 효율성의 균형을 유지하면서 렌더링 품질을 더욱 향상시키는 새로운 시각적 인식 기반 유체 렌더링 방법 ViP-Fluid를 제안합니다. 우리의 접근 방식은 라그랑주 유체 시스템의 다중 입도 시뮬레이션 및 렌더링을 위한 시공간 돌출 모델을 사용하고 시간 가속 전략을 안내하는 PTPE(물리적 프로세스 경과에 대한 인식 임계값) 메트릭을 도입합니다. 일련의 객관적인 실험을 통해 우리는 렌더링 품질과 성능 효율성에 대한 우리 방법의 장점을 보여줍니다. ViP-Fluid는 중심 영역뿐만 아니라 주요 영역과 전체 영역에서도 뛰어난 측정 지표를 보여주며 고해상도 PBF(위치 기반 유체) 벤치마크에 비해 최대 2.15배의 속도 향상을 달성했습니다. 후속 사용자 실험에서는 기존 방법과 최첨단 방법에 비해 ViP-Fluid의 시각적 인식 이점이 더욱 검증되었으며, 가속 전략의 시공간 충실도와 접근 방식에 대한 사용자 선호도가 확인되었습니다.",https://doi.org/10.1109/ISMAR62088.2024.00050,Perception & Cognition,Sensor Fusion,Technical Evaluation,Algorithm / Method
277,2024,VibroArm: Enhancing the Sensation of Forearm Deformation in Virtual Reality Using Vibrotactile Funneling Illusion,VibroArm: Vibrotactile Funneling Illusion을 사용하여 가상 현실에서 팔뚝 변형 감각 향상,"When we enter Virtual Reality (VR) in the first person, the avatar replaces our real body. Within a certain range of mismatches, we tend to identify with our avatar and perceive it as our own body. Previous research has shown that we can even believe that a deformed forearm is our own through haptic and visual feedback. However, the haptic devices used in earlier research were only explored using skin-stretching and weight-shifting. Vibrotactile feedback is the most common technique used to create a haptic VR experience in academia and industry because of its light weight and ease of application. Taking these advantages, we introduce VibroArm, a lightweight wearable haptic system with the vibrotactile funneling illusion that enhances the virtual forearm’s ownership in the forearm deformation illusion. In a perceptual study, we determined that users can make correct direction judgments about vibration on the forearm in different directions. The result shows that all vibration patterns can be sufficiently recognized, with an average recognition rate of $87.17 \%$. Based on our results, we designed 18 vibration patterns and compared their impact on the forearm deformation illusion. Finally, our study shows that using VibroArm in VR applications can significantly improve user realism and enjoyment compared to relying on visual feedback alone.","우리가 1인칭으로 ​​가상 현실(VR)에 들어가면 아바타가 실제 신체를 대체합니다. 일정 범위의 불일치 내에서 우리는 아바타와 동일시하고 그것을 자신의 몸으로 인식하는 경향이 있습니다. 이전 연구에서는 변형된 팔뚝이 촉각과 시각적 피드백을 통해 우리 자신의 것이라고 믿을 수도 있다는 사실이 밝혀졌습니다. 그러나 이전 연구에서 사용된 햅틱 장치는 피부 스트레칭과 체중 이동을 통해서만 연구되었습니다. 진동촉각 피드백은 가벼운 무게와 적용 용이성으로 인해 학계와 산업계에서 햅틱 VR 경험을 만드는 데 가장 일반적으로 사용되는 기술입니다. 이러한 장점을 활용하여 팔뚝 변형 환상에서 가상 팔뚝의 소유권을 향상시키는 진동 촉각 깔대기 환상을 갖춘 경량 웨어러블 햅틱 시스템인 VibroArm을 소개합니다. 지각 연구에서 우리는 사용자가 팔뚝의 다양한 방향에서 진동에 대해 올바른 방향 판단을 내릴 수 있음을 확인했습니다. 그 결과, 평균 인식률은 $87.17\%$로 모든 진동 패턴을 충분히 인식할 수 있는 것으로 나타났다. 우리의 결과를 바탕으로 우리는 18개의 진동 패턴을 설계하고 팔뚝 변형 환상에 미치는 영향을 비교했습니다. 마지막으로, 우리의 연구에 따르면 VR 애플리케이션에서 VibroArm을 사용하면 시각적 피드백에만 의존하는 것에 비해 사용자의 현실감과 즐거움이 크게 향상될 수 있습니다.",https://doi.org/10.1109/ISMAR62088.2024.00066,Perception & Cognition,Haptic / Tactile Feedback,Other,Algorithm / Method; Hardware / Device
278,2024,Virtual Reality Multimodal Stabilization Intervention for Covid-19 Trauma Treatment,코로나19 트라우마 치료를 위한 가상 현실 다중 모드 안정화 개입,"THE COVID-19 pandemic had a serious impact on the public’s mental health and may give rise to post-traumatic symptoms. As the first treatment phase, stabilization focuses on reducing psychological discomfort after experiencing trauma. Previously, stabilization was performed by imaginative guided imagery (GI) and digital audio files. However, this kind of intervention limits the patient’s imagination and ability to recall from memory. Virtual reality (VR) has been considered to have great potential in psychotherapy because of its characteristics of immersion, vividness, and interactivity. Our study proposes multimodal VR stabilization intervention that includes three stabilization techniques: light stream, breathing relaxation, and containment. We combined multisensory (thermal) stimulation and eye-tracking interaction in the VR environment. We conducted a preliminary verification of the treatment effect on groups that were at high risk of experiencing psychological trauma during COVID-19, including healthcare workers and COVID-19 survivors, and conducted a controlled experiment with a previously released audio-based trauma stabilization mobile App. The results show that VR stabilization intervention is feasible, and compared with the App group, the subjective unit of disturbance (SUD) and the objective physiological indicator, the heart rate variability (HRV) of participants in the VR group improved more significantly. Our research experience helps to enrich the research scope of VR technology in the field of trauma treatment through the evidence provided by physiological and psychological measurements.","코로나19 팬데믹은 대중의 정신 건강에 심각한 영향을 미쳤으며 외상 후 증상을 유발할 수 있습니다. 첫 번째 치료 단계로서 안정화는 트라우마를 경험한 후 심리적 불편함을 줄이는 데 중점을 둡니다. 이전에는 상상 유도 이미지(GI)와 디지털 오디오 파일을 통해 안정화가 수행되었습니다. 그러나 이런 종류의 개입은 환자의 상상력과 기억을 회상하는 능력을 제한합니다. 가상현실(VR)은 몰입감, 생생함, 상호작용성 등의 특성으로 인해 심리치료에서 큰 잠재력을 지닌 것으로 여겨져 왔습니다. 우리 연구에서는 빛의 흐름, 호흡 이완, 봉쇄의 세 가지 안정화 기술을 포함하는 다중 모드 VR 안정화 개입을 제안합니다. 우리는 VR 환경에서 다감각(열) 자극과 시선 추적 상호 작용을 결합했습니다. 의료진, 코로나19 생존자 등 코로나19로 인해 심리적 트라우마를 경험할 위험이 높은 집단을 대상으로 치료 효과를 사전 검증하고, 앞서 출시된 오디오 기반 트라우마 안정화 모바일 앱을 이용해 대조 실험을 진행했습니다. 결과는 VR 안정화 개입이 가능하다는 것을 보여주며, 앱 그룹, 주관적 방해 단위(SUD) 및 객관적인 생리학적 지표와 비교하여 VR 그룹 참가자의 심박 변이도(HRV)가 더 크게 향상되었습니다. 우리의 연구 경험은 생리학적, 심리적 측정을 통해 제공되는 증거를 통해 외상 치료 분야에서 VR 기술의 연구 범위를 풍부하게 하는 데 도움이 됩니다.",https://doi.org/10.1109/ISMAR62088.2024.00106,Medical & Healthcare; Interaction & Input,Sensor Fusion,User Study,Algorithm / Method
279,2024,Visceral Interfaces for Privacy Awareness of Eye Tracking in VR,VR에서 시선 추적의 개인 정보 보호 인식을 위한 내장 인터페이스,"Eye tracking is increasingly being integrated into virtual reality (VR) devices to support a wide range of applications. It is used as a method of interaction, to support performance optimizations, and to create adaptive trainingor narrative experiences. However, providing access to eye-tracking data also introduces the ability to monitor user activity, detect and classify a user’s biometric identity, or otherwise reveal sensitive information such as medical conditions. As this technology continues to evolve, users should be made aware of the amount of information they are sharing about themselves to developers and how it can be used. While traditional terms of service may relay this type of information, previous work indicates they are not accessibly conveying privacy-related information to users. Considering this problem, we suggest the application of visceral interfaces that are designed to inform users about eye-tracking data within the VR experience. To this end, we designed and conducted a user study on three visceral interfaces to educate users about their eye-tracking data. Our results suggest that while certain visualizations can be distracting, participants ultimately found them informative and supported the development and availability of such interfaces even if they are not enabled by default or always enabled. Our research contributes to developing informative interfaces specific to eye tracking that promote transparency and privacy awareness in data collection for VR.","다양한 애플리케이션을 지원하기 위해 시선 추적이 가상 현실(VR) 장치에 점점 더 통합되고 있습니다. 이는 성능 최적화를 지원하고 적응형 훈련 또는 내러티브 경험을 생성하기 위한 상호 작용 방법으로 사용됩니다. 그러나 시선 추적 데이터에 대한 액세스를 제공하면 사용자 활동을 모니터링하고, 사용자의 생체 인식 신원을 감지 및 분류하거나, 건강 상태와 같은 민감한 정보를 공개하는 기능도 도입됩니다. 이 기술이 계속해서 발전함에 따라 사용자는 자신에 대해 개발자에게 공유하는 정보의 양과 이를 사용할 수 있는 방법을 인식해야 합니다. 기존 서비스 약관은 이러한 유형의 정보를 전달할 수 있지만 이전 연구에서는 개인 정보 보호 관련 정보를 사용자에게 쉽게 전달하지 못하는 것으로 나타났습니다. 이러한 문제를 고려하여 VR 경험 내에서 시선 추적 데이터를 사용자에게 알리도록 설계된 내장 인터페이스의 적용을 제안합니다. 이를 위해 우리는 시선 추적 데이터에 대해 사용자를 교육하기 위해 세 가지 내장 인터페이스에 대한 사용자 연구를 설계하고 수행했습니다. 우리의 결과는 특정 시각화가 주의를 산만하게 할 수 있지만 참가자는 궁극적으로 그것이 유익하다고 생각하고 기본적으로 활성화되지 않거나 항상 활성화되어 있는 경우에도 그러한 인터페이스의 개발 및 가용성을 지원했음을 시사합니다. 우리의 연구는 VR 데이터 수집의 투명성과 개인 정보 보호 인식을 촉진하는 시선 추적 관련 정보 인터페이스 개발에 기여합니다.",https://doi.org/10.1109/ISMAR62088.2024.00054,Interaction & Input; Privacy & Security,Eye / Gaze Tracking,User Study,Algorithm / Method
280,2024,Warm regards: Influence of thermal haptic feedback during social interactions in VR,따뜻한 안부: VR에서 사회적 상호작용 중 열적 햅틱 피드백이 미치는 영향,"In this paper, we study how thermal haptic feedback can influence social interactions in virtual environments, with an emphasis on persuasion, focus, co-presence, and friendliness. Physical and social warmth have been repeatedly linked in psychological literature, which allows for speculations on the effect of thermal haptics on virtual social interactions. To that effect, we conducted a study on thermal feedback during simulated social interactions with a virtual agent. We tested three conditions: warm, cool, and neutral. Results showed that warm feedback positively influenced users’ perception of the agent and significantly enhanced persuasion and thermal comfort. Multiple users reported the agent feeling less ‘robotic’ and more ‘human’ during the warm condition. Moreover, multiple studies have previously shown the potential of vibrotactile feedback for social interactions. A second study thus evaluated the combination of warmth and vibrations for social interactions. The study included the same protocol and three similar conditions: warmth, vibrations, and warm vibrations. Warmth was perceived as more friendly, while warm vibrations heightened the agent’s virtual presence and persuasion. These results encourage the study of thermal haptics to support positive social interactions. Moreover, they suggest that some haptic feedback are more suited to certain types of social interactions and communication than others.","본 논문에서는 설득, 집중력, 공존 및 친근감을 강조하면서 열적 햅틱 피드백이 가상 환경에서 사회적 상호 작용에 어떻게 영향을 미칠 수 있는지 연구합니다. 신체적, 사회적 따뜻함은 심리학 문헌에서 반복적으로 연결되어 있으며, 이를 통해 열적 촉각이 가상 사회적 상호 작용에 미치는 영향에 대해 추측할 수 있습니다. 이를 위해 우리는 가상 에이전트와의 시뮬레이션된 사회적 상호 작용 중 열 피드백에 대한 연구를 수행했습니다. 우리는 따뜻함, 시원함, 중립의 세 가지 조건을 테스트했습니다. 결과에 따르면 따뜻한 피드백은 에이전트에 대한 사용자의 인식에 긍정적인 영향을 미치고 설득력과 열적 편안함을 크게 향상시키는 것으로 나타났습니다. 여러 사용자는 따뜻한 조건에서 에이전트가 '로봇적' 느낌이 덜하고 '인간적' 느낌이 더 강하다고 보고했습니다. 더욱이, 여러 연구에서는 이전에 사회적 상호 작용에 대한 진동 촉각 피드백의 잠재력을 보여주었습니다. 따라서 두 번째 연구에서는 사회적 상호 작용에 대한 따뜻함과 진동의 조합을 평가했습니다. 이 연구에는 동일한 프로토콜과 세 가지 유사한 조건(따뜻함, 진동, 따뜻한 진동)이 포함되었습니다. 따뜻함은 더 친근한 것으로 인식되었으며, 따뜻한 진동은 에이전트의 가상 존재감과 설득력을 높였습니다. 이러한 결과는 긍정적인 사회적 상호 작용을 지원하기 위한 열 촉각 연구를 장려합니다. 더욱이 그들은 일부 햅틱 피드백이 다른 것보다 특정 유형의 사회적 상호 작용 및 의사 소통에 더 적합하다고 제안합니다.",https://doi.org/10.1109/ISMAR62088.2024.00138,Interaction & Input; Perception & Cognition,Haptic / Tactile Feedback,Simulation,User Study / Empirical Findings
281,2024,Watch Buddy: Evaluating the Impact of an Expressive Virtual Agent on Video Consumption Experience in Augmented Reality,Watch Buddy: 표현이 풍부한 가상 에이전트가 증강 현실의 비디오 소비 경험에 미치는 영향 평가,"The proliferation of personalized video content consumption, amplified by advances in virtual reality (VR) and augmented reality (AR), has introduced new paradigms in media engagement.This paper presents the development and evaluation of “Watch Buddy,” an expressive virtual agent designed to enhance the video-watching experience within an AR environment. The research assesses the impact of the agent’s presence and expressiveness on user satisfaction, technology acceptance, social presence, and emotional intimacy. A comparative user study with 30 participants reveals that the expressive agent significantly improves the viewing experience, as well as the perceived social connection between users and the agent, compared with an inexpressive agent and no agents at all. This paper contributes to VR/AR and human-agent interaction by demonstrating how virtual companions can influence media consumption and foster social connections, particularly for isolated demographics like the elderly or those living alone.","가상 현실(VR)과 증강 현실(AR)의 발전으로 증폭된 개인화된 비디오 콘텐츠 소비의 확산은 미디어 참여에 새로운 패러다임을 도입했습니다. 본 논문은 AR 환경 내에서 비디오 시청 경험을 향상시키기 위해 설계된 표현형 가상 에이전트인 ""Watch Buddy""의 개발 및 평가를 제시합니다. 이 연구에서는 에이전트의 존재감과 표현력이 사용자 만족도, 기술 수용도, 사회적 존재감, 정서적 친밀감에 미치는 영향을 평가합니다. 30명의 참가자를 대상으로 한 비교 사용자 연구에서는 표현형 에이전트가 에이전트가 전혀 없는 비표현형 에이전트에 비해 시청 경험은 물론 사용자와 에이전트 사이의 인식된 사회적 연결을 크게 향상시키는 것으로 나타났습니다. 이 논문은 가상 동반자가 미디어 소비에 어떻게 영향을 미치고 특히 노인이나 혼자 사는 사람들과 같은 고립된 인구통계의 사회적 연결을 촉진할 수 있는지 보여줌으로써 VR/AR 및 인간-에이전트 상호 작용에 기여합니다.",https://doi.org/10.1109/ISMAR62088.2024.00097,Perception & Cognition,Other,User Study,User Study / Empirical Findings
282,2024,Watch Out! XR Mobile Displays Improve the Experience of Co-Located VR Gaming Observers,조심하세요! XR 모바일 디스플레이는 같은 장소에 있는 VR 게임 관찰자의 경험을 향상시킵니다.,"Co-located social gaming with players and observers talking to each other is commonplace. However, it becomes less attractive when using virtual reality (VR): while VR increases players’ presence by immersing them in a virtual world, it also shuts out their observing friends. We investigate whether extended reality (XR) can improve the observer experience by giving them agency over their viewpoint in the game and personalizing the representation they see of the player. In a user study, 24 pairs of players and observers experienced co-located VR gaming. Observers either watched the player’s gameplay on a stationary screen i) from the player’s viewpoint or ii) controlling their viewpoint with a gamepad, or on an XR-enabled mobile screen controlling their viewpoint by moving the screen iii) with the player represented as a generic game avatar or iv) a personalized avatar obtained through live player footage augmentation. Results show that giving observers agency over their viewpoint with an XR-enabled mobile screen provides substantial benefits to presence and observers’ overall experience.","플레이어와 관찰자가 서로 대화하는 동일한 위치의 소셜 게임은 흔한 일입니다. 그러나 가상 현실(VR)을 사용하면 매력이 떨어집니다. VR은 플레이어를 가상 세계에 몰입시켜 플레이어의 존재감을 높이는 반면, 관찰하는 친구도 차단합니다. 우리는 확장 현실(XR)이 게임 내 관점에 대한 주체성을 부여하고 플레이어에 대해 보는 표현을 개인화함으로써 관찰자 경험을 향상시킬 수 있는지 조사합니다. 사용자 연구에서는 24쌍의 플레이어와 관찰자가 같은 장소에서 VR 게임을 경험했습니다. 관찰자는 i) 플레이어의 관점에서 또는 ii) 게임 패드로 자신의 관점을 제어하거나 XR 지원 모바일 화면에서 화면을 이동하여 관점을 제어하는 ​​고정 화면에서 플레이어의 게임 플레이를 시청했습니다. iii) 일반 게임 아바타로 표시된 플레이어와 함께 또는 iv) 라이브 플레이어 영상 확대를 통해 얻은 개인화된 아바타. 결과는 XR 지원 모바일 화면을 통해 관찰자에게 자신의 관점에 대한 주체성을 부여하는 것이 관찰자의 존재감과 전반적인 경험에 상당한 이점을 제공한다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR62088.2024.00139,Perception & Cognition; Collaboration & Social,Optical / Display Technology,User Study,User Study / Empirical Findings
283,2024,Wetness Illusion in Mid-Air,공중에 떠 있는 젖은 환상,"This study explores the impact of combining multiple senses on how people perceive wetness in mid-air. We examine how factors like temperature, pressure, and visual stimuli influence the illusion of wetness on users’ palms. The first user study examines these effects and the complex relationships between these variables. The findings suggest that increased temperature and pressure increase the likelihood of perceiving wetness. Interestingly, we note the influence of visual scene content on wetness perception. This leads to our second user study, which investigates how visual context impacts wetness perception. The results demonstrate a strong link between visual content and wetness perception, offering valuable insights for designing immersive virtual reality experiences that enhance sensory perception.","이 연구는 사람들이 공중에서 습기를 인지하는 방식에 대한 여러 감각의 결합이 미치는 영향을 탐구합니다. 우리는 온도, 압력, 시각적 자극과 같은 요인이 사용자 손바닥이 젖어 있다는 착각에 어떤 영향을 미치는지 조사합니다. The first user study examines these effects and the complex relationships between these variables. 연구 결과에 따르면 온도와 압력이 증가하면 젖음을 인지할 가능성이 높아집니다. 흥미롭게도 우리는 시각적 장면 내용이 젖음 인식에 미치는 영향에 주목합니다. 이는 시각적 맥락이 젖음 인식에 어떻게 영향을 미치는지 조사하는 두 번째 사용자 연구로 이어집니다. 결과는 시각적 콘텐츠와 젖음 인식 사이의 강력한 연관성을 보여주며, 감각 인식을 향상시키는 몰입형 가상 현실 경험을 디자인하는 데 귀중한 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR62088.2024.00100,Interaction & Input,Sensor Fusion,User Study,User Study / Empirical Findings
284,2024,Whirling Interface: Hand-based Motion Matching Selection for Small Target on XR Displays,소용돌이 인터페이스: XR 디스플레이의 작은 대상에 대한 수동 기반 모션 매칭 선택,"We introduce “Whirling Interface,” a selection method for XR displays using bare-hand motion matching gestures as an input technique. We extend the motion matching input method, by introducing different input states to provide visual feedback and guidance to the users. Using the wrist joint as the primary input modality, our technique reduces user fatigue and improves performance while selecting small and distant targets. In a study with 16 participants, we compared the whirling interface with a standard ray casting method using hand gestures. The results demonstrate that the Whirling Interface consistently achieves high success rates, especially for distant targets, averaging 95.58% with a completion time of 5.58 seconds. Notably, it requires a smaller camera sensing field of view of only 21.45° horizontally and 24.7° vertically. Participants reported lower workloads on distant conditions and expressed a higher preference for the Whirling Interface in general. These findings suggest that the Whirling Interface could be a useful alternative input method for XR displays with a small camera sensing FOV or when interacting with small targets.","맨손 모션 매칭 제스처를 입력 기법으로 활용한 XR 디스플레이 선택 방식 'Whirling Interface'를 소개합니다. 우리는 사용자에게 시각적 피드백과 안내를 제공하기 위해 다양한 입력 상태를 도입하여 모션 매칭 입력 방법을 확장합니다. 손목 관절을 기본 입력 양식으로 사용하는 우리의 기술은 작고 먼 대상을 선택하는 동안 사용자 피로를 줄이고 성능을 향상시킵니다. 16명의 참가자를 대상으로 한 연구에서 우리는 손 제스처를 사용하는 표준 레이 캐스팅 방법과 소용돌이 인터페이스를 비교했습니다. 결과는 Whirling 인터페이스가 특히 멀리 있는 대상에 대해 지속적으로 높은 성공률을 달성하여 5.58초의 완료 시간으로 평균 95.58%를 달성한다는 것을 보여줍니다. 특히 가로 21.45°, 세로 24.7°에 불과한 더 작은 카메라 감지 시야가 필요합니다. 참가자들은 멀리 떨어져 있는 조건에서 작업량이 더 적다고 보고했으며 전반적으로 Whirling 인터페이스에 대한 선호도가 더 높았습니다. 이러한 연구 결과는 Whirling 인터페이스가 FOV를 감지하는 작은 카메라가 있거나 작은 대상과 상호 작용할 때 XR 디스플레이에 유용한 대체 입력 방법이 될 수 있음을 시사합니다.",https://doi.org/10.1109/ISMAR62088.2024.00046,Interaction & Input,Hand / Gesture Recognition,User Study,Algorithm / Method; Interaction Technique
285,2024,"Working with Mixed Reality in Public: Effects of Virtual Display Layouts on Productivity, Feeling of Safety, and Social Acceptability","공공 장소에서 혼합 현실 작업: 가상 디스플레이 레이아웃이 생산성, 안전감 및 사회적 수용성에 미치는 영향","Nowadays, Mixed Reality (MR) headsets are a game-changer for knowledge work. Unlike stationary monitors, MR headsets allow users to work with large virtual displays anywhere they wear the headset, whether in a professional office, a public setting like a cafe, or a quiet space like a library. This study compares four different layouts (eye level-close, eye level-far, below eye level-close, below eye level-far) of virtual displays regarding feelings of safety, perceived productivity, and social acceptability when working with MR in public. We test which layout is most preferred by users and seek to understand which factors affect users’ layout preferences. The aim is to derive useful insights for designing better MR layouts. A field study in a public library was conducted using a within-subject design. While the participants interact with a layout, they are asked to work on a planning task. The results from a repeated measure ANOVA show a statistically significant effect on productivity but not on safety and social acceptability. Additionally, we report preferences expressed by the users regarding the layouts and using MR in public.","오늘날 혼합 현실(MR) 헤드셋은 지식 작업의 판도를 바꾸는 도구입니다. 고정식 모니터와 달리 MR 헤드셋을 사용하면 전문적인 사무실, 카페와 같은 공공 장소, 도서관과 같은 조용한 공간 등 헤드셋을 착용한 모든 곳에서 대형 가상 디스플레이를 사용하여 작업할 수 있습니다. 본 연구는 공공 장소에서 MR을 사용할 때 안전감, 인지된 생산성 및 사회적 수용성과 관련하여 가상 디스플레이의 네 가지 다른 레이아웃(눈높이-가까움, 눈높이-멀리, 눈높이 아래-가까움, 눈높이 아래-멀리)을 비교합니다. 우리는 사용자가 어떤 레이아웃을 가장 선호하는지 테스트하고 어떤 요소가 사용자의 레이아웃 선호도에 영향을 미치는지 이해하려고 노력합니다. 목표는 더 나은 MR 레이아웃을 설계하기 위한 유용한 통찰력을 얻는 것입니다. 공공 도서관에서의 현장 연구는 주제 내 설계를 사용하여 수행되었습니다. 참가자는 레이아웃과 상호 작용하는 동안 계획 작업을 수행하라는 요청을 받습니다. 반복 측정 ANOVA의 결과는 생산성에 통계적으로 유의미한 영향을 주지만 안전성과 사회적 수용성에는 영향을 미치지 않는 것으로 나타났습니다. 또한 레이아웃 및 공개 MR 사용과 관련하여 사용자가 표현한 선호도를 보고합니다.",https://doi.org/10.1109/ISMAR62088.2024.00089,Interaction & Input,Optical / Display Technology,User Study,System / Framework
286,2024,XR Prototyping of Mixed Reality Visualizations: Compensating Interaction Latency for a Medical Imaging Robot,혼합 현실 시각화의 XR 프로토타이핑: 의료 영상 로봇의 상호 작용 지연 시간 보상,"Researching novel user experiences in medicine is challenging due to limited access to equipment and strict ethical protocols. Extended Reality (XR) simulation technologies offer a cost-and time-efficient solution for developing interactive systems. Recent work has shown Extended Reality Prototyping (XRP)’s potential, but its applicability to specific domains like controlling complex machinery needs further exploration. This paper explores the benefits and limitations of XRP in controlling a mobile medical imaging robot. We compare two XR visualization techniques to reduce perceived latency between user input and robot activation. Our XRP validation study demonstrates its potential for comparative studies, but identifies a gap in modeling human behavior in the analytic XRP validation framework.",의학 분야에서 새로운 사용자 경험을 연구하는 것은 장비에 대한 제한된 접근과 엄격한 윤리적 프로토콜로 인해 어려운 일입니다. 확장 현실(XR) 시뮬레이션 기술은 대화형 시스템 개발을 위한 비용 및 시간 효율적인 솔루션을 제공합니다. 최근 연구에서는 확장 현실 프로토타이핑(XRP)의 잠재력이 입증되었지만 복잡한 기계 제어와 같은 특정 영역에 적용하려면 추가 조사가 필요합니다. 이 문서에서는 모바일 의료 영상 로봇을 제어할 때 XRP의 이점과 한계를 살펴봅니다. 우리는 사용자 입력과 로봇 활성화 사이의 인지된 대기 시간을 줄이기 위해 두 가지 XR 시각화 기술을 비교합니다. 우리의 XRP 검증 연구는 비교 연구의 잠재력을 보여 주지만 분석 XRP 검증 프레임워크에서 인간 행동 모델링의 격차를 식별합니다.,https://doi.org/10.1109/ISMAR62088.2024.00014,Interaction & Input,Sensor Fusion,Quantitative Experiment,System / Framework; Algorithm / Method
287,2024,XaiR: An XR Platform that Integrates Large Language Models with the Physical World,XaiR: 대규모 언어 모델을 실제 세계와 통합하는 XR 플랫폼,"This paper discusses the integration of Multimodal Large Language Models (MLLMs) with Extended Reality (XR) headsets, focusing on enhancing machine understanding of physical spaces. By combining the contextual capabilities of MLLMs with the sensory inputs from XR, there is potential for more intuitive spatial interactions. However, the integration faces challenges due to the inherent limitations of MLLMs in processing 3D inputs and their significant resource demands for XR headsets. We introduce XaiR, a platform that facilitates integrating MLLMs with XR applications. XaiR uses a split architecture that offloads complex MLLM operations to a server while handling 3D world processing on the headset. This setup manages multiple input modalities, parallel models, and links them with real-time pose data, improving AR content placement in physical scenes. We tested XaiR’s effectiveness with a “cognitive assistant” application that guides users through tasks like making coffee or assembling furniture. Results from a 15-participant study shows over 90% accuracy in task guidance and 85% accuracy in AR content anchoring. Additionally, we evaluate MLLMs against human operators for cognitive assistant tasks which provides insights into the quality of the captured data as well as the current gap in performance for cognitive assistant tasks.","이 문서에서는 MLLM(Multimodal Large Language Model)과 XR(확장 현실) 헤드셋의 통합에 대해 논의하고 물리적 공간에 대한 기계 이해를 높이는 데 중점을 둡니다. MLLM의 상황별 기능과 XR의 감각 입력을 결합하면 보다 직관적인 공간 상호 작용이 가능해집니다. 그러나 3D 입력 처리 시 MLLM의 본질적인 한계와 XR 헤드셋에 대한 상당한 리소스 요구로 인해 통합에 어려움이 있습니다. MLLM과 XR 애플리케이션의 통합을 용이하게 하는 플랫폼인 XaiR을 소개합니다. XaiR은 헤드셋에서 3D 세계 처리를 처리하는 동시에 복잡한 MLLM 작업을 서버로 오프로드하는 분할 아키텍처를 사용합니다. 이 설정은 여러 입력 양식, 병렬 모델을 관리하고 이를 실시간 포즈 데이터와 연결하여 실제 장면에서 AR 콘텐츠 배치를 개선합니다. 우리는 커피 만들기나 가구 조립과 같은 작업을 사용자에게 안내하는 ""인지 보조자"" 애플리케이션을 통해 XaiR의 효율성을 테스트했습니다. 15명이 참여한 연구 결과는 작업 안내의 정확도가 90% 이상, AR 콘텐츠 앵커링의 정확도가 85%를 넘는 것으로 나타났습니다. 또한 캡처된 데이터의 품질과 인지 보조 작업 성능의 현재 격차에 대한 통찰력을 제공하는 인지 보조 작업에 대해 인간 운영자에 대해 MLLM을 평가합니다.",https://doi.org/10.1109/ISMAR62088.2024.00091,Interaction & Input; Perception & Cognition,Deep Learning / Neural Networks,Quantitative Experiment,System / Framework; Hardware / Device
288,2024,"Your Face, Your Anatomy: Flashcard Lenses Enriched with Knowledge Maps for Anatomy Education","당신의 얼굴, 당신의 해부학: 해부학 교육을 위한 지식 지도가 풍부한 플래시카드 렌즈","Traditional anatomy flashcards, with their recognizable static illustrations on the front side and comprehensive lists of concepts on the back, are a long-standing tool for memorizing and refreshing anatomical concepts. This study repurposes such established tool by introducing two key elements: (i) Augmented Reality (AR) lenses acting as magic mirrors enabling users to view anatomical illustrations mapped onto their own faces, and (ii) a knowledge map layer acting as the card’s backside to visually and explicitly illustrate conceptual connections between anatomical reference points. Using Snapchat’s Lens Studio, we crafted a deck of interactive facial anatomy flashcards to assess the potential of AR and knowledge maps for retaining and refreshing anatomical concepts. We conducted a user study involving 44 university-level students. Divided into two groups, participants utilized either flashcard lenses with knowledge maps or traditional flashcards to quickly grasp and refresh anatomical concepts. By employing an approach that integrates anatomical quizzes for objective assessment with surveys and interviews for subjective feedback, our results indicate that anatomy flashcard lenses with knowledge maps offer a more engaging educational experience, yielding higher user preferences and satisfaction levels compared to traditional flashcards. While both approaches showed similar effectiveness in quiz scores, anatomy flashcard lenses with knowledge maps were favored for their usability, significantly reducing temporal demand. These findings underscore the engaging and effective nature of anatomy flashcard lenses with knowledge maps, highlighting them as an alternative tool for the quick retention and review of anatomical concepts.","앞면에는 눈에 띄는 정적인 그림이 있고 뒷면에는 포괄적인 개념 목록이 있는 전통적인 해부학 플래시카드는 해부학적 개념을 암기하고 새로운 생각을 불러일으키는 데 오랫동안 사용되어 온 도구입니다. 이 연구는 두 가지 주요 요소, 즉 (i) 사용자가 자신의 얼굴에 매핑된 해부학적 그림을 볼 수 있도록 하는 마법의 거울 역할을 하는 증강 현실(AR) 렌즈와 (ii) 해부학적 기준점 사이의 개념적 연결을 시각적이고 명시적으로 설명하기 위해 카드의 뒷면 역할을 하는 지식 맵 레이어라는 두 가지 핵심 요소를 도입하여 이러한 기존 도구의 용도를 변경합니다. Snapchat의 Lens Studio를 사용하여 우리는 해부학적 개념을 유지하고 새로 고치기 위한 AR 및 지식 맵의 잠재력을 평가하기 위해 대화형 안면 해부학 플래시카드 덱을 제작했습니다. 우리는 44명의 대학생을 대상으로 사용자 연구를 실시했습니다. 두 그룹으로 나누어 참가자들은 지식 지도가 있는 플래시카드 렌즈나 기존 플래시카드를 활용하여 해부학적 개념을 빠르게 파악하고 새로 고쳤습니다. 객관적인 평가를 위한 해부학적 퀴즈와 주관적인 피드백을 위한 설문 조사 및 인터뷰를 통합한 접근 방식을 채택함으로써 우리의 결과는 지식 맵이 있는 해부학 플래시카드 렌즈가 기존 플래시카드에 비해 더 높은 사용자 선호도와 만족도를 제공하면서 더욱 매력적인 교육 경험을 제공한다는 것을 나타냅니다. 두 접근 방식 모두 퀴즈 점수에서 비슷한 효율성을 보였지만 지식 맵이 있는 해부학 플래시카드 렌즈는 유용성 측면에서 선호되어 시간적 요구를 크게 줄였습니다. 이러한 발견은 지식 맵이 포함된 해부학 플래시카드 렌즈의 매력적이고 효과적인 특성을 강조하며, 해부학적 개념을 빠르게 유지하고 검토할 수 있는 대체 도구임을 강조합니다.",https://doi.org/10.1109/ISMAR62088.2024.00064,Education & Training,Optical / Display Technology,User Study,User Study / Empirical Findings; Survey / Review
289,2023,"""Can You Handle the Truth?"": Investigating the Effects of AR-Based Visualization of the Uncertainty of Deep Learning Models on Users of Autonomous Vehicles","""진실을 감당할 수 있습니까?"": AR 기반 시각화가 자율주행차 사용자에게 딥러닝 모델의 불확실성에 미치는 영향 조사","The recent advances in deep learning have paved the way for autonomous vehicles (AVs) to take charge of more complex tasks in the navigation process. However, predictions of deep learning models are subject to different types of uncertainty that may put the user and the surrounding environment in danger. In this paper, we investigate the effects that AR-based visualizations of 3 types of uncertainties in deep learning modules for path planning in AVs may have on drivers. The uncertainty types of the deep learning models that we consider are: the waypoint uncertainty, the situation uncertainty, and the path uncertainty. We propose 3 concepts to visualize the 3 uncertainty types on a Windshield display. We evaluate our AR-based concepts with a user study $(\mathrm{N}=20)$ using a VR-based immersive environment, to ensure the security of the participants. The results of our evaluation reveal that the absence of uncertainty visualization leads to lower driver engagement. More importantly, the combination of situation uncertainty and path uncertainty visualizations leads to higher driver engagement, and higher trust in the automated vehicle, while inducing an acceptable mental load for the drive.","최근 딥 러닝의 발전으로 자율주행차(AV)가 내비게이션 프로세스에서 더 복잡한 작업을 담당할 수 있는 기반이 마련되었습니다. 그러나 딥러닝 모델의 예측에는 사용자와 주변 환경을 위험에 빠뜨릴 수 있는 다양한 유형의 불확실성이 적용됩니다. 본 논문에서는 AV의 경로 계획을 위한 딥 러닝 모듈의 3가지 유형의 불확실성에 대한 AR 기반 시각화가 운전자에게 미칠 수 있는 영향을 조사합니다. 우리가 고려하는 딥러닝 모델의 불확실성 유형은 경로 불확실성, 상황 불확실성, 경로 불확실성이 있습니다. 우리는 앞 유리 디스플레이에 3가지 불확실성 유형을 시각화하기 위한 3가지 개념을 제안합니다. 참가자의 보안을 보장하기 위해 VR 기반 몰입형 환경을 사용한 사용자 연구 $(\mathrm{N}=20)$를 통해 AR 기반 개념을 평가합니다. 평가 결과, 불확실성 시각화가 없으면 운전자 참여도가 낮아지는 것으로 나타났습니다. 더 중요한 것은 상황 불확실성과 경로 불확실성 시각화의 조합이 운전자의 참여도를 높이고 자동화된 차량에 대한 신뢰를 높이는 동시에 운전에 허용 가능한 정신적 부하를 유도한다는 것입니다.",https://doi.org/10.1109/ISMAR59233.2023.00040,Privacy & Security,Deep Learning / Neural Networks,User Study,User Study / Empirical Findings; Algorithm / Method
290,2023,"""Can You Move It?"": The Design and Evaluation of Moving VR Shots in Sport Broadcast","""움직일 수 있나요?"": 스포츠 방송에서 움직이는 VR 장면의 설계 및 평가","Virtual Reality (VR) broadcasting has seen widespread adoption in major sports events, attributed to its ability to generate a sense of presence, curiosity, and excitement among viewers. However, we have noticed that still shots reveal a limitation in the movement of VR cameras and hinder the VR viewing experience in current VR sports broadcasts. This paper aims to bridge this gap by engaging in a quantitative user analysis to explore the design and impact of dynamic VR shots on viewing experiences. We conducted two user studies in a digital hockey game twin environment and asked participants to evaluate their viewing experience through two questionnaires. Our findings suggested that the viewing experiences demonstrated no notable disparity between still and moving shots for single clips. However, when considering entire events, moving shots improved the viewer’s immersive experience, with no notable increase in sickness compared to still shots. We further discuss the benefits of integrating moving shots into VR sports broadcasts and present a set of design considerations and potential improvements for future VR sports broadcasting.","가상현실(VR) 방송은 시청자들에게 현장감, 호기심, 흥미를 불러일으키는 능력으로 인해 주요 스포츠 행사에서 널리 채택되었습니다. 그러나 우리는 스틸 샷이 VR 카메라의 움직임에 한계를 드러내고 현재 VR 스포츠 방송의 VR 시청 경험을 방해한다는 사실을 알아냈습니다. 본 논문은 다이내믹한 VR 샷이 시청 경험에 미치는 영향과 디자인을 탐구하기 위해 정량적 사용자 분석을 통해 이러한 격차를 해소하는 것을 목표로 합니다. 우리는 디지털 하키 게임 트윈 환경에서 두 가지 사용자 연구를 수행하고 참가자들에게 두 가지 설문지를 통해 시청 경험을 평가하도록 요청했습니다. 우리의 연구 결과에 따르면 시청 환경에서는 단일 클립의 스틸 샷과 움직이는 샷 간에 눈에 띄는 차이가 없는 것으로 나타났습니다. 그러나 전체 이벤트를 고려할 때 무빙 샷은 스틸 샷에 비해 멀미가 눈에 띄게 증가하지 않고 시청자의 몰입도를 향상시켰습니다. 움직이는 장면을 VR 스포츠 방송에 통합함으로써 얻을 수 있는 이점에 대해 더 자세히 논의하고 미래 VR 스포츠 방송을 위한 일련의 디자인 고려 사항과 잠재적인 개선 사항을 제시합니다.",https://doi.org/10.1109/ISMAR59233.2023.00099,Perception & Cognition,Other,User Study,User Study / Empirical Findings
291,2023,"""If It's Not Me It Doesn't Make a Difference"" - The Impact of Avatar Personalization on user Experience and Body Awareness in Virtual Reality","""내가 아니면 차이가 나지 않습니다"" - 아바타 개인화가 가상 현실에서 사용자 경험과 신체 인식에 미치는 영향","Body awareness is relevant for the efficacy of psychotherapy. However, previous work on virtual reality (VR) and avatar-assisted therapy has often overlooked it. We investigated the effect of avatar individualization on body awareness in the context of VR-specific user experience, including sense of embodiment (SoE), plausibility, and sense of presence (SoP). In a between-subject design, 86 participants embodied three avatar types and engaged in VR movement exercises. The avatars were (1) generic and gender-matched, (2) customized from a set of pre-existing options, or (3) personalized photorealistic scans. Compared to the other conditions, participants with personalized avatars reported increased SoE, yet higher eeriness and reduced body awareness. Further, SoE and SoP positively correlated with body awareness across conditions. Our results indicate that VR user experience and body awareness do not always dovetail and do not necessarily predict each other. Future research should work towards a balance between body awareness and SoE.","신체 인식은 심리치료의 효능과 관련이 있습니다. 그러나 가상 현실(VR)과 아바타 지원 치료에 대한 이전 연구에서는 종종 이를 간과했습니다. 우리는 구체화 감각(SoE), 타당성, 존재감(SoP)을 포함한 VR 관련 사용자 경험의 맥락에서 아바타 개별화가 신체 인식에 미치는 영향을 조사했습니다. 주제 간 디자인에서는 86명의 참가자가 세 가지 아바타 유형을 구현하고 VR 동작 연습에 참여했습니다. 아바타는 (1) 일반적이고 성별이 일치하는 아바타, (2) 기존 옵션 세트에서 맞춤화된 아바타, 또는 (3) 개인화되고 사실적인 스캔이었습니다. 다른 조건과 비교하여 개인화된 아바타를 가진 참가자는 SoE가 증가했지만 으스스함이 더 높고 신체 인식이 감소했다고 보고했습니다. 또한 SoE와 SoP는 조건 전반에 걸쳐 신체 인식과 긍정적인 상관관계가 있었습니다. 우리의 결과는 VR 사용자 경험과 신체 인식이 항상 일치하는 것은 아니며 반드시 서로를 예측하지 않는다는 것을 나타냅니다. 향후 연구는 신체 인식과 SoE 사이의 균형을 향해 노력해야 합니다.",https://doi.org/10.1109/ISMAR59233.2023.00063,Perception & Cognition,Other,User Study,User Study / Empirical Findings
292,2023,3D Selection in Mixed Reality: Designing a Two-Phase Technique To Reduce Fatigue,혼합 현실의 3D 선택: 피로를 줄이기 위한 2단계 기술 설계,"Mid-air pointing is widely used for 3D selection in Mixed Reality but leads to arm fatigue. In a first exploratory experiment we study a two-phase design and compare modalities for each phase: mid-air gestures, eye-gaze and microgestures. Results suggest that eye-gaze and microgestures are good candidates to reduce fatigue and improve interaction speed. We therefore propose two 3D selection techniques: Look&MidAir and Look&Micro. Both techniques include a first phase during which users control a cone directed along their eye-gaze. Using the flexion of their non-dominant hand index finger, users pre-select the objects intersecting this cone. If several objects are pre-selected, a disambiguation phase is performed using direct mid-air touch for Look&MidAir or thumb to finger microgestures for Look&Micro. In a second study, we compare both techniques to the standard raycasting technique. Results show that Look&MidAir and Look&Micro perform similarly. However they are 55% faster, perceived easier to use and are less tiring than the baseline. We discuss how the two techniques could be combined for greater flexibility and for object manipulation after selection.","공중 포인팅은 혼합 현실에서 3D 선택에 널리 사용되지만 팔 피로를 유발합니다. 첫 번째 탐색 실험에서 우리는 2단계 설계를 연구하고 각 단계의 양식(공중 제스처, 시선 및 마이크로 제스처)을 비교합니다. 결과는 시선과 미세 제스처가 피로를 줄이고 상호 작용 속도를 향상시키는 데 좋은 후보임을 시사합니다. 따라서 우리는 Look&MidAir와 Look&Micro라는 두 가지 3D 선택 기술을 제안합니다. 두 기술 모두 사용자가 시선을 따라 향하는 원뿔을 제어하는 ​​첫 번째 단계를 포함합니다. 사용자는 주로 쓰지 않는 손 검지의 굴곡을 사용하여 이 원뿔과 교차하는 개체를 미리 선택합니다. 여러 개체가 미리 선택된 경우 Look&MidAir의 경우 직접 공중 터치를 사용하거나 Look&Micro의 경우 엄지손가락에서 손가락으로의 마이크로 제스처를 사용하여 명확화 단계가 수행됩니다. 두 번째 연구에서는 두 기술을 표준 레이캐스팅 기술과 비교합니다. 결과는 Look&MidAir와 Look&Micro가 유사한 성능을 보이는 것으로 나타났습니다. 그러나 기본보다 55% 더 빠르고, 사용하기 더 쉽고 피로도가 적습니다. 유연성을 높이고 선택 후 개체 조작을 위해 두 가지 기술을 결합하는 방법에 대해 논의합니다.",https://doi.org/10.1109/ISMAR59233.2023.00095,Interaction & Input,Other,Technical Evaluation,Algorithm / Method
293,2023,A Closer Look at Dynamic Medical Visualization Techniques,동적 의료 시각화 기술 자세히 살펴보기,"In navigated surgery, physicians perform complex tasks assisted by virtual representations of anatomical structures and surgical tools. Integrating Augmented Reality (AR) in these scenarios enriches the information presented to the surgeon through a range of visualization techniques. Their selection is a crucial task as they represent the primary interface between the system and the surgeon.In this work, we present a novel approach to conveying augmented content using dynamic visualization techniques, allowing users to gather depth and shape information from both pictorial and kinetic cues. We conducted user studies comparing two novel dynamic methods – Object Flow and Wave Propagation – and three state-of-the-art static visualization techniques among medical experts. Our studies provide a detailed comparison of the visualization techniques’ efficacy in conveying shape and depth information from medical data, as well as task load and usability reported by the participants and post hoc analyses. We found that kinetic cues can assist users in understanding complex anatomical structures in medical AR.",내비게이션 수술에서 의사는 해부학적 구조와 수술 도구의 가상 표현을 통해 복잡한 작업을 수행합니다. 이러한 시나리오에 증강 현실(AR)을 통합하면 다양한 시각화 기술을 통해 외과 의사에게 제공되는 정보가 풍부해집니다. 이들의 선택은 시스템과 외과 의사 사이의 주요 인터페이스를 나타내기 때문에 중요한 작업입니다. 이 작업에서 우리는 동적 시각화 기술을 사용하여 증강된 콘텐츠를 전달하는 새로운 접근 방식을 제시하여 사용자가 그림과 운동 단서 모두에서 깊이와 모양 정보를 수집할 수 있도록 합니다. 우리는 의학 전문가들 사이에서 두 가지 새로운 동적 방법인 객체 흐름(Object Flow)과 파동 전파(Wave Propagation)와 세 가지 최첨단 정적 시각화 기술을 비교하는 사용자 연구를 수행했습니다. 우리의 연구는 의료 데이터의 모양과 깊이 정보를 전달하는 시각화 기술의 효율성뿐만 아니라 참가자가 보고한 작업 부하 및 유용성과 사후 분석을 자세히 비교합니다. 우리는 운동 신호가 사용자가 의료 AR의 복잡한 해부학적 구조를 이해하는 데 도움이 될 수 있음을 발견했습니다.,https://doi.org/10.1109/ISMAR59233.2023.00024,Medical & Healthcare,Optical / Display Technology,Technical Evaluation,Algorithm / Method; User Study / Empirical Findings
294,2023,A Comparative Evaluation of AR Embodiments vs. Videos and Figures for Learning Bead Weaving,비드 위빙 학습을 위한 AR 구현과 비디오 및 그림의 비교 평가,"The most common learning materials for handcraft today are videos and figures, which are limited in their ability to express embodied knowledge as an in-person tutor could. We developed WeavAR, an application for headworn augmented reality (AR) displays designed to teach basic bead weaving patterns. WeavAR combines virtual 3D hands showing weaving sequences recorded from an experienced bead weaver and a dynamic 3D bead model showing how the work progresses. Using a mixed within/between-subjects user study (n=30), we compared learning materials (AR to videos and figures) and learning material placement (in the area of work or to the side). Results show that the AR learning materials had comparable effectiveness to video and figures. Hand visualizations were found to lack crucial context, however, making them less useful than the 3D bead model. Extra measures to prevent obstruction are required when placing learning materials at the area of work.","오늘날 가장 일반적인 수공예 학습 자료는 비디오와 그림으로, 직접 교사가 할 수 있는 것처럼 구체화된 지식을 표현하는 능력이 제한되어 있습니다. 우리는 기본적인 비드 직조 패턴을 가르치기 위해 설계된 머리 착용형 증강 현실(AR) 디스플레이용 애플리케이션인 WeavAR을 개발했습니다. WeavAR은 숙련된 비드 직공이 기록한 직조 시퀀스를 보여주는 가상 3D 손과 작업 진행 방식을 보여주는 동적 3D 비드 모델을 결합합니다. 주제 내/과목 간 혼합 사용자 연구(n=30)를 사용하여 학습 자료(AR에서 비디오 및 그림까지)와 학습 자료 배치(작업 영역 또는 측면에서)를 비교했습니다. 결과는 AR 학습 자료가 비디오 및 그림과 동등한 효율성을 가지고 있음을 보여줍니다. 그러나 손 시각화에는 중요한 맥락이 부족하여 3D 비드 모델보다 유용성이 떨어지는 것으로 나타났습니다. 작업 영역에 학습 자료를 배치할 때 방해를 방지하기 위한 추가 조치가 필요합니다.",https://doi.org/10.1109/ISMAR59233.2023.00136,Rendering & Visualization,Optical / Display Technology,User Study,Algorithm / Method
295,2023,A Deep Cybersickness Predictor through Kinematic Data with Encoded Physiological Representation,인코딩된 생리학적 표현이 포함된 운동학적 데이터를 통한 심층 사이버 질병 예측기,"Users would experience individually different sickness symptoms during or after navigating through an immersive virtual environment, generally known as cybersickness. Previous studies have predicted the severity of cybersickness based on physiological and/or kinematic data. However, compared with kinematic data, physiological data rely heavily on biosensors during the collection, which is inconvenient and limited to a few affordable VR devices. In this work, we proposed a deep neural network to predict cybersickness through kinematic data. We introduced the encoded physiological representation to characterize the individual susceptibility; therefore, the predictor could predict cybersickness only based on a user’s kinematic data without counting on biosensors. Fifty-three participants were recruited to attend the user study to collect multimodal data, including kinematic data (navigation speed, head tracking), physiological signals (e.g., electrodermal activity, heart rate), and Simulator Sickness Questionnaire (SSQ). The predictor achieved an accuracy of 97.8% for cybersickness prediction by involving the pre-computed physiological representation to characterize individual differences, providing much convenience for the current cybersickness measurement.","사용자는 일반적으로 사이버멀미라고 알려진 몰입형 가상 환경을 탐색하는 동안이나 탐색한 후에 개별적으로 다른 질병 증상을 경험하게 됩니다. 이전 연구에서는 생리학적 및/또는 운동학적 데이터를 기반으로 사이버 멀미의 심각도를 예측했습니다. 그러나 운동학적 데이터에 비해 생리학적 데이터는 수집 중에 바이오센서에 크게 의존하므로 불편하고 저렴한 VR 장치 몇 개로 제한됩니다. 본 연구에서는 운동학적 데이터를 통해 사이버 멀미를 예측하는 심층 신경망을 제안했습니다. 우리는 개인의 감수성을 특성화하기 위해 인코딩된 생리학적 표현을 도입했습니다. 따라서 예측자는 바이오센서를 사용하지 않고 사용자의 운동학적 데이터만을 기반으로 사이버 멀미를 예측할 수 있습니다. 운동학적 데이터(탐색 속도, 머리 추적), 생리학적 신호(예: 피부 전기 활동, 심박수) 및 시뮬레이터 질병 설문지(SSQ)를 포함한 다중 모드 데이터를 수집하기 위해 사용자 연구에 참석하기 위해 53명의 참가자를 모집했습니다. 예측기는 개인차를 특성화하기 위해 미리 계산된 생리적 표현을 포함함으로써 사이버 멀미 예측에 대해 97.8%의 정확도를 달성하여 현재 사이버 멀미 측정에 많은 편의성을 제공합니다.",https://doi.org/10.1109/ISMAR59233.2023.00130,Interaction & Input; Perception & Cognition,Deep Learning / Neural Networks,User Study; Questionnaire / Survey,Algorithm / Method; Hardware / Device
296,2023,A Mixed Reality Training System for Hand-Object Interaction in Simulated Microgravity Environments,시뮬레이션된 미세 중력 환경에서 손-물체 상호 작용을 위한 혼합 현실 훈련 시스템,"As human exploration of space continues to progress, the use of Mixed Reality (MR) for simulating microgravity environments and facilitating training in hand-object interaction holds immense practical significance. However, hand-object interaction in microgravity presents distinct challenges compared to terrestrial environments due to the absence of gravity. This results in heightened agility and inherent unpredictability of movements that traditional methods struggle to simulate accurately. To this end, we propose a novel MR-based hand-object interaction system in simulated microgravity environments, leveraging physics-based simulations to enhance the interaction between the user’s real hand and virtual objects. Specifically, we introduce a physics-based hand-object interaction model that combines impulse-based simulation with penetration contact dynamics. This accurately captures the intricacies of hand-object interaction in microgravity. By considering forces and impulses during contact, our model ensures realistic collision responses and enables effective object manipulation in the absence of gravity. The proposed system presents a cost-effective solution for users to simulate object manipulation in microgravity. It also holds promise for training space travelers, equipping them with greater immersion to better adapt to space missions. The system reliability and fidelity test verifies the superior effectiveness of our system compared to the state-of-the-art CLAP system.","인간의 우주 탐험이 계속 진행됨에 따라 미세 중력 환경을 시뮬레이션하고 손과 물체의 상호 작용에 대한 훈련을 촉진하기 위해 혼합 현실(MR)을 사용하는 것은 엄청난 실질적인 의미를 갖습니다. 그러나 미세 중력에서 손과 물체의 상호 작용은 중력이 없기 때문에 지상 환경에 비해 뚜렷한 문제를 제시합니다. 이로 인해 기존 방법으로 정확하게 시뮬레이션하기 어려웠던 움직임의 민첩성과 본질적인 예측 불가능성이 높아졌습니다. 이를 위해 우리는 시뮬레이션된 미세중력 환경에서 사용자의 실제 손과 가상 객체 간의 상호 작용을 향상시키기 위해 물리 기반 시뮬레이션을 활용하는 새로운 MR 기반 손-객체 상호 작용 시스템을 제안합니다. 구체적으로, 충격 기반 시뮬레이션과 침투 접촉 역학을 결합한 물리 기반 손-객체 상호 작용 모델을 소개합니다. 이는 미세 중력에서 손과 물체의 상호 작용의 복잡성을 정확하게 포착합니다. 접촉 중 힘과 충격을 고려함으로써 우리 모델은 현실적인 충돌 반응을 보장하고 중력이 없을 때 효과적인 물체 조작을 가능하게 합니다. 제안된 시스템은 사용자가 미세 중력에서 물체 조작을 시뮬레이션할 수 있는 비용 효율적인 솔루션을 제시합니다. 또한 우주 여행자를 훈련시켜 우주 임무에 더 잘 적응할 수 있도록 몰입도를 높여줄 가능성도 있습니다. 시스템 신뢰성 및 충실도 테스트를 통해 최첨단 CLAP 시스템과 비교하여 당사 시스템의 탁월한 효율성을 검증합니다.",https://doi.org/10.1109/ISMAR59233.2023.00031,Interaction & Input; Education & Training,Sensor Fusion,Technical Evaluation; Simulation,Algorithm / Method
297,2023,A Systematic Evaluation of Incongruencies and Their Influence on Plausibility in Virtual Reality,불일치에 대한 체계적 평가와 가상 현실의 타당성에 미치는 영향,"Currently, there is an ongoing debate about the influencing factors of one’s extended reality (XR) experience. Plausibility, congruence, and their role have recently gained more and more attention. One of the latest models to describe XR experiences, the Congruence and Plausibility model (CaP), puts plausibility and congruence right in the center. However, it is unclear what influence they have on the overall XR experience and what influences our perceived plausibility rating. In this paper, we implemented four different incongruencies within a virtual reality scene using breaks in plausibility as an analogy to breaks in presence. These manipulations were either located on the cognitive or perceptual layer of the CaP model. They were also either connected to the task at hand or not. We tested these manipulations in a virtual bowling environment to see which influence they had. Our results show that manipulations connected to the task caused a lower perceived plausibility. Additionally, cognitive manipulations seem to have a larger influence than perceptual manipulations. We were able to cause a break in plausibility with one of our incongruencies. These results show a first direction on how the influence of plausibility in XR can be systematically investigated in the future.","현재 확장 현실(XR) 경험에 영향을 미치는 요소에 대한 지속적인 논쟁이 있습니다. 타당성, 일치성 및 이들의 역할은 최근 점점 더 많은 주목을 받고 있습니다. XR 경험을 설명하는 최신 모델 중 하나인 합동 및 타당성 모델(CaP)은 타당성과 합동을 중심에 둡니다. 그러나 전반적인 XR 경험에 어떤 영향을 미치는지, 그리고 인지된 타당성 등급에 어떤 영향을 미치는지는 불분명합니다. 본 논문에서는 존재감의 중단과 유사하게 타당성의 중단을 사용하여 가상 현실 장면 내에서 네 가지 서로 다른 불일치를 구현했습니다. 이러한 조작은 CaP 모델의 인지 또는 지각 계층에 위치했습니다. 그들은 또한 당면한 작업에 연결되어 있거나 연결되어 있지 않았습니다. 우리는 가상 볼링 환경에서 이러한 조작을 테스트하여 어떤 영향을 미치는지 확인했습니다. 우리의 결과는 작업과 관련된 조작으로 인해 인지된 타당성이 낮아졌다는 것을 보여줍니다. 또한 인지 조작은 지각 조작보다 더 큰 영향을 미치는 것으로 보입니다. 우리는 불일치 중 하나로 타당성을 깨뜨릴 수 있었습니다. 이러한 결과는 향후 XR에서 타당성의 영향을 체계적으로 조사할 수 있는 방법에 대한 첫 번째 방향을 보여줍니다.",https://doi.org/10.1109/ISMAR59233.2023.00105,Perception & Cognition,Other,Other,System / Framework
298,2023,A Systematic Review of Immersive Technologies for Physical Training in Fitness and Sports,피트니스 및 스포츠 신체 훈련을 위한 몰입형 기술의 체계적 검토,"The increased availability of immersive headsets in the games industry has promoted wide adoption of immersive technology amongst consumers. The benefits of spatial freedom and agency for body movements in virtual reality pave the way for everyday fitness and physical training applications. We conducted a systematic review through ACM Digital Library, IEEEXplore, and Scopus, to investigate how immersive technologies have been applied for physical training in fitness and sports. Our review included all peer-reviewed papers, published and written in English, discussing all forms of hardware for VR, AR, and MR technologies, targeted towards healthy population. We excluded applications for clinical populations and treatment of specific diseases, and all non-peer reviewed articles like position papers, workshops, tutorials, and magazine columns. From the shortlisted 144 papers, we summarize the development trends of immersive technologies and highlight the challenges of designing immersive technologies for everyday fitness. We also provide recommendations for future work and highlight the need to support better collaboration with industry partners, trainer-trainee experiences, and social dynamics of sports for designing better experiences.","게임 산업에서 몰입형 헤드셋의 가용성이 높아짐에 따라 소비자들 사이에서 몰입형 기술의 폭넓은 채택이 촉진되었습니다. 가상 현실에서 신체 움직임을 위한 공간적 자유와 기관의 이점은 일상적인 피트니스 및 신체 훈련 애플리케이션의 길을 열어줍니다. 피트니스 및 스포츠 분야의 신체 훈련에 몰입형 기술이 어떻게 적용되는지 조사하기 위해 ACM Digital Library, IEEEXplore 및 Scopus를 통해 체계적인 검토를 수행했습니다. 우리의 검토에는 건강한 인구를 대상으로 하는 VR, AR 및 MR 기술을 위한 모든 형태의 하드웨어에 대해 논의하고 영어로 출판 및 작성된 모든 동료 검토 논문이 포함되었습니다. 임상 모집단 및 특정 질병 치료에 대한 응용 프로그램과 의견서, 워크샵, 자습서 및 잡지 칼럼과 같은 동료 검토가 아닌 모든 기사는 제외했습니다. 최종 후보에 오른 144개 논문에서 우리는 몰입형 기술의 개발 동향을 요약하고 일상적인 피트니스를 위한 몰입형 기술 설계의 과제를 강조합니다. 또한 향후 작업에 대한 권장 사항을 제공하고 더 나은 경험을 디자인하기 위해 업계 파트너, 트레이너-훈련생 경험, 스포츠의 사회적 역학과의 더 나은 협업을 지원해야 할 필요성을 강조합니다.",https://doi.org/10.1109/ISMAR59233.2023.00076,Collaboration & Social; Education & Training,Other,Technical Evaluation,System / Framework; Hardware / Device
299,2023,AMP-IT and WISDOM: Improving 3D Manipulation for High-Precision Tasks in Virtual Reality,AMP-IT 및 WISDOM: 가상 현실에서 고정밀 작업을 위한 3D 조작 개선,"Precise 3D manipulation in virtual reality (VR) is essential for effectively aligning virtual objects. However, state-of-the-art VR manipulation techniques have limitations when high levels of precision are required, including the unnaturalness caused by scaled rotations and the increase in time due to degree-of-freedom (DoF) separation in complex tasks. We designed two novel techniques to address these issues: AMP-IT, which offers direct manipulation with an adaptive scaled mapping for implicit DoF separation, and WISDOM, which offers a combination of Simple Virtual Hand and scaled indirect manipulation with explicit DoF separation. We compared these two techniques against baseline and state-of-the-art manipulation techniques in a controlled experiment. Results indicate that WISDOM and AMP-IT have significant advantages over best-practice techniques regarding task performance, usability, and user preference.","가상 현실(VR)에서의 정밀한 3D 조작은 가상 객체를 효과적으로 정렬하는 데 필수적입니다. 그러나 최첨단 VR 조작 기술은 복잡한 작업에서 회전 크기 조정으로 인한 부자연스러움, 자유도(DoF) 분리로 인한 시간 증가 등 높은 수준의 정밀도가 요구되는 경우 한계가 있습니다. 우리는 이러한 문제를 해결하기 위해 두 가지 새로운 기술을 설계했습니다. 암시적 DoF 분리를 위한 적응형 확장 매핑을 통해 직접 조작을 제공하는 AMP-IT와 명시적인 DoF 분리를 통해 단순 가상 손 및 확장된 간접 조작의 조합을 제공하는 WISDOM입니다. 우리는 통제된 실험에서 이 두 가지 기술을 기준선 및 최첨단 조작 기술과 비교했습니다. 결과는 WISDOM과 AMP-IT가 작업 성능, 유용성 및 사용자 선호도와 관련하여 모범 사례 기술에 비해 상당한 이점을 가지고 있음을 나타냅니다.",https://doi.org/10.1109/ISMAR59233.2023.00045,Interaction & Input,Optical / Display Technology,Technical Evaluation,Algorithm / Method
300,2023,AR Guidance Design for Line Tracing Speed Control,라인 트레이싱 속도 제어를 위한 AR 안내 설계,"In many jobs, workers execute precise line tracing tasks; welding, spray painting, or chiseling, for example. Training and support for such tasks can be done using VR and AR. However, to enable workers to achieve the required precision in movement and timing, the effect of visual guidance on continuous movement needs to be explored. In VR environments, we want to ensure people are trained so that the obtained skill is transferable to a real-world context, whereas, in AR, we want to ensure an ongoing task can be completed successfully when adding visual guidance. To simulate these various contexts, we employ a VR environment to investigate the effectiveness of different visualizations for motion-based guidance in a line tracing task. We tested five different visualizations, including faster and slower arrows on the pen, the same arrows on the line, a dynamic graph on the pen or line, and a ghost object to follow. Each visualization was tested with the same set of five lines of different target speeds (2cm/s to 10 cm/s in steps of 2 cm/s) with a training line of 5 cm/s. Our results show that the example ghost on the line turns out to be the most efficient visualization for allowing users to achieve a specific speed. Users also perceived this visualization as the most engaging and easy to use. These findings have significant implications for the development of AR-based guidance systems, specifically in the realm of speed control, across diverse domains such as industrial applications, training, and entertainment.","많은 작업에서 작업자는 정확한 라인 추적 작업을 수행합니다. 예를 들어 용접, 스프레이 페인팅 또는 치즐링 등이 있습니다. 이러한 업무에 대한 교육과 지원은 VR과 AR을 활용해 이루어질 수 있습니다. 그러나 작업자가 움직임과 타이밍에 필요한 정밀도를 달성할 수 있도록 하려면 지속적인 움직임에 대한 시각적 안내의 효과를 조사해야 합니다. VR 환경에서는 습득한 기술을 실제 상황에 적용할 수 있도록 사람들을 교육하고 싶은 반면, AR에서는 시각적 안내를 추가하여 진행 중인 작업을 성공적으로 완료할 수 있도록 하고 싶습니다. 이러한 다양한 상황을 시뮬레이션하기 위해 VR 환경을 사용하여 라인 추적 작업에서 모션 기반 안내에 대한 다양한 시각화의 효율성을 조사합니다. 우리는 펜의 더 빠르고 느린 화살표, 선의 동일한 화살표, 펜 또는 선의 동적 그래프, 따라가는 고스트 개체를 포함하여 다섯 가지 시각화를 테스트했습니다. 각 시각화는 5cm/s의 훈련 라인을 사용하여 서로 다른 목표 속도(2cm/s ~ 10cm/s, 2cm/s 단계)의 동일한 5개 라인 세트로 테스트되었습니다. 우리의 결과는 라인의 예시 고스트가 사용자가 특정 속도를 달성할 수 있도록 하는 가장 효율적인 시각화임이 밝혀졌다는 것을 보여줍니다. 사용자들은 또한 이 시각화가 가장 매력적이고 사용하기 쉽다고 인식했습니다. 이러한 발견은 특히 산업 응용, 교육, 엔터테인먼트 등 다양한 영역에 걸쳐 속도 제어 영역에서 AR 기반 안내 시스템의 개발에 중요한 영향을 미칩니다.",https://doi.org/10.1109/ISMAR59233.2023.00122,Education & Training,Sensor Fusion,Quantitative Experiment,User Study / Empirical Findings
301,2023,AR-Based Educational Software for Nonspeaking Autistic People - A Feasibility Study,말을 하지 못하는 자폐인을 위한 AR 기반 교육 소프트웨어 - 타당성 조사,"Approximately one-third of individuals with autism are nonspeaking: They cannot communicate effectively using speech. Some traditional accounts suggest that these individuals cannot talk because they lack the symbolic capacity for language. And yet, recent studies have shown that these individuals’ cognitive abilities are vastly underestimated by standardized tests, and that difficulties with motor skills and movement contribute to their difficulty with speech. One consequence of the traditional accounts of nonspeaking autism is that life skills (rather than academic content) tend to be emphasized in schooling. Without access to meaningful academic content, their educational and vocational opportunities are significantly limited. Recent studies have proposed the use of head-mounted Augmented Reality (AR) applications as a means of providing engaging, customizable, and age-appropriate content to this population. Specifically, such applications can address the unique sensory and motor needs of nonspeaking autistic students, e.g., allow them to move freely around the room as they interact with lessons in the application. This paper describes the design and evaluation of the first AR application aimed to facilitate tailored educational experiences for nonspeaking autistic students. After extensive consultations with nonspeaking people, parents, and professionals, we developed our application to run on HoloLens 2 offering lessons and multiple-choice comprehension and spelling questions. We conducted a study involving five nonspeaking autistic participants and two specialized educators. Through a design critique process and an iterative design refinement approach, we show that most of our participants successfully interacted with the application and completed different types of lesson tasks. Based on quantitative data from the study sessions and qualitative feedback from participants and educators, we provide recommendations for UI and UX design that will promote the development and use of such software for this under-served and under-researched population.","자폐증 환자의 약 3분의 1은 말을 하지 못합니다. 이들은 말로 효과적으로 의사소통을 할 수 없습니다. 일부 전통적인 설명에 따르면 이러한 개인은 언어에 대한 상징적 능력이 부족하기 때문에 말할 수 없습니다. 그러나 최근 연구에 따르면 이러한 개인의 인지 능력은 표준화된 테스트에 의해 크게 과소평가되고 있으며 운동 기술 및 운동의 어려움이 언어 장애의 원인이 되는 것으로 나타났습니다. 말을 하지 못하는 자폐증에 대한 전통적인 설명의 한 가지 결과는 학교 교육에서 (학업 내용보다는) 생활 기술이 강조되는 경향이 있다는 것입니다. 의미 있는 학술 콘텐츠에 접근하지 못하면 교육 및 직업 기회가 상당히 제한됩니다. 최근 연구에서는 이 인구 집단에게 매력적이고 사용자 정의 가능하며 연령에 적합한 콘텐츠를 제공하는 수단으로 머리 장착형 증강 현실(AR) 애플리케이션의 사용을 제안했습니다. 특히, 이러한 애플리케이션은 말을 하지 못하는 자폐 학생의 고유한 감각 및 운동 요구 사항을 해결할 수 있습니다. 예를 들어 애플리케이션의 수업과 상호 작용하면서 교실 내에서 자유롭게 이동할 수 있습니다. 본 논문에서는 말을 하지 못하는 자폐증 학생들을 위한 맞춤형 교육 경험을 촉진하기 위한 최초의 AR 애플리케이션의 설계 및 평가에 대해 설명합니다. 말을 하지 못하는 사람, 부모 및 전문가와 광범위한 상담을 마친 후 우리는 HoloLens 2에서 강의와 객관식 이해력 및 철자법 문제를 제공하는 애플리케이션을 개발했습니다. 우리는 말을 하지 못하는 자폐증 참가자 5명과 전문 교육자 2명을 대상으로 연구를 실시했습니다. 디자인 비평 프로세스와 반복적인 디자인 개선 접근 방식을 통해 대부분의 참가자가 애플리케이션과 성공적으로 상호 작용하고 다양한 유형의 수업 작업을 완료했음을 보여줍니다. 연구 세션의 정량적 데이터와 참가자 및 교육자의 정성적 피드백을 바탕으로 우리는 서비스가 부족하고 연구가 부족한 인구 집단을 위해 이러한 소프트웨어의 개발 및 사용을 촉진할 UI 및 UX 디자인에 대한 권장 사항을 제공합니다.",https://doi.org/10.1109/ISMAR59233.2023.00069,Education & Training; Display & Optics,Other,User Study,Hardware / Device
302,2023,AR-supported Human-Robot Collaboration: Facilitating Workspace Awareness and Parallelized Assembly Tasks,AR 지원 인간-로봇 협업: 작업 공간 인식 및 병렬화된 조립 작업 촉진,"While technologies for human-robot collaboration are rapidly advancing, plenty of aspects still need further investigation, such as ensuring workspace awareness, enabling the operator to reschedule tasks on the fly, and how users prefer to coordinate and collaborate with robots. To address these, we propose an Augmented Reality interface that supports human-robot collaboration in an assembly task by (1) enabling the inspection of planned and ongoing robot processes through dynamic task lists and a path visualization, (2) allowing the operator to also delegate tasks to the robot, and (3) presenting step-by-step assembly instructions. We evaluate our AR interface in comparison to a state-of-the-art tablet interface in a user study, where participants collaborated with a robot arm in a shared workspace to complete an assembly task. Our findings confirm the feasibility and potential of AR-assisted human-robot collaboration, while pointing to some central challenges that require further work.","인간-로봇 협업을 위한 기술이 빠르게 발전하고 있지만, 작업 공간 인식 보장, 운영자가 즉시 작업 일정을 조정할 수 있도록 지원, 사용자가 로봇과 조정 및 협업하는 방식을 선호하는 등 많은 측면에 대해 추가 조사가 여전히 필요합니다. 이러한 문제를 해결하기 위해 우리는 (1) 동적 작업 목록 및 경로 시각화를 통해 계획되고 진행 중인 로봇 프로세스를 검사할 수 있게 하고, (2) 작업자가 로봇에 작업을 위임할 수 있게 하고, (3) 단계별 조립 지침을 제시함으로써 조립 작업에서 인간-로봇 협업을 지원하는 증강 현실 인터페이스를 제안합니다. 우리는 참가자들이 공유 작업 공간에서 로봇 팔과 협력하여 조립 작업을 완료한 사용자 연구에서 최첨단 태블릿 인터페이스와 비교하여 AR 인터페이스를 평가했습니다. 우리의 연구 결과는 AR 지원 인간-로봇 협업의 타당성과 잠재력을 확인하는 동시에 추가 작업이 필요한 몇 가지 핵심 과제를 지적합니다.",https://doi.org/10.1109/ISMAR59233.2023.00123,Collaboration & Social; Interaction & Input,Other,User Study; Technical Evaluation,User Study / Empirical Findings
303,2023,ARCHIE2: An Augmented Reality Interface with Plant Detection for Future Planetary Surface Greenhouses,ARCHIE2: 미래의 행성 표면 온실을 위한 식물 감지 기능을 갖춘 증강 현실 인터페이스,"More than 50 years after the last human set foot on the Moon during the Apollo 17 mission, humans aim to return to the Moon in this decade. This time, humanity plans to establish lunar habitats for a sustainable longer presence. An integrated part of these lunar habitats will be planetary surface greenhouses. These greenhouses will produce food, process air, recycle water, and improve the psychological well-being of humans. Past research has shown that a large amount of crew time, a scarce and valuable resource in spaceflight, is needed for maintenance and repairs in a planetary surface greenhouse, leaving less time for crop cultivation and science activities. In this paper, we present the concept of an augmented reality interface named ARCHIE2 to reduce crew time and the workload of astronauts and remote support teams on Earth to operate a planetary surface greenhouse. ARCHIE2 allows users to visualize status information on plants, technical systems, and environmental parameters in the greenhouse or other features supporting the greenhouse operations using an augmented reality headset. In particular, we report on the implementation and performance of the ARCHIE2 plant detection module that runs locally on the augmented reality headset. Using images with a resolution of 320$\times$l92 pixels, arugula selvatica plants were detected using an artificial neural network (based on a YOLOv5s model) from a horizontal distance up to 50 cm with an average inference time of 602 ms and an average of 48 FPS. Based on that, the plants were augmented with labels to visualize relevant plant-specific information supporting astronauts in the maintenance of the plants.","아폴로 17호 임무 중 마지막 인류가 달에 발을 디딘 지 50여년이 지난 지금, 인류는 10년 안에 달로 돌아가는 것을 목표로 하고 있습니다. 이번에 인류는 지속 가능한 더 오랜 존재를 위해 달 서식지를 구축할 계획입니다. 이러한 달 서식지의 통합된 부분은 행성 표면 온실이 될 것입니다. 이러한 온실은 식량을 생산하고, 공기를 처리하고, 물을 재활용하고, 인간의 심리적 웰빙을 향상시킵니다. 과거 연구에 따르면 우주 비행에서 부족하고 귀중한 자원인 많은 승무원 시간이 행성 표면 온실의 유지 관리 및 수리에 필요하므로 작물 재배 및 과학 활동에 소요되는 시간이 줄어드는 것으로 나타났습니다. 본 논문에서는 행성 표면 온실을 운영하기 위해 지구상의 우주 비행사 및 원격 지원 팀의 승무원 시간과 작업량을 줄이기 위한 ARCHIE2라는 증강 현실 인터페이스의 개념을 제시합니다. ARCHIE2를 사용하면 사용자는 증강 현실 헤드셋을 사용하여 온실의 식물, 기술 시스템 및 환경 매개변수에 대한 상태 정보 또는 온실 운영을 지원하는 기타 기능을 시각화할 수 있습니다. 특히 증강 현실 헤드셋에서 로컬로 실행되는 ARCHIE2 식물 감지 모듈의 구현 및 성능에 대해 보고합니다. 320$\times$l92 픽셀의 해상도를 가진 이미지를 사용하여 arugula selvatica 식물은 인공 신경망(YOLOv5s 모델 기반)을 사용하여 수평 거리 최대 50cm에서 평균 추론 시간 602ms 및 평균 48FPS로 감지되었습니다. 이를 바탕으로 식물 유지 관리에 있어 우주비행사를 지원하는 관련 식물별 정보를 시각화하기 위해 식물에 라벨을 추가했습니다.",https://doi.org/10.1109/ISMAR59233.2023.00075,Perception & Cognition,Deep Learning / Neural Networks,Quantitative Experiment,Algorithm / Method
304,2023,ARPuzzle: Evaluating the Effectiveness of Collaborative Augmented Reality,AR퍼즐: 협업 증강 현실의 효율성 평가,"Collaborative Augmented Reality (CAR) offers disruptive ways for people to collaborate. However, this emerging technology must improve its acceptance, efficiency, and usability to scale up and, for example, support augmented operations executed by technicians. This paper presents our CAR system and its experimentation during a cooperative puzzle-solving task. Our system provides collaborators with a shared virtual space allowing verbal and non-verbal interpersonal communications, and intuitive interactions with shared virtual replicas of real objects. Our system also integrates avatars embodied by remote users. We conducted a dual-user study comparing colocated and remote solving of a puzzle virtual replica with its real solving. We evaluated task performance, collaboration, mutual awareness, spatial presence, and copresence, usability, and preference. We found that, if real is preferred and more efficient than our CAR system, CAR is reaching favorable usability levels. We also found that remote augmented reality including full-body avatars offers similar results to colocated augmented reality. This preliminary work paves the way for future research aiming to support and enhance the design and making of Collaborative Augmented Reality systems dedicated to augmented operations.","협업 증강 현실(CAR)은 사람들이 협업할 수 있는 파괴적인 방법을 제공합니다. 그러나 이 새로운 기술은 확장을 위해 수용성, 효율성 및 유용성을 개선해야 하며, 예를 들어 기술자가 실행하는 증강된 작업을 지원해야 합니다. 이 논문은 협력적인 퍼즐 해결 작업 중 CAR 시스템과 그 실험을 제시합니다. 우리 시스템은 공동 작업자에게 언어 및 비언어적 대인 커뮤니케이션이 가능하고 실제 객체의 공유 가상 복제본과의 직관적인 상호 작용이 가능한 공유 가상 공간을 제공합니다. 우리 시스템은 원격 사용자가 구현한 아바타도 통합합니다. 우리는 퍼즐 가상 복제본의 공동 배치 및 원격 해결과 실제 해결을 비교하는 이중 사용자 연구를 수행했습니다. 작업 수행도, 협업, 상호 인식, 공간적 존재감, 공존성, 사용성 및 선호도를 평가했습니다. 우리는 실제가 CAR 시스템보다 선호되고 더 효율적이라면 CAR이 유리한 유용성 수준에 도달하고 있음을 발견했습니다. 또한 전신 아바타를 포함한 원격 증강 현실이 함께 위치한 증강 현실과 유사한 결과를 제공한다는 것을 발견했습니다. 이 예비 작업은 증강 운영 전용 협업 증강 현실 시스템의 설계 및 제작을 지원하고 향상시키는 것을 목표로 하는 미래 연구의 길을 열어줍니다.",https://doi.org/10.1109/ISMAR59233.2023.00079,Collaboration & Social; Perception & Cognition,Other,User Study,System / Framework
305,2023,Active Engagement with Virtual Reality Reduces Stress and Increases Positive Emotions,가상 현실을 통한 적극적인 참여는 스트레스를 줄이고 긍정적인 감정을 증가시킵니다,"Stress, anxiety, and depression negatively affect productivity and the global economy with an estimated annual cost of ${\$}$1 trillion U.S. dollars, according to the World Health Organization. Moreover, prolonged daily stress—even if minor—can lead to severe health consequences, including cancer and various mental disorders. Virtual reality (VR) has been shown to be a promising tool for relieving daily stressors given its accessibility and its projected availability as compared to visiting with mental health professionals. Prior work in this area has mostly focused on the restorative effects of nature simulations, demonstrating that passively experiencing immersive nature scenes improves positive affect. However, aside from providing opportunities for exercise, little is known about how active VR engagement can improve one’s mental health. To address this research gap, this paper presents a new, active form of VR therapy and assesses its effectiveness as compared to passive VR experiences. We developed VR Drawing—inspired by art therapy, which promotes positive emotions through artistic creation—and VR Throwing—inspired by “rage rooms”, which allow people to release negative emotions via intentional destruction. In a between- participants study (n = 64), we found that both VR Drawing and VR Throwing significantly reduced participants’ stress levels and increased positive affect when compared to passively watching nature scenes in VR. Linear regression models suggest that the total number of user interactions positively affects improvement in positive emotions for VR Drawing, but has a negative impact on positive emotions for VR Throwing. This study provides empirical evidence of how active VR experiences may reduce stress and offers guidelines for creating future VR applications to promote psychological well-being.","세계보건기구(WHO)에 따르면 스트레스, 불안, 우울증은 생산성과 세계 경제에 부정적인 영향을 미치며 연간 비용이 미화 1조 달러로 추산됩니다. 더욱이, 장기간의 일일 스트레스는 경미하더라도 암 및 다양한 정신 질환을 포함하여 건강에 심각한 결과를 초래할 수 있습니다. 가상 현실(VR)은 정신 건강 전문가를 방문하는 것과 비교하여 접근성과 예상 가용성을 고려할 때 일상적인 스트레스 요인을 완화하는 유망한 도구로 나타났습니다. 이 분야의 이전 연구는 주로 자연 시뮬레이션의 회복 효과에 중점을 두고 몰입형 자연 장면을 수동적으로 경험하면 긍정적인 감정이 향상된다는 것을 보여주었습니다. 그러나 운동 기회를 제공하는 것 외에 적극적인 VR 참여가 정신 건강을 어떻게 향상시킬 수 있는지에 대해서는 알려진 바가 거의 없습니다. 이러한 연구 격차를 해결하기 위해 이 논문은 새롭고 능동적인 형태의 VR 치료를 제시하고 수동적 VR 경험과 비교하여 그 효과를 평가합니다. 우리는 예술적 창작을 통해 긍정적인 감정을 촉진하는 미술 치료에서 영감을 받은 VR 드로잉과 의도적인 파괴를 통해 사람들이 부정적인 감정을 발산할 수 있는 ""분노의 방""에서 영감을 얻은 VR 던지기를 개발했습니다. 참가자 간 연구(n = 64)에서 VR 그리기와 VR 던지기 모두 VR에서 수동적으로 자연 장면을 보는 것과 비교할 때 참가자의 스트레스 수준을 크게 줄이고 긍정적인 감정을 증가시키는 것으로 나타났습니다. 선형 회귀 모델은 총 사용자 상호 작용 수가 VR 그리기에 대한 긍정적인 감정 향상에 긍정적인 영향을 미치지만 VR 던지기에 대한 긍정적인 감정에는 부정적인 영향을 미친다는 것을 보여줍니다. 이 연구는 적극적인 VR 경험이 어떻게 스트레스를 줄일 수 있는지에 대한 실증적 증거를 제공하고 심리적 웰빙을 촉진하기 위한 미래 VR 애플리케이션을 만들기 위한 지침을 제공합니다.",https://doi.org/10.1109/ISMAR59233.2023.00067,Interaction & Input,Deep Learning / Neural Networks,User Study,User Study / Empirical Findings
306,2023,Adaptive Color Structured Light for Calibration and Shape Reconstruction,보정 및 형상 재구성을 위한 적응형 색상 구조광,"Color structured light (SL) plays an important role in spatial augmented reality and shape reconstruction. Compared to traditional non-color multi-shot SL, it has the advantage of fewer projections, and can even achieve single-shot. However, distortions caused by ambient light and imaging devices limit color SL’s applicability and accuracy. A common solution is to apply color adaptation techniques to cancel the disturbances. Previous studies focus on either robust fixed color patterns or adaptation approaches that may require preliminary geometric calibrations. In this paper, we propose an approach that can efficiently adapt color SL to arbitrary ambient light and imaging devices’ color responses, without device response function calibration or geometric calibration. First, we design a novel algorithm to quickly find the most distinct colors that are easily separable under a new environment and device setup. Then, we design a maximum a posteriori (MAP)-based color detection algorithm that can utilize ambient light and device priors to robustly detect the SL colors. In experiments, our adaptive color SL outperforms previous methods in both calibration and shape reconstruction tasks across a variety of setups.","컬러 구조광(SL)은 공간 증강 현실과 형상 재구성에서 중요한 역할을 합니다. 기존의 무색 멀티샷 SL에 비해 투영 수가 적고 단일 샷도 달성할 수 있다는 장점이 있습니다. 그러나 주변광과 이미징 장치로 인한 왜곡으로 인해 컬러 SL의 적용 가능성과 정확성이 제한됩니다. 일반적인 해결책은 색상 적응 기술을 적용하여 방해를 제거하는 것입니다. 이전 연구에서는 예비 기하학적 보정이 필요할 수 있는 강력한 고정 색상 패턴이나 적응 접근 방식에 중점을 두었습니다. 본 논문에서는 장치 응답 기능 보정이나 기하학적 보정 없이 컬러 SL을 임의의 주변광 및 이미징 장치의 색상 반응에 효율적으로 적용할 수 있는 접근 방식을 제안합니다. 먼저, 새로운 환경과 기기 설정에서 쉽게 분리할 수 있는 가장 뚜렷한 색상을 빠르게 찾을 수 있는 새로운 알고리즘을 설계합니다. 그런 다음 주변광과 장치 사전을 활용하여 SL 색상을 강력하게 감지할 수 있는 최대 사후(MAP) 기반 색상 감지 알고리즘을 설계합니다. 실험에서 적응형 색상 SL은 다양한 설정에 걸쳐 보정 및 모양 재구성 작업 모두에서 이전 방법보다 성능이 뛰어났습니다.",https://doi.org/10.1109/ISMAR59233.2023.00141,Interaction & Input,Sensor Fusion,Quantitative Experiment,Algorithm / Method
307,2023,An Exploration of The Effects of Head-Centric Rest Frames On Egocentric Distance Judgments in VR,VR에서 자기 중심적 거리 판단에 대한 머리 중심 받침대 프레임의 효과 탐색,"Users tend to underestimate distances in virtual reality (VR), and several efforts have been directed toward finding the causes and developing tools that mitigate this phenomenon. One hypothesis that stands out in the field of spatial perception is the rest frame hypothesis (RFH), which states that visual frames of reference (RFs), defined as fixed reference points of view in a virtual environment (VE), contribute to minimizing sensory mismatch. RFs have been shown to promote better eye-gaze stability and focus, reduce VR sickness, and improve visual search, along with other benefits. However, their effect on distance perception in VEs has not been evaluated. In this paper, we use a blind walking task to explore the effect of three head-centric RFs (mesh mask, nose, and hat) on egocentric distance estimation. We found that at near and mid-field distances, certain RFs can improve the user’s distance estimation accuracy and reduce distance underestimation. These findings mean that the addition of head-centric RFs, a simple avatar augmentation method, can lead to meaningful improvements in distance judgments, user experience, and task performance in VR.","사용자는 가상 현실(VR)에서 거리를 과소평가하는 경향이 있으며, 이러한 현상을 완화하는 도구를 개발하고 원인을 찾는 데 여러 가지 노력이 기울여졌습니다. 공간 지각 분야에서 눈에 띄는 가설 중 하나는 가상 환경(VE)에서 고정된 기준점으로 정의되는 시각적 기준 프레임(RF)이 감각 불일치를 최소화하는 데 기여한다는 RFH(Rest Frame Hypothese)입니다. RF는 눈의 시선 안정성과 초점을 향상시키고, VR 멀미를 줄이고, 시각적 검색을 향상시키는 등 기타 이점을 제공하는 것으로 나타났습니다. 그러나 VE의 거리 인식에 미치는 영향은 평가되지 않았습니다. 본 논문에서는 맹인 걷기 작업을 사용하여 자기 중심 거리 추정에 대한 세 가지 머리 중심 RF(메시 마스크, 코, 모자)의 효과를 탐색합니다. 우리는 근거리 및 중거리에서 특정 RF가 사용자의 거리 추정 정확도를 향상시키고 거리 과소 추정을 줄일 수 있음을 발견했습니다. 이러한 연구 결과는 간단한 아바타 증강 방법인 머리 중심 RF를 추가하면 VR에서 거리 판단, 사용자 경험 및 작업 성능이 의미 있게 향상될 수 있음을 의미합니다.",https://doi.org/10.1109/ISMAR59233.2023.00041,Perception & Cognition,Other,Quantitative Experiment,User Study / Empirical Findings; Algorithm / Method
308,2023,"Auditory, Vibrotactile, or Visual? Investigating the Effective Feedback Modalities to Improve Standing Balance in Immersive Virtual Reality for People with Balance Impairments Due to Type 2 Diabetes","청각, 진동촉각 또는 시각? 제2형 당뇨병으로 인해 균형 장애가 있는 사람들을 위한 몰입형 가상 현실에서 기립 균형을 개선하기 위한 효과적인 피드백 방식 조사","Immersive Virtual Reality (VR) users often experience difficulties with maintaining their balance. This issue poses a significant challenge to the widespread usability and accessibility of VR, particularly for individuals with balance impairments. Previous studies have confirmed the existence of balance problems in VR, but little attention has been given to addressing them. To investigate the impact of different feedback modalities (auditory, vibrotactile, and visual) on balance in immersive VR, we conducted a study with 50 participants, consisting of 25 individuals with balance impairments due to type 2 diabetes and 25 without balance impairments. Participants were asked to perform standing reach and grasp tasks. Our findings indicated that auditory and vibrotactile techniques improved balance significantly (p<.001) in immersive VR for participants with and without balance impairments, while visual techniques only improved balance significantly for participants with balance impairments. Also, auditory and vibrotactile feedback techniques improved balance significantly more than visual techniques. Spatial auditory feedback outperformed other conditions significantly for all people. This study presents implementations and comparisons of potential strategies that can be implemented in future VR environments to enhance standing balance and promote the broader adoption of VR.","몰입형 가상 현실(VR) 사용자는 균형을 유지하는 데 어려움을 겪는 경우가 많습니다. 이 문제는 특히 균형 장애가 있는 개인의 경우 VR의 광범위한 사용성과 접근성에 심각한 문제를 제기합니다. 이전 연구에서는 VR에 균형 문제가 있음이 확인되었지만 이를 해결하는 데는 거의 관심이 없었습니다. 몰입형 VR의 균형에 대한 다양한 피드백 양식(청각, 진동 촉각 및 시각)의 영향을 조사하기 위해 제2형 당뇨병으로 인해 균형 장애가 있는 개인 25명과 균형 장애가 없는 25명으로 구성된 50명의 참가자를 대상으로 연구를 수행했습니다. 참가자들은 서서 손을 뻗고 잡는 작업을 수행하도록 요청 받았습니다. 우리 연구 결과에 따르면 청각 및 진동 촉각 기술은 균형 장애가 있는 참가자와 없는 참가자의 몰입형 VR에서 균형을 크게 향상시킨 반면(p<.001), 시각적 기술은 균형 장애가 있는 참가자의 균형을 크게 향상시키는 것으로 나타났습니다. 또한 청각 및 진동촉각 피드백 기술은 시각적 기술보다 균형 감각을 훨씬 더 향상시켰습니다. 공간적 청각 피드백은 모든 사람들에게 다른 조건보다 훨씬 더 나은 성능을 보였습니다. 본 연구에서는 서 있는 균형을 향상하고 VR의 광범위한 채택을 촉진하기 위해 미래의 VR 환경에서 구현할 수 있는 잠재적인 전략의 구현 및 비교를 제시합니다.",https://doi.org/10.1109/ISMAR59233.2023.00072,Perception & Cognition; Accessibility,Haptic / Tactile Feedback,User Study,User Study / Empirical Findings
309,2023,Augmented Reality Rehabilitative and Exercise Games (ARREGs): A Systematic Review and Future Considerations,증강 현실 재활 및 운동 게임(ARREG): 체계적인 검토 및 향후 고려 사항,"Augmented Reality (AR) and exergames have been trending areas of interest in healthcare spaces for rehabilitation and exercise. This work reviews 25 papers featuring AR rehabilitative/exercise games and paints a picture of the literature landscape. The included results span twelve years, with the oldest paper published in 2010 and the most recent work in 2022. More specifically, this work contributes a bank of representative ARREGs and a synthesis of measurement strategies for player perceptions of Augmented Reality Rehabilitative and Exercise Game (ARREG) experiences, the elements that comprise the exergame experience, the intended use cases of ARREGs, whether participants are actually representative users, the utilized devices and AR modalities, the measures used to capture rehabilitative success, and the measures used to capture participant perceptions. Informed by the literature body, our most significant contribution is nine considerations for future ARREG development.","증강 현실(AR)과 엑서게임은 재활 및 운동을 위한 의료 공간에서 관심을 끄는 분야입니다. 이 작품은 AR 재활/운동 게임을 주제로 한 25편의 논문을 검토하고 문학계의 모습을 그립니다. 포함된 결과는 12년에 걸쳐 있으며 가장 오래된 논문은 2010년에 출판되었고 가장 최근 작업은 2022년에 출판되었습니다. 보다 구체적으로 이 작업은 대표적인 ARREG 은행과 증강 현실 재활 및 운동 게임(ARREG) 경험에 대한 플레이어 인식을 위한 측정 전략의 종합, 엑서게임 경험을 구성하는 요소, ARREG의 의도된 사용 사례, 참가자가 실제로 대표적인 사용자인지 여부, 활용된 장치 및 AR 양식, 캡처에 사용된 측정에 기여합니다. 재활 성공 및 참가자 인식을 포착하는 데 사용되는 조치. 문헌 기관에서 정보를 얻은 가장 중요한 기여는 향후 ARREG 개발을 위한 9가지 고려 사항입니다.",https://doi.org/10.1109/ISMAR59233.2023.00118,Medical & Healthcare,Other,User Study,Survey / Review
310,2023,Be Real in Scale: Swing for True Scale in Dual Camera Mode,실제 크기로 구현: 듀얼 카메라 모드에서 실제 크기를 위한 스윙,"Many mobile AR apps that use the front-facing camera can benefit significantly from knowing the metric scale of the user’s face. However, the true scale of the face is hard to measure because monocular vision suffers from a fundamental ambiguity in scale. The methods based on prior knowledge about the scene either have a large error or are not easily accessible. In this paper, we propose a new method to measure the face scale by a simple user interaction: the user only needs to swing the phone to capture two selfies while using the recently popular Dual Camera mode. This mode allows simultaneous streaming of the front camera and the rear cameras and has become a key feature in many social apps. A computer vision method is applied to first estimate the absolute motion of the phone from the images captured by two rear cameras, and then calculate the point cloud of the face by triangulation. We develop a prototype mobile app to validate the proposed method. Our user study shows that the proposed method is favored compared to existing methods because of its high accuracy and ease of use. Our method can be built into Dual Camera mode and can enable a wide range of applications (e.g., virtual try-on for online shopping, true-scale 3D face modeling, gaze tracking, and face anti-spoofing) by introducing true scale to smartphone-based XR. The code is available at https://github.com/ruiyu0/Swing-for-True-Scale.","전면 카메라를 사용하는 많은 모바일 AR 앱은 사용자 얼굴의 척도를 알면 상당한 이점을 얻을 수 있습니다. 그러나 단안 시력은 근본적으로 규모가 모호하기 때문에 얼굴의 실제 규모를 측정하기가 어렵습니다. 현장에 대한 사전 지식을 기반으로 한 방법은 오류가 크거나 접근이 쉽지 않습니다. 본 논문에서는 간단한 사용자 상호작용으로 얼굴 크기를 측정하는 새로운 방법을 제안합니다. 사용자는 최근 인기 있는 듀얼 카메라 모드를 사용하면서 휴대폰을 흔들기만 하면 두 장의 셀카를 촬영할 수 있습니다. 이 모드는 전면 카메라와 후면 카메라의 동시 스트리밍을 허용하며 많은 소셜 앱의 핵심 기능이 되었습니다. 컴퓨터 비전 방법을 적용해 두 대의 후면 카메라가 촬영한 이미지에서 휴대폰의 절대 움직임을 먼저 추정한 후 삼각 측량을 통해 얼굴의 포인트 클라우드를 계산합니다. 제안된 방법을 검증하기 위해 프로토타입 모바일 앱을 개발합니다. 우리의 사용자 연구에 따르면 제안된 방법은 높은 정확성과 사용 용이성 때문에 기존 방법에 비해 선호되는 것으로 나타났습니다. 우리의 방법은 듀얼 카메라 모드에 구축될 수 있으며 스마트폰 기반 XR에 진정한 규모를 도입함으로써 광범위한 애플리케이션(예: 온라인 쇼핑을 위한 가상 체험, 실제 크기의 3D 얼굴 모델링, 시선 추적 및 얼굴 스푸핑 방지)을 지원할 수 있습니다. 코드는 https://github.com/ruiyu0/Swing-for-True-Scale에서 확인할 수 있습니다.",https://doi.org/10.1109/ISMAR59233.2023.00140,Interaction & Input,Cloud / Edge Computing,User Study,Algorithm / Method
311,2023,Beyond Well-Intentioned: An HCI Students' Ethical Assessment of Their Own XR Designs,선의를 넘어: 자신의 XR 디자인에 대한 HCI 학생들의 윤리적 평가,"Foreseeing the impact of augmented and virtual reality applications on users and society is challenging. Thus, efforts to establish an ethical mindset and include technology assessment techniques in HCI education are increasing. However, XR educational courses fostering students’ reasoning and perceived responsibility in designing ethical applications are lacking. We, therefore, developed the explorative design method Reality Composer to investigate and foster the students’ assessment of the ethical impact of and responsibilities in XR application design. We conducted a workshop with 40 international HCI master students applying this method and analyzed the resulting application concepts regarding the students’ ethical assessment. Our findings show that they critically discussed their concepts’ impact and identified potential countermeasures for negative social implications. However, they overestimated the users’ responsibility to securely use XR applications as well as a positive design intention. We contribute with our findings and developed method to understand students’ potential and derive future course design implications.",증강 현실과 가상 현실 애플리케이션이 사용자와 사회에 미치는 영향을 예측하는 것은 어렵습니다. 이에 윤리적 사고방식을 확립하고 기술평가기법을 HCI 교육에 포함시키려는 노력이 늘어나고 있다. 그러나 윤리적 애플리케이션 설계에 있어 학생들의 추론과 책임감을 키우는 XR 교육 과정은 부족합니다. 따라서 우리는 XR 애플리케이션 디자인의 윤리적 영향과 책임에 대한 학생들의 평가를 조사하고 육성하기 위해 탐색적 디자인 방법인 Reality Composer를 개발했습니다. 우리는 이 방법을 적용한 40명의 국제 HCI 석사 학생들과 워크숍을 진행하고 학생들의 윤리적 평가에 대한 적용 개념을 분석했습니다. 우리의 연구 결과에 따르면 그들은 개념의 영향을 비판적으로 논의하고 부정적인 사회적 영향에 대한 잠재적인 대응책을 식별했습니다. 그러나 그들은 XR 애플리케이션을 안전하게 사용하려는 사용자의 책임과 긍정적인 디자인 의도를 과대평가했습니다. 우리는 학생들의 잠재력을 이해하고 향후 코스 설계에 미치는 영향을 도출하기 위해 우리의 연구 결과와 개발된 방법에 기여합니다.,https://doi.org/10.1109/ISMAR59233.2023.00020,Education & Training,Other,Other,Algorithm / Method; User Study / Empirical Findings
312,2023,Comparative Analysis of Artefact Interaction and Manipulation Techniques in VR Museums: A Study of Performance and User Experience,VR 박물관의 유물 상호작용과 조작 기법의 비교 분석: 성능과 사용자 경험에 관한 연구,"For museums in Virtual Reality (VR), various interaction and manipulation techniques could be employed for users to engage with artefact interactions. This study examined four combinations of interaction (controller-based and hand-tracking) and manipulation (direct and indirect) techniques, assessing user performance and experience with these interaction techniques in a virtual museum environment. We conducted a within-subjects experiment and asked participants to perform a series of transform manipulation tasks using the four techniques. Participants’ task completion time was measured. They also provided feedback on acceptance, learnability, presence, sickness, and fatigue, and gave an overall ranking through post-experiment questionnaires and interviews. The results revealed that controller-based direct manipulation outperformed the other techniques in terms of task performance and user experience, with hand-tracking indirect manipulation being the least efficient and the least preferred option. The study offers insights for future research and development in refining interaction and manipulation techniques and designing more user-friendly VR museum experiences.","가상 현실(VR) 박물관의 경우 사용자가 유물 상호 작용에 참여할 수 있도록 다양한 상호 작용 및 조작 기술을 사용할 수 있습니다. 이 연구에서는 상호 작용(컨트롤러 기반 및 손 추적)과 조작(직접 및 간접) 기술의 네 가지 조합을 조사하여 가상 박물관 환경에서 이러한 상호 작용 기술에 대한 사용자 성능과 경험을 평가했습니다. 우리는 피험자 내 실험을 수행하고 참가자들에게 네 가지 기술을 사용하여 일련의 변환 조작 작업을 수행하도록 요청했습니다. 참가자의 작업 완료 시간을 측정했습니다. 또한 수용성, 학습성, 존재감, 질병, 피로도에 대한 피드백을 제공하고 실험 후 설문지와 인터뷰를 통해 전반적인 순위를 매겼습니다. 결과는 컨트롤러 기반 직접 조작이 작업 성능 및 사용자 경험 측면에서 다른 기술을 능가하는 것으로 나타났으며, 손 추적 간접 조작은 가장 효율적이지 않고 가장 선호도가 낮은 옵션이었습니다. 이 연구는 상호 작용 및 조작 기술을 개선하고 보다 사용자 친화적인 VR 박물관 경험을 설계하는 데 있어 향후 연구 개발에 대한 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR59233.2023.00091,Interaction & Input,Optical / Display Technology,User Study,Interaction Technique
313,2023,Comparative Analysis of Change Blindness in Virtual Reality and Augmented Reality Environments,가상현실과 증강현실 환경의 변화맹점 비교 분석,"Change blindness is a phenomenon where an individual fails to notice alterations in a visual scene when a change occurs during a brief interruption or distraction. Understanding this phenomenon is specifically important for the technique that uses a visual stimulus, such as Virtual Reality (VR) or Augmented Reality (AR). Previous research had primarily focused on 2D environments or conducted limited controlled experiments in 3D immersive environments. In this paper, we design and conduct two formal user experiments to investigate the effects of different visual attention-disrupting conditions (Flickering and Head-Turning) and object alternative conditions (Removal, Color Alteration, and Size Alteration) on change blindness detection in VR and AR environments. Our results reveal that participants detected changes more quickly and had a higher detection rate with Flickering compared to Head-Turning. Furthermore, they spent less time detecting changes when an object disappeared compared to changes in color or size. Additionally, we provide a comparison of the results between VR and AR environments.","변화맹(Change Blindness)은 짧은 방해나 방해로 인해 시각적 장면의 변화를 인지하지 못하는 현상입니다. 이러한 현상을 이해하는 것은 가상 현실(VR)이나 증강 현실(AR)과 같은 시각적 자극을 사용하는 기술에 특히 중요합니다. 이전 연구에서는 주로 2D 환경에 초점을 맞추거나 3D 몰입형 환경에서 제한적으로 제어된 실험을 수행했습니다. 본 논문에서는 다양한 시각적 주의를 방해하는 조건(깜박임 및 머리 회전)과 객체 대체 조건(제거, 색상 변경 및 크기 변경)이 VR 및 AR 환경에서 변경 맹목 감지에 미치는 영향을 조사하기 위해 두 가지 공식 사용자 실험을 설계하고 수행합니다. 우리의 결과는 참가자들이 Head-Turning에 비해 깜박임으로 변화를 더 빨리 감지하고 감지율이 더 높았다는 것을 보여줍니다. 또한 색상이나 크기의 변화에 ​​비해 물체가 사라졌을 때 변화를 감지하는 데 소요되는 시간이 더 짧았습니다. 또한 VR과 AR 환경 간의 결과 비교를 제공합니다.",https://doi.org/10.1109/ISMAR59233.2023.00115,Interaction & Input,Sensor Fusion,User Study,User Study / Empirical Findings; Algorithm / Method
314,2023,Comparing Visualizations to Help a Teacher Effectively Monitor Students in a VR Classroom,교사가 VR 교실에서 학생들을 효과적으로 모니터링하는 데 도움이 되는 시각화 비교,"Educational virtual reality (VR) applications are the most recent addition to the learning management tools in this modern age. Due to health concerns, financial concerns, and convenience, people are looking for alternate ways to teach and learn. An efficient VR-based teaching interface could enhance student engagement, learning outcomes, and overall educational experience. Typically, teachers in a VR classroom do not have a way to know what students are doing since students are not visible. An efficient teaching interface should include some mechanism for a teacher to monitor students and alert the teacher if a student is trying to catch the attention of the teacher. An ideal interface would be one, which helps a teacher effectively monitor students while teaching without increasing the cognitive load of the teacher. In this paper, we present a comparative study of two such student monitoring interfaces. In the first interface, the student activity related information is shown using icons near the student avatar (representing a student in the VR environment). While in the second interface, a set of centrally-arranged emoticon-like visual indicators are present in addition to the student avatar, and the student activity related information is shown near the student emoticon. We present a detailed user experiment comparing the two interfaces in terms of teaching management, student monitoring capability, cognitive load, and user preference. Participants preferred and performed better with Indicator-located interface over avatar-located interface.","교육용 가상 현실(VR) 애플리케이션은 현대 시대의 학습 관리 도구에 가장 최근에 추가된 것입니다. 건강 문제, 재정 문제, 편리성 때문에 사람들은 가르치고 배우는 대체 방법을 찾고 있습니다. 효율적인 VR 기반 교육 인터페이스는 학생 참여, 학습 결과 및 전반적인 교육 경험을 향상시킬 수 있습니다. 일반적으로 VR 교실의 교사는 학생들이 보이지 않기 때문에 학생들이 무엇을 하고 있는지 알 수 없습니다. 효율적인 교육 인터페이스에는 교사가 학생을 모니터링하고 학생이 교사의 관심을 끌려고 하면 교사에게 경고할 수 있는 몇 가지 메커니즘이 포함되어야 합니다. 이상적인 인터페이스는 교사가 교사의 인지 부하를 늘리지 않고 가르치는 동안 학생을 효과적으로 모니터링하는 데 도움이 되는 인터페이스입니다. 본 논문에서는 이러한 두 가지 학생 모니터링 인터페이스에 대한 비교 연구를 제시합니다. 첫 번째 인터페이스에서는 학생 아바타(VR 환경에서 학생을 표현) 근처에 학생 활동 관련 정보가 아이콘으로 표시됩니다. 두 번째 인터페이스에서는 학생 아바타 외에 중앙에 배치된 이모티콘 형태의 시각적 표시 세트가 존재하며, 학생 이모티콘 근처에 학생 활동 관련 정보가 표시됩니다. 우리는 교육 관리, 학생 모니터링 기능, 인지 부하 및 사용자 선호도 측면에서 두 인터페이스를 비교하는 자세한 사용자 실험을 제시합니다. 참가자들은 아바타 위치 인터페이스보다 표시기 위치 인터페이스를 선호하고 더 나은 성과를 거두었습니다.",https://doi.org/10.1109/ISMAR59233.2023.00059,Education & Training; Perception & Cognition,Optical / Display Technology,User Study,Interaction Technique
315,2023,Compass+Ring: A Multimodal Menu to Improve Interaction Performance and Comfortability in One-handed Scenarios,나침반+링: 한 손 시나리오에서 상호 작용 성능과 편안함을 향상시키는 다중 모드 메뉴,"In numerous applications, an excellent interface design should allow users to perform secondary tasks as naturally as possible without affecting the main task. Multimodal handheld menus are regularly the preferred user interface that meets the natural switching of primary and secondary tasks. However, existing multimodal handheld menus have some limitations under single-handed conditions, or the comfort needs improvement. To address these issues, this paper proposes a novel multimodal handheld menu: Compass+Ring. The “compass” integrates gesture, gaze, and speech into a pie menu, whereas the “ring” serves as a shortcut menu. The Compass menu improves interaction performance and comfortability in one-handed scenarios, and the Ring menu alleviates eye fatigue when both hands are free. We evaluated five handheld menus: Touch, Gaze+Pinch, Speech+Pinch, Bangles, and Compass+Ring. We first analyze the usability of these menus in three different scenarios, and then conduct a user study about these menus in geometry matching and line drawing tasks. The results show that the Bangles menu and the Compass+Ring menu are more suitable for one-handed scenarios than the other three menus, and the Compass+Ring menu is superior to the Bangles menu in terms of efficiency and hand fatigue. In addition, participants indicate that the Ring menu can reduce eye strain for the Compass menu in two-handed scenarios and increase haptic perception.","수많은 애플리케이션에서 뛰어난 인터페이스 디자인은 사용자가 주 작업에 영향을 주지 않고 보조 작업을 최대한 자연스럽게 수행할 수 있도록 해야 합니다. 다중 모드 휴대용 메뉴는 기본 작업과 보조 작업의 자연스러운 전환을 충족하는 일반적으로 선호되는 사용자 인터페이스입니다. 그러나 기존의 멀티모달 핸드헬드 메뉴는 한 손으로 조작할 때 일부 제한이 있거나 편의성 개선이 필요합니다. 이러한 문제를 해결하기 위해 본 논문에서는 새로운 다중 모드 휴대용 메뉴인 Compass+Ring을 제안합니다. ""나침반""은 제스처, 시선, 음성을 파이 메뉴에 통합하는 반면 ""링""은 바로가기 메뉴 역할을 합니다. 나침반 메뉴는 한 손 사용 시나리오에서 상호 작용 성능과 편안함을 향상시키고, 링 메뉴는 양손이 자유로울 때 눈의 피로를 완화합니다. 우리는 터치, 시선+핀치, 말하기+핀치, 뱅글, 나침반+링 등 5가지 휴대용 메뉴를 평가했습니다. We first analyze the usability of these menus in three different scenarios, and then conduct a user study about these menus in geometry matching and line drawing tasks. 그 결과, 한 손 사용 시나리오에는 다른 세 가지 메뉴보다 팔찌 메뉴와 나침반+반지 메뉴가 더 적합한 것으로 나타났으며, 효율성과 손 피로도 측면에서는 나침반+반지 메뉴가 팔찌 메뉴보다 우수한 것으로 나타났다. 또한 참가자들은 링 메뉴가 양손 사용 시나리오에서 나침반 메뉴에 대한 눈의 피로를 줄이고 촉각 인식을 향상시킬 수 있음을 나타냅니다.",https://doi.org/10.1109/ISMAR59233.2023.00062,Interaction & Input,Haptic / Tactile Feedback,User Study,User Study / Empirical Findings
316,2023,Cueing Sequential 6DoF Rigid-Body Transformations in Augmented Reality,증강 현실에서 순차적 6DoF 강체 변환 큐잉,"Augmented reality (AR) has been used to guide users in multi-step tasks, providing information about the current step (cueing) or future steps (precueing). However, existing work exploring cueing and precueing a series of rigid-body transformations requiring rotation has only examined one-degree-of-freedom (DoF) rotations alone or in conjunction with 3DoF translations. In contrast, we address sequential tasks involving 3DoF rotations and 3DoF translations. We built a testbed to compare two types of visualizations for cueing and precueing steps. In each step, a user picks up an object, rotates it in 3D while translating it in 3D, and deposits it in a target 6DoF pose. Action-based visualizations show the actions needed to carry out a step and goal-based visualizations show the desired end state of a step. We conducted a user study to evaluate these visualizations and the efficacy of precueing. Participants performed better with goal-based visualizations than with action-based visualizations, and most effectively with goal-based visualizations aligned with the Euler axis. However, only a few of our participants benefited from precues, most likely because of the cognitive load of 3D rotations.","증강 현실(AR)은 사용자에게 다단계 작업을 안내하고 현재 단계(큐잉) 또는 미래 단계(프리큐잉)에 대한 정보를 제공하는 데 사용되었습니다. 그러나 회전이 필요한 일련의 강체 변환에 대한 큐잉 및 프리큐를 탐색하는 기존 작업에서는 1자유도(DoF) 회전만 단독으로 또는 3DoF 변환과 함께 검사했습니다. 대조적으로, 우리는 3DoF 회전 및 3DoF 변환과 관련된 순차적 작업을 다룹니다. 우리는 큐잉 및 프리큐 단계에 대한 두 가지 유형의 시각화를 비교하기 위해 테스트베드를 구축했습니다. 각 단계에서 사용자는 객체를 집어 3D로 변환하면서 3D로 회전하고 목표 6DoF 포즈에 배치합니다. 작업 기반 시각화는 단계를 수행하는 데 필요한 작업을 보여주고, 목표 기반 시각화는 원하는 단계의 최종 상태를 보여줍니다. 우리는 이러한 시각화와 프리큐의 효율성을 평가하기 위해 사용자 연구를 수행했습니다. 참가자들은 행동 기반 시각화보다 목표 기반 시각화를 통해 더 나은 성과를 거두었으며, 오일러 축에 맞춰 정렬된 목표 기반 시각화를 통해 가장 효과적으로 수행했습니다. 그러나 참가자 중 극소수만이 프리큐의 혜택을 받았는데, 이는 3D 회전의 인지 부하 때문일 가능성이 높습니다.",https://doi.org/10.1109/ISMAR59233.2023.00050,Tracking & Localization,Other,User Study,User Study / Empirical Findings
317,2023,DEAMP: Dominant-Eye-Aware Foveated Rendering with Multi-Parameter optimization,DEAMP: 다중 매개변수 최적화를 통한 우세 눈 인식 포비티드 렌더링,"The increasing use of high-resolution displays and the demand for interactive frame rates presents a major challenge to widespread adoption of virtual reality. Foveated rendering address this issue by lowering pixel sampling rate at the periphery of the display. How-ever, existing techniques do not fully exploit the feature of human binocular vision, i.e., the dominant eye. In this paper, we propose a Dominant-Eye-Aware foveated rendering method optimized with Multi-Parameter foveation (DEAMP). Specifically, we control the level of foveation for both eyes with two distinct sets of foveation parameters. To achieve this, each eye’s visual field is divided into three nested layers based on eccentricity. Multiple parameters govern the level of foveation of each layer, respectively. We conduct user studies to evaluate our method. Experimental results demonstrate that DEAMP is superior in terms of rendering time and reduces the disparity between pixel sampling rate and the visual acuity fall-off model while maintaining the perceptual quality.","고해상도 디스플레이의 사용 증가와 대화형 프레임 속도에 대한 요구는 가상 현실의 광범위한 채택에 큰 과제를 제시합니다. 포비티드 렌더링은 디스플레이 주변의 픽셀 샘플링 속도를 낮춤으로써 이 문제를 해결합니다. 그러나 기존 기술은 인간의 양안시 기능, 즉 주안을 완전히 활용하지 못합니다. 본 논문에서는 DEAMP(Multi-Parameter Foveation)에 최적화된 Dominant-Eye-Aware 포비티드 렌더링 방법을 제안합니다. 구체적으로, 우리는 두 가지 별개의 초점 매개변수 세트를 사용하여 양쪽 눈의 초점 수준을 제어합니다. 이를 달성하기 위해 각 눈의 시야는 편심을 기준으로 세 개의 중첩 레이어로 나뉩니다. 여러 매개변수가 각 레이어의 포브레이션 수준을 각각 제어합니다. 우리는 우리의 방법을 평가하기 위해 사용자 연구를 수행합니다. 실험 결과는 DEAMP가 렌더링 시간 측면에서 우수하고 지각 품질을 유지하면서 픽셀 샘플링 속도와 시력 저하 모델 간의 차이를 줄이는 것으로 나타났습니다.",https://doi.org/10.1109/ISMAR59233.2023.00078,Rendering & Visualization,Eye / Gaze Tracking,Quantitative Experiment,Algorithm / Method
318,2023,Deep Learning-based Simulator Sickness Estimation from 3D Motion,3D 모션을 통한 딥러닝 기반 시뮬레이터 멀미 추정,"This paper presents a novel solution for estimating simulator sickness in HMDs using machine learning and 3D motion data, informed by user-labeled simulator sickness data and user analysis. We conducted a novel VR user study, which decomposed motion data and used an instant dial-based sickness scoring mechanism. We were able to emulate typical VR usage and collect user simulator sickness scores. Our user analysis shows that translation and rotation differently impact user simulator sickness in HMDs. In addition, users’ demographic information and self-assessed simulator sickness susceptibility data are collected and show some indication of potential simulator sickness. Guided by the findings from the user study, we developed a novel deep learning-based solution to better estimate simulator sickness with decomposed 3D motion features and user profile information. The model was trained and tested using the 3D motion dataset with user-labeled simulator sickness and profiles collected from the user study. The results show higher estimation accuracy when using the 3D motion data compared with methods based on optical flow extracted from the recorded video, as well as improved accuracy when decomposing the motion data and incorporating user profile information.","이 논문은 사용자 라벨이 지정된 시뮬레이터 멀미 데이터와 사용자 분석을 통해 얻은 기계 학습 및 3D 모션 데이터를 사용하여 HMD의 시뮬레이터 멀미를 추정하기 위한 새로운 솔루션을 제시합니다. 우리는 모션 데이터를 분해하고 인스턴트 다이얼 기반 질병 점수 매기기 메커니즘을 사용하는 새로운 VR 사용자 연구를 수행했습니다. 우리는 일반적인 VR 사용을 에뮬레이션하고 사용자 시뮬레이터 멀미 점수를 수집할 수 있었습니다. 우리의 사용자 분석에 따르면 변환과 회전은 HMD의 사용자 시뮬레이터 멀미에 서로 다른 영향을 미칩니다. 또한 사용자의 인구통계 정보와 자가 평가된 시뮬레이터 멀미 민감성 데이터가 수집되어 잠재적인 시뮬레이터 멀미에 대한 몇 가지 징후를 보여줍니다. 사용자 연구 결과를 바탕으로 우리는 분해된 3D 모션 기능과 사용자 프로필 정보를 사용하여 시뮬레이터 멀미를 더 잘 예측할 수 있는 새로운 딥 러닝 기반 솔루션을 개발했습니다. 이 모델은 사용자가 라벨을 붙인 시뮬레이터 멀미 및 사용자 연구에서 수집된 프로필이 포함된 3D 모션 데이터 세트를 사용하여 훈련 및 테스트되었습니다. 그 결과, 녹화된 영상에서 추출된 광류 기반 방법에 비해 3차원 동작 데이터를 사용할 때 추정 정확도가 더 높고, 동작 데이터 분해 및 사용자 프로필 정보 통합 시 정확도가 향상되는 것으로 나타났다.",https://doi.org/10.1109/ISMAR59233.2023.00018,Interaction & Input,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method; User Study / Empirical Findings
319,2023,DeepMetricEye: Metric Depth Estimation in Periocular VR Imagery,DeepMetricEye: 안구 주변 VR 이미지의 미터법 깊이 추정,"Despite the enhanced realism and immersion provided by VR headsets, users frequently encounter adverse effects such as digital eye strain (DES), dry eye, and potential long-term visual impairment due to excessive eye stimulation from VR displays and pressure from the mask. Recent VR headsets are increasingly equipped with eye-oriented monocular cameras to segment ocular feature maps. Yet, to compute the incident light stimulus and observe periocular condition alterations, it is imperative to transform these relative measurements into metric dimensions. To bridge this gap, we propose a lightweight framework derived from the U-Net 3 + deep learning backbone that we re-optimised, to estimate measurable periocular depth maps. Compatible with any VR headset equipped with an eye-oriented monocular camera, our method reconstructs three-dimensional periocular regions, providing a metric basis for related light stimulus calculation protocols and medical guidelines. Navigating the complexities of data collection, we introduce a Dynamic Periocular Data Generation (DPDG) environment based on UE MetaHuman, which synthesises thousands of training images from a small quantity of human facial scan data. Evaluated on a sample of 36 participants, our method exhibited notable efficacy in the periocular global precision evaluation experiment, and the pupil diameter measurement.","VR 헤드셋이 제공하는 향상된 현실감과 몰입감에도 불구하고 사용자는 VR 디스플레이의 과도한 눈 자극과 마스크의 압력으로 인해 디지털 눈의 피로(DES), 안구 건조, 잠재적인 장기적인 시각 장애 등의 부작용을 자주 경험합니다. 최근 VR 헤드셋에는 안구 특징 맵을 분할하기 위해 눈 중심의 단안 카메라가 점점 더 많이 장착되고 있습니다. 그러나 입사광 자극을 계산하고 안구 주위 상태 변경을 관찰하려면 이러한 상대 측정값을 미터법 차원으로 변환하는 것이 필수적입니다. 이러한 격차를 해소하기 위해 우리는 측정 가능한 눈주위 깊이 맵을 추정하기 위해 다시 최적화한 U-Net 3 + 딥 러닝 백본에서 파생된 경량 프레임워크를 제안합니다. 눈 지향 단안 카메라가 장착된 모든 VR 헤드셋과 호환되는 당사의 방법은 3차원 안구 주위 영역을 재구성하여 관련 광 자극 계산 프로토콜 및 의료 지침에 대한 미터법 기반을 제공합니다. 데이터 수집의 복잡성을 탐색하면서 소량의 인간 얼굴 스캔 데이터에서 수천 개의 훈련 이미지를 합성하는 UE MetaHuman을 기반으로 하는 DPDG(동적 주변 데이터 생성) 환경을 소개합니다. 36명의 참가자 표본을 대상으로 평가한 결과, 우리의 방법은 안구 주위 전체 정밀도 평가 실험과 동공 직경 측정에서 주목할만한 효능을 나타냈습니다.",https://doi.org/10.1109/ISMAR59233.2023.00058,Perception & Cognition; Education & Training,Deep Learning / Neural Networks,Quantitative Experiment,Algorithm / Method
320,2023,Detecting Teacher Expertise in an Immersive VR Classroom: Leveraging Fused Sensor Data with Explainable Machine Learning Models,몰입형 VR 교실에서 교사의 전문성 감지: 설명 가능한 머신 러닝 모델로 융합된 센서 데이터 활용,"Currently, VR technology is increasingly being used in applications to enable immersive yet controlled research settings. One such area of research is expertise assessment, where novel technological approaches to collecting process data, specifically eye tracking, in combination with explainable models, can provide insights into assessing and training novices, as well as fostering expertise development. We present a machine learning approach to predict teacher expertise by leveraging data from an off-the-shelf VR device collected in a VirATec study. By fusing eye-tracking and controller-tracking data, teachers’ recognition and handling of disruptive events in the classroom are taken into account or considered. Three classification models were compared, including SVM, Random Forest, and LightGBM, with Random Forest achieving the best ROC-AUC score of 0.768 in predicting teacher expertise. The SHAP approach to model interpretation revealed informative features (e.g., fixations on identified disruptive students) for distinguishing teacher expertise. Our study serves as a pioneering effort in assessing teacher expertise using eye tracking within an interactive virtual setting, paving the way for future research and advancements in the field.","현재 VR 기술은 몰입감 넘치면서도 통제된 연구 설정을 가능하게 하는 응용 분야에서 점점 더 많이 사용되고 있습니다. 이러한 연구 분야 중 하나는 전문 지식 평가입니다. 프로세스 데이터 수집, 특히 시선 추적에 대한 새로운 기술 접근 ​​방식을 설명 가능한 모델과 결합하여 초보자 평가 및 교육에 대한 통찰력을 제공하고 전문 지식 개발을 육성할 수 있습니다. 우리는 VirATec 연구에서 수집된 기성 VR 장치의 데이터를 활용하여 교사의 전문성을 예측하는 기계 학습 접근 방식을 제시합니다. 시선 추적과 컨트롤러 추적 데이터를 융합함으로써 교실에서 발생하는 파괴적인 사건에 대한 교사의 인식 및 처리가 고려됩니다. SVM, Random Forest 및 LightGBM을 포함한 세 가지 분류 모델을 비교했으며 Random Forest는 교사 전문성 예측에서 최고의 ROC-AUC 점수 0.768을 달성했습니다. 모델 해석에 대한 SHAP 접근 방식은 교사 전문성을 구별하기 위한 유익한 특징(예: 식별된 파괴적인 학생에 대한 고정)을 보여주었습니다. 우리의 연구는 대화형 가상 환경에서 시선 추적을 사용하여 교사의 전문성을 평가하는 선구적인 노력으로 해당 분야의 향후 연구 및 발전을 위한 길을 닦습니다.",https://doi.org/10.1109/ISMAR59233.2023.00083,Education & Training; Interaction & Input,Eye / Gaze Tracking,Technical Evaluation,Algorithm / Method; Hardware / Device
321,2023,DualStream: Spatially Sharing Selves and Surroundings using Mobile Devices and Augmented Reality,DualStream: 모바일 기기와 증강 현실을 활용하여 자신과 주변 환경을 공간적으로 공유,"In-person human interaction relies on our spatial perception of each other and our surroundings. Current remote communication tools partially address each of these aspects. Video calls convey real user representations but without spatial interactions. Augmented and Virtual Reality (AR/VR) experiences are immersive and spatial but often use virtual environments and characters instead of real-life representations. Bridging these gaps, we introduce DualStream, a system for synchronous mobile AR remote communication that captures, streams, and displays spatial representations of users and their surroundings. DualStream supports transitions between user and environment representations with different levels of visuospatial fidelity, as well as the creation of persistent shared spaces using environment snapshots. We demonstrate how DualStream can enable spatial communication in real-world contexts, and support the creation of blended spaces for collaboration. A formative evaluation of DualStream revealed that users valued the ability to interact spatially and move between representations, and could see DualStream fitting into their own remote communication practices in the near future. Drawing from these findings, we discuss new opportunities for designing more widely accessible spatial communication tools, centered around the mobile phone.","대면 인간 상호 작용은 서로와 주변 ​​환경에 대한 공간 인식에 의존합니다. 현재의 원격 통신 도구는 이러한 각 측면을 부분적으로 다루고 있습니다. 영상 통화는 실제 사용자 표현을 전달하지만 공간적 상호 작용은 없습니다. 증강 및 가상 현실(AR/VR) 경험은 몰입감 있고 공간적이지만 실제 표현 대신 가상 환경과 캐릭터를 사용하는 경우가 많습니다. 이러한 격차를 해소하기 위해 우리는 사용자와 주변 환경의 공간적 표현을 캡처, 스트리밍 및 표시하는 동기식 모바일 AR 원격 통신 시스템인 DualStream을 소개합니다. DualStream은 다양한 수준의 시공간 충실도를 갖춘 사용자와 환경 표현 간 전환은 물론 환경 스냅샷을 사용한 영구 공유 공간 생성을 지원합니다. 우리는 DualStream이 어떻게 실제 상황에서 공간 커뮤니케이션을 가능하게 하고 협업을 위한 혼합 공간 생성을 지원하는지 보여줍니다. DualStream에 대한 형성적 평가에 따르면 사용자는 공간적으로 상호 작용하고 표현 간을 이동할 수 있는 능력을 높이 평가했으며 가까운 미래에 DualStream이 자신의 원격 통신 방식에 적합할 것으로 예상됩니다. 이러한 결과를 바탕으로 우리는 휴대폰을 중심으로 더욱 광범위하게 접근 가능한 공간 통신 도구를 설계할 수 있는 새로운 기회에 대해 논의합니다.",https://doi.org/10.1109/ISMAR59233.2023.00028,Collaboration & Social; Perception & Cognition,Optical / Display Technology,Simulation,System / Framework
322,2023,EEG-Based Error Detection Can Challenge Human Reaction Time in a VR Navigation Task,EEG 기반 오류 감지는 VR 탐색 작업에서 인간의 반응 시간에 도전할 수 있습니다.,"Error perception is known to elicit distinct brain patterns, which can be used to improve the usability of systems facilitating human-computer interactions, such as brain-computer interfaces. This requires a high-accuracy detection of erroneous events, e.g., misinterpretations of the user’s intention by the interface, to allow for suitable reactions of the system. In this work, we concentrate on steering-based navigation tasks. We present a combined electroencephalography-virtual reality (VR) study investigating different approaches for error detection and simultaneously exploring the corrective human behavior to erroneous events in a VR flight simulation. We could classify different errors allowing us to analyze neural signatures of unexpected changes in the VR. Moreover, the presented models could detect errors faster than participants naturally responded to them. This work could contribute to developing adaptive VR applications that exclusively rely on the user’s physiological information.",오류 인식은 뇌-컴퓨터 인터페이스와 같이 인간-컴퓨터 상호 작용을 촉진하는 시스템의 유용성을 향상시키는 데 사용할 수 있는 뚜렷한 뇌 패턴을 이끌어내는 것으로 알려져 있습니다. 이를 위해서는 시스템의 적절한 반응을 가능하게 하기 위해 잘못된 이벤트(예: 인터페이스에 의한 사용자 의도의 잘못된 해석)를 매우 정확하게 감지해야 합니다. 이 작업에서는 스티어링 기반 탐색 작업에 중점을 둡니다. 우리는 오류 감지를 위한 다양한 접근 방식을 조사하는 동시에 VR 비행 시뮬레이션에서 잘못된 이벤트에 대한 교정 인간 행동을 탐색하는 결합된 뇌파검사-가상 현실(VR) 연구를 제시합니다. 다양한 오류를 분류하여 VR의 예상치 못한 변화에 대한 신경 신호를 분석할 수 있습니다. 더욱이 제시된 모델은 참가자가 자연스럽게 반응하는 것보다 더 빠르게 오류를 감지할 수 있었습니다. 이 작업은 사용자의 생리학적 정보에만 의존하는 적응형 VR 애플리케이션 개발에 기여할 수 있습니다.,https://doi.org/10.1109/ISMAR59233.2023.00113,Interaction & Input,Sensor Fusion,User Study,Algorithm / Method; User Study / Empirical Findings
323,2023,Edge-Centric Space Rescaling with Redirected Walking for Dissimilar Physical-Virtual Space Registration,서로 다른 물리적-가상 공간 등록을 위한 리디렉션 보행을 통한 엣지 중심 공간 재조정,"We propose a novel space-rescaling technique for registering dissimilar physical-virtual spaces by utilizing the effects of adjusting physical space with redirected walking. Achieving a seamless and immersive Virtual Reality (VR) experience requires overcoming the spatial heterogeneities between the physical and virtual spaces and accurately aligning the VR environment with the user’s tracked physical space. However, existing space-matching algorithms that rely on one-to-one scale mapping are inadequate when dealing with highly dissimilar physical and virtual spaces, and redirected walking controllers could not utilize basic geometric information from physical space in the virtual space due to coordinate distortion. To address these issues, we apply relative translation gains to partitioned space grids based on the main interactable object’s edge, which enables space-adaptive modification effects of physical space without coordinate distortion. Our evaluation results demonstrate the effectiveness of our algorithm in aligning the main object’s edge, surface, and wall, as well as securing the largest registered area compared to alternative methods under all conditions. These findings can be used to create an immersive play area for VR content where users can receive passive feedback from the plane and edge in their physical environment.","우리는 방향 전환을 통한 물리적 공간 조정 효과를 활용하여 서로 다른 물리적-가상 공간을 등록하는 새로운 공간 크기 조정 기법을 제안합니다. 원활하고 몰입도 높은 가상 현실(VR) 경험을 달성하려면 물리적 공간과 가상 공간 사이의 공간적 이질성을 극복하고 VR 환경을 사용자가 추적하는 물리적 공간과 정확하게 정렬해야 합니다. 그러나 일대일 스케일 매핑에 의존하는 기존 공간 매칭 알고리즘은 매우 다른 물리적 공간과 가상 공간을 처리하는 데 부적절하며 방향이 지정된 보행 컨트롤러는 좌표 왜곡으로 인해 물리적 공간의 기본 기하학적 정보를 가상 공간에서 활용할 수 없습니다. 이러한 문제를 해결하기 위해 우리는 주요 상호 작용 가능한 객체의 가장자리를 기반으로 분할된 공간 그리드에 상대 변환 이득을 적용하여 좌표 왜곡 없이 물리적 공간의 공간 적응 수정 효과를 가능하게 합니다. 우리의 평가 결과는 모든 조건에서 주요 물체의 가장자리, 표면 및 벽을 정렬하고 다른 방법에 비해 가장 큰 등록 영역을 확보하는 데 있어 우리 알고리즘의 효율성을 입증합니다. 이러한 결과는 사용자가 물리적 환경의 평면과 가장자리로부터 수동적 피드백을 받을 수 있는 VR 콘텐츠를 위한 몰입형 플레이 영역을 만드는 데 사용될 수 있습니다.",https://doi.org/10.1109/ISMAR59233.2023.00098,Interaction & Input,Redirected Walking / Locomotion,Other,Algorithm / Method; User Study / Empirical Findings
324,2023,Effect of Grip Style on Peripersonal Target Pointing in VR Head Mounted Displays,VR 헤드 마운트 디스플레이에서 개인 주변 대상 포인팅에 대한 그립 스타일의 영향,"When working in Virtual Reality (VR), the user’s performance is affected by how the user holds the input device (e.g., controller), typically using either a precision or a power grip. Previous work examined these grip styles for 3D pointing at targets at different depths in peripersonal space and found that participants had a lower error rate with the precision grip but identified no difference in movement speed, throughput, or interaction with target depth. Yet, this previous experiment was potentially affected by tracking differences between devices. This paper reports an experiment that partially replicates and extends the previous study by evaluating the effect of grip style on the 3D selection of nearby targets with the same device. Furthermore, our experiment re-investigates the effect of the vergence-accommodation conflict (VAC) present in current stereo displays on 3D pointing in peripersonal space. Our results show that grip style significantly affects user performance. We hope that our results are useful for researchers and designers when creating virtual environments.","가상 현실(VR)에서 작업할 때 사용자의 성능은 일반적으로 정밀 그립이나 파워 그립을 사용하여 사용자가 입력 장치(예: 컨트롤러)를 잡는 방식에 따라 영향을 받습니다. 이전 연구에서는 개인 주변 공간의 다양한 깊이에 있는 대상을 3D 포인팅하기 위해 이러한 그립 스타일을 조사한 결과 참가자가 정밀 그립을 사용하면 오류율이 더 낮았지만 이동 속도, 처리량 또는 대상 깊이와의 상호 작용에는 차이가 없음을 확인했습니다. 그러나 이 이전 실험은 장치 간의 차이를 추적하여 잠재적으로 영향을 받았습니다. 이 논문은 동일한 장치를 사용하여 근처 대상의 3D 선택에 대한 그립 스타일의 효과를 평가함으로써 이전 연구를 부분적으로 복제하고 확장하는 실험을 보고합니다. 또한, 우리의 실험은 주변 공간의 3D 포인팅에 대한 현재 스테레오 디스플레이에 존재하는 VAC(수렴-조절 충돌)의 효과를 재조사합니다. 우리의 결과는 그립 스타일이 사용자 성능에 큰 영향을 미치는 것으로 나타났습니다. 우리의 결과가 연구자와 디자이너가 가상 환경을 만들 때 도움이 되기를 바랍니다.",https://doi.org/10.1109/ISMAR59233.2023.00057,Interaction & Input; Display & Optics,Other,Quantitative Experiment,User Study / Empirical Findings
325,2023,Effects of Interaction with Virtual Pets on Self-Disclosure in Mixed Reality,혼합 현실에서 가상 애완동물과의 상호 작용이 자기 공개에 미치는 영향,"Self-disclosure involves revealing information about oneself to others and is critical in relationship formation to develop trust and understanding, leading to emotional intimacy. In psychotherapy, inducing self-disclosure is critical for a therapist to clearly understand clients and suggest relevant solutions. Companion animals have been known to increase human self-disclosure; hence, we hypothesized that virtual animals could have the same effect, which can be strengthened through interaction. To verify this hypothesis, we implemented a mixed-reality-based interaction between humans and virtual cats through Unity. Participants could interact with the virtual cat using hand gestures and voice commands. Psychological states related to self-disclosure were evaluated using questionnaires after the interaction. Furthermore, participants’ responses to the virtual cats were compared with their responses to non-interactive virtual contents. Participants exhibited higher willingness for self-disclosure with virtual cats compared with virtual humans. Interacting with virtual cats was also found to encourage more self-disclosure than interactions with non-interactive states content. Therefore, virtual animals can induce self-disclosure and can be used in psychotherapy. Our findings demonstrate that virtual animals can be used to provide solutions to mental health problems and can be widely applied in the field of psychotherapy.","자기 공개는 자신에 대한 정보를 다른 사람에게 공개하는 것을 의미하며, 신뢰와 이해를 발전시켜 정서적 친밀감을 형성하는 관계 형성에 매우 중요합니다. 심리치료에서 치료사가 내담자를 명확하게 이해하고 적절한 해결책을 제시하려면 자기 공개를 유도하는 것이 중요합니다. 반려동물은 인간의 자기 공개를 증가시키는 것으로 알려져 있습니다. 따라서 우리는 가상 동물도 상호작용을 통해 강화될 수 있는 동일한 효과를 가질 수 있다는 가설을 세웠습니다. 이 가설을 검증하기 위해 Unity를 통해 인간과 가상 고양이 간의 혼합 현실 기반 상호 작용을 구현했습니다. 참가자들은 손짓과 음성 명령을 사용하여 가상 고양이와 상호작용할 수 있었습니다. 자기 공개와 관련된 심리적 상태는 상호 작용 후 설문지를 사용하여 평가되었습니다. 또한 가상 고양이에 대한 참가자의 반응과 비대화형 가상 콘텐츠에 대한 참가자의 반응을 비교했습니다. 참가자들은 가상 인간에 비해 가상 고양이와 함께 자기 공개에 대한 의지가 더 높았습니다. 가상 고양이와의 상호작용은 비대화형 상태 콘텐츠와의 상호작용보다 자기 공개를 더 많이 장려하는 것으로 나타났습니다. 따라서 가상동물은 자기공개를 유도할 수 있으며 심리치료에 활용될 수 있다. 우리의 연구 결과는 가상 동물이 정신 건강 문제에 대한 해결책을 제공하는 데 사용될 수 있으며 심리 치료 분야에 널리 적용될 수 있음을 보여줍니다.",https://doi.org/10.1109/ISMAR59233.2023.00014,Interaction & Input,Hand / Gesture Recognition,User Study,User Study / Empirical Findings
326,2023,"Effects of Opaque, Transparent and Invisible Hand Visualization Styles on Motor Dexterity in a Virtual Reality Based Purdue Pegboard Test",가상 현실 기반 Purdue Pegboard 테스트에서 불투명하고 투명하며 보이지 않는 손 시각화 스타일이 운동 능력에 미치는 영향,"The virtual hand interaction technique is one of the most common interaction techniques used in virtual reality (VR) systems. A VR application can be designed with different hand visualization styles, which might impact motor dexterity. In this paper, we aim to investigate the effects of three different hand visualization styles — transparent, opaque, and invisible — on participants’ performance through a VR-based Purdue Pegboard Test (PPT). A total of 24 participants were recruited and instructed to place pegs on the board as quickly and accurately as possible. The results indicated that using the invisible hand visualization significantly increased the number of task repetitions completed compared to the opaque hand visualization. However, no significant difference was observed in participants’ preference for the hand visualization styles. These findings suggest that an invisible hand visualization may enhance performance in the VR-based PPT, potentially indicating the advantages of a less obstructive hand visualization style. We hope our results can guide developers, researchers, and practitioners when designing novel virtual hand interaction techniques.","가상손 상호작용 기술은 가상현실(VR) 시스템에서 사용되는 가장 일반적인 상호작용 기술 중 하나이다. VR 애플리케이션은 다양한 손 시각화 스타일로 디자인될 수 있으며, 이는 운동 능력에 영향을 미칠 수 있습니다. 본 논문에서는 VR 기반 Purdue Pegboard Test(PPT)를 통해 투명, 불투명, 투명 등 세 가지 손 시각화 스타일이 참가자의 성과에 미치는 영향을 조사하는 것을 목표로 합니다. 총 24명의 참가자를 모집하여 가능한 한 빠르고 정확하게 보드에 말뚝을 놓도록 지시했습니다. 결과는 보이지 않는 손 시각화를 사용하는 것이 불투명한 손 시각화에 비해 완료된 작업 반복 횟수를 크게 증가시키는 것으로 나타났습니다. 그러나 손 시각화 스타일에 대한 참가자의 선호도에는 유의미한 차이가 관찰되지 않았습니다. 이러한 결과는 보이지 않는 손 시각화가 VR 기반 PPT의 성능을 향상시킬 수 있으며 잠재적으로 덜 방해적인 손 시각화 스타일의 장점을 나타낼 수 있음을 시사합니다. 우리의 결과가 개발자, 연구원 및 실무자가 새로운 가상 손 상호 작용 기술을 설계할 때 지침이 되기를 바랍니다.",https://doi.org/10.1109/ISMAR59233.2023.00087,Interaction & Input,Other,User Study,User Study / Empirical Findings
327,2023,Effects of Speed of a Collocated Virtual Walker and Proximity Toward a Static Virtual Character on Avoidance Movement Behavior,배치된 가상 워커의 속도와 정적 가상 캐릭터에 대한 근접성이 회피 이동 행위에 미치는 영향,"We explored the avoidance movement behaviors of study participants immersed in a virtual reality environment. We placed a static virtual character at the midpoint between the start and target spot for the avoidance task, and a virtual walker character in front of the starting spot and scripted it to reach the target spot. Participants were placed behind the virtual walker in order to measure its influence on participants’ behavior. We developed nine experimental conditions assigned to the virtual walker character by following a 3 (speed: slow vs. normal vs. fast walking speed) $\times 3$ (proximity: close vs. middle vs. far proximity to the static virtual character) study design. For this within-group study, we collected data from 22 study participants to explore how speed and proximity walking patterns assigned to a virtual walker character could impact participants’ avoidance movement behaviors and decisions. Our data revealed that 1) the speed factor impacted the participants’ avoidance movement behavior; 2) the proximity factor did not significantly impact the participants’ avoidance movement behavior; 3) the virtual walker character did not significantly impact participants’ avoidance decisions regarding the static virtual character; 4) in all examined conditions, the side-by-side distances between the participants and the static virtual character were inside the social space according to the proxemics model; and 5) in conditions in which a slow virtual walker character was present or in the condition of normal speed and far proximity, we observed an increased number of participants pass the virtual walker character.","가상 현실 환경에 몰입한 연구 참가자의 회피 동작 행동을 탐색했습니다. 회피 작업을 위해 시작 지점과 목표 지점 사이의 중간 지점에 정적 가상 캐릭터를 배치하고, 시작 지점 앞에 가상 워커 캐릭터를 배치하고 목표 지점에 도달하도록 스크립팅했습니다. 참가자의 행동에 미치는 영향을 측정하기 위해 참가자를 가상 보행기 뒤에 배치했습니다. 우리는 3(속도: 느린 vs. 보통 vs. 빠른 걷기 속도) $\times 3$(근접성: 정적 가상 캐릭터에 가까운 vs. 중간 vs. 먼 근접성) 연구 설계에 따라 가상 워커 캐릭터에 할당된 9가지 실험 조건을 개발했습니다. 이 그룹 내 연구를 위해 우리는 22명의 연구 참가자로부터 데이터를 수집하여 가상 워커 캐릭터에 할당된 속도 및 근접 걷기 패턴이 참가자의 회피 움직임 행동 및 결정에 어떤 영향을 미칠 수 있는지 조사했습니다. 우리의 데이터에 따르면 1) 속도 요인이 참가자의 회피 운동 행동에 영향을 미쳤습니다. 2) 근접 요인은 참가자의 회피 운동 행동에 큰 영향을 미치지 않았습니다. 3) 가상 워커 캐릭터는 정적 가상 캐릭터에 관한 참가자의 회피 결정에 큰 영향을 미치지 않았습니다. 4) 모든 조사 조건에서 참가자와 정적 가상 캐릭터 사이의 나란한 거리는 근접학 모델에 따라 사회적 공간 내부에 있었습니다. 5) 느린 가상 보행기 캐릭터가 존재하는 조건이나 정상 속도 및 원거리 근접 조건에서는 가상 보행기 캐릭터를 통과하는 참가자 수가 증가하는 것을 관찰했습니다.",https://doi.org/10.1109/ISMAR59233.2023.00109,Collaboration & Social,Optical / Display Technology,User Study,Algorithm / Method
328,2023,Effects of Visual Presentation Near the Mouth on Cross-Modal Effects of Multisensory Flavor Perception and Ease of Eating,입 근처의 시각적 표현이 다감각적 향미 인식과 식사 용이성의 교차 모드 효과에 미치는 영향,"Various studies have suggested that altering the appearance of food can impact multisensory flavor perception. The cross-modal effect of such visual changes on gustation may allow for the presentation of food tastes that are difficult to express with simple combinations of taste stimuli. This cross-modal effect of visual changes on gustation holds potential for applications in gustatory displays. However, the current limitation of existing Head-Mounted Displays (HMDs) is their restricted vertical Field of View (FoV), which prohibits the display of images near the mouth while eating. This limitation may impede the cross-modal effect of visual changes on multisensory flavor perception. Additionally, the lack of visibility around the mouth area challenges the ease of eating. To address these issues, we design a Video See-Through (VST)-HMD with an expanded vertical FoV (approx. 100 [deg]). Using the HMD, we investigated how presenting visual information near the mouth affects the cross-modal effects of flavor perception and ease of eating. In our experiment, machine learning techniques were utilized to alter the appearance of food. However, the result showed no significant differences in the amount of cross-modal effects or the ease of eating between the groups with and without visual information near the mouth. As a discussion of this result, the participants may not direct their visual attention to the food when they put the food in their mouths. The experiment also examined whether visual changes alter the taste as well as the smell and texture of the food. The findings demonstrated that visual changes could present the smell and texture of the food following the modifications. This result was confirmed irrespective of the visibility near the mouth.","다양한 연구에 따르면 음식의 외관을 변경하면 다감각적 맛 인식에 영향을 미칠 수 있다고 합니다. 미각에 대한 이러한 시각적 변화의 교차적 효과는 미각 자극의 단순한 조합으로는 표현하기 어려운 음식 맛의 표현을 가능하게 할 수 있습니다. 미각에 대한 시각적 변화의 이러한 교차 모드 효과는 미각 디스플레이에 적용할 가능성이 있습니다. 그러나 현재 기존 헤드마운트디스플레이(HMD)의 한계는 수직 시야(FoV)가 제한되어 있어 식사하는 동안 입 근처의 이미지를 표시할 수 없다는 점입니다. 이러한 제한은 다감각적 풍미 인식에 대한 시각적 변화의 교차 모드 효과를 방해할 수 있습니다. 또한, 입 주변의 가시성이 부족하여 식사의 용이성이 저하됩니다. 이러한 문제를 해결하기 위해 우리는 확장된 수직 FoV(약 100도)를 갖춘 VST(Video See-Through)-HMD를 설계했습니다. 우리는 HMD를 사용하여 입 근처에 시각적 정보를 제공하는 것이 풍미 인식 및 먹기 용이성의 교차 모드 효과에 어떻게 영향을 미치는지 조사했습니다. 우리 실험에서는 기계 학습 기술을 활용하여 음식의 모양을 변경했습니다. 그러나 그 결과, 입 근처에 시각적 정보가 있는 그룹과 없는 그룹 사이에 교차 모드 효과의 양이나 식사 용이성에는 큰 차이가 없는 것으로 나타났습니다. 이 결과에 대한 논의로서 참가자들은 음식을 입에 넣을 때 음식에 시각적 주의를 기울이지 않을 수 있습니다. 실험에서는 시각적인 변화가 음식의 맛은 물론 냄새와 질감에도 영향을 미치는지 여부도 조사했습니다. 연구 결과에 따르면 시각적 변화는 수정 후 식품의 냄새와 질감을 나타낼 수 있음을 보여주었습니다. 이는 입 근처의 시야 여부와 관계없이 확인된 결과이다.",https://doi.org/10.1109/ISMAR59233.2023.00106,Display & Optics; Perception & Cognition,Sensor Fusion,User Study,User Study / Empirical Findings
329,2023,Empirical Evaluation of the Effects of Visuo-Auditory Perceptual Information on Head Oriented Tracking of Dynamic Objects in VR,VR에서 동적 객체의 머리 지향 추적에 대한 시청각 지각 정보의 효과에 대한 실증적 평가,"As virtual reality (VR) technology sees more use in various fields, there is a greater need to understand how to effectively design dynamic virtual environments. As of now, there is still uncertainty in how well users of a VR system are capable of tracking moving targets in a virtual space. In this work, we examined the influence of sensory modality and visual feedback on the accuracy of head-gaze moving target tracking. To this end, a between subjects study was conducted wherein participants would receive targets that were visual, auditory, or audiovisual. Each participant performed two blocks of experimental trials, with a calibration block in between. Results indicate that audiovisual targets promoted greater improvement in tracking performance over single-modality targets, and that audio-only targets are more difficult to track than those of other modalities.","가상 현실(VR) 기술이 다양한 분야에서 더 많이 사용됨에 따라 역동적인 가상 환경을 효과적으로 설계하는 방법에 대한 이해가 더욱 필요해졌습니다. 현재로서는 VR 시스템 사용자가 가상 ​​공간에서 움직이는 표적을 얼마나 잘 추적할 수 있는지에 대해서는 여전히 불확실성이 있습니다. 본 연구에서는 감각 양식과 시각적 피드백이 머리 시선 이동 표적 추적의 정확도에 미치는 영향을 조사했습니다. 이를 위해 참가자가 시각적, 청각적 또는 시청각적 목표를 받는 피험자 간 연구가 수행되었습니다. 각 참가자는 그 사이에 보정 블록을 사용하여 두 블록의 실험 시험을 수행했습니다. 결과는 시청각 목표가 단일 양식 목표에 비해 추적 성능의 더 큰 향상을 촉진했으며 오디오 전용 목표는 다른 양식의 목표보다 추적하기가 더 어렵다는 것을 나타냅니다.",https://doi.org/10.1109/ISMAR59233.2023.00124,Audio & Sound,Other,User Study,User Study / Empirical Findings
330,2023,Enhancing Perception and Immersion in Pre-Captured Environments through Learning-Based Eye Height Adaptation,학습 기반 눈 높이 적응을 통해 사전 캡처된 환경에서 인지 및 몰입도 향상,"Pre-captured immersive environments using omnidirectional cameras provide a wide range of virtual reality applications. Previous research has shown that manipulating the eye height in egocentric virtual environments can significantly affect distance perception and immersion. However, the influence of eye height in pre-captured real environments has received less attention due to the difficulty of altering the perspective after finishing the capture process. To explore this influence, we first propose a pilot study that captures real environments with multiple eye heights and asks participants to judge the egocentric distances and immersion. If a significant influence is confirmed, an effective image-based approach to adapt pre-captured real-world environments to the user’s eye height would be desirable. Motivated by the study, we propose a learning-based approach for synthesizing novel views for omnidirectional images with altered eye heights. This approach employs a multitask architecture that learns depth and semantic segmentation in two formats, and generates high-quality depth and semantic segmentation to facilitate the inpainting stage. With the improved omnidirectional-aware layered depth image, our approach synthesizes natural and realistic visuals for eye height adaptation. Quantitative and qualitative evaluation shows favorable results against state-of-the-art methods, and an extensive user study verifies improved perception and immersion for pre-captured real-world environments.","전방향 카메라를 사용하여 사전 캡처된 몰입형 환경은 광범위한 가상 현실 애플리케이션을 제공합니다. 이전 연구에 따르면 자기중심적인 가상 환경에서 눈 높이를 조작하면 거리 인식과 몰입도에 큰 영향을 미칠 수 있는 것으로 나타났습니다. 그러나 사전 캡처된 실제 환경에서 눈 높이의 영향은 캡처 프로세스를 마친 후 시점을 변경하기 어렵기 때문에 덜 주목을 받았습니다. 이러한 영향을 탐구하기 위해 우리는 먼저 다양한 눈 높이로 실제 환경을 포착하고 참가자에게 자기중심적 거리와 몰입도를 판단하도록 요청하는 파일럿 연구를 제안합니다. 상당한 영향이 확인되면 사전 캡처된 실제 환경을 사용자의 눈 높이에 맞게 조정하는 효과적인 이미지 기반 접근 방식이 바람직할 것입니다. 연구에 영감을 받아 우리는 눈 높이가 변경된 전방향 이미지에 대한 새로운 뷰를 합성하기 위한 학습 기반 접근 방식을 제안합니다. 이 접근 방식은 두 가지 형식으로 깊이와 의미론적 분할을 학습하고 고품질 깊이 및 의미론적 분할을 생성하여 인페인팅 단계를 용이하게 하는 멀티태스크 아키텍처를 사용합니다. 개선된 전방향 인식 계층 깊이 이미지를 통해 우리의 접근 방식은 눈 높이 적응을 위해 자연스럽고 사실적인 시각을 합성합니다. 정량적 및 정성적 평가는 최첨단 방법에 비해 유리한 결과를 보여주며, 광범위한 사용자 연구를 통해 사전 캡처된 실제 환경에 대한 향상된 인식 및 몰입도가 검증되었습니다.",https://doi.org/10.1109/ISMAR59233.2023.00055,Perception & Cognition,Computer Vision,User Study,Algorithm / Method; User Study / Empirical Findings
331,2023,Enhancing Seamless Walking in Virtual Reality: Application of Bone-Conduction Vibration in Redirected Walking,가상 현실에서 원활한 보행 강화: 방향 전환에 골전도 진동 적용,"This study explored bone-conduction vibration (BCV) in redirected walking (RDW), a technology for seamless walking in large virtual spaces within confined physical areas, enhancing obstacle avoidance performance using nonelectrical vestibular stimulation without the side effects caused by electrical stimulation. We proposed four different BCV stimulation methods and evaluated their detection threshold (DT) extension performance and user experience in virtual reality (VR) conditions. The DT was successfully expanded from at least 23% to 45% under all BCV conditions while preserving the immersion and presence. Notably, user comfort increased when content sound was used for vestibular stimulation. Under the extended DT condition, a simulation study demonstrated that all BCV stimulation methods facilitated uninterrupted walking over extended distances when applying RDW to users with random movements. Thus, this research established the viability of using BCV in RDW applications and the potential for incorporating content sound into BCV stimulation techniques.","본 연구에서는 제한된 물리적 영역 내의 넓은 가상 공간에서 원활한 보행을 위한 기술인 RDW(Redirected Walking)의 골전도 진동(BCV)을 탐색하여 전기 자극으로 인한 부작용 없이 비전기 전정 자극을 사용하여 장애물 회피 성능을 향상시켰습니다. 우리는 네 가지 BCV 자극 방법을 제안하고 가상 현실(VR) 조건에서 감지 임계값(DT) 확장 성능과 사용자 경험을 평가했습니다. 몰입감과 존재감을 유지하면서 모든 BCV 조건에서 DT가 최소 23%에서 45%로 성공적으로 확장되었습니다. 특히, 전정 자극을 위해 콘텐츠 사운드를 사용했을 때 사용자 편의성이 향상되었습니다. 확장된 DT 조건에서 시뮬레이션 연구에 따르면 모든 BCV 자극 방법은 무작위로 움직이는 사용자에게 RDW를 적용할 때 확장된 거리에서 중단 없이 걷기를 촉진하는 것으로 나타났습니다. 따라서 이 연구는 RDW 애플리케이션에서 BCV를 사용할 수 있는 가능성과 컨텐츠 사운드를 BCV 자극 기술에 통합할 수 있는 가능성을 확립했습니다.",https://doi.org/10.1109/ISMAR59233.2023.00135,Perception & Cognition,Sensor Fusion,Simulation,Algorithm / Method
332,2023,Evaluating 3D User Interaction Techniques on Spatial Working Memory for 3D Scatter Plot Exploration in Immersive Analytics,몰입형 분석에서 3D 산점도 탐색을 위한 공간 작업 메모리에 대한 3D 사용자 상호 작용 기술 평가,"This work evaluates three 3D user interaction techniques to investigate their visuo-spatial working memory support for users’ data exploration in immersive analytics. Two techniques are the common VR locomotion technique, Walking and Teleportation, while the other one is Grab, an object manipulation technique. We present two formal user studies in VR and AR. Our study is designed based on the Corsi block-tapping Task, a psychological test for assessing visuo-spatial working memory. Our study results show that Walking supports spatial memory best, and Grab follows. Though Teleportation is found to support it the least, participants rated Teleportation as the easiest way to move in the VR study. We also compare the Walking and Grab results in the VR and AR studies and discuss differences. At last, we discuss our limitations and future work.","이 연구에서는 몰입형 분석에서 사용자의 데이터 탐색을 위한 시공간 작업 메모리 지원을 조사하기 위해 세 가지 3D 사용자 상호 작용 기술을 평가합니다. 두 가지 기술은 일반적인 VR 이동 기술인 걷기(Walk)와 순간이동(Teleportation)이고, 다른 하나는 객체 조작 기술인 그랩(Grab)입니다. 우리는 VR과 AR에 대한 두 가지 공식적인 사용자 연구를 제시합니다. 우리의 연구는 시공간 작업 기억을 평가하기 위한 심리 테스트인 Corsi 블록 탭핑 작업을 기반으로 설계되었습니다. 우리의 연구 결과에 따르면 걷기는 공간 기억을 가장 잘 지원하고 그랩이 뒤따르는 것으로 나타났습니다. 순간이동은 이를 가장 적게 지원하는 것으로 밝혀졌지만 참가자들은 VR 연구에서 순간이동을 이동하는 가장 쉬운 방법으로 평가했습니다. 또한 VR 및 AR 연구에서 걷기 및 잡기 결과를 비교하고 차이점에 대해 논의합니다. 마지막으로 우리의 한계와 향후 작업에 대해 논의합니다.",https://doi.org/10.1109/ISMAR59233.2023.00066,Interaction & Input,Redirected Walking / Locomotion,User Study,Interaction Technique
333,2023,Evaluating the Feasibility of Predicting Information Relevance During Sensemaking with Eye Gaze Data,시선 데이터를 이용한 센스메이킹 중 정보 관련성 예측의 타당성 평가,"Eye gaze patterns vary based on reading purpose and complexity, and can provide insights into a reader’s perception of the content. We hypothesize that during a complex sensemaking task with many text-based documents, we will be able to use eye-tracking data to predict the importance of documents and words, which could be the basis for intelligent suggestions made by the system to an analyst. We introduce a novel eye-gaze metric called ‘GazeScore’ that predicts an analyst’s perception of the relevance of each document and word when they perform a sensemaking task. We conducted a user study to assess the effectiveness of this metric and found strong evidence that documents and words with high GazeScores are perceived as more relevant, while those with low GazeScores were considered less relevant. We explore potential real-time applications of this metric to facilitate immersive sensemaking tasks by offering relevant suggestions.","시선 패턴은 독서 목적과 복잡성에 따라 달라지며 콘텐츠에 대한 독자의 인식에 대한 통찰력을 제공할 수 있습니다. 우리는 텍스트 기반 문서가 많은 복잡한 감지 작업 중에 시선 추적 데이터를 사용하여 문서와 단어의 중요성을 예측할 수 있으며, 이는 시스템이 분석가에게 제공하는 지능적인 제안의 기초가 될 수 있다고 가정합니다. 분석가가 감각 작업을 수행할 때 각 문서와 단어의 관련성에 대한 인식을 예측하는 'GazeScore'라는 새로운 시선 측정 항목을 소개합니다. 우리는 이 지표의 효율성을 평가하기 위해 사용자 연구를 수행했으며 GazeScore가 높은 문서와 단어는 더 관련성이 높은 것으로 인식되는 반면, GazeScore가 낮은 문서와 단어는 관련성이 낮은 것으로 간주된다는 강력한 증거를 발견했습니다. 우리는 관련 제안을 제공함으로써 몰입형 센스메이킹 작업을 촉진하기 위해 이 측정항목의 잠재적인 실시간 적용을 탐색합니다.",https://doi.org/10.1109/ISMAR59233.2023.00086,Tracking & Localization,Optical / Display Technology,User Study,System / Framework
334,2023,Expansion of Detection Thresholds for Hand Redirection using Noisy Tendon Electrical Stimulation,시끄러운 힘줄 전기 자극을 사용하여 손 방향 전환에 대한 감지 임계값 확장,"To increase the flexibility of haptic feedback in virtual reality (VR), hand redirection (HR) has been proposed to shift the hand’s virtual position from its actual position. To expand the range of HR applications, a method to broaden the detection threshold (DT), which is the maximum amount of shift that can be applied without the user noticing, is required. Multisensory integration studies have revealed that the reliability of senses affects the weight of integration. To expand the DTs of HR, we propose a method to increase visual dominance in the integration of vision and proprioception by introducing noise to the latter, thereby decreasing its reliability through weak Gaussian white noise electrical stimulation ($\sigma$=0.5mA). The results of a user study comprising 22 participants (11 women and 11 men) confirm that noisy electrical stimulation significantly expands the DTs of HR with the mean range of DTs ($R_{DT}$) was 20.48° (SD =7.90) with electrical stimulation and 19.15° (SD =7.11) without electrical stimulation. Interestingly, this effect was only observed in women. The average $R_{DT}$ for men was 15.36° (SD =6.13) and 15.18°(SD=5.58), whereas that for women was 25. 61°(SD=5.89) and 23.12°(SD=6.21), with and without electrical stimulation, respectively. Electrical stimulation was mostly tolerable for the participants and did not affect embodiment or presence ratings. These results suggest that expansion of the DT without disturbing the user’s VR experience is feasible.","가상 현실(VR)에서 촉각 피드백의 유연성을 높이기 위해 손의 가상 위치를 실제 위치에서 이동시키는 손 방향 전환(HR)이 제안되었습니다. HR 적용 범위를 확대하기 위해서는 사용자가 눈치채지 못하게 적용할 수 있는 최대 이동량인 DT(Detection Threshold)를 넓힐 수 있는 방법이 필요하다. 다감각 통합 연구에 따르면 감각의 신뢰성이 통합의 가중치에 영향을 미치는 것으로 나타났습니다. HR의 DT를 확장하기 위해 우리는 시각과 고유 감각의 통합에서 시각 지배력을 높이는 방법을 후자에 도입하여 약한 가우스 백색 잡음 전기 자극($\sigma$=0.5mA)을 통해 신뢰도를 낮추는 방법을 제안합니다. 22명의 참가자(여성 11명, 남성 11명)로 구성된 사용자 연구 결과는 시끄러운 전기 자극이 HR의 DT를 크게 확장한다는 것을 확인했습니다. 평균 DT 범위($R_{DT}$)는 전기 자극의 경우 20.48°(SD =7.90), 전기 자극이 없는 경우 19.15°(SD =7.11)였습니다. 흥미롭게도 이 효과는 여성에게서만 관찰되었습니다. 남성의 평균 $R_{DT}$는 15.36°(SD =6.13) 및 15.18°(SD=5.58)인 반면, 여성의 경우 전기 자극 유무에 따라 각각 25.61°(SD=5.89) 및 23.12°(SD=6.21)였습니다. 전기 자극은 참가자들에게 대부분 견딜 수 있었으며 구체화 또는 존재 등급에 영향을 미치지 않았습니다. 이러한 결과는 사용자의 VR 경험을 방해하지 않고 DT를 확장하는 것이 가능함을 시사합니다.",https://doi.org/10.1109/ISMAR59233.2023.00119,Interaction & Input; Perception & Cognition,Sensor Fusion,User Study,Algorithm / Method
335,2023,Exploring Effective Immersive Approaches to Visualizing WiFi,WiFi 시각화를 위한 효과적인 몰입형 접근 방식 탐색,"WiFi networks are essential to our daily lives, but their signals are not visible to us. Therefore, it is challenging to evaluate the health of a network or make changes to ensure an optimal configuration. Traditional visualization approaches, such as contour lines, are not intuitive and lead to challenges in the analysis and comprehension of networks. In this paper, we introduce two novel visualizations: Wavelines and Stacked Bars. We then compared these visualizations to the state-of-the-art visualization technique of contour lines. We carried out a user study with 32 participants to validate that our novel visualizations can improve user confidence, accuracy, and completion time for the tasks of router localization, ranking of signal strengths, channel interference, and router coverage. We selected these tasks after extensive discussions with domain experts. We believe that our findings will assist network analysts in visually understanding our increasingly rich signal environments.","WiFi 네트워크는 우리 일상 생활에 필수적이지만 그 신호는 우리에게 보이지 않습니다. 따라서 네트워크 상태를 평가하거나 최적의 구성을 보장하기 위해 변경하는 것은 어렵습니다. 등고선과 같은 기존 시각화 접근 방식은 직관적이지 않으며 네트워크 분석 및 이해에 어려움을 겪습니다. 이 문서에서는 웨이브라인과 누적 막대라는 두 가지 새로운 시각화를 소개합니다. 그런 다음 이러한 시각화를 등고선의 최첨단 시각화 기술과 비교했습니다. 우리는 새로운 시각화가 라우터 위치 파악, 신호 강도 순위 지정, 채널 간섭 및 라우터 적용 범위 작업에 대한 사용자 신뢰도, 정확성 및 완료 시간을 향상시킬 수 있음을 검증하기 위해 32명의 참가자를 대상으로 사용자 연구를 수행했습니다. 우리는 도메인 전문가와의 광범위한 논의를 거쳐 이러한 작업을 선택했습니다. 우리는 우리의 연구 결과가 네트워크 분석가가 점점 더 풍부해지는 신호 환경을 시각적으로 이해하는 데 도움이 될 것이라고 믿습니다.",https://doi.org/10.1109/ISMAR59233.2023.00088,Tracking & Localization,Other,User Study; Quantitative Experiment,User Study / Empirical Findings; Algorithm / Method
336,2023,Exploring Trajectory Data in Augmented Reality: A Comparative Study of Interaction Modalities,증강 현실의 궤적 데이터 탐색: 상호 작용 양식에 대한 비교 연구,"The visual exploration of trajectory data is crucial in domains such as animal behavior, molecular dynamics, and transportation. With the emergence of immersive technology, trajectory data, which is often inherently three-dimensional, can be analyzed in stereoscopic 3D, providing new opportunities for perception, engagement, and understanding. However, the interaction with the presented data remains a key challenge. While most applications depend on hand tracking, we see eye tracking as a promising yet under-explored interaction modality, while challenges such as imprecision or inadvertently triggered actions need to be addressed. In this work, we explore the potential of eye gaze interaction for the visual exploration of trajectory data within an AR environment. We integrate hand- and eye-based interaction techniques specifically designed for three common use cases and address known eye tracking challenges. We refine our techniques and setup based on a pilot user study (n=6) and find in a follow-up study (n=20) that gaze interaction can compete with hand-tracked interaction regarding effectiveness, efficiency, and task load for selection and cluster exploration tasks. However, time step analysis comes with higher answer times and task load. In general, we find the results and preferences to be user-dependent. Our work contributes to the field of immersive data exploration, underscoring the need for continued research on eye tracking interaction.","궤적 데이터의 시각적 탐색은 동물 행동, 분자 역학 및 운송과 같은 영역에서 매우 중요합니다. 몰입형 기술의 출현으로 본질적으로 3차원인 경우가 많은 궤도 데이터를 입체적인 3D로 분석하여 인식, 참여 및 이해를 위한 새로운 기회를 제공할 수 있습니다. 그러나 제시된 데이터와의 상호 작용은 여전히 ​​중요한 과제로 남아 있습니다. 대부분의 응용 프로그램은 손 추적에 의존하지만 시선 추적은 유망하지만 아직 충분히 탐구되지 않은 상호 작용 방식으로 보고 있으며, 부정확성 또는 부주의하게 트리거된 작업과 같은 문제는 해결해야 합니다. 이 작업에서 우리는 AR 환경 내에서 궤적 데이터의 시각적 탐색을 위한 시선 상호 작용의 잠재력을 탐구합니다. 우리는 세 가지 일반적인 사용 사례를 위해 특별히 설계된 손 및 눈 기반 상호 작용 기술을 통합하고 알려진 시선 추적 문제를 해결합니다. 우리는 파일럿 사용자 연구(n=6)를 기반으로 기술과 설정을 개선하고 후속 연구(n=20)에서 시선 상호 작용이 선택 및 클러스터 탐색 작업에 대한 효율성, 효율성 및 작업 부하와 관련하여 수동 추적 상호 작용과 경쟁할 수 있음을 확인했습니다. 그러나 시간 단계 분석에는 응답 시간과 작업 부하가 더 높습니다. 일반적으로 결과와 선호도는 사용자에 따라 다릅니다. 우리의 작업은 몰입형 데이터 탐색 분야에 기여하며 시선 추적 상호 작용에 대한 지속적인 연구의 필요성을 강조합니다.",https://doi.org/10.1109/ISMAR59233.2023.00094,Interaction & Input,Eye / Gaze Tracking,User Study,User Study / Empirical Findings
337,2023,"Exploring the Effects of VR Activities on Stress Relief: A Comparison of Sitting-in-Silence, VR Meditation, and VR Smash Room","VR 활동이 스트레스 해소에 미치는 영향 탐색: 침묵, VR 명상, VR 스매시룸 비교","In our lives, we encounter various stressors that may cause negative mental and bodily reactions to make us feel frustrated, angry, or irritated. Effective methods to manage or reduce stress and anxiety are essential for a healthy life, and several stress-management approaches are found to be useful for stress relief, such as meditation, taking a rest, walking around nature, or even breaking things in a smash room. Previous research has revealed that certain experiences in virtual reality (VR) are effective for reducing stress as traditional real-world methods. However, it is still unclear how the stress relief effects are associated with other factors like individual user profile in terms of different treatment activities. In this paper, we report our findings from a formal user study that investigates the effects of two virtual activities: (1) VR Meditation and (2) VR Smash Room experience, compared with a traditional Sitting-in-Silence method. Our results show that VR Meditation has a better stress relief effect compared to VR Smash Room and Sitting-in-Silence, and the effects of the treatments are correlated with the participants’ personalities. We discuss the findings and implications addressing potential benefits/impacts of different stress-relief activities in VR.","우리는 살면서 부정적인 정신적, 육체적 반응을 일으켜 좌절감, 분노, 짜증을 느끼게 하는 다양한 스트레스 요인을 접하게 됩니다. 스트레스와 불안을 효과적으로 관리하거나 줄이는 방법은 건강한 삶에 필수적이며, 명상, 휴식, 자연 산책, 심지어 방에서 물건 부수기 등 여러 가지 스트레스 관리 접근법이 스트레스 해소에 유용한 것으로 밝혀졌습니다. 이전 연구에서는 가상 현실(VR)에서의 특정 경험이 전통적인 현실 세계 방법처럼 스트레스를 줄이는 데 효과적인 것으로 나타났습니다. 그러나 다양한 치료 활동 측면에서 스트레스 해소 효과가 개별 사용자 프로필과 같은 다른 요인과 어떻게 연관되어 있는지는 여전히 불분명합니다. 본 논문에서는 (1) VR 명상과 (2) VR 스매시 룸 경험이라는 두 가지 가상 활동의 효과를 전통적인 침묵 방식과 비교하여 조사한 공식 사용자 연구 결과를 보고합니다. 우리의 결과는 VR 명상이 VR 스매시 룸 및 침묵에 앉아 있는 것에 비해 더 나은 스트레스 해소 효과를 가지며 치료 효과는 참가자의 성격과 상관 관계가 있음을 보여줍니다. 우리는 VR에서 다양한 스트레스 해소 활동의 잠재적 이점/영향을 다루는 연구 결과와 의미에 대해 논의합니다.",https://doi.org/10.1109/ISMAR59233.2023.00103,Other,Optical / Display Technology,User Study,User Study / Empirical Findings; Algorithm / Method
338,2023,Exploring the Effects of Virtually-Augmented Display Sizes on Users' Spatial Memory in Smartwatches,스마트워치에서 가상으로 증강된 디스플레이 크기가 사용자의 공간 메모리에 미치는 영향 탐색,"The small display size of the smartwatches makes it difficult to display large amounts of information on the device. Prior work explored leveraging a second device (e.g., Head-mounted displays) to extend the space where users can access large information space with virtual displays anchored on their wrists. Though researchers showed that having an additional virtual screen increased information bandwidth, little is known about the effect of virtual display sizes on users’ performance. In this paper, we examined the impact of display sizes on spatial memory, workload, and user experience to better understand the prospects of virtually-augmented displays for smartwatches. Results from a user study revealed that a 4.8 inches display size can be the “sweet spot” for the virtually-augmented displays to ensure improved spatial memory performance and better user experience with less workload. Finally, we provided a set of design guidelines focusing to display size, spatial memory, user experience, and workload for designing virtually augmented user interfaces for smartwatches.","스마트워치는 디스플레이 크기가 작아서 많은 양의 정보를 기기에 표시하기가 어렵습니다. 이전 작업에서는 두 번째 장치(예: 머리 장착형 디스플레이)를 활용하여 사용자가 손목에 고정된 가상 디스플레이를 통해 대규모 정보 공간에 액세스할 수 있는 공간을 확장하는 방법을 연구했습니다. 연구자들은 추가 가상 화면을 사용하면 정보 대역폭이 증가한다는 사실을 보여주었지만 가상 디스플레이 크기가 사용자 성능에 미치는 영향에 대해서는 알려진 바가 거의 없습니다. 본 논문에서는 스마트워치용 가상 증강 디스플레이의 전망을 더 잘 이해하기 위해 디스플레이 크기가 공간 메모리, 작업 부하 및 사용자 경험에 미치는 영향을 조사했습니다. 사용자 연구 결과에 따르면 4.8인치 디스플레이 크기는 향상된 공간 메모리 성능과 더 적은 작업 부하로 더 나은 사용자 경험을 보장하기 위한 가상 증강 디스플레이의 ""최적 지점""이 될 수 있는 것으로 나타났습니다. 마지막으로 우리는 스마트워치용 가상 증강 사용자 인터페이스를 디자인하기 위한 디스플레이 크기, 공간 메모리, 사용자 경험 및 작업량에 초점을 맞춘 일련의 디자인 지침을 제공했습니다.",https://doi.org/10.1109/ISMAR59233.2023.00070,Interaction & Input; Display & Optics,Optical / Display Technology,User Study,User Study / Empirical Findings
339,2023,Exploring the Impact of User and System Factors on Human-AI Interactions in Head-Worn Displays,머리 착용 디스플레이에서 인간-AI ​​상호 작용에 대한 사용자 및 시스템 요소의 영향 탐색,"Empowered by the rich sensory capabilities and the advancements in artificial intelligence (AI), head-worn displays (HWD) could understand the user’s contexts and provide just-in-time assistance to users’ tasks to augment their everyday lives. However, there has been limited understanding of how users perceive interacting with AI services, and how different factors impact the user experience in HWD applications. In this research, we investigated broadly what user and system factors play important roles in human-AI experiences during an AI-assisted spatial task. We conducted a user study to simulate an everyday scenario where augmented reality (AR) glasses could provide suggestions/assistance. We researched three AI system factors (performance, initiation, transparency) with multiple user factors (personality traits, trust propensity, and prior trust with AI). We not only identified the impact of user traits such as the levels of conscientiousness and prior trust with the AI, but also found interesting interactions between them and system factors such as AI’s performance and initiation strategy. Based on the findings, we suggest that future AI assistance on HWD needs to take users’ individual characteristics into account and customize the system design accordingly.","풍부한 감각 기능과 인공 지능(AI)의 발전을 통해 머리 착용 디스플레이(HWD)는 사용자의 상황을 이해하고 사용자의 작업에 적시 지원을 제공하여 일상 생활을 강화할 수 있습니다. 그러나 사용자가 AI 서비스와의 상호 작용을 어떻게 인식하는지, 다양한 요인이 HWD 애플리케이션의 사용자 경험에 어떤 영향을 미치는지에 대한 이해가 제한적이었습니다. 본 연구에서 우리는 AI 지원 공간 작업 중 인간-AI ​​경험에 중요한 역할을 하는 사용자 및 시스템 요소가 무엇인지 광범위하게 조사했습니다. 우리는 증강 현실(AR) 안경이 제안/지원을 제공할 수 있는 일상적인 시나리오를 시뮬레이션하기 위해 사용자 연구를 수행했습니다. 우리는 다양한 사용자 요소(성격 특성, 신뢰 성향, AI에 대한 사전 신뢰)를 사용하여 세 가지 AI 시스템 요소(성능, 개시, 투명성)를 연구했습니다. 우리는 AI에 대한 성실성 및 사전 신뢰 수준과 같은 사용자 특성의 영향을 식별했을 뿐만 아니라 AI 성능 및 시작 전략과 같은 시스템 요소와 사용자 특성 사이의 흥미로운 상호 작용을 발견했습니다. 연구 결과를 바탕으로 HWD에 대한 향후 AI 지원은 사용자의 개별 특성을 고려하고 이에 따라 시스템 설계를 맞춤화해야 한다고 제안합니다.",https://doi.org/10.1109/ISMAR59233.2023.00025,Interaction & Input,Sensor Fusion,User Study,User Study / Empirical Findings
340,2023,Fabric Thermal Display using Ultrasonic Waves,초음파를 이용한 직물 열전사 디스플레이,"This paper presents a fabric-based thermal display of a polyester fabric material combined with thermally-conductive materials using an ultrasound haptic display. We first empirically test the thermal generation process in five fabric materials by applying 40 kHz ultrasonic waves to the fabric materials. We also examine their thermal characteristics by applying different frequencies and amplitudes of ultrasonic cues. We show that polyester demonstrates the best thermal performance. We then combine it with thermally-conductive materials, including copper and aluminum, and compare them with the fabric-only condition. Two user studies show that our approach of combining a fabric material with copper and aluminum outperforms fabric-only conditions in thermal perception and thermal level identification. We integrate polyester with aluminum into a glove to explore the use cases in VR and share our findings, insights, limitations, and future works.","이 논문은 초음파 햅틱 디스플레이를 사용하여 열 전도성 물질과 결합된 폴리에스테르 직물 소재의 직물 기반 열 디스플레이를 제시합니다. 먼저 직물 소재에 40kHz 초음파를 적용하여 5가지 직물 소재의 열 발생 과정을 실증적으로 테스트합니다. 또한 초음파 단서의 다양한 주파수와 진폭을 적용하여 열 특성을 조사합니다. 우리는 폴리에스터가 최고의 열 성능을 보인다는 것을 보여줍니다. 그런 다음 구리, 알루미늄 등 열전도 소재를 결합해 패브릭만 사용한 상태와 비교합니다. 두 가지 사용자 연구에 따르면 직물 재료를 구리 및 알루미늄과 결합하는 접근 방식은 열 인식 및 열 수준 식별에서 직물 단독 조건보다 성능이 뛰어난 것으로 나타났습니다. 우리는 폴리에스테르와 알루미늄을 장갑에 통합하여 VR의 사용 사례를 탐색하고 연구 결과, 통찰력, 한계 및 향후 작업을 공유합니다.",https://doi.org/10.1109/ISMAR59233.2023.00048,Interaction & Input,Haptic / Tactile Feedback,Case Study / Application Demo,User Study / Empirical Findings
341,2023,FingerButton: Enabling Controller-Free Transitions between Real and Virtual Environments,FingerButton: 컨트롤러 없이 실제 환경과 가상 환경 간 전환 가능,"With the recent Virtual Reality (VR) Head-Mounted Displays (HMDs), users can seamlessly transition between the virtual and real worlds with techniques such as passthrough. These techniques leverage on-device cameras to capture the real world and show users a view of their physical surroundings while wearing the HMDs. However, they often require users to hold a controller or frequently tap on the HMD, limiting the potential for hands-free interaction and thereby hindering a truly immersive and natural VR experience. To address this limitation, we designed FingerButton, a finger-worn push button device that enables seamless transitions between real and virtual environments. We conducted two studies, where the first one explored a set of hand gestures for transitioning between two environments that are commonly used for “mode switching” within realities. In the second study, we compared FingerButton with the best two-hand gesture identified in the first study and other commercially available solutions (e.g., double tap) for a between-reality selection task. The results show that the physical finger button is faster and user-preferred than other techniques for the transition tasks. Overall, this research contributes to understanding and improving the interaction techniques for fluid switching between the real and virtual worlds, thereby enhancing VR user experiences.","최근 가상 현실(VR) 머리 장착 디스플레이(HMD)를 통해 사용자는 패스스루(passthrough)와 같은 기술을 통해 가상 세계와 현실 세계 사이를 원활하게 전환할 수 있습니다. 이러한 기술은 온디바이스 카메라를 활용하여 실제 세계를 캡처하고 사용자에게 HMD를 착용하는 동안 실제 주변 환경을 보여줍니다. 그러나 사용자가 컨트롤러를 잡거나 HMD를 자주 탭해야 하는 경우가 많아 핸즈프리 상호 작용의 가능성이 제한되어 진정한 몰입감과 자연스러운 VR 경험을 방해합니다. 이러한 한계를 해결하기 위해 우리는 실제 환경과 가상 환경 사이를 원활하게 전환할 수 있는 손가락 착용 푸시 버튼 장치인 FingerButton을 설계했습니다. 우리는 두 가지 연구를 수행했는데, 첫 번째 연구에서는 현실 내에서 ""모드 전환""에 일반적으로 사용되는 두 환경 간 전환을 위한 일련의 손 제스처를 탐색했습니다. 두 번째 연구에서는 현실 간 선택 작업을 위해 첫 번째 연구에서 확인된 최고의 양손 제스처와 기타 상용 솔루션(예: 더블 탭)을 FingerButton과 비교했습니다. 결과는 실제 손가락 버튼이 전환 작업에 대한 다른 기술보다 더 빠르고 사용자가 선호하는 것으로 나타났습니다. 전반적으로 본 연구는 현실 세계와 가상 세계 사이의 유동적인 전환을 위한 상호 작용 기술을 이해하고 개선하여 VR 사용자 경험을 향상시키는 데 기여합니다.",https://doi.org/10.1109/ISMAR59233.2023.00068,Interaction & Input; Display & Optics,Hand / Gesture Recognition,Simulation,Algorithm / Method
342,2023,Free-form Conversation with Human and Symbolic Avatars in Mixed Reality,혼합 현실에서 인간 및 상징적 아바타와의 자유로운 형식 대화,"The integration of large language models and mixed reality technologies has enabled users to engage in free-form conversations with virtual agents across different “realities”. However, if and how the agent’s visual representation, especially when combined with mixed reality environments, will affect the conversation content or user experience is not yet fully understood. In this work, we design and conduct a user study involving two types of visual representations (a human avatar and a symbolic avatar) and two mixed reality environments (virtual reality and augmented reality), facilitating a free-form conversation experience with GPT-3 powered agents. We found evidence that the use of virtual or augmented realities can influence conversation content. Users chatting with avatars in virtual reality made significantly more references to the location or the space, suggesting they tended to perceive conversations as occurring in the agent’s space, whereas the physical AR environment was perhaps more perceived as the user’s space. Conversations with the human avatar improve user recall of the conversation, even though there is no evidence of increased information extracted during the conversation. These observations and our analysis of post-study questionnaires suggest that human avatars can positively impact user memory and experience. We hope our findings and the open-source implementation will help facilitate future research on free-form conversational agents in mixed reality.","대규모 언어 모델과 혼합 현실 기술의 통합을 통해 사용자는 다양한 ""현실""에서 가상 에이전트와 자유로운 형식의 대화에 참여할 수 있습니다. 그러나 에이전트의 시각적 표현, 특히 혼합 현실 환경과 결합될 때 에이전트의 시각적 표현이 대화 내용이나 사용자 경험에 어떤 영향을 미치는지는 아직 완전히 이해되지 않았습니다. 이 작업에서 우리는 두 가지 유형의 시각적 표현(인간 아바타와 상징적 아바타)과 두 가지 혼합 현실 환경(가상 현실과 증강 현실)을 포함하는 사용자 연구를 설계하고 수행하여 GPT-3 기반 에이전트와의 자유로운 형식의 대화 경험을 촉진합니다. 우리는 가상 또는 증강 현실의 사용이 대화 내용에 영향을 미칠 수 있다는 증거를 발견했습니다. 가상 현실에서 아바타와 채팅하는 사용자는 위치나 공간을 훨씬 더 많이 참조하여 대화를 에이전트의 공간에서 일어나는 것으로 인식하는 경향이 있는 반면, 물리적 AR 환경은 아마도 사용자의 공간으로 더 많이 인식하는 것으로 나타났습니다. 인간 아바타와의 대화는 대화 중에 추출된 정보가 증가했다는 증거가 없더라도 대화에 대한 사용자 기억력을 향상시킵니다. 이러한 관찰과 연구 후 설문지 분석은 인간 아바타가 사용자 기억과 경험에 긍정적인 영향을 미칠 수 있음을 시사합니다. 우리의 연구 결과와 오픈 소스 구현이 혼합 현실에서 자유 형식 대화 에이전트에 대한 향후 연구를 촉진하는 데 도움이 되기를 바랍니다.",https://doi.org/10.1109/ISMAR59233.2023.00090,Perception & Cognition,Deep Learning / Neural Networks,User Study,User Study / Empirical Findings
343,2023,Giant Finger: A Novel Visuo-Somatosensory Approach to Simulating Lower Body Movements in Virtual Reality,거대 손가락: 가상 현실에서 하체 움직임을 시뮬레이션하기 위한 새로운 시각-체성감각 접근법,"Surreal experience in virtual reality (VR) occurs when visual experience is accompanied by congruent somatosensation. Thus, VR contents that require physical actions are often bounded to our physical capabilities to maintain somatosensory consistency. Alternatively, users often choose less immersive but safer interfaces that offer a wider action variability. In either case, this situation compromises the potential for a hyper-realistic experience. To address this, we introduce “Giant Finger,” a concept that replicates human lower body movements through two enlarged virtual fingers in VR. Through a user study, we affirmed Giant Finger’s ownership using proprioceptive drift and questionnaire responses. We also compared Giant Finger’s capability to perform a variety of tasks with existing methods. Despite its minimalistic approach, Giant Finger demonstrated a high level of efficacy in supporting lower body movements, with ownership and presence comparable to those of the body-leaning method with whole-body motion. Giant Finger can replace the sensations of real legs or support locomotion in confined spaces by providing proprioceptive illusions to the virtual lower body. The applications showcased in this paper suggest that Giant Finger can enable new forms of movement with high action variability and immersion in various fields such as gaming, industry, and accessibility.","가상 현실(VR)에서의 초현실적 경험은 시각적 경험이 일치하는 체성 감각을 동반할 때 발생합니다. 따라서 신체적 행동이 필요한 VR 콘텐츠는 체성 감각의 일관성을 유지하기 위한 신체적 능력에 국한되는 경우가 많습니다. 또는 사용자는 몰입도가 낮지만 더 넓은 작업 가변성을 제공하는 더 안전한 인터페이스를 선택하는 경우가 많습니다. 두 경우 모두 이 상황은 초현실적인 경험의 가능성을 손상시킵니다. 이를 해결하기 위해 VR에서 확대된 두 개의 가상 손가락을 통해 인간의 하체 움직임을 재현하는 개념인 ""Giant Finger""를 소개합니다. 사용자 연구를 통해 우리는 고유 감각 드리프트와 설문지 응답을 사용하여 Giant Finger의 소유권을 확인했습니다. 또한 Giant Finger의 다양한 작업 수행 능력을 기존 방법과 비교했습니다. 자이언트 핑거는 미니멀한 접근 방식에도 불구하고 하체의 움직임을 지지하는 데 있어 높은 수준의 효능을 보여주었으며, 몸을 기울여 전신을 움직이는 방식에 버금가는 소유감과 존재감을 보여주었습니다. Giant Finger는 실제 다리의 감각을 대체하거나 가상 하체에 고유 감각 환상을 제공하여 제한된 공간에서 운동을 지원할 수 있습니다. 본 논문에서 소개한 애플리케이션은 Giant Finger가 게임, 산업, 접근성 등 다양한 분야에서 높은 액션 가변성과 몰입도를 갖춘 새로운 형태의 움직임을 가능하게 할 수 있음을 시사합니다.",https://doi.org/10.1109/ISMAR59233.2023.00038,Interaction & Input; Perception & Cognition,Sensor Fusion,User Study,Algorithm / Method
344,2023,High-Frame-Rate Projection with Thousands of Frames Per Second Based on the Multi-Bit Superimposition Method,멀티비트 중첩 방법을 기반으로 초당 수천 프레임의 높은 프레임 속도 투사,"The growing need for high-frame-rate projectors in the fields of dynamic projection mapping (DPM) and three-dimensional (3D) displays has increased. Conventional methods allow for an increase in the frame rate to as much as 2,841 frames per second (fps) for 8-bit image projection, using digital light processing (DLP) technology when the minimum digital mirror device (DMD) control time is $44 \mu \mathrm{s}$. However, this rate needs to be further augmented to suit specific applications. In this study, we developed a novel high-frame-rate projection method, which divides the bit depth of an image among multiple projectors and simultaneously projects them in synchronization. The simultaneously projected bit images are superimposed such that a high-bit-depth image is generated within a reduced single-frame duration. Additionally, we devised an optimization process to determine the system parameters necessary for attaining maximum brightness. We constructed a prototype system utilizing two high-frame-rate projectors and validated the feasibility of using our system to project 8-bit images at a rate of 5,600 fps. Furthermore, the quality assessment of our projected image exhibited superior performance in comparison to a dithered image.","DPM(다이나믹 프로젝션 매핑) 및 3차원(3D) 디스플레이 분야에서 높은 프레임 속도의 프로젝터에 대한 수요가 증가하고 있습니다. 기존 방법에서는 최소 디지털 미러 장치(DMD) 제어 시간이 $44 \mu \mathrm{s}$일 때 디지털 조명 처리(DLP) 기술을 사용하여 8비트 이미지 프로젝션에 대해 프레임 속도를 초당 2,841프레임(fps)까지 높일 수 있습니다. 그러나 특정 애플리케이션에 맞게 이 속도를 더욱 강화해야 합니다. 본 연구에서 우리는 여러 프로젝터에서 이미지의 비트 심도를 나누어 동시에 동기화하여 투사하는 새로운 높은 프레임 속도 투사 방법을 개발했습니다. 동시에 투사된 비트 이미지는 중첩되어 감소된 단일 프레임 지속 시간 내에 높은 비트 깊이의 이미지가 생성됩니다. 또한, 우리는 최대 밝기를 달성하는 데 필요한 시스템 매개변수를 결정하기 위한 최적화 프로세스를 고안했습니다. 우리는 두 개의 높은 프레임 속도 프로젝터를 활용하여 프로토타입 시스템을 구축하고 우리 시스템을 사용하여 5,600fps의 속도로 8비트 이미지를 투사하는 타당성을 검증했습니다. 또한 투사된 이미지의 품질 평가는 디더링된 이미지에 비해 우수한 성능을 보였습니다.",https://doi.org/10.1109/ISMAR59233.2023.00089,Display & Optics,Sensor Fusion,Quantitative Experiment,Algorithm / Method
345,2023,How Visualising Emotions Affects Interpersonal Trust and Task Collaboration in a Shared Virtual Space,감정 시각화가 공유 가상 공간에서 대인 신뢰 및 작업 협업에 미치는 영향,"Emotion is dynamic. Changes in emotion can be hard to process during face-to-face interaction, yet transferring them into a shared virtual space becomes more challenging. This research first explores nine visual representations to amplify emotions in a virtual space, leading to a bi-directional emotion-sharing system (FeelMoji i/o). The second study investigates the effect of explicit emotion-sharing in interpersonal trust and task collaboration through three conditions - verbal only, verbal+positive visual, and verbal+honest visual using FeelMoji through the proposal of a framework of four factors (usability, integrity, behaviour, and collaboration). The results indicate that FeelMoji yields frequent emotion consensus as task milestones and positive interdependent behaviours between collaborators, which help develop conversations, affirm decision-making, and build familiarity and trust between strangers. Moreover, we discuss how our study can inspire future investigation in human-AI agent behaviours and large-scale multi-user virtual environments.","감정은 역동적입니다. 감정의 변화는 대면 상호작용 중에 처리하기 어려울 수 있지만 이를 공유된 가상 공간으로 전달하는 것은 더욱 어렵습니다. 본 연구는 먼저 가상 공간에서 감정을 증폭시키기 위한 9가지 시각적 표현을 탐색하여 양방향 감정 공유 시스템(FeelMoji i/o)으로 이어집니다. 두 번째 연구에서는 4가지 요소(사용성, 성실성, 행동, 협업) 프레임워크 제안을 통해 FeelMoji를 사용하여 언어 전용, 언어+긍정적 시각, 언어+정직한 시각의 세 가지 조건을 통해 명시적인 감정 공유가 대인 신뢰와 작업 협업에 미치는 영향을 조사했습니다. 결과는 FeelMoji가 대화를 발전시키고, 의사 결정을 확인하며, 낯선 사람들 사이에 친밀감과 신뢰를 구축하는 데 도움이 되는 공동 작업자 간의 작업 이정표 및 긍정적인 상호 의존적 행동으로 빈번한 감정 합의를 생성한다는 것을 나타냅니다. 또한 우리의 연구가 인간-AI ​​에이전트 행동 및 대규모 다중 사용자 가상 환경에 대한 향후 조사에 어떻게 영감을 줄 수 있는지 논의합니다.",https://doi.org/10.1109/ISMAR59233.2023.00100,Collaboration & Social; Interaction & Input,Other,Questionnaire / Survey,System / Framework
346,2023,Human Behavior Analysis in Human-Robot Cooperation with AR Glasses,AR 안경을 사용한 인간-로봇 협력의 인간 행동 분석,"To achieve efficient human-robot cooperation, it is necessary to work in close proximity while ensuring safety. However, in conventional robot control, maintaining a certain distance between humans and robots is required for safety, owing to control uncertainties and unexpected human actions, which can limit the efficiency of robot operations. Therefore, this study aims to establish a human-robot cooperation aiding system that concerns both safety and efficiency in a close proximity situation. We propose two Augmented Reality (AR) interfaces to display robot information via AR glasses, allowing workers to see the robot information while focusing on their task and avoiding collisions with the robot. AR glasses can give hands-free communication required for a work environment like warehouses or convenience store backyards, and multiple information levels, simple or informative, to balance accuracy and easiness of human recognition ability. We conducted a comparative evaluation experiments with 24 participants and found that both safety and efficiency were improved using the proposed user interfaces (UIs). We also collected the position, head motion, and eye-tracking data from the AR glasses to gain insight into human behavior during the tasks for each UI. Consequently, we clarified the behavior of the participants under each condition and how they contributed to safety and efficiency.","효율적인 인간-로봇 협력을 위해서는 안전을 보장하면서 긴밀하게 작업해야 합니다. 그러나 기존의 로봇 제어에서는 제어의 불확실성과 예상치 못한 인간의 행동으로 인해 안전을 위해 인간과 로봇 사이에 일정 거리를 유지하는 것이 필요하며, 이는 로봇 동작의 효율성을 제한할 수 있다. 따라서 본 연구에서는 근접 상황에서 안전성과 효율성을 모두 고려한 인간-로봇 협력 지원 시스템을 구축하는 것을 목표로 한다. AR 안경을 통해 로봇 정보를 표시하는 두 가지 증강 현실(AR) 인터페이스를 제안합니다. 이를 통해 작업자는 작업에 집중하고 로봇과의 충돌을 피하면서 로봇 정보를 볼 수 있습니다. AR 글래스는 창고나 편의점 뒷마당과 같은 작업 환경에 필요한 핸즈프리 통신과 단순 또는 정보의 다양한 정보 수준을 제공하여 인간 인식 능력의 정확성과 용이함의 균형을 맞출 수 있습니다. 24명의 참가자를 대상으로 비교 평가 실험을 진행한 결과, 제안된 사용자 인터페이스(UI)를 사용하여 안전성과 효율성이 모두 향상되었음을 확인했습니다. 또한 각 UI 작업 중 인간 행동에 대한 통찰력을 얻기 위해 AR 안경에서 위치, 머리 동작, 시선 추적 데이터를 수집했습니다. 결과적으로 우리는 각 조건에서 참가자의 행동과 그들이 안전과 효율성에 어떻게 기여했는지를 명확히 했습니다.",https://doi.org/10.1109/ISMAR59233.2023.00016,Interaction & Input,Other,User Study,System / Framework
347,2023,Hype D-Live: XR Live Music System to Entertain Passengers for Anxiety Reduction in Autonomous Vehicles,Hype D-Live: 자율주행 차량의 불안을 줄이기 위해 승객에게 즐거움을 선사하는 XR 라이브 음악 시스템,"Passengers in autonomous vehicles enjoy the comfort of being free from driving tasks, but they inevitably experience anxiety caused by autonomous vehicle stress (AVS). AVS encompasses vehicle behavior stress due to unpredictable acceleration, and external environmental stress due to potential collisions. Past research has explored approaches to improve passengers’ comfort through behavior control and information presentation. However, methods that utilize stressful vehicle behavior in Extended Reality (XR) entertainment to distract from AVS-related anxiety are limited. Hence, the goal of this study was to maximize passenger comfort in automated vehicles. To achieve this goal, we implemented an XR entertainment system that utilizes vehicle behavior and evaluated its effect on reducing anxiety. In this study, we proposed “Hype D-Live”, an XR live music system designed to reduce anxiety by providing multimodal visual, auditory, force, and vestibular stimuli using a hemispherical display and motion platform mounted on a vehicle. We developed system functions to adjust the force and vestibular senses according to the excitement level of the music and the direction of stressful acceleration and to reproduce moshing, a characteristic behavior at live music venues. However, we hypothesized that passengers might not fully enjoy the entertainment and could experience anxiety if the video content makes them aware of the external environment. Therefore, we conducted an experiment with a within-participant design, involving 24 participants (14 males and 10 females), comparing 3 types of video content for XR entertainment inside the autonomous vehicle: a real external environment, a virtual simulation of the external environment, and a virtual live music venue. The Wilcoxon signed rank test with the Bonferroni correction after the Friedman test revealed that, without the moshing function, the virtual live music venue video significantly enhanced enjoyment and reduced anxiety, compared to the real one.","자율주행차 탑승자는 운전 업무로부터 해방되는 편안함을 누리지만, 자율주행차 스트레스(AVS)로 인한 불안감을 피할 수 없습니다. AVS는 예측할 수 없는 가속으로 인한 차량 거동 스트레스와 잠재적인 충돌로 인한 외부 환경 스트레스를 포괄합니다. 과거 연구에서는 행동 제어 및 정보 제시를 통해 승객의 편안함을 향상시키는 접근 방식을 탐구했습니다. 그러나 확장 현실(XR) 엔터테인먼트에서 스트레스가 많은 차량 동작을 활용하여 AVS 관련 불안을 분산시키는 방법은 제한되어 있습니다. 따라서 본 연구의 목표는 자동화된 차량에서 승객의 편안함을 극대화하는 것이었습니다. 이를 위해 차량 거동을 활용한 XR 엔터테인먼트 시스템을 구현하고 불안 감소 효과를 평가했습니다. 본 연구에서는 차량에 장착된 반구형 디스플레이와 모션 플랫폼을 사용하여 시각, 청각, 힘, 전정 자극을 복합적으로 제공하여 불안을 감소시키도록 설계된 XR 라이브 음악 시스템인 ""Hype D-Live""를 제안했습니다. 우리는 음악의 흥분 수준과 스트레스가 많은 가속의 방향에 따라 힘과 전정 감각을 조정하고 라이브 음악 공연장의 특징적인 행동인 모싱을 재현하는 시스템 기능을 개발했습니다. 그러나 영상 콘텐츠를 통해 외부 환경을 인식하게 되면 탑승객이 엔터테인먼트를 충분히 즐기지 못하고 불안감을 느낄 수 있다는 가설을 세웠습니다. 따라서 우리는 24명의 참가자(남자 14명, 여자 10명)가 참여하는 참가자 내 설계 실험을 수행하여 자율주행차 내부의 XR 엔터테인먼트를 위한 3가지 유형의 비디오 콘텐츠인 실제 외부 환경, 외부 환경의 가상 시뮬레이션, 가상 라이브 음악 공연장을 비교했습니다. Friedman 테스트 후 Bonferroni 보정을 적용한 Wilcoxon 부호 순위 테스트에서는 모싱 기능이 없는 가상 라이브 음악 공연장 영상이 실제 영상에 비해 즐거움을 크게 높이고 불안감을 줄이는 것으로 나타났습니다.",https://doi.org/10.1109/ISMAR59233.2023.00029,Display & Optics,Sensor Fusion,User Study,System / Framework; Algorithm / Method
348,2023,Identifying Virtual Reality Users Across Domain-Specific Tasks: A Systematic Investigation of Tracked Features for Assembly,도메인별 작업 전반에 걸쳐 가상 현실 사용자 식별: 조립을 위해 추적된 기능에 대한 체계적인 조사,"Recently, there has been much interest in using virtual reality (VR) tracking data to authenticate or identify users. Most prior research has relied on task-specific characteristics but newer studies have begun investigating task-agnostic, domain-specific approaches. In this paper, we present one of the first systematic investigations of how different combinations of VR tracked devices (i.e., the headset, dominant hand controller, and non-dominant hand controller) and their spatial representations (i.e., position and/or rotation as Euler angles, quaternions, or 6D) affect identification accuracy for domain-specific approaches. We conducted a user study $( n =45)$ involving participants learning how to assemble two distinct full-scale constructions. Our results indicate that more tracked devices improve identification accuracies for the same assembly task, but only headset features afford the best accuracies across the domain-specific tasks. Our results also indicate that spatial features involving position and any rotation yield better accuracies than either alone.","최근 가상현실(VR) 추적 데이터를 활용해 사용자를 인증하거나 식별하는 것에 대한 관심이 높아지고 있다. 대부분의 이전 연구는 작업별 특성에 의존했지만 최신 연구에서는 작업에 구애받지 않고 도메인별 접근 방식을 조사하기 시작했습니다. 이 논문에서는 VR 추적 장치(예: 헤드셋, 주로 사용하는 손 컨트롤러, 비주로 사용하는 손 컨트롤러)의 다양한 조합과 해당 공간 표현(예: 오일러 각도, 쿼터니언 또는 6D와 같은 위치 및/또는 회전)이 도메인별 접근 방식의 식별 정확도에 어떻게 영향을 미치는지에 대한 최초의 체계적인 조사 중 하나를 제시합니다. 우리는 참가자들이 두 개의 서로 다른 실물 크기 구조물을 조립하는 방법을 배우는 사용자 연구 $(n =45)$를 실시했습니다. 우리의 결과는 추적된 장치가 많을수록 동일한 조립 작업에 대한 식별 정확도가 향상되지만 헤드셋 기능만이 도메인별 작업 전반에 걸쳐 최고의 정확도를 제공한다는 것을 나타냅니다. 우리의 결과는 또한 위치 및 회전과 관련된 공간적 특징이 둘 중 하나보다 더 나은 정확도를 제공한다는 것을 나타냅니다.",https://doi.org/10.1109/ISMAR59233.2023.00054,Education & Training,Other,User Study,User Study / Empirical Findings; Hardware / Device
349,2023,Interaction between AR Cue Types and Environmental Conditions in Autonomous Vehicles,자율주행차의 AR 큐 유형과 환경 조건 간의 상호 작용,"As one of autonomous vehicles, conditional autonomous vehicles is expected to become popular in the near future. Conditional autonomous vehicles can send a take-over request (TOR) to a driver, and if they are immersed in non-driving-related tasks (NDRT), they will struggle to accommodate this request. Previous studies have shown that providing augmented reality (AR) information on traffic situations (status cues) or driver actions (command cues) can improve TOR performance. However, we are not aware of any studies comparing the types of AR cues (state versus command cues) and their interactions with environmental factors. Therefore, the current study investigated this and evaluated the TOR performance of 42 drivers. We used a 2 (environments: day and night) $\times$ 4 (AR cue types: without, status, command, and combined cues) mixed-subject experimental design, and dependent measures included driving, cognitive, and NDRT performances. The results suggest that overall driving and cognitive performance were significantly improved by the command AR cue. In contrast, the status AR cue improved the TOR performance in nighttime environments. The performance of AR cues can vary depending on environmental factors, and AR cue designs for autonomous vehicles should consider this interaction for successful collaboration between drivers and vehicles.","조건부 자율주행차는 자율주행차 중 하나로 가까운 미래에 대중화될 것으로 예상됩니다. 조건부 자율주행차는 운전자에게 인수 요청(TOR)을 보낼 수 있는데, 운전 외 업무(NDRT)에 몰두해 있으면 이 요청을 수용하기 위해 애쓰게 됩니다. 이전 연구에서는 교통 상황(상태 단서) 또는 운전자 행동(명령 단서)에 대한 증강 현실(AR) 정보를 제공하면 TOR 성능을 향상시킬 수 있는 것으로 나타났습니다. 그러나 AR 단서(상태 대 명령 단서)의 유형과 환경 요인과의 상호 작용을 비교하는 연구는 없습니다. 따라서 본 연구에서는 이를 조사하여 42명의 운전자를 대상으로 TOR 성능을 평가하였다. 우리는 2(환경: 낮과 밤) $\times$ 4(AR 큐 유형: 없음, 상태, 명령 및 결합된 큐) 혼합 주제 실험 설계를 사용했으며 종속 측정에는 운전, 인지 및 NDRT 성능이 포함되었습니다. 결과는 명령 AR 큐에 의해 전반적인 운전 및 인지 성능이 크게 향상되었음을 시사합니다. 대조적으로, 상태 AR 신호는 야간 환경에서 TOR 성능을 향상시켰습니다. AR 큐의 성능은 환경 요인에 따라 달라질 수 있으며, 자율주행차를 위한 AR 큐 디자인은 운전자와 차량 간의 성공적인 협업을 위해 이러한 상호 작용을 고려해야 합니다.",https://doi.org/10.1109/ISMAR59233.2023.00052,Interaction & Input,Other,Questionnaire / Survey,Other
350,2023,Investigating Psychological Ownership in a Shared AR Space: Effects of Human and Object Reality and Object Controllability,공유 AR 공간에서 심리적 소유권 조사: 인간과 객체 현실 및 객체 제어 가능성의 영향,"Augmented reality (AR) provides users with a unique social space where virtual objects are natural parts of the real world. The users can interact with 3D virtual objects and virtual humans projected onto the physical environment. This work examines perceived ownership based on the reality of objects and partners, as well as object controllability in a shared AR setting. Our formal user study with 28 participants shows a sense of possession, control, separation, and partner presence affect their perceived ownership of a shared object. Finally, we discuss the findings and present a conclusion.","증강 현실(AR)은 가상 객체가 현실 세계의 자연스러운 일부인 독특한 소셜 공간을 사용자에게 제공합니다. 사용자는 물리적 환경에 투영된 3D 가상 개체 및 가상 인간과 상호 작용할 수 있습니다. 이 연구에서는 개체와 파트너의 현실을 기반으로 인식된 소유권과 공유 AR 설정의 개체 제어 가능성을 조사합니다. 28명의 참가자를 대상으로 한 공식 사용자 연구에서는 소유감, 통제력, 분리감, 파트너 존재감이 공유 객체에 대한 인지된 소유권에 영향을 미치는 것으로 나타났습니다. 마지막으로 연구 결과를 논의하고 결론을 제시합니다.",https://doi.org/10.1109/ISMAR59233.2023.00102,Collaboration & Social,Other,User Study,User Study / Empirical Findings
351,2023,Investigating the Effects of Selective Information Presentation in Intensive Care Units Using Virtual Reality,가상 현실을 활용한 중환자실의 선택적 정보 제시 효과 조사,"Medical personnel working in intensive care units (ICUs) are continuously exposed to a multitude of alarms emanating from various monitoring devices, such as cardiac monitors, ventilators, or infusion pumps. The sheer volume of alarms, coupled with high false positive rates, can lead to alarm fatigue. This phenomenon compromises patient safety and places an additional burden on nurses who must diligently prioritize and respond to alarms in the highly dynamic environment. While the testing of stress-reducing strategies in a real ICU is challenging, virtual reality (VR) represents a powerful tool and methodology to simulate an ICU environment and test optimization scenarios for alarm display strategies. For example, redistributing alarms to responsible individuals (personalized information presentation) has been proposed as a solution, but testing in real ICU environments is not applicable due to critical patient safety. In this paper, we present a VR simulation of an ICU to simulate comparable stress situations, as well as to assess the impact of a selective and personalized alarm representation strategy in an evaluation study in two conditions. A stress condition mirrors the current ubiquitous audible alarm distribution in most ICUs, where alarms are heard non-patient-specific throughout the ward. In an experimental condition, alarms are filtered patient-specific to reduce information overload and noise pollution. Our user study with medical personnel and novices shows that stress levels can be simulated with our system as indicated by physiological responses. Further, we show that the perceived task load can be reduced with selective information presentation. We discuss the potential benefits of ICU simulations as a methodology and personalized alarm distribution as a first potential strategy for future technologies in ICUs.","중환자실(ICU)에서 근무하는 의료진은 심장 모니터, 인공호흡기, 주입 펌프 등 다양한 모니터링 장치에서 발생하는 수많은 경보에 지속적으로 노출됩니다. 높은 오탐률과 함께 엄청난 양의 알람으로 인해 알람 피로가 발생할 수 있습니다. 이러한 현상은 환자의 안전을 위협하고 매우 역동적인 환경에서 경보의 우선순위를 부지런히 정하고 이에 대응해야 하는 간호사에게 추가적인 부담을 줍니다. 실제 ICU에서 스트레스 감소 전략을 테스트하는 것은 어려운 일이지만, 가상 현실(VR)은 ICU 환경을 시뮬레이션하고 알람 표시 전략에 대한 최적화 시나리오를 테스트하는 강력한 도구 및 방법론을 나타냅니다. 예를 들어, 알람을 책임자에게 재분배하는 것(개인화된 정보 제공)이 해결책으로 제안되었지만 실제 ICU 환경에서의 테스트는 중요한 환자 안전으로 인해 적용되지 않습니다. 본 논문에서는 비교 가능한 스트레스 상황을 시뮬레이션하고 두 가지 조건에 대한 평가 연구에서 선택적 및 개인화된 경보 표현 전략의 영향을 평가하기 위해 ICU의 VR 시뮬레이션을 제시합니다. 스트레스 상태는 현재 대부분의 ICU에서 널리 퍼져 있는 청각 경보 분포를 반영하며, 경보는 병동 전체에서 환자별로 특정하지 않게 들립니다. 실험 조건에서는 정보 과부하 및 소음 공해를 줄이기 위해 경보가 환자별로 필터링됩니다. 의료진 및 초보자를 대상으로 한 사용자 연구에서는 스트레스 수준이 생리학적 반응으로 표시된 대로 시스템을 사용하여 시뮬레이션될 수 있음을 보여줍니다. Further, we show that the perceived task load can be reduced with selective information presentation. 우리는 방법론으로서 ICU 시뮬레이션의 잠재적 이점과 ICU의 미래 기술을 위한 첫 번째 잠재적 전략인 개인화된 경보 배포에 대해 논의합니다.",https://doi.org/10.1109/ISMAR59233.2023.00034,Medical & Healthcare,Sensor Fusion,Simulation,User Study / Empirical Findings; Algorithm / Method
352,2023,Is Foveated Rendering Perception Affected by Users' Motion?,포비티드 렌더링 인식이 사용자의 움직임에 영향을 받나요?,"Virtual reality (VR) is gaining increasing popularity across various domains, but the current state of technology imposes limitations on the level of realism and complexity achievable in computer graphics when displayed through VR head-mounted devices (HMDs). To improve the user experience in HMDs, optimization techniques are needed to enhance performance without sacrificing quality. One such technique is Foveated Rendering (FR), which leverages the human visual system to optimize resource usage. FR degrades the image quality at the periphery of the human vision, where visual acuity is lower, to save resources. This paper aims to investigate if the perception of the peripheral area is affected whenever users are in movement in a VR environment. Our findings show a significant correlation between speed movement and Foveated rendering parameters in both scenarios. The least amount of degradation was observed in the idle state and the most in the high-speed state, indicating that users perceive less degradation at higher speeds. These results are particularly relevant for path-tracing-based algorithms, due to the possibility of reducing the number of rays required for the rendering whenever there is movement.","가상 현실(VR)은 다양한 영역에서 점점 더 인기를 얻고 있지만, 현재의 기술 상태는 VR 헤드 장착 장치(HMD)를 통해 표시할 때 컴퓨터 그래픽에서 달성할 수 있는 현실감과 복잡성 수준에 제한을 가하고 있습니다. HMD의 사용자 경험을 개선하려면 품질을 희생하지 않고 성능을 향상시키는 최적화 기술이 필요합니다. 그러한 기술 중 하나가 인간의 시각 시스템을 활용하여 리소스 사용을 최적화하는 FR(Foveated Rendering)입니다. FR은 시력이 낮은 인간의 시각 주변 부분의 이미지 품질을 저하시켜 자원을 절약합니다. 본 논문은 VR 환경에서 사용자가 움직일 때마다 주변 영역에 대한 인식이 영향을 받는지 조사하는 것을 목표로 한다. 우리의 연구 결과는 두 시나리오 모두에서 속도 이동과 Foveated 렌더링 매개변수 사이의 중요한 상관관계를 보여줍니다. 유휴 상태에서는 성능 저하가 가장 적고 고속 상태에서는 성능 저하가 가장 많이 나타나 사용자는 고속에서 성능 저하를 덜 느낀다는 것을 알 수 있습니다. 이러한 결과는 움직임이 있을 때마다 렌더링에 필요한 광선 수를 줄일 수 있기 때문에 경로 추적 기반 알고리즘과 특히 관련이 있습니다.",https://doi.org/10.1109/ISMAR59233.2023.00127,Display & Optics; Perception & Cognition,Eye / Gaze Tracking,Other,Algorithm / Method
353,2023,Is this the vReal Life? Manipulating Visual Fidelity of Immersive Environments for Medical Task Simulation,이게 vReal Life인가요? 의료 작업 시뮬레이션을 위한 몰입형 환경의 시각적 충실도 조작,"Recent developments and research advances contribute to an ever-increasing trend towards quality levels close to what we experience in reality. In this work, we investigate how different degrees of these quality characteristics affect user performance, qualia of user experience (UX), and sense of presence in an example medical task. To this end, a two-way within-subjects design user study was conducted, in which three different levels of visual fidelity were compared. In addition, two different interaction modalities were considered: (1) the use of conventional VR controllers and (2) natural hand interaction using 3D-printed, spatially-registered replicas of medical devices, to interact with their virtual representations. Consistent results indicate that higher degrees of visual fidelity evoke a higher sense of presence and UX. However, user performance was less affected. Moreover, no differences were detected between both interaction modalities for the examined task. Future work should investigate the discovered interaction effects between quality levels and interaction modalities in more detail and examine whether these results can be reproduced in tasks that require more precision. This work provides insights into the implications to consider when studying interactions in VR and paves the way for investigations into early phases of medical product development and workflow analysis.","최근의 개발과 연구 발전은 우리가 현실에서 경험하는 것과 가까운 품질 수준을 향해 계속해서 증가하는 추세에 기여하고 있습니다. 이 연구에서는 이러한 품질 특성의 다양한 정도가 사용자 성능, 사용자 경험(UX) 품질 및 의료 작업 예시에서 존재감에 어떻게 영향을 미치는지 조사합니다. 이를 위해 세 가지 다른 수준의 시각적 충실도를 비교하는 양방향 개체 내 디자인 사용자 연구가 수행되었습니다. 또한 두 가지 서로 다른 상호 작용 양식이 고려되었습니다. (1) 기존 VR 컨트롤러를 사용하는 것과 (2) 3D 인쇄되고 공간적으로 등록된 의료 기기 복제품을 사용하여 가상 표현과 상호 작용하는 자연스러운 손 상호 작용입니다. 일관된 결과는 시각적 충실도가 높을수록 존재감과 UX가 높아진다는 것을 나타냅니다. 그러나 사용자 성능에는 덜 영향을 미쳤습니다. 더욱이, 검사된 작업에 대한 두 상호 작용 양식 간에는 차이가 발견되지 않았습니다. 향후 연구에서는 품질 수준과 상호 작용 양식 사이에서 발견된 상호 작용 효과를 더 자세히 조사하고 이러한 결과가 더 정밀해야 하는 작업에서 재현될 수 있는지 여부를 조사해야 합니다. 이 작업은 VR에서 상호 작용을 연구할 때 고려해야 할 의미에 대한 통찰력을 제공하고 의료 제품 개발 및 워크플로 분석의 초기 단계에 대한 조사를 위한 길을 열어줍니다.",https://doi.org/10.1109/ISMAR59233.2023.00134,Interaction & Input; Perception & Cognition,Sensor Fusion,User Study,Design Guidelines
354,2023,LeanOn: Simulating Balance Vehicle Locomotion in Virtual Reality,LeanOn: 가상 현실에서 균형 차량 이동 시뮬레이션,"Locomotion plays a critical role in user experience in Virtual Reality (VR). This work presents a novel locomotion device, LeanOn, which aims to enhance immersion and feedback experience in VR. Inspired by balance vehicles, LeanOn is a leaning-based locomotion device that allows users to control their location by tilting a board on two balance wheels, with rotation enabled by two buttons near users’ feet. To create a more realistic riding experience, LeanOn is equipped with a terrain vibration system that generates varying levels of vibration based on the roughness of the terrain. We conducted a within-subjects experiment $(\mathrm{N}=24)$ and compared the use of LeanOn and joystick steering in four aspects: cybersickness, spatial presence, feedback experience, and task performance. Participants used LeanOn with and without the vibration system to investigate the necessity of tactile feedback. The results showed that LeanOn significantly improved users’ feedback experience, including autotelic, expressivity, harmony, and immersion, and maintained similar levels of cybersickness and spatial presence, compared to joystick steering. Our work contributes to the field of VR locomotion by validating a leaning-based steering prototype and showing its positive effect on improving users’ feedback experience in VR. We also showed that tactile feedback in locomotion is necessary to further enhance immersion in VR.","Locomotion은 가상 현실(VR)의 사용자 경험에서 중요한 역할을 합니다. 이 작품은 VR에서 몰입감과 피드백 경험을 향상시키는 것을 목표로 하는 새로운 이동 장치인 LeanOn을 제시합니다. 저울차에서 영감을 받은 LeanOn은 사용자가 발 근처에 있는 두 개의 버튼으로 회전할 수 있도록 두 개의 균형 바퀴에 있는 보드를 기울여 위치를 제어할 수 있는 기울임 기반 이동 장치입니다. 보다 현실적인 라이딩 경험을 만들기 위해 LeanOn에는 지형의 거칠기에 따라 다양한 수준의 진동을 생성하는 지형 진동 시스템이 장착되어 있습니다. 우리는 피험자 내 실험 $(\mathrm{N}=24)$을 수행하고 LeanOn과 조이스틱 조종의 사용을 사이버 멀미, 공간 존재감, 피드백 경험 및 작업 수행의 네 가지 측면에서 비교했습니다. 참가자들은 진동 시스템 유무에 관계없이 LeanOn을 사용하여 촉각 피드백의 필요성을 조사했습니다. 그 결과, LeanOn은 조이스틱 조향과 비교하여 자율성, 표현성, 조화, 몰입도 등 사용자의 피드백 경험을 크게 향상시켰으며, 사이버 멀미 및 공간적 존재감도 유사한 수준을 유지하는 것으로 나타났습니다. 우리의 연구는 기울기 기반 조향 프로토타입을 검증하고 VR에서 사용자의 피드백 경험을 향상시키는 데 긍정적인 효과를 보여줌으로써 VR 이동 분야에 기여합니다. 또한 VR에 대한 몰입감을 더욱 향상시키기 위해서는 운동 시 촉각 피드백이 필요하다는 것을 보여주었습니다.",https://doi.org/10.1109/ISMAR59233.2023.00056,Perception & Cognition; Interaction & Input,Sensor Fusion,User Study,System / Framework
355,2023,Leap to the Eye: Implicit Gaze-based Interaction to Reveal Invisible Objects for Virtual Environment Exploration,눈으로 도약: 가상 환경 탐색을 위해 보이지 않는 물체를 드러내는 암시적 시선 기반 상호 작용,"Cinematic virtual reality (CVR) brings viewers a novel and immersive movie-watching experience. However, they may miss story events and scene transitions that the director has designed as key points hidden in the VR scene. In this paper, we introduce implicit gaze-based interaction for enhancing the exploration experience in CVR. In contrast to most research on gaze-based selection of objects or explicit guidance of attention using visual cues, we focus on implicit interaction that utilizes the user’s natural gaze and attention to explore the scene. We design and implement different gaze trigger methods for implicit interaction, making the interaction more intuitive and natural when users reveal the invisible objects. We implemented the adaptive collider technique, offering users a better sense of exploration than raycasting and spotlight techniques. We have also conducted user studies to compare animation sequences for visual feedback, with each animation sequence offering different storytelling techniques. One of the sequences is better suited for describing spaces in the virtual world, while the other sequence offers users the feeling of constructing a world through their gaze.","시네마틱 가상 현실(CVR)은 시청자에게 새롭고 몰입도 높은 영화 감상 경험을 선사합니다. 하지만 VR 장면에 숨겨진 핵심 포인트로 감독이 디자인한 스토리 이벤트와 장면 전환 등은 놓칠 수도 있다. 본 논문에서는 CVR에서 탐색 경험을 향상시키기 위한 암시적 시선 기반 상호 작용을 소개합니다. 시선 기반 객체 선택이나 시각적 단서를 사용한 명시적 주의 유도에 대한 대부분의 연구와 달리 우리는 장면을 탐색하기 위해 사용자의 자연스러운 시선과 주의를 활용하는 암시적 상호 작용에 중점을 둡니다. 우리는 암시적 상호 작용을 위해 다양한 응시 트리거 방법을 설계하고 구현하여 사용자가 보이지 않는 개체를 공개할 때 상호 작용을 더욱 직관적이고 자연스럽게 만듭니다. 우리는 적응형 충돌기 기술을 구현하여 사용자에게 레이캐스팅 및 스포트라이트 기술보다 더 나은 탐색 감각을 제공했습니다. 또한 시각적 피드백을 위해 애니메이션 시퀀스를 비교하기 위한 사용자 연구를 수행했으며, 각 애니메이션 시퀀스는 서로 다른 스토리텔링 기술을 제공합니다. 시퀀스 중 하나는 가상 세계의 공간을 묘사하는 데 더 적합한 반면, 다른 시퀀스는 사용자에게 응시를 통해 세상을 구성하는 느낌을 제공합니다.",https://doi.org/10.1109/ISMAR59233.2023.00036,Interaction & Input,Other,Simulation,Algorithm / Method
356,2023,Leveraging Motion Tracking for Intuitive Interactions in a Tablet-Based 3D Scene Annotation System,태블릿 기반 3D 장면 주석 시스템에서 직관적인 상호 작용을 위해 모션 추적 활용,"In the rapidly evolving field of computer vision, efficient and accurate annotation of 3D scenes plays a crucial role. While automation has streamlined this process, manual intervention is still essential for obtaining precise annotations. Existing annotation tools often lack intuitive interactions and efficient interfaces, particularly when it comes to annotating complex elements such as 3D bounding boxes, 6D human poses, and semantic relationships in a 3D scene. Therefore, it is often time-consuming and error-prone. Emerging technologies such as augmented reality (AR) and virtual reality (VR) have shown potential to provide an immersive and interactive environment for annotators to label objects and their relationships. However, the cost and accessibility of these technologies can be a barrier to their widespread adoption. This work introduces a novel tablet-based system that utilizes built-in motion tracking to facilitate an efficient and intuitive 3D scene annotation process. The system supports a variety of annotation tasks and leverages the tracking and mobility features of the tablet to enhance user interactions. Through a thorough user study investigating three distinct tasks - creating bounding boxes, adjusting human poses, and annotating scene relationships - we evaluate the effectiveness and usability of two interaction methods: touch-based interactions and hybrid interactions that utilize both touch and device motion tracking. Our results suggest that leveraging the tablet’s motion tracking feature could lead to more intuitive and efficient annotation processes. This work contributes to the understanding of tablet-based interaction and the potential it holds for annotating complex 3D scenes.","빠르게 발전하는 컴퓨터 비전 분야에서 3D 장면의 효율적이고 정확한 주석은 중요한 역할을 합니다. 자동화로 인해 이 프로세스가 간소화되었지만 정확한 주석을 얻으려면 수동 개입이 여전히 필수적입니다. 기존 주석 도구에는 직관적인 상호 작용과 효율적인 인터페이스가 부족한 경우가 많습니다. 특히 3D 경계 상자, 6D 인간 포즈, 3D 장면의 의미 관계와 같은 복잡한 요소에 주석을 달 때 더욱 그렇습니다. 따라서 시간이 많이 걸리고 오류가 발생하기 쉬운 경우가 많습니다. 증강 현실(AR) 및 가상 현실(VR)과 같은 최신 기술은 주석자가 개체와 개체 관계에 레이블을 지정할 수 있는 몰입형 대화형 환경을 제공할 수 있는 잠재력을 보여주었습니다. 그러나 이러한 기술의 비용과 접근성은 광범위한 채택에 장벽이 될 수 있습니다. 이 작업은 효율적이고 직관적인 3D 장면 주석 프로세스를 촉진하기 위해 내장된 동작 추적을 활용하는 새로운 태블릿 기반 시스템을 소개합니다. 이 시스템은 다양한 주석 작업을 지원하고 태블릿의 추적 및 이동 기능을 활용하여 사용자 상호 작용을 향상시킵니다. 경계 상자 생성, 사람 포즈 조정, 장면 관계 주석 달기라는 세 가지 고유한 작업을 조사한 철저한 사용자 연구를 통해 터치 기반 상호 작용과 터치 및 장치 동작 추적을 모두 활용하는 하이브리드 상호 작용이라는 두 가지 상호 작용 방법의 효율성과 유용성을 평가합니다. 우리의 결과는 태블릿의 동작 추적 기능을 활용하면 보다 직관적이고 효율적인 주석 처리가 가능하다는 것을 보여줍니다. 이 작업은 태블릿 기반 상호 작용에 대한 이해와 복잡한 3D 장면에 주석을 달 수 있는 잠재력을 이해하는 데 도움이 됩니다.",https://doi.org/10.1109/ISMAR59233.2023.00071,Interaction & Input; Content Authoring,Computer Vision,User Study,User Study / Empirical Findings
357,2023,LiVRSono - Virtual Reality Training with Haptics for Intraoperative Ultrasound,LiVRSono - 수술 중 초음파를 위한 햅틱을 사용한 가상 현실 교육,"One of the biggest challenges in using ultrasound (US) is learning to create a spatial mental model of the interior of the scanned object based on the US image and the probe position. As intraoperative ultrasound (IOUS) cannot be easily trained on patients, we present LiVRSono, an immersive VR application to train this skill. The immersive environment, including an US simulation with patientspecific data as well as haptics to support hand-eye coordination, provides a realistic setting. Four clinically relevant training scenarios were identified based on the described learning goal and the workflow of IOUS for liver. The realism of the setting and the training scenarios were evaluated with eleven physicians, of which six participants are experts in IOUS for liver and five participants are potential users of the training system. The setting, handling of the US probe, and US image were considered realistic enough for the learning goal. Regarding the haptic feedback, a limitation is the restricted workspace of the input device. Three of the four training scenarios were rated as meaningful and effective. A pilot study regarding learning outcome shows positive results, especially with respect to confidence and perceived competence. Besides the drawbacks of the input device, our training system provides a realistic learning environment with meaningful scenarios to train the creation of a mental 3D model when performing IOUS. We also identified important improvements to the training scenarios to further enhance the training experience.","초음파(US) 사용 시 가장 큰 과제 중 하나는 US 이미지와 프로브 위치를 기반으로 스캔한 개체 내부의 공간 정신 모델을 만드는 방법을 학습하는 것입니다. 수술 중 초음파(IOUS)는 환자에게 쉽게 훈련할 수 없기 때문에 이 기술을 훈련하기 위한 몰입형 VR 애플리케이션인 LiVRSono를 제시합니다. 환자별 데이터를 사용한 미국 시뮬레이션과 손과 눈의 협응을 지원하는 햅틱을 포함한 몰입형 환경은 현실적인 설정을 제공합니다. 설명된 학습 목표와 간에 대한 IOUS의 워크플로우를 기반으로 임상적으로 관련된 4가지 훈련 시나리오가 식별되었습니다. 설정의 현실성과 훈련 시나리오는 11명의 의사를 통해 평가되었으며, 그 중 6명의 참가자는 간에 대한 IOUS 전문가이고 5명의 참가자는 훈련 시스템의 잠재적 사용자입니다. US 프로브의 설정, 조작 및 US 이미지는 학습 목표에 충분히 현실적이라고 간주되었습니다. 햅틱 피드백과 관련하여 제한 사항은 입력 장치의 제한된 작업 공간입니다. 네 가지 훈련 시나리오 중 세 가지가 의미 있고 효과적인 것으로 평가되었습니다. 학습 결과에 관한 예비 연구는 특히 자신감과 인지된 역량과 관련하여 긍정적인 결과를 보여줍니다. 입력 장치의 단점 외에도 당사의 훈련 시스템은 IOUS를 수행할 때 정신적인 3D 모델 생성을 훈련할 수 있는 의미 있는 시나리오를 갖춘 현실적인 학습 환경을 제공합니다. 또한 훈련 경험을 더욱 향상시키기 위해 훈련 시나리오에 대한 중요한 개선 사항을 확인했습니다.",https://doi.org/10.1109/ISMAR59233.2023.00114,Education & Training; Medical & Healthcare,Sensor Fusion,User Study,System / Framework
358,2023,MR.Sketch. Immediate 3D Sketching via Mixed Reality Drawing Canvases,MR.스케치. 혼합 현실 드로잉 캔버스를 통한 즉각적인 3D 스케치,"Sketching is a fundamental technique for early design and form finding. Digital 3D sketching can improve the early design phase by improving spatial understanding and enriching the design with additional information. However, the tools used for sketching should not hinder the expression and thought process inherent in form finding. Methods already exist for using 2D pen-on-tablet input for 3D sketching via stroke projection onto 3D drawing canvases. However, positioning the canvas and sketching lines are separate work steps. This breaks the flow of the designer’s thought process. We propose a novel technique for mixed reality 3D sketching that involves the use of viewport-attached drawing canvases, spatial meshing and intersection canvas visualisation. By combining the inside-out tracking capabilities of current portable consumer devices with stylus-on-tablet freehand drawing input, we transform 2D to 3D projective sketching into a more seamless experience. Results of a pilot user study with 16 participants show significant user preference for our technique, as well as increased sketching speed and immediacy.","스케치는 초기 디자인 및 형태 찾기를 위한 기본 기술입니다. 디지털 3D 스케치는 공간 이해를 향상하고 추가 정보로 설계를 풍부하게 하여 초기 설계 단계를 개선할 수 있습니다. 그러나 스케치에 사용되는 도구는 형태 찾기에 내재된 표현과 사고 과정을 방해해서는 안 됩니다. 3D 그리기 캔버스에 스트로크 투영을 통해 3D 스케치에 2D 펜 온 태블릿 입력을 사용하는 방법이 이미 존재합니다. 그러나 캔버스 위치 지정과 선 스케치는 별도의 작업 단계입니다. 이는 디자이너의 사고 과정의 흐름을 깨뜨립니다. 우리는 뷰포트에 연결된 드로잉 캔버스, 공간 메싱 및 교차 캔버스 시각화를 사용하는 혼합 현실 3D 스케치를 위한 새로운 기술을 제안합니다. 현재 휴대용 소비자 장치의 내부 추적 기능과 태블릿의 스타일러스 프리핸드 드로잉 입력을 결합하여 2D에서 3D 투영 스케치를 보다 원활한 경험으로 변환합니다. 16명의 참가자를 대상으로 한 파일럿 사용자 연구 결과는 우리 기술에 대한 상당한 사용자 선호도와 향상된 스케치 속도 및 즉각성을 보여줍니다.",https://doi.org/10.1109/ISMAR59233.2023.00015,Tracking & Localization,Other,User Study,Algorithm / Method; User Study / Empirical Findings
359,2023,MRMAC: Mixed Reality Multi-user Asymmetric Collaboration,MRMAC: 혼합 현실 다중 사용자 비대칭 협업,"We present MRMAC, a Mixed Reality Multi-user Asymmetric Collaboration system that allows remote users to teleport virtually into a real-world collaboration space to communicate and collaborate with local users. Our system enables telepresence for remote users by live-streaming the physical environment of local users using a 360° camera while blending 3D virtual assets into the mixed-reality collaboration space. Our novel client-server architecture enables asymmetric collaboration for multiple AR and VR users and incorporates avatars, view controls, as well as synchronized low-latency audio, video, and asset streaming. We evaluated our implementation with two baseline conditions: conventional 2D and standard 360° videoconferencing. Results show that MRMAC outperformed both baselines in inducing a sense of presence, improving task performance, usability, and overall user preference, demonstrating its potential for immersive multi-user telecollaboration.","우리는 원격 사용자가 가상으로 실제 협업 공간으로 순간이동하여 로컬 사용자와 통신하고 협업할 수 있도록 하는 혼합 현실 다중 사용자 비대칭 협업 시스템인 MRMAC를 제시합니다. 우리 시스템은 360° 카메라를 사용하여 로컬 사용자의 물리적 환경을 라이브 스트리밍하는 동시에 3D 가상 자산을 혼합 현실 협업 공간에 혼합함으로써 원격 사용자를 위한 텔레프레즌스를 가능하게 합니다. 우리의 새로운 클라이언트-서버 아키텍처는 여러 AR 및 VR 사용자를 위한 비대칭 협업을 가능하게 하며 아바타, 뷰 컨트롤은 물론 동기화된 짧은 지연 시간의 오디오, 비디오 및 자산 스트리밍을 통합합니다. 우리는 기존 2D 및 표준 360° 화상회의라는 두 가지 기본 조건을 사용하여 구현을 평가했습니다. 결과에 따르면 MRMAC는 존재감을 유도하고 작업 성능, 유용성 및 전반적인 사용자 선호도를 향상시키는 데 있어 두 기준 모두를 능가하여 몰입형 다중 사용자 원격 협업의 잠재력을 입증했습니다.",https://doi.org/10.1109/ISMAR59233.2023.00074,Collaboration & Social; Perception & Cognition,Cloud / Edge Computing,Quantitative Experiment,System / Framework
360,2023,Merging Camera and Object Haptic Motion Effects for Improved 4D Experiences,향상된 4D 경험을 위해 카메라와 개체 햅틱 모션 효과 병합,"(Haptic) motion effects refer to the vestibular stimuli generated by a motion platform and delivered to the whole body of a user sitting on the platform. Motion effects are an essential tool for creating vivid sensory experiences in various extended reality (XR) applications, ranging from training simulators to recent 4D rides, films, and games for entertainment. For the latter purpose, motion effects emphasize audiovisual events occurring in the scene, such as camera motion, the movement of an object of interest, and special sounds. Recent research developed several algorithms to produce motion effects from the audiovisual stream automatically. However, these algorithms are designed for a single class of motion effects, and extension to multiple motion effect classes remains unexplored. In this paper, we propose an algorithmic framework that merges camera and object motion effects into one motion effect while preserving the perceptual consequences of the two effects. We validate the framework’s perceptual performance through a user study. To our knowledge, this work is one of the first successful reports of merging different kinds of motion effects for improved XR experiences.","(햅틱) 모션 효과는 모션 플랫폼에서 생성되어 플랫폼에 앉은 사용자의 몸 전체에 전달되는 전정 자극을 말합니다. 모션 효과는 훈련 시뮬레이터부터 최신 4D 놀이기구, 영화, 엔터테인먼트 게임에 이르기까지 다양한 확장 현실(XR) 애플리케이션에서 생생한 감각 경험을 만드는 데 필수적인 도구입니다. 후자의 경우 모션 효과는 카메라 모션, 관심 개체의 움직임, 특수 사운드 등 장면에서 발생하는 시청각 이벤트를 강조합니다. 최근 연구에서는 시청각 스트림에서 모션 효과를 자동으로 생성하는 여러 알고리즘을 개발했습니다. 그러나 이러한 알고리즘은 단일 클래스의 모션 효과용으로 설계되었으며 여러 모션 효과 클래스로의 확장은 아직 연구되지 않은 상태로 남아 있습니다. 본 논문에서는 두 효과의 지각 결과를 보존하면서 카메라와 객체 모션 효과를 하나의 모션 효과로 병합하는 알고리즘 프레임워크를 제안합니다. 우리는 사용자 연구를 통해 프레임워크의 인지 성능을 검증합니다. 우리가 아는 바로는 이 작업은 향상된 XR 경험을 위해 다양한 종류의 모션 효과를 병합한 최초의 성공적인 보고서 중 하나입니다.",https://doi.org/10.1109/ISMAR59233.2023.00120,Audio & Sound,Sensor Fusion,User Study,System / Framework
361,2023,Meta360: Exploring User-Specific and Robust Viewport Prediction in360-Degree Videos through Bi-Directional LSTM and Meta-Adaptation,Meta360: 양방향 LSTM 및 메타 적응을 통해 360도 비디오에서 사용자별 강력한 뷰포트 예측 탐색,"Viewport prediction is a critical aspect of virtual reality (VR) video streaming, directly impacting user experience in adaptive streaming. However, most existing algorithms treat users as homogeneous entities and overlook the variations in user behaviors and video content. Additionally, they often struggle with long-term predictions and intense movement. Our research sheds light on the importance of considering user behavior variations and leveraging advanced techniques to optimize robust viewport prediction in VR video streaming. First, we address these limitations by conducting a comprehensive feature analysis on existing datasets to uncover distinctive user behaviors. Building upon these findings, we propose a novel approach that utilizes the power of Bidirectional Long Short-Term Memory (BiLSTM) networks and meta-learning. The BiLSTM architecture effectively captures long-term dependencies, which can strengthen the robustness of viewport prediction especially in longterm prediction and intense movement. Additionally, meta-learning enables personalized adaptation to individual users’ viewing behaviors. Through extensive evaluations on diverse datasets, our algorithm Meta360 demonstrates superior performance in terms of accuracy and robustness compared to state-of-the-art methods.","뷰포트 예측은 가상 현실(VR) 비디오 스트리밍의 중요한 측면으로, 적응형 스트리밍의 사용자 경험에 직접적인 영향을 미칩니다. 그러나 대부분의 기존 알고리즘은 사용자를 동질적인 개체로 취급하고 사용자 행동과 비디오 콘텐츠의 변화를 간과합니다. 또한 장기적인 예측과 강렬한 움직임으로 인해 어려움을 겪는 경우가 많습니다. 우리의 연구는 VR 비디오 스트리밍에서 강력한 뷰포트 예측을 최적화하기 위해 사용자 행동 변화를 고려하고 고급 기술을 활용하는 것의 중요성을 조명합니다. 첫째, 기존 데이터 세트에 대한 포괄적인 기능 분석을 수행하여 독특한 사용자 행동을 찾아냄으로써 이러한 제한 사항을 해결합니다. 이러한 발견을 바탕으로 우리는 BiLSTM(양방향 장단기 기억) 네트워크와 메타 학습의 힘을 활용하는 새로운 접근 방식을 제안합니다. BiLSTM 아키텍처는 장기 종속성을 효과적으로 포착하여 특히 장기 예측 및 강렬한 움직임에서 뷰포트 예측의 견고성을 강화할 수 있습니다. 또한 메타 학습을 통해 개별 사용자의 시청 행동에 맞춤화된 적응이 가능합니다. 다양한 데이터 세트에 대한 광범위한 평가를 통해 우리의 알고리즘 Meta360은 최첨단 방법에 비해 정확성과 견고성 측면에서 탁월한 성능을 보여줍니다.",https://doi.org/10.1109/ISMAR59233.2023.00080,Interaction & Input,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method; User Study / Empirical Findings
362,2023,Minilag Filter for Jitter Elimination of Pose Trajectory in AR Environment,AR 환경에서 포즈 궤적의 지터 제거를 위한 Minilag 필터,"In AR applications, the jitter of virtual objects can weaken the sense of integration with the real environment. This jitter is often caused by noise in the pose obtained by 3D tracking or localization methods, especially in monocular vision systems without IMU support. Filtering the pose is an effective method to eliminate jitter, however, it can also cause significant lag in the filtered pose, seriously degrading the AR experience. Existing filters struggle to simultaneously reduce jitter while maintaining low lag. In this paper, we propose a novel Minilag filter, which achieves excellent pose smoothing while significantly reducing the lag through backtracking update and compensation strategies, and has excellent real-time performance. We represent the rotation in the pose in the Lie algebra and filter it in locally Euclidean space, ensuring that the filtering of rotation is consistent with that of vectors. We also analyze the noise distribution and characteristics in the tracked pose, providing a theoretical basis for setting filter parameters. We evaluated the proposed filter using both objective mathematical metrics and a user study, and the experimental results demonstrate that our method achieves state-of-the-art performance.","AR 애플리케이션에서 가상 객체의 지터는 실제 환경과의 통합감을 약화시킬 수 있습니다. 이 지터는 특히 IMU를 지원하지 않는 단안 비전 시스템에서 3D 추적 또는 위치 파악 방법으로 얻은 포즈의 노이즈로 인해 발생하는 경우가 많습니다. 포즈를 필터링하는 것은 지터를 제거하는 효과적인 방법이지만, 필터링된 포즈에 상당한 지연이 발생하여 AR 경험이 심각하게 저하될 수도 있습니다. 기존 필터는 낮은 지연을 유지하는 동시에 지터를 줄이는 데 어려움을 겪고 있습니다. 본 논문에서는 역추적 업데이트 및 보상 전략을 통해 지연을 크게 줄이면서 뛰어난 포즈 스무딩을 달성하고 실시간 성능이 뛰어난 새로운 Minilag 필터를 제안합니다. 거짓말 대수학에서 포즈의 회전을 표현하고 이를 로컬 유클리드 공간에서 필터링하여 회전 필터링이 벡터 필터링과 일치하는지 확인합니다. 또한 추적된 포즈의 노이즈 분포와 특성을 분석하여 필터 매개변수 설정을 위한 이론적 기초를 제공합니다. 우리는 객관적인 수학적 측정법과 사용자 연구를 모두 사용하여 제안된 필터를 평가했으며 실험 결과는 우리의 방법이 최첨단 성능을 달성했음을 보여줍니다.",https://doi.org/10.1109/ISMAR59233.2023.00111,Tracking & Localization,Sensor Fusion,User Study,System / Framework
363,2023,Mixed Reality 3D Teleconsultation for Emergency Decompressive Craniotomy: An Evaluation with Medical Residents,응급 감압 개두술을 위한 혼합 현실 3D 원격 상담: 전공의를 대상으로 한 평가,"Enabling collaborative telepresence in healthcare, especially surgical procedures, presents a critical challenge. The decompressive craniotomy procedure stands out as particularly complex and time-sensitive. The current teleconsultation approach relies on 2D color cameras, often offering only a fixed view and limited visual capabilities between experts and surgeons. However, teleconsultation can be addressed with Mixed Reality and immersive technology to potentially enable a better consultation of the procedure. We conducted an extensive user study focusing on decompressive craniotomy to investigate the advantages and challenges of our 3D teleconsultation system compared to a 2D video-based consultation system. Our 3D teleconsultation system leverages real-time 3D reconstruction of the patient and environment to empower experts to provide guidance and create virtual 3D annotations. The study utilized 3D-printed head models to perform a lifelike surgical intervention. It involved 14 medical residents and demonstrated an in-vitro 17% improvement in accurately describing the incision size on the patient’s head, contributing to potentially improved patient outcomes.","의료, 특히 수술 절차에서 협업 텔레프레즌스를 활성화하는 것은 중요한 과제입니다. 감압 개두술 절차는 특히 복잡하고 시간에 민감합니다. 현재의 원격 상담 접근 방식은 2D 컬러 카메라에 의존하며, 종종 전문가와 외과 의사 사이에 고정된 보기와 제한된 시각적 기능만 제공합니다. 그러나 혼합 현실과 몰입형 기술을 사용하면 원격 상담을 통해 잠재적으로 절차에 대한 더 나은 상담이 가능해집니다. 우리는 2D 비디오 기반 상담 시스템과 비교하여 3D 원격 상담 시스템의 장점과 문제점을 조사하기 위해 감압 개두술에 초점을 맞춘 광범위한 사용자 연구를 수행했습니다. 당사의 3D 원격 상담 시스템은 환자와 환경의 실시간 3D 재구성을 활용하여 전문가가 지침을 제공하고 가상 3D 주석을 생성할 수 있도록 지원합니다. 이 연구에서는 실제와 같은 수술을 수행하기 위해 3D 프린팅된 머리 모델을 활용했습니다. 14명의 전공의가 참여했으며 시험관 내에서 환자 머리의 절개 크기를 정확하게 설명하는 데 17%의 개선이 있었으며 잠재적으로 환자 결과가 개선되는 데 기여했습니다.",https://doi.org/10.1109/ISMAR59233.2023.00081,Medical & Healthcare; Collaboration & Social,3D Reconstruction,User Study,Algorithm / Method
364,2023,MonoVAN: Visual Attention for Self-Supervised Monocular Depth Estimation,MonoVAN: 자가 지도 단안 깊이 추정을 위한 시각적 주의,"Depth estimation is crucial in various computer vision applications, including autonomous driving, robotics, and virtual and augmented reality. An accurate scene depth map is beneficial for localization, spatial registration, and tracking. It converts 2D images into precise 3D coordinates for accurate positioning, seamlessly aligns virtual and real objects in applications like AR, and enhances object tracking by distinguishing distances. The self-supervised monocular approach is particularly promising as it eliminates the need for complex and expensive data acquisition setups relying solely on a standard RGB camera. Recently, transformer-based architectures have become popular to solve this problem, but at high quality, they suffer from high computational cost and poor perception of small details as they focus more on global information. In this paper, we propose a novel fully convolutional network for monocular depth estimation, called MonoVAN, which incorporates the visual attention mechanism and applies super-resolution techniques in decoder to better capture fine-grained details in depth maps. To the best of our knowledge, this work pioneers the use of a convolutional visual attention in the context of depth estimation. Our experiments on outdoor KITTI benchmark and the indoor NYUv2 dataset show that our approach outperforms the most advanced self-supervised methods, including such state-of-the-art models as transformer-based VTDepth from ISMAR’22 and hybrid convolutional-transformer MonoFormer from AAAI’23, while having a comparable or even fewer number of parameters in our model than competitors. We also validate the impact of each proposed improvement in isolation, providing evidence of its significant contribution. Code and weights are available at https://github.com/IlyaInd/MonoVAN.","깊이 추정은 자율 주행, 로봇공학, 가상 및 증강 현실을 포함한 다양한 컴퓨터 비전 애플리케이션에서 매우 중요합니다. 정확한 장면 깊이 맵은 위치 파악, 공간 등록 및 추적에 유용합니다. 정확한 위치 지정을 위해 2D 이미지를 정밀한 3D 좌표로 변환하고, AR과 같은 애플리케이션에서 가상 객체와 실제 객체를 원활하게 정렬하며, 거리를 구별하여 객체 추적을 향상시킵니다. 자가 감독 단안 접근 방식은 표준 RGB 카메라에만 의존하는 복잡하고 값비싼 데이터 수집 설정이 필요하지 않기 때문에 특히 유망합니다. 최근에는 이 문제를 해결하기 위해 변환기 기반 아키텍처가 인기를 얻었지만 고품질에서는 전역 정보에 더 집중하기 때문에 높은 계산 비용과 작은 세부 사항에 대한 인식이 좋지 않은 문제가 있습니다. 본 논문에서는 시각적 주의 메커니즘을 통합하고 디코더에 초해상도 기술을 적용하여 깊이 맵에서 세밀한 세부 사항을 더 잘 캡처하는 MonoVAN이라는 단안 깊이 추정을 위한 새로운 완전 합성곱 네트워크를 제안합니다. 우리가 아는 한, 이 작업은 깊이 추정의 맥락에서 컨볼루셔널 시각적 주의 사용을 개척했습니다. 실외 KITTI 벤치마크 및 실내 NYUv2 데이터 세트에 대한 실험에서는 우리의 접근 방식이 ISMAR'22의 변환기 기반 VTDepth 및 AAAI'23의 하이브리드 컨볼루셔널 변환기 MonoFormer와 같은 최첨단 모델을 포함하여 가장 진보된 자체 감독 방법보다 성능이 뛰어나면서도 우리 모델의 매개 변수 수가 경쟁사보다 비슷하거나 더 적다는 것을 보여줍니다. 또한 제안된 각 개선 사항의 영향을 개별적으로 검증하여 상당한 기여에 대한 증거를 제공합니다. 코드와 가중치는 https://github.com/IlyaInd/MonoVAN에서 확인할 수 있습니다.",https://doi.org/10.1109/ISMAR59233.2023.00138,Tracking & Localization; Perception & Cognition,Deep Learning / Neural Networks; Computer Vision,Technical Evaluation,Algorithm / Method; Dataset / Benchmark
365,2023,Multi-Focus Querying of the Human Genome Information on Desktop and in Virtual Reality: an Evaluation,데스크톱 및 가상 현실에서 인간 게놈 정보의 다중 초점 쿼리: 평가,"The human genome is incredibly information-rich, consisting of approximately 25,000 protein-coding genes spread out over 3.2 billion nucleotide base pairs contained within 24 unique chromosomes. The genome is important in maintaining spatial context, which assists in understanding gene interactions and relationships. However, existing methods of genome visualization that utilize spatial awareness are inefficient and prone to limitations in presenting gene information and spatial context. This study proposed an innovative approach to genome visualization and exploration utilizing virtual reality. To determine the optimal placement of gene information and evaluate its essentiality in a VR environment, we implemented and conducted a user study with three different interaction methods. Two interaction methods were developed in virtual reality to determine if gene information is better suited to be embedded within the chromosome ideogram or separate from the ideogram. The final ideogram interaction method was performed on a desktop and served as a benchmark to evaluate the potential benefits associated with the use of VR. Our study findings reveal a preference for VR, despite longer task completion times. In addition, the placement of gene information within the visualization had a notable impact on the ability of a user to complete tasks. Specifically, gene information embedded within the chromosome ideogram was better suited for single target identification and summarization tasks, while separating gene information from the ideogram better supported region comparison tasks.","인간 게놈은 믿을 수 없을 정도로 정보가 풍부하며, 24개의 독특한 염색체 내에 포함된 32억 개가 넘는 뉴클레오티드 염기쌍에 퍼져 있는 약 25,000개의 단백질 코딩 유전자로 구성되어 있습니다. 게놈은 유전자 상호 작용과 관계를 이해하는 데 도움이 되는 공간적 맥락을 유지하는 데 중요합니다. 그러나 공간 인식을 활용하는 기존의 게놈 시각화 방법은 유전자 정보와 공간적 맥락을 제시하는 데 있어 비효율적이고 한계가 있기 쉽습니다. 본 연구에서는 가상현실을 활용한 게놈 시각화 및 탐색에 대한 혁신적인 접근 방식을 제안했습니다. VR 환경에서 유전자 정보의 최적 배치를 결정하고 그 필수성을 평가하기 위해 세 가지 상호 작용 방법을 사용하여 사용자 연구를 구현하고 수행했습니다. 유전자 정보가 염색체 표의 문자 내에 삽입되거나 표의 문자와 분리되는 것이 더 적합한지 결정하기 위해 가상 현실에서 두 가지 상호 작용 방법이 개발되었습니다. 최종 표의 문자 상호 작용 방법은 데스크톱에서 수행되었으며 VR 사용과 관련된 잠재적 이점을 평가하기 위한 벤치마크로 사용되었습니다. 우리의 연구 결과에 따르면 작업 완료 시간이 길어지더라도 VR을 선호하는 것으로 나타났습니다. 또한 시각화 내 유전자 정보 배치는 사용자가 작업을 완료하는 능력에 주목할만한 영향을 미쳤습니다. 특히, 염색체 표의문자 내에 포함된 유전자 정보는 단일 표적 식별 및 요약 작업에 더 적합했으며, 표의문자에서 유전자 정보를 분리하는 것이 지역 비교 작업을 더 잘 지원했습니다.",https://doi.org/10.1109/ISMAR59233.2023.00129,Interaction & Input; Perception & Cognition,Other,Technical Evaluation,User Study / Empirical Findings; Algorithm / Method
366,2023,Multi-modal classification of cognitive load in a VR-based training system,VR 기반 훈련 시스템에서 인지 부하의 다중 모드 분류,"Training systems are used in many industries, ranging from surgery to space missions to rehabilitation. Virtual Reality (VR) is a technology that has been incorporated as an effective tool in such training systems to simulate the environment, especially in situations where the training can’t take place in the actual environment. For a training environment and task to be effective, it must sufficiently challenge the trainee. One parameter that can be used to measure this is cognitive load (CL), which is defined as the amount of working memory used while performing a learning task. This parameter needs to be sufficiently high to maximize learning but not too high as to overload the trainee. However, the challenge is to detect this state using objective physiological measures, which can be collected during the entire task. This paper presents a study to classify CL using a combination of Electroencephalogram (EEG) and Electrodermal Activity (EDA) signals during a procedural VR training task. Thirty participants undertook a study where they built a designated model within a given time over multiple levels that were constructed to induce low to high CL. Features generated from the data were subject to feature selection (FS), which was undertaken using the Mutual Information (MI) technique. Binary classification models were developed using Support Vector Machines (SVM), Random Forest (RF), k-Nearest Neighbors (kNN), Extreme Gradient Boosting (Xgboost) and Multi-Layer Perceptrons (MLP). Results illustrated that the Xgboost classifier performed the best with an F1-score of $0.831 \pm 0.030$ and accuracy of $0.805 \pm 0.033.$ SHAP analysis of the features illustrated greater contributions from the frontal and occipital regions of the brain and frequency domain features from tonic skin conductance.","훈련 시스템은 수술부터 우주 임무, 재활에 이르기까지 다양한 산업 분야에서 사용됩니다. 가상현실(VR)은 특히 실제 환경에서 훈련을 할 수 없는 상황에서 환경을 시뮬레이션하기 위해 훈련 시스템에 효과적인 도구로 통합된 기술입니다. 훈련 환경과 작업이 효과적이려면 훈련생에게 충분히 도전적이어야 합니다. 이를 측정하는 데 사용할 수 있는 매개변수 중 하나는 인지 부하(CL)이며, 이는 학습 작업을 수행하는 동안 사용되는 작업 메모리의 양으로 정의됩니다. 이 매개변수는 학습을 최대화하기 위해 충분히 높아야 하지만 훈련생에게 과부하가 걸릴 정도로 너무 높으면 안 됩니다. 그러나 과제는 전체 작업 중에 수집할 수 있는 객관적인 생리학적 측정을 사용하여 이 상태를 감지하는 것입니다. 이 논문에서는 절차적 VR 훈련 작업 중에 뇌전도(EEG)와 피부 전기 활동(EDA) 신호의 조합을 사용하여 CL을 분류하는 연구를 제시합니다. 30명의 참가자는 낮은 수준에서 높은 수준의 CL을 유도하도록 구성된 여러 수준에 걸쳐 지정된 시간 내에 지정된 모델을 구축하는 연구에 착수했습니다. 데이터에서 생성된 특징은 상호 정보(MI) 기술을 사용하여 수행된 특징 선택(FS)을 거쳤습니다. 이진 분류 모델은 SVM(Support Vector Machines), RF(Random Forest), kNN(k-Nearest Neighbors), Xgboost(Extreme Gradient Boosting) 및 MLP(Multi-Layer Perceptrons)를 사용하여 개발되었습니다. 결과는 Xgboost 분류기가 $0.831 \pm 0.030$의 F1 점수와 $0.805 \pm 0.033$의 정확도로 가장 잘 수행되었음을 보여줍니다. 기능에 대한 SHAP 분석은 뇌의 전두엽 및 후두부 영역과 강장성 피부 전도도의 주파수 도메인 특징에서 더 큰 기여를 보여주었습니다.",https://doi.org/10.1109/ISMAR59233.2023.00065,Interaction & Input; Education & Training,Sensor Fusion,User Study,Algorithm / Method
367,2023,MultiVibes: What if your VR Controller had 10 Times more Vibrotactile Actuators?,MultiVibes: VR 컨트롤러에 진동촉각 액추에이터가 10배 더 많다면 어떨까요?,"Consumer-grade virtual reality (VR) controllers are typically equipped with one vibrotactile actuator, allowing to create simple and non-spatialized tactile sensations through the vibration of the entire controller. Leveraging the funneling effect, an illusion in which multiple vibrations are perceived as a single one, we propose MultiVibes, a VR controller capable of rendering spatialized sensations at different locations on the user’s hand and fingers. The designed prototype includes ten vibrotactile actuators, directly in contact with the skin of the hand, limiting the propagation of vibrations through the controller. We evaluated MultiVibes through two controlled experiments. The first one focused on the ability of users to recognize spatio-temporal patterns, while the second one focused on the impact of MultiVibes on the users’ haptic experience when interacting with virtual objects they can feel. Taken together, the results show that MultiVibes is capable of providing accurate spatialized feedback and that users prefer MultiVibes over recent VR controllers.","소비자 등급 가상 현실(VR) 컨트롤러에는 일반적으로 하나의 진동 촉각 액추에이터가 장착되어 전체 컨트롤러의 진동을 통해 간단하고 비공간적인 촉각 감각을 생성할 수 있습니다. 여러 개의 진동이 하나의 진동으로 인식되는 환상인 퍼넬링 효과를 활용하여 사용자의 손과 손가락의 다양한 위치에 공간화된 감각을 렌더링할 수 있는 VR 컨트롤러인 MultiVibes를 제안합니다. 설계된 프로토타입에는 손 피부와 직접 접촉하는 10개의 진동 촉각 액추에이터가 포함되어 있어 컨트롤러를 통한 진동 전파를 제한합니다. 우리는 두 가지 통제된 실험을 통해 MultiVibe를 평가했습니다. 첫 번째는 사용자가 시공간 패턴을 인식하는 능력에 초점을 맞춘 반면, 두 번째는 사용자가 느낄 수 있는 가상 객체와 상호 작용할 때 MultiVibe가 사용자의 햅틱 경험에 미치는 영향에 중점을 두었습니다. 종합해보면, 결과는 MultiVibes가 정확한 공간화된 피드백을 제공할 수 있으며 사용자가 최신 VR 컨트롤러보다 MultiVibes를 선호한다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR59233.2023.00085,Interaction & Input,Haptic / Tactile Feedback,User Study,User Study / Empirical Findings
368,2023,Now I Wanna Be a Dog: Exploring the Impact of Audio and Tactile Feedback on Animal Embodiment,이제 나는 개가 되고 싶어: 동물 구현에 대한 오디오 및 촉각 피드백의 영향 탐구,"Embodying a virtual creature or animal in Virtual Reality (VR) is becoming common, and can have numerous beneficial impacts. For instance, it can help actors improve their performance of a computer-generated creature, or it can endow the user with empathy towards threatened animal species. However, users must feel a sense of embodiment towards their virtual representation, commonly achieved by providing congruent sensory feedback. Providing effective visuo-motor feedback in dysmorphic bodies can be challenging due to human-animal morphology differences. Thus, the purpose of this study was to experiment with the inclusion of audio and audio-tactile feedback to begin unveiling their influence towards animal avatar embodiment. Two experiments were conducted to examine the effects of different sensory feedback on participants’ embodiment in a dog avatar in an Immersive Virtual Environment (IVE). The first experiment (n= 24) included audio, tactile, audio-tactile, and baseline conditions. The second experiment (n= 34) involved audio and baseline conditions only.","가상 현실(VR)에서 가상 생물이나 동물을 구현하는 것이 일반화되고 있으며 수많은 유익한 영향을 미칠 수 있습니다. 예를 들어, 배우가 컴퓨터로 생성된 생물의 연기를 향상시키는 데 도움이 될 수 있고, 사용자에게 멸종 위기에 처한 동물 종에 대한 공감 능력을 부여할 수 있습니다. 그러나 사용자는 일반적으로 일치하는 감각 피드백을 제공함으로써 달성되는 가상 표현에 대한 구체화 감각을 느껴야 합니다. 이형체에서 효과적인 시각-운동 피드백을 제공하는 것은 인간과 동물의 형태적 차이로 인해 어려울 수 있습니다. 따라서 이 연구의 목적은 동물 아바타 구현에 대한 영향을 밝히기 시작하기 위해 오디오 및 오디오 촉각 피드백을 포함하는 실험을 시작하는 것이었습니다. 몰입형 가상 환경(IVE)에서 개 아바타의 참가자 구현에 대한 다양한 감각 피드백의 효과를 조사하기 위해 두 가지 실험이 수행되었습니다. 첫 번째 실험(n= 24)에는 청각, 촉각, 청각-촉각 및 기본 조건이 포함되었습니다. 두 번째 실험(n= 34)에는 오디오 및 기본 조건만 포함되었습니다.",https://doi.org/10.1109/ISMAR59233.2023.00107,Perception & Cognition,Haptic / Tactile Feedback,User Study,User Study / Empirical Findings
369,2023,Perception and Proxemics with Virtual Humans on Transparent Display Installations in Augmented Reality,증강현실의 투명 디스플레이 설치물에 대한 가상인간의 인지와 근접학,"It is not uncommon for science fiction movies to portray futuristic user interfaces that can only be realized decades later with state-of-the-art technology. In this work, we present a prototypical augmented reality (AR) installation that was inspired by the movie The Time Machine (2002). It consists of a transparent screen that acts as a window through which users can see the stereoscopic projection of a three-dimensional virtual human (VH). However, there are some key differences between the vision of this technology and the way VHs on these displays are actually perceived. In particular, the additive light model of these displays causes darker VHs to appear more transparent, while light in the physical environment further increases transparency, which may affect the way VHs are perceived, to what degree they are trusted, and the distances one maintains from them in a spatial setting. In this paper, we present a user study in which we investigate how transparency in the scope of transparent AR screens affects the perception of a VH’s appearance, social presence with the VH, and the social space around users as defined by proxemics theory. Our results indicate that appearances are comparatively robust to transparency, while social presence improves in darker physical environments, and proxemic distances to the VH largely depend on one’s distance from the screen but are not noticeably affected by transparency. Overall, our results suggest that such transparent AR screens can be an effective technology for facilitating social interactions between users and VHs in a shared physical space.","공상과학 영화에서 수십 년 후에야 최첨단 기술로 실현될 수 있는 미래 지향적인 사용자 인터페이스를 묘사하는 것은 드문 일이 아닙니다. 이 작품에서는 영화 The Time Machine(2002)에서 영감을 받은 프로토타입 증강 현실(AR) 설치물을 선보입니다. 이는 사용자가 3차원 가상 인간(VH)의 입체 투영을 볼 수 있는 창 역할을 하는 투명 스크린으로 구성됩니다. 그러나 이 기술의 비전과 이러한 디스플레이의 VH가 실제로 인식되는 방식에는 몇 가지 주요 차이점이 있습니다. 특히, 이러한 디스플레이의 추가 조명 모델은 더 어두운 VH가 더 투명하게 나타나는 반면, 물리적 환경의 빛은 투명성을 더욱 증가시켜 VH가 인식되는 방식, 신뢰도 및 공간 설정에서 유지하는 거리에 영향을 미칠 수 있습니다. 본 논문에서는 투명한 AR 화면 범위의 투명성이 VH의 외모 인식, VH와의 사회적 존재, 근접학 이론으로 정의된 사용자 주변의 사회적 공간에 어떤 영향을 미치는지 조사하는 사용자 연구를 제시합니다. 우리의 결과는 외관이 상대적으로 투명도에 강하고 사회적 존재감이 어두운 물리적 환경에서 향상되며 VH까지의 근접 거리는 화면과의 거리에 크게 좌우되지만 투명도에 의해 눈에 띄게 영향을 받지 않는다는 것을 나타냅니다. 전반적으로 우리의 결과는 이러한 투명한 AR 화면이 공유된 물리적 공간에서 사용자와 VH 간의 사회적 상호 작용을 촉진하는 효과적인 기술이 될 수 있음을 시사합니다.",https://doi.org/10.1109/ISMAR59233.2023.00053,Perception & Cognition; Interaction & Input,Other,User Study,Algorithm / Method
370,2023,Perceptual Tolerance of Split-Up Effect for Near-Eye Light Field Display,근안 라이트 필드 디스플레이에 대한 분할 효과의 지각 허용 오차,"When the light field of a scene is generated with a finite number of subviews, the defocused regions would appear to be split-up if a camera is used to capture the light field. Yet, the split-up effect is unnoticeable when the light field is viewed directly through a human eye. In this paper, we attribute the unobservability of the split-up effect to the decrease in visual acuity as a function of retinal eccentricity and to the low-pass filtering property of visual attention. Theoretical and experimental results are provided to support our claim. Furthermore, we set an observability criterion for the split-up effect and discuss design strategies for performance improvement of light field displays.",제한된 수의 하위 뷰를 사용하여 장면의 라이트 필드가 생성되는 경우 카메라를 사용하여 라이트 필드를 캡처하면 초점이 흐려진 영역이 분할된 것처럼 보입니다. 그러나 인간의 눈을 통해 라이트 필드를 직접 볼 때 분할 효과는 눈에 띄지 않습니다. 이 논문에서 우리는 분할 효과의 관찰 불가능성을 망막 이심률의 함수로서 시력 감소와 시각적 주의의 저역 통과 필터링 특성에 기인합니다. 우리의 주장을 뒷받침하기 위해 이론적 및 실험적 결과가 제공됩니다. 또한 분할 효과에 대한 관찰 가능성 기준을 설정하고 라이트 필드 디스플레이의 성능 향상을 위한 설계 전략을 논의합니다.,https://doi.org/10.1109/ISMAR59233.2023.00032,Display & Optics,Optical / Display Technology,Other,Other
371,2023,Performance Impact of Immersion and Collaboration in Visual Data Analysis,시각적 데이터 분석에서 몰입도와 협업이 성능에 미치는 영향,"Immersive Analytics is a recent field of study that focuses on utilizing emerging extended reality technologies to bring visual data analysis from the 2D screen to the real/virtual world. The effectiveness of Immersive Analytics, when compared to traditional systems, has been widely studied in this field’s corpus, usually concluding that the immersive solution is superior. However, when it comes to comparing collaborative to single-user immersive analytics, the literature is lacking in user studies. As such, we developed a comprehensive experimental study with the objective of quantifying and analysing the impact that both immersion and collaboration have on the visual data analysis process. A two-variable (immersion: desktop/virtual reality; number of users: solo/pair) full factorial study was conceived with a mixed design (within-subject for immersion and between subject for number of users). Each of the 24 solo and 24 pairs of participants solved five visual data analysis tasks in both a head-mounted display-based virtual world and a desktop computer environment. The results show that, in terms of task time to completion, there were no significant differences between desktop and virtual reality, or between the solo and pair conditions. However, it was possible to conclude that collaboration is more beneficial the more complex the task is in both desktop and virtual reality, and that for less complex tasks, collaboration can be a hindrance. System Usability Scale scores were significantly better in the virtual reality condition than the desktop one, especially when working in pairs. As for user preference, the virtual reality system was significantly more favoured both as a visual data analysis platform and a collaborative data analysis platform over the desktop system. All supplemental materials are available at https://osf.io/k94u5/.","몰입형 분석(Immersive Analytics)은 새로운 확장 현실 기술을 활용하여 2D 화면의 시각적 데이터 분석을 실제/가상 세계로 가져오는 데 중점을 두는 최근 연구 분야입니다. 몰입형 분석의 효율성은 기존 시스템과 비교할 때 이 분야의 코퍼스에서 광범위하게 연구되어 왔으며 일반적으로 몰입형 솔루션이 우수하다는 결론을 내립니다. 그러나 협업 분석과 단일 사용자 몰입형 분석을 비교할 때 사용자 연구에 대한 문헌은 부족합니다. 따라서 우리는 몰입과 협업이 시각적 데이터 분석 프로세스에 미치는 영향을 정량화하고 분석하기 위한 목적으로 포괄적인 실험 연구를 개발했습니다. 2변수(몰입: 데스크탑/가상 현실, 사용자 수: 솔로/페어) 전체 요인 연구는 혼합 설계(몰입을 위한 대상 내 및 사용자 수에 대한 대상 간)로 고안되었습니다. 24명의 솔로 참가자와 24쌍의 참가자는 각각 헤드마운트 디스플레이 기반 가상 세계와 데스크톱 컴퓨터 환경 모두에서 5가지 시각적 데이터 분석 과제를 해결했습니다. 결과는 작업 완료까지의 시간 측면에서 데스크톱과 가상 현실, 또는 솔로와 페어 조건 간에 큰 차이가 없음을 보여줍니다. 그러나 데스크톱과 가상 현실 모두에서 작업이 복잡할수록 협업이 더 유익하고, 덜 복잡한 작업의 경우 협업이 방해가 될 수 있다는 결론을 내릴 수 있었습니다. 시스템 사용성 척도 점수는 데스크톱 환경보다 가상 현실 환경에서 훨씬 더 좋았으며, 특히 두 사람이 함께 작업할 때 더욱 그렇습니다. 사용자 선호도에서는 가상 현실 시스템이 데스크톱 시스템보다 시각적 데이터 분석 플랫폼과 협업 데이터 분석 플랫폼 모두에서 훨씬 더 선호되었습니다. 모든 보충 자료는 https://osf.io/k94u5/에서 확인할 수 있습니다.",https://doi.org/10.1109/ISMAR59233.2023.00093,Display & Optics; Collaboration & Social,Other,User Study,System / Framework
372,2023,PhyVR: Physics-based Multi-material and Free-hand Interaction in VR,PhyVR: VR의 물리 기반 다중 재료 및 자유 손 상호 작용,"The realistic interaction with physical phenomena is a crucial aspect of human-computer interaction (HCI) in virtual reality (VR). However, the real-time performance of physical simulation, interactive computation, and rendering is the bottleneck of physics-based VR HCI. To address these challenges, we propose a novel physics-oriented framework for multi-material objects and free-hand interaction, termed PhyVR. This framework enables users to interact with diverse virtual phenomena dynamically. At the algorithm level, we develop a unified particle system to describe both the virtual multi-materials and the user’s avatar for the efficiency issue, optimize collision detection, and accelerate the HCI algorithms with a variable fine-coarse particle sampling scheme. At the rendering level, we introduce a hybrid particle-grid anisotropic algorithm for surface reconstruction, enabling real-time and visually convincing fluid rendering. Comprehensive experiments and user studies demonstrate that our framework effectively captures various physical interaction phenomena, providing an enhanced user experience and paving the way for expanding VR-related HCI applications.","물리적 현상과 사실적인 상호작용은 가상현실(VR)에서 인간-컴퓨터 상호작용(HCI)의 중요한 측면입니다. 그러나 물리 시뮬레이션, 인터랙티브 계산, 렌더링의 실시간 성능은 물리 기반 VR HCI의 병목 현상입니다. 이러한 문제를 해결하기 위해 우리는 PhyVR이라고 불리는 다중 재료 개체 및 자유 손 상호 작용을 위한 새로운 물리학 중심 프레임워크를 제안합니다. 이 프레임워크를 통해 사용자는 다양한 가상 현상과 동적으로 상호 작용할 수 있습니다. 알고리즘 수준에서는 효율성 문제를 위해 가상 다중 재료와 사용자 아바타를 모두 설명하고 충돌 감지를 최적화하며 가변 미세 거친 입자 샘플링 방식으로 HCI 알고리즘을 가속화하는 통합 입자 시스템을 개발합니다. 렌더링 수준에서는 표면 재구성을 위한 하이브리드 입자 그리드 이방성 알고리즘을 도입하여 시각적으로 설득력 있는 실시간 유체 렌더링을 가능하게 합니다. 포괄적인 실험과 사용자 연구는 우리의 프레임워크가 다양한 물리적 상호 작용 현상을 효과적으로 포착하여 향상된 사용자 경험을 제공하고 VR 관련 HCI 애플리케이션 확장을 위한 길을 닦는다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR59233.2023.00060,Interaction & Input,Sensor Fusion,Simulation,System / Framework; Algorithm / Method
373,2023,PianoSyncAR: Enhancing Piano Learning through Visualizing Synchronized Hand Pose Discrepancies in Augmented Reality,PianoSyncAR: 증강 현실에서 동기화된 손 자세 불일치 시각화를 통해 피아노 학습 향상,"Motor skill acquisition involves learning from spatiotemporal discrepancies between target and self-generated motions. However, in dexterous skills with numerous degrees of freedom, understanding and correcting these motor errors are challenging. This issue becomes crucial for experienced individuals who seek for mastering and sophisticating their skills, where even subtle errors need to be minimized. To enable efficient optimization of body posture in piano learning, we present PianoSyncAR, an augmented reality system that superimposes the time-varying complex hand postures of a teacher over the hand of a learner. Through a user study with 12 pianists, we demonstrate several advantages of the proposed system over conventional tablet-screen, which implicate the potential of AR training as a complementary tool for video-based skill learning in piano playing.",운동 기술 습득에는 목표 동작과 자체 생성 동작 사이의 시공간적 불일치로부터 학습하는 것이 포함됩니다. 그러나 자유도가 다양한 능숙한 기술에서는 이러한 운동 오류를 이해하고 수정하는 것이 어렵습니다. 이 문제는 미묘한 오류라도 최소화해야 하는 자신의 기술을 숙달하고 정교화하려는 숙련된 개인에게 매우 중요합니다. 피아노 학습에서 신체 자세를 효율적으로 최적화하기 위해 우리는 학습자의 손 위에 교사의 시간에 따라 변하는 복잡한 손 자세를 겹쳐주는 증강 현실 시스템인 PianoSyncAR를 제시합니다. 12명의 피아니스트를 대상으로 한 사용자 연구를 통해 우리는 기존 태블릿 화면에 비해 제안된 시스템의 몇 가지 장점을 보여주었습니다. 이는 피아노 연주에서 비디오 기반 기술 학습을 위한 보완 도구로서 AR 훈련의 잠재력을 암시합니다.,https://doi.org/10.1109/ISMAR59233.2023.00101,Education & Training,Hand / Gesture Recognition,User Study,User Study / Empirical Findings
374,2023,PinchLens: Applying Spatial Magnification and Adaptive Control-Display Gain for Precise Selection in Virtual Reality,PinchLens: 가상 현실에서 정확한 선택을 위해 공간 확대 및 적응형 제어-디스플레이 이득 적용,"We present PinchLens, a new free-hand target selection technique for acquiring small and dense targets in Virtual Reality. Traditional pinch-based selection does not allow people to precisely manipulate small and dense objects effectively due to tracking and perceptual inaccuracies. Our approach combines spatial magnification, an adaptive control-display gain, and visual feedback to improve selection accuracy. When a user starts the pinching selection process, a magnifying bubble expands the scale of nearby targets, an adaptive control-to-display ratio is applied to the user’s hand for precision, and a cursor is displayed at the estimated pinch point for enhanced visual feedback. We performed a user study to compare our technique to traditional pinch selection and several variations to isolate the impact of each of the technique’s features. The results showed that PinchLens significantly outperformed traditional pinch selection, reducing error rates from 18.9% to 1.9%. Furthermore, we found that magnification was the dominant feature to produce this improvement, while the adaptive control-display gain and visual cursor of pinch were also helpful in several conditions.","가상 현실에서 작고 조밀한 타겟을 획득하기 위한 새로운 자유 타겟 선택 기술인 PinchLens를 소개합니다. 기존의 핀치 기반 선택에서는 추적 및 지각의 부정확성으로 인해 사람들이 작고 조밀한 개체를 효과적으로 정밀하게 조작할 수 없습니다. 우리의 접근 방식은 공간 확대, 적응형 제어 디스플레이 이득 및 시각적 피드백을 결합하여 선택 정확도를 향상시킵니다. 사용자가 핀치 선택 프로세스를 시작하면 돋보기 버블이 주변 대상의 크기를 확장하고, 정확성을 위해 적응형 컨트롤-디스플레이 비율이 사용자의 손에 적용되며, 향상된 시각적 피드백을 위해 예상 핀치 지점에 커서가 표시됩니다. 우리는 우리 기술을 전통적인 핀치 선택 및 여러 변형과 비교하여 각 기술 기능의 영향을 분리하기 위해 사용자 연구를 수행했습니다. 결과에 따르면 PinchLens는 기존 핀치 선택보다 훨씬 뛰어난 성능을 보여 오류율을 18.9%에서 1.9%로 줄였습니다. 또한, 우리는 이러한 개선을 이루는 데 배율이 주요 특징인 반면 적응형 제어 디스플레이 이득과 핀치의 시각적 커서도 여러 조건에서 도움이 된다는 것을 발견했습니다.",https://doi.org/10.1109/ISMAR59233.2023.00139,Interaction & Input; Display & Optics,Other,Quantitative Experiment,Algorithm / Method; User Study / Empirical Findings
375,2023,Point & Portal: A New Action at a Distance Technique For Virtual Reality,포인트 및 포털: 가상 현실을 위한 새로운 원거리 액션 기술,"This paper introduces Point & Portal, a novel Virtual Reality (VR) interaction technique, inspired by Point & Teleport. This new technique enables users to configure portals using pointing actions, and supports seamless action at a distance and navigation without requiring line of sight. By supporting multiple portals, Point & Portalenables users to create dynamic portal configurations to manage multiple remote tasks. Additionally, this paper introduces Relative Portal Positioning for reliable portal interactions, and the concept of maintaining Level Portals. In a comparative user study, Point & Portal demonstrated significant advantages over the traditional Point & Teleport technique to bring interaction devices within-arm’s reach. In the presence of obstacles, Point & Portal exhibited faster speed, lower cognitive load and was preferred by participants. Overall, participants required less physical movement, pointing actions, and reported higher involvement and “good” usability.","본 논문에서는 Point & Teleport에서 영감을 받은 새로운 가상현실(VR) 상호작용 기술인 Point & Portal을 소개합니다. 이 새로운 기술을 통해 사용자는 포인팅 작업을 사용하여 포털을 구성할 수 있으며, 원거리에서의 원활한 작업과 시선 없이 탐색을 지원합니다. Point & Portal은 여러 포털을 지원함으로써 사용자가 동적 포털 구성을 생성하여 여러 원격 작업을 관리할 수 있도록 해줍니다. Additionally, this paper introduces Relative Portal Positioning for reliable portal interactions, and the concept of maintaining Level Portals. 비교 사용자 연구에서 Point & Portal은 상호 작용 장치를 팔 범위 내로 가져오는 기존 Point & Teleport 기술에 비해 상당한 이점을 보여주었습니다. 장애물이 있는 경우 Point & Portal은 더 빠른 속도와 낮은 인지 부하를 보여 참가자들이 선호했습니다. 전반적으로 참가자들은 신체적인 움직임과 포인팅 동작이 덜 필요했으며 더 높은 참여도와 ""좋은"" 유용성을 보고했습니다.",https://doi.org/10.1109/ISMAR59233.2023.00026,Interaction & Input; Perception & Cognition,Other,User Study,Algorithm / Method
376,2023,QAVA-DPC: Eye-Tracking Based Quality Assessment and Visual Attention Dataset for Dynamic Point Cloud in 6 DoF,QAVA-DPC: 6 DoF의 동적 포인트 클라우드에 대한 시선 추적 기반 품질 평가 및 시각적 주의 데이터 세트,"Perceptual quality assessment of Dynamic Point Cloud (DPC) contents plays an important role in various Virtual Reality (VR) applications that involve human beings as the end user, understanding and modeling perceptual quality assessment is greatly enriched by insights from visual attention. However, incorporating aspects of visual attention in DPC quality models is largely unexplored, as ground-truth visual attention data is scarcely available. This paper presents a dataset containing subjective opinion scores and visual attention maps of DPCs, collected in a VR environment using eye-tracking technology. The data was collected during a subjective quality assessment experiment, in which subjects were instructed to watch and rate DPCs at various degradation levels under 6 degrees-of-freedom inspection, using a head-mounted display. The dataset comprises 5 reference DPC contents, with each reference encoded at 3 distortion levels using 3 different codecs, amounting to a total of 9 degraded DPC contents. Moreover, it includes 1,000 gaze trials from 40 participants, resulting in 15,000 visual attention maps in total. The curated dataset can serve as authentic benchmark data for assessing the performance of objective DPC quality metrics. Additionally, it establishes a link between quality assessment and visual attention within the context of DPC. This work deepens our understanding of DPC quality and visual attention, driving progress in the realm of VR experiences and perception.","DPC(Dynamic Point Cloud) 콘텐츠의 지각 품질 평가는 인간을 최종 사용자로 포함하는 다양한 가상 현실(VR) 애플리케이션에서 중요한 역할을 하며, 지각 품질 평가를 이해하고 모델링하는 것은 시각적 주의로부터 얻은 통찰력으로 크게 강화됩니다. 그러나 실제 시각적 주의 데이터를 거의 사용할 수 없기 때문에 DPC 품질 모델에 시각적 주의 측면을 통합하는 것은 거의 탐구되지 않았습니다. 본 논문은 시선 추적 기술을 사용하여 VR 환경에서 수집된 DPC의 주관적 의견 점수와 시각적 주의 맵이 포함된 데이터 세트를 제시합니다. 데이터는 주관적 품질 평가 실험 중에 수집되었으며, 이 실험에서 피험자는 머리 장착 디스플레이를 사용하여 6자유도 검사 하에 다양한 성능 저하 수준에서 DPC를 관찰하고 평가하도록 지시 받았습니다. 데이터 세트는 5개의 참조 DPC 콘텐츠로 구성되며, 각 참조는 3개의 서로 다른 코덱을 사용하여 3개의 왜곡 수준으로 인코딩되어 총 9개의 저하된 DPC 콘텐츠에 해당합니다. 또한 40명의 참가자로부터 1,000번의 시선 시도가 포함되어 총 15,000개의 시각적 주의 지도가 생성됩니다. 선별된 데이터세트는 객관적인 DPC 품질 지표의 성능을 평가하기 위한 실제 벤치마크 데이터 역할을 할 수 있습니다. 또한 DPC 맥락 내에서 품질 평가와 시각적 주의 사이의 연결을 설정합니다. 이 작업은 DPC 품질과 시각적 관심에 대한 이해를 심화시켜 VR 경험과 인식 영역의 발전을 촉진합니다.",https://doi.org/10.1109/ISMAR59233.2023.00021,Display & Optics; Perception & Cognition,Cloud / Edge Computing,User Study; Technical Evaluation,Dataset / Benchmark
377,2023,RC-SMPL: Real-time Cumulative SMPL-based Avatar Body Generation,RC-SMPL: 실시간 누적 SMPL 기반 아바타 신체 생성,"We present a novel method for avatar body generation that cumulatively updates the texture and normal map in real-time. Multiple images or videos have been broadly adopted to create detailed 3D human models that capture more realistic user identities in both Augmented Reality (AR) and Virtual Reality (VR) environments. However, this approach has a higher spatiotemporal cost because it requires a complex camera setup and extensive computational resources. For lightweight reconstruction of personalized avatar bodies, we design a system that progressively captures the texture and normal values using a single RGBD camera to generate the widely-accepted 3D parametric body model, SMPL-X. Quantitatively, our system maintains real-time performance while delivering reconstruction quality comparable to the state-of-the-art method. Moreover, user studies reveal the benefits of real-time avatar creation and its applicability in various collaborative scenarios. By enabling the production of high-fidelity avatars at a lower cost, our method provides more general way to create personalized avatar in AR/VR applications, thereby fostering more expressive self-representation in the metaverse.","텍스처와 노멀맵을 실시간으로 누적 업데이트하는 새로운 아바타 신체 생성 방법을 제시합니다. 증강 현실(AR) 및 가상 현실(VR) 환경 모두에서 보다 사실적인 사용자 신원을 캡처하는 상세한 3D 인간 모델을 생성하기 위해 여러 이미지 또는 비디오가 광범위하게 채택되었습니다. 그러나 이 접근 방식은 복잡한 카메라 설정과 광범위한 계산 리소스가 필요하기 때문에 시공간 비용이 더 높습니다. 개인화된 아바타 신체의 가벼운 재구성을 위해 우리는 널리 수용되는 3D 파라메트릭 신체 모델인 SMPL-X를 생성하기 위해 단일 RGBD 카메라를 사용하여 텍스처와 일반 값을 점진적으로 캡처하는 시스템을 설계합니다. 정량적으로 우리 시스템은 실시간 성능을 유지하면서 최첨단 방법에 필적하는 재구성 품질을 제공합니다. 또한, 사용자 연구는 실시간 아바타 생성의 이점과 다양한 협업 시나리오에서의 적용 가능성을 보여줍니다. 저렴한 비용으로 고품질 아바타를 제작할 수 있게 함으로써 우리의 방법은 AR/VR 애플리케이션에서 개인화된 아바타를 만드는 보다 일반적인 방법을 제공하여 메타버스에서 보다 표현력 있는 자기 표현을 촉진합니다.",https://doi.org/10.1109/ISMAR59233.2023.00023,Rendering & Visualization,Optical / Display Technology,Quantitative Experiment,Algorithm / Method
378,2023,Real-time Retargeting of Deictic Motion to Virtual Avatars for Augmented Reality Telepresence,증강 현실 텔레프레즌스를 위한 가상 아바타에 대한 실제 모션의 실시간 리타겟팅,"Avatar-mediated augmented reality telepresence aims to enable distant users to collaborate remotely through avatars. When two spaces involved in telepresence are dissimilar, with different object sizes and arrangements, the avatar movement must be adjusted to convey the user’s intention rather than directly following their motion, which poses a significant challenge. In this paper, we propose a novel neural network-based framework for real-time retargeting of users’ deictic motions (pointing at and touching objects) to virtual avatars in dissimilar environments. Our framework translates the user’s deictic motion, acquired from a sparse set of tracking signals, to the virtual avatar’s deictic motion for a corresponding remote object in real-time. One of the main features of our framework is that a single trained network can generate natural deictic motions for various sizes of users. To this end, our network includes two sub-networks: AngleNet and MotionNet. AngleNet maps the angular state of the user’s motion into a latent representation, which is subsequently converted by MotionNet into the avatar’s pose, considering the user’s scale. We validate the effectiveness of our method in terms of deictic intention preservation and movement naturalness through quantitative comparison with alternative approaches. Additionally, we demonstrate the utility of our approach through several AR telepresence scenarios.","아바타를 통한 증강 현실 텔레프레즌스의 목표는 멀리 떨어져 있는 사용자가 아바타를 통해 원격으로 협업할 수 있도록 하는 것입니다. 텔레프레즌스와 관련된 두 공간이 서로 다르고 물체의 크기와 배열이 서로 다른 경우, 아바타의 움직임을 직접 따르기보다는 사용자의 의도를 전달하도록 아바타의 움직임을 조정해야 하는데 이는 중요한 과제입니다. 본 논문에서는 서로 다른 환경에서 가상 아바타에 대한 사용자의 동작(물체를 가리키거나 만지는 동작)을 실시간으로 리타겟팅하기 위한 새로운 신경망 기반 프레임워크를 제안합니다. 우리의 프레임워크는 희박한 추적 신호 세트에서 얻은 사용자의 직시적 모션을 해당 원격 개체에 대한 가상 아바타의 직시적 모션으로 실시간 변환합니다. 우리 프레임워크의 주요 기능 중 하나는 훈련된 단일 네트워크가 다양한 규모의 사용자를 위해 자연스러운 직설적 모션을 생성할 수 있다는 것입니다. 이를 위해 우리 네트워크에는 AngleNet과 MotionNet이라는 두 개의 하위 네트워크가 포함되어 있습니다. AngleNet은 사용자 모션의 각도 상태를 잠재 표현으로 매핑한 후 사용자의 규모를 고려하여 MotionNet에 의해 아바타의 포즈로 변환됩니다. 우리는 대체 접근법과의 정량적 비교를 통해 서술적 의도 보존 및 움직임 자연성 측면에서 우리 방법의 효율성을 검증합니다. 또한 여러 AR 텔레프레즌스 시나리오를 통해 우리 접근 방식의 유용성을 보여줍니다.",https://doi.org/10.1109/ISMAR59233.2023.00104,Interaction & Input; Perception & Cognition,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method
379,2023,Reality Distortion Room: A Study of User Locomotion Responses to Spatial Augmented Reality Effects,현실 왜곡실: 공간 증강 현실 효과에 대한 사용자 운동 반응 연구,"Reality Distortion Room (RDR) is a proof-of-concept augmented reality system using projection mapping and unencumbered interaction with the Microsoft RoomAlive system to study a user’s locomotive response to visual effects that seemingly transform the physical room the user is in. This study presents five effects that augment the appearance of a physical room to subtly encourage user motion. Our experiment demonstrates users’ reactions to the different distortion and augmentation effects in a standard living room, with the distortion effects projected as wall grids, furniture holograms, and small particles in the air. The augmented living room can give the impression of becoming elongated, wrapped, shifted, elevated, and enlarged. The study results support the implementation of AR experiences in limited physical spaces by providing an initial understanding of how users can be subtly encouraged to move throughout a room.","RDR(Reality Distortion Room)은 프로젝션 매핑과 Microsoft RoomAlive 시스템과의 자유로운 상호 작용을 사용하여 사용자가 있는 물리적 공간을 겉보기에 변화시키는 시각적 효과에 대한 사용자의 기관차 반응을 연구하는 개념 증명 증강 현실 시스템입니다. 이 연구에서는 물리적 공간의 외관을 강화하여 사용자 동작을 미묘하게 장려하는 5가지 효과를 제시합니다. 우리의 실험은 벽 격자, 가구 홀로그램 및 공기 중의 작은 입자로 투영되는 왜곡 효과를 통해 표준 거실의 다양한 왜곡 및 확대 효과에 대한 사용자의 반응을 보여줍니다. 증강된 거실은 늘어나거나, 감싸지거나, 이동하거나, 높아지거나, 넓어지는 듯한 느낌을 줄 수 있습니다. 연구 결과는 사용자가 방 전체를 이동하도록 미묘하게 유도할 수 있는 방법에 대한 초기 이해를 제공함으로써 제한된 물리적 공간에서 AR 경험의 구현을 지원합니다.",https://doi.org/10.1109/ISMAR59233.2023.00137,Interaction & Input,Redirected Walking / Locomotion,Other,System / Framework
380,2023,Remote Monitoring and Teleoperation of Autonomous Vehicles - Is Virtual Reality an Option?,자율 주행 차량의 원격 모니터링 및 원격 조작 - 가상 현실이 선택 사항입니까?,"While the promise of autonomous vehicles has led to significant scientific and industrial progress, fully automated, SAE level 5 conform cars will likely not see mass adoption anytime soon. Instead, in many applications, human supervision, such as remote monitoring and teleoperation, will be required for the foreseeable future. While Virtual Reality (VR) has been proposed as one potential interface for teleoperation, its benefits and drawbacks over physical monitoring and teleoperation solutions have not been thoroughly investigated. To this end, we contribute three user studies, comparing and quantifying the performance of and subjective feedback for a VR-based system with an existing monitoring and teleoperation system, which is in industrial use today. Through these three user studies, we contribute to a better understanding of future virtual monitoring and teleoperation solutions for autonomous vehicles. The results of our first user study (n= 16) indicate that a VR interface replicating the physical interface does not outperform the physical interface. It also quantifies the negative effects that combined monitoring and teleoperating tasks have on users irrespective of the interface being used. The results of the second user study (n= 24) indicate that the perceptual and ergonomic issues caused by VR outweigh its benefits, like better concentration through isolation. The third follow-up user study (n= 24) specifically targeted the perceptual and ergonomic issues of VR; the subjective feedback of this study indicates that newer-generation VR headsets have the potential to catch up with the current physical displays.","자율주행차에 대한 약속이 상당한 과학적, 산업적 발전을 가져왔지만, 완전 자동화된 SAE 레벨 5 준수 자동차는 조만간 대량 채택을 볼 수 없을 것 같습니다. 대신, 많은 응용 분야에서 가까운 미래에는 원격 모니터링 및 원격 조작과 같은 인간 감독이 필요할 것입니다. 가상 현실(VR)은 원격 조작을 위한 하나의 잠재적인 인터페이스로 제안되었지만 물리적 모니터링 및 원격 조작 솔루션에 비해 장점과 단점은 철저히 조사되지 않았습니다. 이를 위해 우리는 오늘날 산업용으로 사용되는 기존 모니터링 및 원격 조작 시스템과 VR 기반 시스템의 성능 및 주관적인 피드백을 비교하고 정량화하는 세 가지 사용자 연구를 제공합니다. 이 세 가지 사용자 연구를 통해 우리는 자율주행차를 위한 미래의 가상 모니터링 및 원격조작 솔루션에 대한 더 나은 이해에 기여합니다. 첫 번째 사용자 연구 결과(n= 16)는 물리적 인터페이스를 복제하는 VR 인터페이스가 물리적 인터페이스보다 성능이 뛰어나지 않음을 나타냅니다. 또한 사용되는 인터페이스에 관계없이 모니터링과 원격 조작 작업이 결합되어 사용자에게 미치는 부정적인 영향을 수량화합니다. 두 번째 사용자 연구(n= 24)의 결과는 VR로 인해 발생하는 지각 및 인체공학적 문제가 격리를 통한 집중력 향상과 같은 이점보다 더 크다는 것을 나타냅니다. 세 번째 후속 사용자 연구(n= 24)는 특히 VR의 지각적, 인체공학적 문제를 대상으로 했습니다. 이 연구의 주관적인 피드백은 차세대 VR 헤드셋이 현재의 물리적 디스플레이를 따라잡을 수 있는 잠재력을 가지고 있음을 나타냅니다.",https://doi.org/10.1109/ISMAR59233.2023.00061,Interaction & Input,Optical / Display Technology,User Study,User Study / Empirical Findings
381,2023,RenderFusion: Balancing Local and Remote Rendering for Interactive 3D Scenes,RenderFusion: 대화형 3D 장면을 위한 로컬 및 원격 렌더링 균형 조정,"Many modern-day XR devices (e.g. mobile headsets, phones, etc.) lack the computing resources required to render complex 3D scenes in real-time. Typically, to render a high-resolution scene on a lightweight XR device, 3D designers arduously decimate and fine-tune the objects. As an alternative, remote rendering systems can utilize powerful nearby servers to stream rendering results to a client. While this is a promising solution, it can introduce a variety of latency and reliability issues, especially under variable network conditions. In this paper, we present a distributed rendering system that combines both remote rendering and on-device, “local” rendering to add robustness to network fluctuations and device workloads. To maximize user QoE, our approach dynamically swaps an object’s rendering medium, adjusting for client workload, low frame rates, and several perceptual characteristics. To model these characteristics, we perform a study under simulated conditions to measure how users perceive latency and complexity differences between objects in a scene. Using the results of the study, we then provide an algorithm for choosing the optimal object rendering medium, based on rendering complexity as well as network and latency models, ensuring that a target frame rate will be met. Finally, we evaluate this algorithm on a prototype implementation that can provide cross-platform split rendering using web technologies.","많은 최신 XR 장치(예: 모바일 헤드셋, 휴대폰 등)에는 복잡한 3D 장면을 실시간으로 렌더링하는 데 필요한 컴퓨팅 리소스가 부족합니다. 일반적으로 경량 XR 장치에서 고해상도 장면을 렌더링하기 위해 3D 디자이너는 개체를 힘들게 제거하고 미세 조정합니다. 대안으로, 원격 렌더링 시스템은 근처에 있는 강력한 서버를 활용하여 렌더링 결과를 클라이언트에 스트리밍할 수 있습니다. 이는 유망한 솔루션이지만, 특히 가변적인 네트워크 조건에서는 다양한 대기 시간 및 안정성 문제가 발생할 수 있습니다. 본 논문에서는 원격 렌더링과 장치 내 ""로컬"" 렌더링을 결합하여 네트워크 변동 및 장치 작업 부하에 견고성을 추가하는 분산 렌더링 시스템을 제시합니다. 사용자 QoE를 극대화하기 위해 우리의 접근 방식은 객체의 렌더링 매체를 동적으로 교환하여 클라이언트 작업량, 낮은 프레임 속도 및 여러 가지 지각 특성을 조정합니다. 이러한 특성을 모델링하기 위해 우리는 시뮬레이션된 조건에서 연구를 수행하여 사용자가 장면 내 개체 간의 지연 시간과 복잡성 차이를 어떻게 인식하는지 측정합니다. 그런 다음 연구 결과를 사용하여 렌더링 복잡성과 네트워크 및 대기 시간 모델을 기반으로 최적의 객체 렌더링 매체를 선택하기 위한 알고리즘을 제공하여 목표 프레임 속도가 충족되도록 합니다. 마지막으로 웹 기술을 사용하여 크로스 플랫폼 분할 렌더링을 제공할 수 있는 프로토타입 구현에서 이 알고리즘을 평가합니다.",https://doi.org/10.1109/ISMAR59233.2023.00046,Rendering & Visualization,Cloud / Edge Computing,Quantitative Experiment,Algorithm / Method; System / Framework
382,2023,Scene-independent Localization by Learning Residual Coordinate Map with Cascaded Localizers,캐스케이드 로컬라이저를 사용한 잔여 좌표 맵 학습을 통한 장면 독립적 위치화,"Visual localization plays an essential role in a variety of different fields. The indirect learning based method obtains an excellent performance, but it requests a training process in the target scene before the localization. To achieve deep scene-independent localization, we start by proposing the representation called residual coordinate map between a pair of images. Based on the structure, we put forward a network called SILocNet with the proposed residual coordinate map as the output. The network consists of feature extraction, multi-level feature fusion and transformer based coordinate decoder. Moreover, considering the dynamic scene, we introduce an additional segmentation branch that distinguishes fixed and dynamic parts to promote network perception. With SILocNet in place, a cascaded localizer design is presented for reducing the accumulative error. Meanwhile, the simple mathematical analysis behind the cascaded localizers is also provided. To verify how well our algorithm could perform, we conduct experiments on static 7 Scenes, ScanNet and dynamic TUM RGB-D. In particular, we train the network on ScanNet and test it on 7 Scenes and TUM RGB-D to demonstrate the generality performance. All experiments demonstrate superior performance to other existing methods. Additionally, the effects of the cascaded localizer design, feature fusion, transformer based coordinate decoder and segmentation loss are also discussed.","시각적 위치 파악은 다양한 분야에서 필수적인 역할을 합니다. 간접 학습 기반 방법은 우수한 성능을 얻지만 위치 파악 전에 대상 장면에서 훈련 과정을 요청합니다. 장면 독립적인 심층 위치 파악을 위해 우리는 한 쌍의 이미지 사이에 잔여 좌표 맵이라는 표현을 제안하는 것부터 시작합니다. Based on the structure, we put forward a network called SILocNet with the proposed residual coordinate map as the output. 네트워크는 특징 추출, 다중 레벨 특징 융합 및 변환기 기반 좌표 디코더로 구성됩니다. 또한 동적 장면을 고려하여 고정 부분과 동적 부분을 구분하는 추가 분할 분기를 도입하여 네트워크 인식을 향상시킵니다. SILocNet을 사용하면 누적 오류를 줄이기 위한 계단식 로컬라이저 설계가 제공됩니다. 한편, 계단식 로컬라이저 뒤에 있는 간단한 수학적 분석도 제공됩니다. 우리 알고리즘이 얼마나 잘 수행될 수 있는지 확인하기 위해 정적 7 Scenes, ScanNet 및 동적 TUM RGB-D에 대한 실험을 수행합니다. 특히, ScanNet에서 네트워크를 훈련시키고 7개의 Scene과 TUM RGB-D에서 테스트하여 일반성 성능을 보여줍니다. 모든 실험은 기존의 다른 방법보다 우수한 성능을 보여줍니다. 또한 계단식 로컬라이저 설계, 기능 융합, 변환기 기반 좌표 디코더 및 분할 손실의 효과도 논의됩니다.",https://doi.org/10.1109/ISMAR59233.2023.00022,Education & Training,Deep Learning / Neural Networks; Computer Vision,Technical Evaluation,Algorithm / Method
383,2023,See or Hear? Exploring the Effect of Visual/Audio Hints and Gaze-assisted Instant Post-task Feedback for Visual Search Tasks in AR,보거나 듣습니까? AR의 시각적 검색 작업에 대한 시각적/음성 힌트 및 시선 지원 즉각적인 작업 후 피드백의 효과 탐색,"Augmented reality (AR) is emerging in visual search tasks for increasingly immersive interactions with virtual objects. We propose an AR approach providing visual and audio hints along with gaze-assisted instant post-task feedback for search tasks based on mobile head-mounted display (HMD). The target case was a book-searching task, in which we aimed to explore the effect of the hints together with the task feedback with two hypotheses. H1: Since visual and audio hints can positively affect AR search tasks, the combination outperforms the individuals. H2: The gaze-assisted instant post-task feedback can positively affect AR search tasks. The proof-of-concept was demonstrated by an AR app in HMD and a comprehensive user study (n=96) consisting of two sub-studies, Study I (n=48) without task feedback and Study II (n=48) with task feedback. Following quantitative and qualitative analysis, our results partially verified H1 and completely verified H2, enabling us to conclude that the synthesis of visual and audio hints conditionally improves the AR visual search task efficiency when coupled with task feedback.","증강 현실(AR)은 가상 객체와의 점점 더 몰입적인 상호 작용을 위한 시각적 검색 작업에서 등장하고 있습니다. 우리는 모바일 헤드마운트디스플레이(HMD)를 기반으로 검색 작업에 대한 시선 지원 즉각적인 작업 후 피드백과 함께 시각적 및 오디오 힌트를 제공하는 AR 접근 방식을 제안합니다. 대상 사례는 책 검색 작업으로, 두 가지 가설을 사용하여 작업 피드백과 함께 힌트의 효과를 탐색하는 것을 목표로 했습니다. H1: 시각적 및 청각적 힌트는 AR 검색 작업에 긍정적인 영향을 미칠 수 있으므로 조합이 개인보다 성능이 뛰어납니다. H2: 응시 지원 즉각적인 작업 후 피드백은 AR 검색 작업에 긍정적인 영향을 미칠 수 있습니다. 개념 증명은 HMD의 AR 앱과 2개의 하위 연구, 즉 작업 피드백이 없는 연구 I(n=48)과 작업 피드백이 있는 연구 II(n=48)로 구성된 포괄적인 사용자 연구(n=96)를 통해 시연되었습니다. 정량적 및 정성적 분석에 따라 우리의 결과는 H1을 부분적으로 검증하고 H2를 완전히 검증하여 시각적 및 청각적 힌트의 합성이 작업 피드백과 결합될 때 조건부로 AR 시각적 검색 작업 효율성을 향상시킨다는 결론을 내릴 수 있습니다.",https://doi.org/10.1109/ISMAR59233.2023.00128,Display & Optics; Interaction & Input,Optical / Display Technology,User Study,User Study / Empirical Findings
384,2023,"Self-Calibrating Dynamic Projection Mapping System for Dynamic, Deformable Surfaces with Jitter Correction and Occlusion Handling",지터 교정 및 폐색 처리 기능을 갖춘 역동적이고 변형 가능한 표면을 위한 자체 보정 동적 프로젝션 매핑 시스템,"Dynamic projection mapping (DPM) is becoming increasingly popular, enabling viewers to visualize information on moving and deformable surfaces. Examples include large data visualization on the moving walls of tents deployed in austere remote locations during emergency management or defense operations. A DPM system typically comprises a RGB-D camera and a projector. In this paper, we present the first fully functional DPM system that auto-calibrates (without any physical props like planar checkerboard or rigid 3D objects) and creates a comprehensible display in the presence of large and fast movements by managing jitter and occlusion by passing objects.Prior DPM systems need specific calibration props, manual inputs and in order to deliver sub-pixel calibration accuracy. Recalibration in the face of movement or change in system setup becomes a time consuming process where the calibration prop needs to be brought back. When rendering content using DPM, errors in calibration are exacerbated and the noise in the depth camera leads to jitter, making the projection unreadable or incomprehensible. Occlusion may disrupt operations completely by jumbling up even the unoccluded parts of the display.In this paper we propose key hardware-agnostic methods for DPM calibration and rendering to make DPM systems easily deployable, stable and legible. First, we present a novel projector-camera calibration that does not need synchronization of the devices and leverages the moving surface itself, a counter-intuitive proposition. We project ArUCo markers on the moving surface and use corresponding detected features of these markers in the RGB and depth camera over multiple frames to accurately estimate the intrinsics and extrinsics of both the projector and the RGB-D camera. Second, we present a DPM rendering method that uses Kalman filtering models to reduce jitter and predict the surface shape in the presence of short term occlusions by other static objects. This results in the first DPM system, to the best of our knowledge, that can auto-calibrate in minutes and can render high resolution content like high-resolution text or images comprehensible even in the presence of fast movements, deformations and occlusions. We compare and evaluate the accuracy with prior methods and analyze the effect of surface movement on the calibration accuracy.","동적 프로젝션 매핑(DPM)이 점점 인기를 얻고 있으며 시청자가 움직이고 변형 가능한 표면에 대한 정보를 시각화할 수 있습니다. 예를 들어 비상 관리 또는 방어 작전 중 열악한 원격 위치에 배치된 텐트의 움직이는 벽에 대한 대규모 데이터 시각화가 포함됩니다. DPM 시스템은 일반적으로 RGB-D 카메라와 프로젝터로 구성됩니다. 이 문서에서는 자동 보정(평면 체커보드 또는 견고한 3D 객체와 같은 물리적 소품 없이)을 수행하고 객체 통과에 따른 지터 및 폐색을 관리하여 크고 빠른 움직임이 있을 때 이해하기 쉬운 디스플레이를 생성하는 최초의 완전한 기능을 갖춘 DPM 시스템을 제시합니다. 이전 DPM 시스템에는 하위 픽셀 보정 정확도를 제공하기 위해 특정 보정 소품, 수동 입력이 필요합니다. 시스템 설정의 이동 또는 변경에 따른 재보정은 보정 소품을 다시 가져와야 하는 시간 소모적인 프로세스가 됩니다. DPM을 사용하여 콘텐츠를 렌더링할 때 보정 오류가 악화되고 깊이 카메라의 노이즈로 인해 지터가 발생하여 프로젝션을 읽을 수 없거나 이해할 수 없게 됩니다. 폐색은 디스플레이의 폐색되지 않은 부분까지 뒤죽박죽으로 만들어 작업을 완전히 방해할 수 있습니다. 이 문서에서는 DPM 시스템을 쉽게 배포하고 안정적이며 읽을 수 있도록 만들기 위해 DPM 보정 및 렌더링을 위한 하드웨어에 구애받지 않는 주요 방법을 제안합니다. 첫째, 우리는 장치의 동기화가 필요하지 않고 움직이는 표면 자체를 활용하는 반직관적인 제안인 새로운 프로젝터-카메라 교정을 제시합니다. 우리는 움직이는 표면에 ArUCo 마커를 투사하고 여러 프레임에 걸쳐 RGB 및 깊이 카메라에서 이러한 마커의 해당 감지 기능을 사용하여 프로젝터와 RGB-D 카메라 모두의 고유 및 외부 특성을 정확하게 추정합니다. 둘째, 칼만 필터링 모델을 사용하여 지터를 줄이고 다른 정적 객체에 의한 단기 폐색이 있는 경우 표면 모양을 예측하는 DPM 렌더링 방법을 제시합니다. This results in the first DPM system, to the best of our knowledge, that can auto-calibrate in minutes and can render high resolution content like high-resolution text or images comprehensible even in the presence of fast movements, deformations and occlusions. 우리는 이전 방법과 정확도를 비교 및 ​​평가하고 표면 움직임이 교정 정확도에 미치는 영향을 분석합니다.",https://doi.org/10.1109/ISMAR59233.2023.00044,Rendering & Visualization; Interaction & Input,Optical / Display Technology,Quantitative Experiment,Algorithm / Method; Hardware / Device
385,2023,Shopping in between Realities-Using an Augmented Virtuality Smartphone in a Virtual Supermarket,현실 사이에서의 쇼핑 - 가상 슈퍼마켓에서 증강된 가상 스마트폰을 사용하여,"In this project, the full spectrum provided by Milgram’s RealityVirtuality Continuum is utilized to enhance presence, usability, and interactions in an authentic Virtual Reality (VR) supermarket simulation used as a standardized evaluation platform for mHealth apps. We introduce a unique Unity-based modeling platform for supermarket environments and the option to design high-quality customized products. To achieve that, solutions are demonstrated by focusing on a recognizable replica of a discounter and utilizing Augmented Virtuality (AV) to include a physical smartphone in the virtual simulation. The user is able to manipulate the simulation from within the smartphone app through this versatile, highly usability-centered controller. To achieve reliable tracking of the smartphone screen, we propose a hybrid approach, which combines fiducial marker tracking with data acquired through a WiFi connection between the VR system and the smartphone. Furthermore, the AV concept is utilized to build scenarios for Mixed Reality (MR) use cases such as simulated AR to navigate to a chosen product in the market. After an initial pre-study with important insights to strengthen the platform, a broad user study involving the physical smartphone with a simulated AR scenario has been conducted. The goals with 30 participants were to evaluate spatial presence, involvement, experienced realism (IPQ), and usability of the system (SUS). Results showed ”Good” (SUS) usability and recorded data as well as the participants’ feedback brought important insights. We plan to release this unique VR supermarket platform to contribute to the science community and mHealth industry.","이 프로젝트에서는 Milgram의 RealityVirtuality Continuum이 제공하는 전체 스펙트럼을 활용하여 mHealth 앱의 표준화된 평가 플랫폼으로 사용되는 진정한 가상 현실(VR) 슈퍼마켓 시뮬레이션에서 존재감, 유용성 및 상호 작용을 향상시킵니다. 슈퍼마켓 환경을 위한 고유한 Unity 기반 모델링 플랫폼과 고품질 맞춤형 제품을 디자인할 수 있는 옵션을 소개합니다. 이를 달성하기 위해 인식 가능한 할인점 복제본에 초점을 맞추고 증강 가상(AV)을 활용하여 가상 시뮬레이션에 실제 스마트폰을 포함시키는 솔루션을 시연합니다. 사용자는 이 다재다능하고 유용성 중심의 컨트롤러를 통해 스마트폰 앱 내에서 시뮬레이션을 조작할 수 있습니다. 스마트폰 화면의 안정적인 추적을 달성하기 위해 VR 시스템과 스마트폰 간의 WiFi 연결을 통해 획득한 데이터와 기준 마커 추적을 결합하는 하이브리드 접근 방식을 제안합니다. 또한 AV 개념은 시뮬레이션된 AR과 같은 혼합 현실(MR) 사용 사례에 대한 시나리오를 구축하여 시장에서 선택한 제품을 탐색하는 데 활용됩니다. 플랫폼 강화를 위한 중요한 통찰력을 갖춘 초기 사전 연구 이후, 시뮬레이션된 AR 시나리오를 갖춘 실제 스마트폰과 관련된 광범위한 사용자 연구가 수행되었습니다. 30명의 참가자를 대상으로 한 목표는 공간적 존재감, 참여도, 경험적 현실성(IPQ) 및 시스템 유용성(SUS)을 평가하는 것이었습니다. 결과는 “Good”(SUS) 사용성을 보여주었고, 기록된 데이터와 참가자들의 피드백은 중요한 통찰력을 가져왔습니다. 우리는 과학 커뮤니티와 모바일 헬스 산업에 기여하기 위해 이 독특한 VR 슈퍼마켓 플랫폼을 출시할 계획입니다.",https://doi.org/10.1109/ISMAR59233.2023.00133,Tracking & Localization; Interaction & Input,Sensor Fusion,User Study; Questionnaire / Survey,System / Framework; Algorithm / Method
386,2023,SiTAR: Situated Trajectory Analysis for In-the-Wild Pose Error Estimation,SiTAR: 실제 자세 오류 추정을 위한 위치 궤적 분석,"Virtual content instability caused by device pose tracking error remains a prevalent issue in markerless augmented reality (AR), especially on smartphones and tablets. However, when examining environments which will host AR experiences, it is challenging to determine where those instability artifacts will occur; we rarely have access to ground truth pose to measure pose error, and even if pose error is available, traditional visualizations do not connect that data with the real environment, limiting their usefulness. To address these issues we present SiTAR (Situated Trajectory Analysis for Augmented Reality), the first situated trajectory analysis system for AR that incorporates estimates of pose tracking error. We start by developing the first uncertainty-based pose error estimation method for visual-inertial simultaneous localization and mapping (VI-SLAM), which allows us to obtain pose error estimates without ground truth; we achieve an average accuracy of up to 96.1% and an average FI score of up to 0.77 in our evaluations on four VI-SLAM datasets. Next, we present our SiTAR system, implemented for ARCore devices, combining a backend that supplies uncertainty-based pose error estimates with a frontend that generates situated trajectory visualizations. Finally, we evaluate the efficacy of SiTAR in realistic conditions by testing three visualization techniques in an in-the-wild study with 15 users and 13 diverse environments; this study reveals the impact both environment scale and the properties of surfaces present can have on user experience and task performance.","장치 자세 추적 오류로 인한 가상 콘텐츠 불안정성은 마커 없는 증강 현실(AR), 특히 스마트폰과 태블릿에서 널리 퍼져 있는 문제로 남아 있습니다. 그러나 AR 경험을 호스팅할 환경을 검사할 때 불안정성 아티팩트가 발생하는 위치를 결정하는 것은 어렵습니다. 포즈 오류를 측정하기 위해 Ground Truth 포즈에 액세스할 수 있는 경우가 거의 없으며, 포즈 오류가 사용 가능하더라도 기존 시각화는 해당 데이터를 실제 환경과 연결하지 않아 유용성이 제한됩니다. 이러한 문제를 해결하기 위해 우리는 포즈 추적 오류 추정을 통합하는 최초의 AR용 위치 궤적 분석 시스템인 SiTAR(증강 현실을 위한 위치 궤적 분석)을 제시합니다. 우리는 시각적 관성 동시 위치 파악 및 매핑(VI-SLAM)을 위한 최초의 불확실성 기반 포즈 오류 추정 방법을 개발하는 것부터 시작합니다. 이를 통해 지상 진실 없이 포즈 오류 추정치를 얻을 수 있습니다. 우리는 4개의 VI-SLAM 데이터 세트에 대한 평가에서 최대 96.1%의 평균 정확도와 최대 0.77의 평균 FI 점수를 달성했습니다. 다음으로, 불확실성 기반 자세 오류 추정치를 제공하는 백엔드와 위치 궤적 시각화를 생성하는 프런트엔드를 결합하여 ARCore 장치용으로 구현된 SiTAR 시스템을 제시합니다. 마지막으로 우리는 15명의 사용자와 13개의 다양한 환경을 대상으로 한 실제 연구에서 세 가지 시각화 기술을 테스트하여 현실적인 조건에서 SiTAR의 효능을 평가했습니다. 이 연구는 환경 규모와 현재 표면의 특성이 사용자 경험과 작업 성능에 미칠 수 있는 영향을 보여줍니다.",https://doi.org/10.1109/ISMAR59233.2023.00043,Tracking & Localization,SLAM / Spatial Mapping; Sensor Fusion,Technical Evaluation,Algorithm / Method; Dataset / Benchmark
387,2023,SimpleMapping: Real-Time Visual-Inertial Dense Mapping with Deep Multi-View Stereo,SimpleMapping: 심층 다중 뷰 스테레오를 사용한 실시간 시각적 관성 조밀 매핑,"We present a real-time visual-inertial dense mapping method capable of performing incremental 3D mesh reconstruction with high quality using only sequential monocular images and inertial measurement unit (IMU) readings. 6-DoF camera poses are estimated by a robust feature-based visual-inertial odometry (VIO), which also generates noisy sparse 3D map points as a by-product. We propose a sparse point aided multi-view stereo neural network (SPA-MVSNet) that can effectively leverage the informative but noisy sparse points from the VIO system. The sparse depth from VIO is firstly completed by a single-view depth completion network. This dense depth map, although naturally limited in accuracy, is then used as a prior to guide our MVS network in the cost volume generation and regularization for accurate dense depth prediction. Predicted depth maps of keyframe images by the MVS network are incrementally fused into a global map using TSDF-Fusion. We extensively evaluate both the proposed SPA-MVSNet and the entire dense mapping system on several public datasets as well as our own dataset, demonstrating the system’s impressive generalization capabilities and its ability to deliver high-quality 3D reconstruction online. Our proposed dense mapping system achieves a 39.7% improvement in F-score over existing systems when evaluated on the challenging scenarios of the EuRoC dataset.","순차 단안 이미지와 관성 측정 장치(IMU) 판독값만을 사용하여 고품질의 증분 3D 메시 재구성을 수행할 수 있는 실시간 시각적 관성 조밀 매핑 방법을 제시합니다. 6-DoF 카메라 포즈는 강력한 기능 기반 VIO(시각 관성 주행 거리 측정)에 의해 추정되며, 이는 부산물로 시끄러운 희소 3D 맵 포인트도 생성합니다. 우리는 VIO 시스템에서 유익하지만 잡음이 많은 희소 점을 효과적으로 활용할 수 있는 희소 점 지원 다중 뷰 스테레오 신경망(SPA-MVSNet)을 제안합니다. VIO의 희소 깊이는 먼저 단일 뷰 깊이 완성 네트워크에 의해 완성됩니다. 이 조밀한 깊이 맵은 정확도가 본질적으로 제한되어 있지만 정확한 조밀한 깊이 예측을 위한 비용 볼륨 생성 및 정규화에서 MVS 네트워크를 안내하기 위한 사전 지침으로 사용됩니다. MVS 네트워크에 의해 예측된 키프레임 이미지의 깊이 맵은 TSDF-Fusion을 사용하여 전역 맵으로 점진적으로 융합됩니다. 우리는 제안된 SPA-MVSNet과 여러 공개 데이터세트 및 자체 데이터세트에 대한 전체 밀도 매핑 시스템을 광범위하게 평가하여 시스템의 인상적인 일반화 기능과 고품질 3D 재구성을 온라인으로 제공하는 능력을 보여줍니다. 우리가 제안한 조밀한 매핑 시스템은 EuRoC 데이터 세트의 까다로운 시나리오에서 평가할 때 기존 시스템에 비해 F-점수가 39.7% 향상되었습니다.",https://doi.org/10.1109/ISMAR59233.2023.00042,Tracking & Localization; Rendering & Visualization,3D Reconstruction; Sensor Fusion,Quantitative Experiment,Algorithm / Method
388,2023,Smell of Fire Increases Behavioural Realism in Virtual Reality: A Case Study on a Recreated MGM Grand Hotel Fire,불 냄새로 가상 현실에서 행동 현실감 향상: MGM 그랜드 호텔 화재 재현 사례 연구,"Virtual reality allows creating highly immersive visual and auditory experiences, making users feel physically present in the environment. This makes it an ideal platform to simulate dangerous scenarios, including fire evacuation, and study human behaviour without exposing users to harmful elements. However, human perception of the surroundings is based on the integration of multiple sensory cues (visual, auditory, tactile, or/and olfactory) present in the environment. When some of the sensory stimuli are missing in the virtual experience, it can break the illusion of being there in the environment and could lead to actions that deviate from normal behaviour. In this work, we added an olfactory cue in a well-documented historic hotel fire scenario that was recreated in VR, and examined the effects of the olfactory cue on human behaviour. We conducted a between subject study on 40 naive participants. Our results show that the addition of the olfactory cue could increase behavioural realism. We found that 80% of the studied actions for the VR with olfactory cue condition matched the ones performed by the survivors. In comparison, only 40% of the participants’ actions for VR only condition were similar to the survivors.","가상 현실을 사용하면 몰입도가 높은 시각적, 청각적 경험을 만들어 사용자가 환경에 물리적으로 존재하는 듯한 느낌을 받을 수 있습니다. 이는 화재 대피를 포함한 위험한 시나리오를 시뮬레이션하고 사용자를 유해한 요소에 노출시키지 않고 인간 행동을 연구하는 데 이상적인 플랫폼입니다. 그러나 주변 환경에 대한 인간의 인식은 환경에 존재하는 여러 감각 단서(시각, 청각, 촉각 또는/및 후각)의 통합을 기반으로 합니다. 가상 경험에서 감각 자극 중 일부가 누락되면 해당 환경에 있다는 환상이 깨지고 정상적인 행동에서 벗어나는 행동으로 이어질 수 있습니다. 이 작업에서 우리는 VR로 재현된 잘 기록된 역사적인 호텔 화재 시나리오에 후각 신호를 추가하고 후각 신호가 인간 행동에 미치는 영향을 조사했습니다. 우리는 40명의 순진한 참가자를 대상으로 주제 간 연구를 실시했습니다. 우리의 결과는 후각 신호를 추가하면 행동 현실감을 높일 수 있음을 보여줍니다. 우리는 후각 신호 조건이 있는 VR에 대해 연구된 행동의 80%가 생존자가 수행한 행동과 일치한다는 것을 발견했습니다. 이에 비해 VR 전용 조건에 대한 참가자의 행동 중 40%만이 생존자와 유사했습니다.",https://doi.org/10.1109/ISMAR59233.2023.00097,Perception & Cognition,Sensor Fusion,User Study,System / Framework
389,2023,"Spaces to Think: A Comparison of Small, Large, and Immersive Displays for the Sensemaking Process","생각할 공간: 센스메이킹 과정을 위한 소형, 대형, 몰입형 디스플레이 비교","Analysts need to process large amounts of data in order to extract concepts, themes, and plans of action based upon their findings. Different display technologies offer varying levels of space and interaction methods that change the way users can process data using them. In a comparative study, we investigated how the use of single traditional monitor, a large, high-resolution two-dimensional monitor, and immersive three-dimensional space using the Immersive Space to Think approach impact the sensemaking process. We found that user satisfaction grows and frustration decreases as available space increases. We observed specific strategies users employ in the various conditions to assist with the processing of datasets. We also found an increased usage of spatial memory as space increased, which increases performance in artifact position recall tasks. In future systems supporting sensemaking, we recommend using display technologies that provide users with large amounts of space to organize information and analysis artifacts.","분석가는 자신이 발견한 내용을 기반으로 개념, 주제 및 실행 계획을 추출하기 위해 많은 양의 데이터를 처리해야 합니다. 다양한 디스플레이 기술은 사용자가 이를 사용하여 데이터를 처리할 수 있는 방식을 변화시키는 다양한 수준의 공간과 상호 작용 방법을 제공합니다. 비교 연구에서 우리는 단일 기존 모니터, 대형 고해상도 2차원 모니터, Immersive Space to Think 접근 방식을 사용한 몰입형 3차원 공간의 사용이 센스메이킹 프로세스에 어떤 영향을 미치는지 조사했습니다. 사용 가능한 공간이 늘어남에 따라 사용자 만족도는 높아지고 불만은 줄어드는 것으로 나타났습니다. 우리는 데이터 세트 처리를 돕기 위해 다양한 조건에서 사용자가 사용하는 특정 전략을 관찰했습니다. 또한 공간이 증가함에 따라 공간 메모리 사용량이 증가하여 아티팩트 위치 리콜 작업의 성능이 향상되는 것을 발견했습니다. 센스메이킹을 지원하는 미래 시스템에서는 사용자에게 정보를 정리하고 아티팩트를 분석할 수 있는 넓은 공간을 제공하는 디스플레이 기술을 사용하는 것이 좋습니다.",https://doi.org/10.1109/ISMAR59233.2023.00125,Interaction & Input,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method; User Study / Empirical Findings
390,2023,Specifying Volumes of Interest for Industrial Use Cases,산업용 사용 사례에 대한 관심 볼륨 지정,"Creating digital representations of industrial facilities presents substantial opportunities for the industry. The development of digital twins opens up many prospective applications for industrial plants, such as facilitating virtual training environments, streamlining change management, and providing real-time understanding of remote scenes. However, creating and maintaining these digital twins involves complex tasks, including scanning industrial plants and annotating their subsystems. Many use cases necessitate specifying a Volume of Interest (VoI) for further processing while presenting severe environmental challenges for segmentation quality and worker safety. Besides that, widespread adoption also depends on creating methods that are easy to use, even for workers with limited 3D interaction knowledge. Currently, no commonly implemented method fulfills all these diverse requirements. This paper introduces an approach for specifying a VoI in industrial scenarios. Our approach defines a volume by intersecting the projection of hand-drawn surroundings on a small number of pictures of the target volume, utilizing Augmented Reality and Voxel Carving. It can successfully handle various sizes of target volumes and delivers an appropriately detailed result. Applying qualitative discussions and a quantitative evaluation, we ensure our application meets all requirements posed by the scenarios. This simple interaction metaphor, tailored to specific use cases, can serve as a versatile pattern for various digital twin scenarios. It offers a valuable alternative to 3D primitive-based segmentation methods.","산업 시설의 디지털 표현을 만드는 것은 업계에 상당한 기회를 제공합니다. 디지털 트윈의 개발은 가상 교육 환경 촉진, 변경 관리 간소화, 원격 장면에 대한 실시간 이해 제공 등 산업 플랜트에 대한 다양한 응용 분야를 열어줍니다. 그러나 이러한 디지털 트윈을 생성하고 유지 관리하려면 산업 플랜트를 스캔하고 해당 하위 시스템에 주석을 추가하는 등 복잡한 작업이 필요합니다. 많은 사용 사례에서는 추가 처리를 위해 관심 볼륨(VoI)을 지정해야 하며 분할 품질 및 작업자 안전에 대한 심각한 환경 문제를 제시합니다. 그 외에도 3D 상호 작용 지식이 부족한 작업자라도 사용하기 쉬운 방법을 만드는 것이 널리 채택되는 데에도 달려 있습니다. 현재 일반적으로 구현되는 방법은 이러한 다양한 요구 사항을 모두 충족하지 않습니다. 본 문서에서는 산업 시나리오에서 VoI를 지정하는 접근 방식을 소개합니다. 우리의 접근 방식은 증강 현실과 복셀 조각(Voxel Carving)을 활용하여 대상 볼륨의 소수의 그림에 손으로 그린 ​​주변 환경의 투영을 교차시켜 볼륨을 정의합니다. 다양한 크기의 대상 볼륨을 성공적으로 처리할 수 있으며 적절하고 상세한 결과를 제공합니다. 정성적 논의와 정량적 평가를 적용하여 애플리케이션이 시나리오에 따른 모든 요구 사항을 충족하는지 확인합니다. 특정 사용 사례에 맞게 조정된 이 간단한 상호 작용 비유는 다양한 디지털 트윈 시나리오에 대한 다목적 패턴으로 사용될 수 있습니다. 이는 3D 기본 기반 분할 방법에 대한 귀중한 대안을 제공합니다.",https://doi.org/10.1109/ISMAR59233.2023.00092,Interaction & Input,Computer Vision,Quantitative Experiment,Algorithm / Method
391,2023,State-Aware Configuration Detection for Augmented Reality Step-by-Step Tutorials,증강 현실을 위한 상태 인식 구성 감지 단계별 튜토리얼,"Presenting tutorials in augmented reality is a compelling application area, but previous attempts have been limited to objects with only a small numbers of parts. Scaling augmented reality tutorials to complex assemblies of a large number of parts is difficult, because it requires automatically discriminating many similar-looking object configurations, which poses a challenge for current object detection techniques. In this paper, we seek to lift this limitation. Our approach is inspired by the observation that, even though the number of assembly steps may be large, their order is typically highly restricted: Some actions can only be performed after others. To leverage this observation, we enhance a state-of-the-art object detector to predict the current assembly state by conditioning on the previous one, and to learn the constraints on consecutive states. This learned ‘consecutive state prior’ helps the detector disambiguate configurations that are otherwise too similar in terms of visual appearance to be reliably discriminated. Via the state prior, the detector is also able to improve the estimated probabilities that a state detection is correct. We experimentally demonstrate that our technique enhances the detection accuracy for assembly sequences with a large number of steps and on a variety of use cases, including furniture, Lego and origami. Additionally, we demonstrate the use of our algorithm in an interactive augmented reality application.","증강 현실에서 튜토리얼을 제시하는 것은 매력적인 응용 분야이지만 이전 시도는 부품 수가 적은 객체로 제한되었습니다. 증강 현실 튜토리얼을 다수의 부품으로 구성된 복잡한 어셈블리로 확장하는 것은 어렵습니다. 왜냐하면 유사하게 보이는 많은 객체 구성을 자동으로 식별해야 하기 때문에 현재 객체 감지 기술에 대한 과제를 제기하기 때문입니다. 본 논문에서는 이러한 한계를 해소하고자 한다. 우리의 접근 방식은 조립 단계 수가 많더라도 순서가 일반적으로 매우 제한적이라는 관찰에서 영감을 얻었습니다. 일부 작업은 다른 작업 후에만 수행될 수 있습니다. 이 관찰을 활용하기 위해 최첨단 개체 감지기를 강화하여 이전 어셈블리를 조건으로 현재 어셈블리 상태를 예측하고 연속 상태에 대한 제약 조건을 학습합니다. 이렇게 학습된 '연속 상태 사전'은 탐지기가 시각적 외관 측면에서 너무 유사하여 안정적으로 식별할 수 없는 구성을 명확하게 하는 데 도움이 됩니다. 이전 상태를 통해 검출기는 상태 검출이 올바른 추정 확률을 향상시킬 수도 있습니다. 우리는 우리의 기술이 가구, 레고, 종이접기를 포함한 다양한 사용 사례와 많은 단계로 구성된 조립 시퀀스의 감지 정확도를 향상시킨다는 것을 실험적으로 보여줍니다. 또한 대화형 증강 현실 애플리케이션에서 알고리즘을 사용하는 방법을 보여줍니다.",https://doi.org/10.1109/ISMAR59233.2023.00030,Education & Training,Computer Vision,Technical Evaluation,Algorithm / Method
392,2023,Studying User Perceptible Misalignment in Simulated Dynamic Facial Projection Mapping,시뮬레이션된 동적 얼굴 프로젝션 매핑에서 사용자가 인지할 수 있는 정렬 불량 연구,"High-speed dynamic facial projection mapping (DFPM) is an advanced technology that aims to create perceptual changes in facial appearance by overlapping images based on facial position and shape. Compared to traditional monitor-based augmented reality systems, DFPM offers a higher level of immersion because users can directly observe digital content on their faces. However, DFPM suffers from misalignment issues owing to a slight temporal delay from sensing to projection, which reduces the level of immersion. To the best of our knowledge, no previous study has established the necessary latency requirements to avoid perceptible misalignment and achieve an immersive experience. Furthermore, conventional DFPM works followed latency requirements that were not reported for the DFPM scenario. Therefore, this study measured the latency that provided a just-noticeable difference (JND) in DFPM under different facial motion conditions, using the weighted up-down two-alternative forced-choice method. The results showed that user-perceptible misalignment was influenced by facial motion types and their velocities. Additionally, it was found that an average latency of 3.87 ms was necessary to avoid perceptible misalignment in the DFPM system when the translation speed was 0.5 m/s, which contradicts the commonly held belief regarding the required latency threshold.","DFPM(고속 동적 얼굴 프로젝션 매핑)은 얼굴 위치와 모양을 기반으로 이미지를 겹쳐서 얼굴 모양의 지각 변화를 만드는 것을 목표로 하는 첨단 기술입니다. 기존 모니터 기반 증강현실 시스템에 비해 DFPM은 사용자가 디지털 콘텐츠를 얼굴에서 직접 관찰할 수 있어 더 높은 몰입감을 제공한다. 그러나 DFPM은 감지에서 투사까지 약간의 시간적 지연으로 인해 정렬 불량 문제가 발생하여 몰입도가 떨어집니다. 우리가 아는 한, 인지 가능한 정렬 오류를 방지하고 몰입형 경험을 달성하는 데 필요한 대기 시간 요구 사항을 확립한 이전 연구는 없습니다. 또한 기존 DFPM 작업은 DFPM 시나리오에 대해 보고되지 않은 대기 시간 요구 사항을 따랐습니다. 따라서 본 연구에서는 가중치 상향-하위 2대 대안 강제 선택 방법을 사용하여 다양한 얼굴 동작 조건에서 DFPM에서 JND(Just-Noticeable Difference)를 제공하는 지연 시간을 측정했습니다. 결과는 사용자가 인지할 수 있는 정렬 불량이 얼굴 동작 유형과 속도에 영향을 받는 것으로 나타났습니다. 또한 변환 속도가 0.5m/s일 때 DFPM 시스템에서 감지할 수 있는 정렬 오류를 방지하려면 3.87ms의 평균 대기 시간이 필요한 것으로 나타났습니다. 이는 필요한 대기 시간 임계값에 대해 일반적으로 받아들여지는 믿음과 모순됩니다.",https://doi.org/10.1109/ISMAR59233.2023.00064,Interaction & Input,Sensor Fusion,Quantitative Experiment,System / Framework
393,2023,Supporting Co-Presence in Populated Virtual Environments by Actor Takeover of Animated Characters,애니메이션 캐릭터의 배우 인수를 통해 인구가 많은 가상 환경에서 공존 지원,"Online social virtual worlds are now becoming widely available on consumer devices including virtual reality headsets. One goal of a virtual world could be to give a user an experience of a crowded environment with many virtual humans. However, gathering enough personnel to control the necessary number of avatars for creating a realistic scene is usually difficult. Additionally, current technology is not capable of fully simulating avatars with behaviours, especially when interaction with users is required. In this paper, we develop a system that enables an actor to take over control of one of a set of avatars. We built an immersive interface that allows an actor to select an avatar to take over and then segue into the currently playing animation. By allowing one person to take control of multiple avatars, we can enhance the plausibility of environments inhabited by simulated characters. In an experiment, we show that in a cafe scenario, one actor can take over the roles of a barista and two customers. Experiment participants reported experiencing the scene as if it were populated by more than one actor. This system and experiment demonstrate the feasibility of one actor controlling multiple avatars sequentially, thus enhancing users’ feelings of being in a populated environment.","이제 온라인 소셜 가상 세계는 가상 현실 헤드셋을 포함한 소비자 장치에서 널리 사용 가능해지고 있습니다. 가상 세계의 한 가지 목표는 사용자에게 많은 가상 인간이 있는 혼잡한 환경 경험을 제공하는 것입니다. 그러나 사실적인 장면을 만들기 위해 필요한 아바타 수를 제어할 만큼 충분한 인력을 모으는 것은 일반적으로 어렵습니다. 또한, 현재 기술로는 특히 사용자와의 상호작용이 필요할 때 행동으로 아바타를 완벽하게 시뮬레이션할 수 없습니다. 본 논문에서는 배우가 아바타 세트 중 하나를 제어할 수 있는 시스템을 개발합니다. 우리는 배우가 인계할 아바타를 선택한 다음 현재 재생 중인 애니메이션으로 이어갈 수 있는 몰입형 인터페이스를 구축했습니다. 한 사람이 여러 아바타를 제어할 수 있도록 함으로써 시뮬레이션된 캐릭터가 거주하는 환경의 타당성을 향상시킬 수 있습니다. 실험에서 우리는 카페 시나리오에서 한 명의 배우가 바리스타와 두 명의 고객의 역할을 맡을 수 있음을 보여줍니다. 실험 참가자들은 마치 한 명 이상의 배우가 등장하는 것처럼 그 장면을 경험했다고 보고했습니다. 이 시스템과 실험은 한 배우가 여러 아바타를 순차적으로 제어하여 인구가 많은 환경에 있는 사용자의 느낌을 향상시키는 가능성을 보여줍니다.",https://doi.org/10.1109/ISMAR59233.2023.00110,Perception & Cognition; Interaction & Input,Sensor Fusion,Simulation,Hardware / Device
394,2023,TENETvr: Comprehensible Temporal Teleportation in Time-Varying Virtual Environments,TENETvr: 시변 가상 환경에서 이해 가능한 시간 순간 이동,"The iterative design process of virtual environments commonly generates a history of revisions that each represent the state of the scene at a different point in time. Browsing through these discrete time points by common temporal navigation interfaces like time sliders, however, can be inaccurate and lead to an uncomfortably high number of visual changes in a short time. In this paper, we therefore present a novel technique called TENETvr (Temporal Exploration and Navigation in virtual Environments via Teleportation) that allows for efficient teleportation-based travel to time points in which a particular object of interest changed. Unlike previous systems, we suggest that changes affecting other objects in the same time span should also be mediated before the teleport to improve predictability. We therefore propose visualizations for nine different types of additions, property changes, and deletions. In a formal user study with 20 participants, we confirmed that this addition leads to significantly more efficient change detection, lower task loads, and higher usability ratings, therefore reducing temporal disorientation.","가상 환경의 반복적인 설계 프로세스는 일반적으로 각기 다른 시점의 장면 상태를 나타내는 개정 내역을 생성합니다. 그러나 시간 슬라이더와 같은 일반적인 시간 탐색 인터페이스를 통해 이러한 개별 시점을 탐색하는 것은 부정확할 수 있으며 짧은 시간에 불편할 정도로 많은 시각적 변화가 발생할 수 있습니다. 따라서 본 논문에서는 특정 관심 객체가 변경된 시점으로 효율적인 순간 이동 기반 이동을 가능하게 하는 TENETvr(Temporal Exploration and Navigation in virtual Environments via Teleportation)이라는 새로운 기술을 제시합니다. 이전 시스템과 달리 예측 가능성을 높이기 위해 동일한 기간에 다른 개체에 영향을 미치는 변경 사항도 순간 이동 전에 중재되어야 한다고 제안합니다. 따라서 우리는 9가지 유형의 추가, 속성 변경 및 삭제에 대한 시각화를 제안합니다. 20명의 참가자를 대상으로 한 공식 사용자 연구에서 우리는 이러한 추가가 훨씬 더 효율적인 변경 감지, 낮은 작업 부하 및 높은 유용성 등급으로 이어져 시간적 방향 감각 상실을 줄이는 것으로 확인했습니다.",https://doi.org/10.1109/ISMAR59233.2023.00108,Rendering & Visualization,Optical / Display Technology,User Study,Algorithm / Method
395,2023,The Effect of Visual and Auditory Modality Mismatching between Distraction and Warning on Pedestrian Street Crossing Behavior,주의 분산과 경고 사이의 시각 및 청각 양식 불일치가 보행자 횡단 행동에 미치는 영향,"Augmented reality (AR) headsets could provide useful information to users, but they may also be a source of distraction. Previous works have explored using AR to enhance pedestrian safety by providing real-time warnings, but there has been little research on the impact of modality matching between distractions and warnings on pedestrian street crossing behaviour. We conducted a VR experiment using a within-subjects 2-by-2 design (N=24) with four conditions: (auditory distraction, visual distraction) $\times$ (auditory warning, visual warning). When experienced conditions with mismatched modalities, participants exhibited more cautious street crossing behaviours, such as reduced walking speed, and increased scan range after receiving the warning, and significantly faster reaction times to the incoming vehicle. The participants also expressed a preference for warnings to be presented in a modality different from the distraction. Our findings suggest that in the context of utilizing AR for pedestrian road safety, future AR interfaces should incorporate a warning modality that differs from the one causing distraction.","증강 현실(AR) 헤드셋은 사용자에게 유용한 정보를 제공할 수 있지만 방해 요소가 될 수도 있습니다. 이전 연구에서는 실시간 경고를 제공하여 보행자 안전을 강화하기 위해 AR을 사용하는 방법을 모색했지만 보행자의 횡단 행동에 대한 산만함과 경고 사이의 양식 일치에 대한 연구는 거의 없었습니다. 우리는 (청각 산만, 시각적 산만) $\times$ (청각 경고, 시각적 경고)의 4가지 조건으로 피험자 내 2x2 디자인(N=24)을 사용하여 VR 실험을 수행했습니다. 일치하지 않는 양식의 조건을 경험했을 때 참가자들은 경고를 받은 후 보행 속도 감소, 스캔 범위 증가, 다가오는 차량에 대한 반응 시간이 훨씬 빨라지는 등 보다 신중한 도로 횡단 행동을 보였습니다. 참가자들은 또한 주의가 산만해지는 것과는 다른 방식으로 경고가 표시되는 것을 선호한다고 표현했습니다. 우리의 연구 결과는 보행자의 도로 안전을 위해 AR을 활용하는 맥락에서 미래의 AR 인터페이스에는 산만함을 유발하는 것과는 다른 경고 양식이 포함되어야 함을 시사합니다.",https://doi.org/10.1109/ISMAR59233.2023.00121,Audio & Sound,Other,User Study,User Study / Empirical Findings
396,2023,The Effect of an Exergame on the Shadow Play Skill Based on Muscle Memory for Young Female Participants: The Case of Forehand Drive in Table Tennis,엑서게임이 젊은 여성 참가자의 근육 기억 기반 그림자 놀이 능력에 미치는 영향: 탁구 포핸드 드라이브 사례를 중심으로,"Learning and practicing table tennis with traditional methods is a long, tedious process and may even lead to the internalization of incorrect techniques if not supervised by a coach. To overcome these issues, the presented study proposes an exergame with the aim of enhancing young female novice players’ performance by boosting muscle memory, making practice more interesting, and decreasing the probability of faulty training. Specifically, we propose an exergame based on skeleton tracking and a virtual avatar to support correct shadow practice to learn forehand drive technique without the presence of a coach. We recruited 44 schoolgirls aged between 8 and 12 years without a background in playing table tennis and divided them into control and experimental groups. We examined their stroke skills (via the Mott-Lockhart test) and the error coefficient of their forehand drives (using a ball machine) in the pre-test, post-test, and follow-up tests (10 days after the post-test). Our results showed that the experimental group had progress in the short and long term, while the control group had an improvement only in the short term. Further, the scale of improvement in the experimental group was significantly higher than in the control group. Given that the early stages of learning, particularly in girls children, are important in the internalization of individual skills in would-be athletes, this method could support promoting correct training for young females.","전통적인 방법으로 탁구를 배우고 연습하는 것은 길고 지루한 과정이며, 코치의 감독 없이는 잘못된 기술을 내면화하는 결과를 낳을 수도 있습니다. 이러한 문제점을 극복하기 위해 본 연구에서는 젊은 여성 초보 선수들의 근육 기억력 향상을 통해 경기력을 향상시키고, 연습을 더욱 흥미롭게 하며, 잘못된 훈련 확률을 낮추는 것을 목표로 하는 엑서게임을 제안한다. 구체적으로 코치 없이도 포핸드 드라이브 기술을 학습할 수 있도록 올바른 그림자 연습을 지원하는 스켈레톤 트래킹과 가상 아바타 기반의 엑서게임을 제안한다. 우리는 탁구에 대한 배경 지식이 없는 8세에서 12세 사이의 여학생 44명을 모집하여 통제 그룹과 실험 그룹으로 나누었습니다. 사전 테스트, 사후 테스트 및 후속 테스트(사후 테스트 10일 후)를 통해 스트로크 기술(Mott-Lockhart 테스트를 통해)과 포핸드 드라이브(볼 머신 사용)의 오류 계수를 조사했습니다. 우리의 결과는 실험 그룹은 단기 및 장기적으로 개선이 있었던 반면, 통제 그룹은 단기적으로만 개선이 있었던 것으로 나타났습니다. 또한, 실험군의 개선 정도는 대조군에 비해 유의하게 높았다. 학습의 초기 단계, 특히 여아의 경우 운동선수가 될 개인의 기술을 내면화하는 데 중요하다는 점을 고려하면 이 방법은 젊은 여성을 위한 올바른 훈련을 촉진하는 데 도움이 될 수 있습니다.",https://doi.org/10.1109/ISMAR59233.2023.00132,Education & Training; Perception & Cognition,Other,User Study,Algorithm / Method
397,2023,The Work Avatar Face-Off: Knowledge Worker Preferences for Realism in Meetings,작업 아바타 대결: 회의의 현실성에 대한 지식 근로자 선호도,"While avatars have grown in popularity in social settings, their use in the workplace is still debatable. We conducted a large-scale survey to evaluate knowledge worker sentiment towards avatars, particularly the effects of realism on their acceptability for work meetings. Our survey of 2509 knowledge workers from multiple countries rated five avatar styles for use by managers, known colleagues and unknown colleagues. In all scenarios, participants favored higher realism, but fully realistic avatars were sometimes perceived as uncanny. Less realistic avatars were rated worse when interacting with an unknown colleague or manager, as compared to a known colleague. Avatar acceptability varied by country, with participants from the United States and South Korea rating avatars more favorably. We supplemented our quantitative findings with a thematic analysis of open-ended responses to provide a comprehensive understanding of factors influencing work avatar choices. In conclusion, our results show that realism had a significant positive correlation with acceptability. Non-realistic avatars were seen as fun and playful, but only suitable for occasional use.","아바타는 소셜 환경에서 인기가 높아졌지만 직장에서의 사용에 대해서는 여전히 논란의 여지가 있습니다. 우리는 아바타에 대한 지식 근로자의 감정, 특히 업무 회의에 대한 수용 가능성에 대한 사실주의의 영향을 평가하기 위해 대규모 설문조사를 실시했습니다. 여러 국가의 지식 근로자 2,509명을 대상으로 한 설문 조사에서는 관리자, 알려진 동료, 알려지지 않은 동료가 사용할 수 있는 5가지 아바타 스타일을 평가했습니다. 모든 시나리오에서 참가자들은 더 높은 현실성을 선호했지만 완전히 현실적인 아바타는 때때로 기괴한 것으로 인식되었습니다. 덜 현실적인 아바타는 알려진 동료에 비해 알려지지 않은 동료나 관리자와 상호 작용할 때 더 나쁜 평가를 받았습니다. 아바타 수용도는 국가별로 다양했으며, 미국과 한국의 참가자들은 아바타를 더 호의적으로 평가했습니다. 우리는 작업 아바타 선택에 영향을 미치는 요인에 대한 포괄적인 이해를 제공하기 위해 개방형 응답에 대한 주제별 분석을 통해 정량적 조사 결과를 보완했습니다. 결론적으로, 우리의 결과는 사실주의가 수용성과 유의미한 양의 상관관계를 가지고 있음을 보여줍니다. 비현실적인 아바타는 재미 있고 유쾌한 것으로 보였지만 가끔 사용하는 데에만 적합했습니다.",https://doi.org/10.1109/ISMAR59233.2023.00112,Perception & Cognition,Optical / Display Technology,User Study,User Study / Empirical Findings
398,2023,TouchRay: Towards Low-effort Object Selection at Any Distance in DeskVR,TouchRay: DeskVR의 어떤 거리에서도 쉽게 개체를 선택할 수 있도록,"DeskVR allows users to experience Virtual Reality (VR) while sitting at a desk without requiring extensive movements. This makes it better suited for professional work environments where productivity over extended periods is essential. However, tasks that typically resort to mid-air gestures might not be suitable for DeskVR. In this paper, we focus on the fundamental task of object selection. We present TouchRay, an object selection technique conceived specifically for DeskVR that enables users to select objects at any distance while resting their hands on the desk. It also allows selecting objects’ sub-components by traversing their corresponding hierarchical trees. We conducted a user evaluation comparing TouchRay against state-of-the-art techniques targeted at traditional VR. Results revealed that participants could successfully select objects in different settings, with consistent times and on par with the baseline techniques in complex tasks, without requiring mid-air gestures.",DeskVR을 사용하면 사용자는 큰 움직임 없이 책상에 앉아 가상 현실(VR)을 경험할 수 있습니다. 따라서 장기간에 걸친 생산성이 필수적인 전문적인 작업 환경에 더 적합합니다. 그러나 일반적으로 공중 제스처에 의존하는 작업은 DeskVR에 적합하지 않을 수 있습니다. 본 논문에서는 객체 선택의 기본 작업에 중점을 둡니다. 우리는 사용자가 책상 위에 손을 얹고 있는 동안 어떤 거리에서든 객체를 선택할 수 있도록 DeskVR을 위해 특별히 고안된 객체 선택 기술인 TouchRay를 소개합니다. 또한 해당 계층 트리를 탐색하여 개체의 하위 구성 요소를 선택할 수도 있습니다. 우리는 기존 VR을 겨냥한 최첨단 기술과 TouchRay를 비교하는 사용자 평가를 실시했습니다. 결과에 따르면 참가자들은 공중 제스처 없이도 복잡한 작업의 기본 기술과 일관된 시간과 동등한 수준으로 다양한 설정에서 개체를 성공적으로 선택할 수 있는 것으로 나타났습니다.,https://doi.org/10.1109/ISMAR59233.2023.00116,Interaction & Input,Optical / Display Technology,Technical Evaluation,Algorithm / Method
399,2023,Towards Eyeglasses Refraction in Appearance-based Gaze Estimation,외모 기반 시선 추정에서 안경 굴절에 대하여,"For myopia and hyperopia subjects, eyeglasses would change the position of objects in their views, leading to different eyeball rotations for the same gaze target (Fig. 1). Existing appearance-based gaze estimation methods ignore this effect, while this paper investigates it and proposes an effective method to consider it in gaze estimation, achieving noticeable improvements. Specifically, we discover that the appearance-gaze mapping differs for spectacled and unspectacled conditions, and the deviations are nearly consistent with the physical laws of the ideal lens. Based on this discovery, we propose a novel multi-task training strategy that encourages networks to regress gaze and classify the wearing conditions simultaneously. We apply the proposed strategy to some popular methods, including supervised and unsupervised ones, and evaluate them on different datasets with various backbones. The results show that the multi-task training strategy could be used on the existing methods to improve the performance of gaze estimation. To the best of our knowledge, we are the first to clearly reveal and explicitly consider eyeglasses refraction in appearance-based gaze estimation. Data and code are available at https://github.com/StoryMY/RefractionGaze.","근시 및 원시 피험자의 경우 안경은 시야에서 물체의 위치를 ​​변경하여 동일한 시선 대상에 대해 안구 회전이 달라집니다(그림 1). 기존의 외모 기반 시선 추정 방법은 이러한 효과를 무시하는 반면, 본 논문에서는 이를 조사하고 시선 추정에 고려할 수 있는 효과적인 방법을 제안하여 눈에 띄는 개선을 달성했습니다. 특히, 우리는 외관-시선 매핑이 안경을 쓴 조건과 안경을 쓰지 않은 조건에 따라 다르며 편차가 이상적인 렌즈의 물리적 법칙과 거의 일치한다는 것을 발견했습니다. 이 발견을 바탕으로 우리는 네트워크가 시선을 회귀하고 착용 조건을 동시에 분류하도록 장려하는 새로운 다중 작업 훈련 전략을 제안합니다. 우리는 제안된 전략을 감독 및 비지도 방식을 포함한 일부 인기 있는 방법에 적용하고 다양한 백본을 갖춘 다양한 데이터 세트에서 이를 평가합니다. 결과는 다중 작업 훈련 전략이 시선 추정 성능을 향상시키기 위해 기존 방법에 사용될 수 있음을 보여줍니다. 우리가 아는 한, 우리는 외모 기반 시선 추정에서 안경 굴절을 명확하게 밝히고 명시적으로 고려한 최초의 사람입니다. 데이터와 코드는 https://github.com/StoryMY/RefractionGaze에서 확인할 수 있습니다.",https://doi.org/10.1109/ISMAR59233.2023.00084,Interaction & Input,Sensor Fusion,Technical Evaluation,Algorithm / Method
400,2023,Towards a Framework for Validating XR Prototyping for Performance Evaluations of Simulated User Experiences,시뮬레이션된 사용자 경험의 성능 평가를 위한 XR 프로토타이핑 검증을 위한 프레임워크를 향하여,"Extended Reality (XR) technology has matured in recent years, leading to increased use of XR simulations for prototyping novel human-centered interfaces, approximating advanced display hardware, or exploring future user experiences, before realising them in real-world scenarios. However, the validity of utilizing XR prototyping (XRP) as a method for gathering performance data on novel user experiences is still underexplored, i.e, it is not clear if results gathered in simulations can be transferred to a real experience. To address this gap, we propose a validation framework that supports establishing equivalence of performance measures gathered with real products and simulated products and, thus, improve ecological validity of XRP. To demonstrate the utility of the framework, we conduct an exemplary validation study using a Varjo XR-3, a state-of-the-art XR head-mounted display (HMD). The study focuses on steering a small drone and comparing it to interactions with its real-world counterpart. We identify functional fidelity, i.e., functional similarity between real and simulated product, as well as simulation overhead from wearing an HMD as major confounding factors for XRP.","확장 현실(XR) 기술은 최근 몇 년간 성숙해졌으며, 실제 시나리오에서 구현하기 전에 새로운 인간 중심 인터페이스 프로토타입화, 고급 디스플레이 하드웨어 근사화 또는 미래 사용자 경험 탐구를 위한 XR 시뮬레이션 사용이 증가했습니다. 그러나 새로운 사용자 경험에 대한 성능 데이터를 수집하는 방법으로 XR 프로토타이핑(XRP)을 활용하는 타당성은 여전히 ​​과소 탐구되고 있습니다. 즉, 시뮬레이션에서 수집된 결과가 실제 경험으로 전달될 수 있는지는 확실하지 않습니다. 이러한 격차를 해결하기 위해 우리는 실제 제품과 시뮬레이션 제품으로 수집된 성능 측정의 동등성 설정을 지원하고 이를 통해 XRP의 생태학적 유효성을 향상시키는 검증 프레임워크를 제안합니다. 프레임워크의 유용성을 입증하기 위해 최첨단 XR 헤드 장착 디스플레이(HMD)인 Varjo XR-3을 사용하여 예시적인 검증 연구를 수행합니다. 이 연구는 소형 드론을 조종하고 이를 실제 드론과의 상호 작용과 비교하는 데 중점을 둡니다. 우리는 기능적 충실도, 즉 실제 제품과 시뮬레이션 제품 간의 기능적 유사성과 HMD 착용으로 인한 시뮬레이션 오버헤드를 XRP의 주요 혼란 요인으로 식별합니다.",https://doi.org/10.1109/ISMAR59233.2023.00096,Display & Optics; Interaction & Input,Sensor Fusion,Simulation,System / Framework
401,2023,Training for Open-Ended Drilling through a Virtual Reality Simulation,가상 현실 시뮬레이션을 통한 개방형 드릴링 교육,"Virtual Reality (VR) can support effective and scalable training of psychomotor skills in manufacturing. However, many industry training modules offer experiences that are close-ended and do not allow for human error. We aim to address this gap in VR training tools for psychomotor skills training by exploring an open-ended approach to the system design. We designed a VR training simulation prototype to perform open-ended practice of drilling using a 3-axis milling machine. The simulation employs near “endto-end” instruction through a safety module, a setup and drilling tutorial, open-ended practice complete with warnings of mistakes and failures, and a function to assess the geometries and locations of drilled holes against an engineering drawing. We developed and conducted a user study within an undergraduate-level introductory fabrication course to investigate the impact of open-ended VR practice on learning outcomes. Study results reveal positive trends, with the VR group successfully completing the machining task of drilling at a higher rate (75% vs 64%), with fewer mistakes (1.75 vs 2.14 score), and in less time (17.67 mins vs 21.57 mins) compared to the control group. We discuss our findings and limitations and implications for the design of open-ended VR training systems for learning psychomotor skills.","가상 현실(VR)은 제조 분야에서 정신 운동 기술에 대한 효과적이고 확장 가능한 교육을 지원할 수 있습니다. 그러나 많은 업계 교육 모듈은 폐쇄형 경험을 제공하고 인적 오류를 허용하지 않습니다. 우리는 시스템 설계에 대한 개방형 접근 방식을 탐색하여 정신 운동 기술 훈련을 위한 VR 훈련 도구의 이러한 격차를 해결하는 것을 목표로 합니다. 우리는 3축 밀링 머신을 사용하여 개방형 드릴링 연습을 수행하기 위해 VR 교육 시뮬레이션 프로토타입을 설계했습니다. 시뮬레이션에서는 안전 모듈, 설정 및 드릴링 튜토리얼, 실수 및 실패에 대한 경고가 포함된 개방형 연습, 엔지니어링 도면에 대해 드릴 구멍의 형상 및 위치를 평가하는 기능을 통해 거의 ""엔드 투 엔드"" 지침을 사용합니다. 우리는 개방형 VR 실습이 학습 결과에 미치는 영향을 조사하기 위해 학부 수준 입문 제작 과정 내에서 사용자 연구를 개발하고 실시했습니다. 연구 결과에 따르면 VR 그룹은 대조군에 비해 더 높은 비율(75% 대 64%), 더 적은 실수(1.75 대 2.14점), 더 짧은 시간(17.67분 대 21.57분)으로 드릴링 가공 작업을 성공적으로 완료하는 등 긍정적인 추세를 보여줍니다. 우리는 정신 운동 기술 학습을 위한 개방형 VR 훈련 시스템 설계에 대한 우리의 연구 결과, 한계 및 시사점을 논의합니다.",https://doi.org/10.1109/ISMAR59233.2023.00051,Education & Training,Sensor Fusion,User Study,User Study / Empirical Findings
402,2023,User Experience of Collaborative Co-located Mixed Reality: a User Study in Teaching Veterinary Radiation Safety Rules,공동 배치 혼합 현실의 사용자 경험: 수의학 방사선 안전 규칙 교육에 대한 사용자 연구,"As part of the clinical training during their degree course, veterinary students learn how to safely obtain radiographs in horses. However, this can sometimes be challenging due to ethical considerations related to the use of live animals and the fact that it is strictly dependent on the caseload of the veterinary teaching hospital. This networked setup allows the lecturer to guide students in equine radiographic techniques without requiring actual horses or an X-ray machine. A study involving veterinary students showed promising results regarding the effectiveness of using MR to teach radiation safety while performing radiographic techniques on horses. In addition to performance metrics, we employed questionnaires, including MREQ, VRSQ, and UEQ, to collect demographic data and participant feedback. Participants praised the system’s pedagogical effectiveness and overall user experience. The immersive MR experience created a sense of presence and co-presence, underscoring the potential for broader applications of co-located MR in radiology and other areas.","학위 과정 중 임상 훈련의 일환으로 수의과 학생들은 말의 방사선 사진을 안전하게 촬영하는 방법을 배웁니다. 그러나 이는 살아있는 동물의 사용과 관련된 윤리적 고려 사항과 수의과 교육 병원의 업무량에 엄격하게 의존한다는 사실로 인해 때로는 어려울 수 있습니다. 이 네트워크 설정을 통해 강사는 실제 말이나 X선 기계 없이도 학생들에게 말 방사선 촬영 기술을 지도할 수 있습니다. 수의과 학생들을 대상으로 한 연구에서는 말에게 방사선 촬영 기술을 수행하는 동안 방사선 안전을 가르치기 위해 MR을 사용하는 효과에 관한 유망한 결과가 나타났습니다. 성과 지표 외에도 MREQ, VRSQ, UEQ 등의 설문지를 사용하여 인구 통계 데이터와 참가자 피드백을 수집했습니다. 참가자들은 시스템의 교육적 효율성과 전반적인 사용자 경험을 높이 평가했습니다. 몰입형 MR 경험은 존재감과 공존감을 만들어내며, 방사선과 및 기타 분야에서 같은 위치에 있는 MR의 광범위한 적용 가능성을 강조합니다.",https://doi.org/10.1109/ISMAR59233.2023.00073,Education & Training; Collaboration & Social,Other,User Study,Algorithm / Method
403,2023,User Self-Motion Modulates the Perceptibility of Jitter for World-locked Objects in Augmented Reality,사용자 셀프 모션은 증강 현실에서 세계에 고정된 객체에 대한 지터의 인지도를 조절합니다,"A key feature of augmented reality (AR) is the ability to display virtual content that appears stationary as users move throughout the physical world (‘world-locked rendering’). Imperfect world-locked rendering gives rise to perceptual artifacts that can negatively impact user experience. One example is random variation in the position of virtual objects that are intended to be stationary (jitter’). The human visual system is highly attuned to detect moving objects, and moreover it can disambiguate between the retinal velocities that arise from object motion and self-motion, respectively. In this study, we investigated how the perceptibility of AR object jitter varies as a function of user self-motion. Using a commercially available AR HMD to display a 3D textured cube, we measured sensitivity to added jitter versus a no-jitter reference using a two-interval forced choice task. Three user motion conditions (stationary, head rotation, and walking) and three object placement conditions (floating in free space, on a desk, and against a wall) were tested in a full factorial design. We hypothesized that (1) as users move their head and eyes during self-motion, their sensitivity to jitter will decrease, due to added retinal velocity; and (2) rendering virtual objects near physical surfaces will increase sensitivity to jitter, by providing proximal veridical visual cues. Psychometric thresholds indicated that users were significantly less sensitive to jitter during self-motion than when they were stationary, consistent with hypothesis (1). Users were also more sensitive to jitter in one of the two object placement conditions, providing partial support for hypothesis (2). To generalize beyond distinct user motion and object placement conditions, we also analyzed eye tracking data. The amount of retinal slip (i.e. how much gaze drifted across the virtual object) predicted jitter thresholds better than recorded head movements alone, suggesting a retinally-driven decrease in jitter sensitivity during self-motion. These results can be used to inform requirements for AR world-locked rendering systems, as well as how these may be updated dynamically using online measurement of user head and eye movements.","증강 현실(AR)의 핵심 기능은 사용자가 물리적 세계에서 움직일 때 정지해 있는 것처럼 보이는 가상 콘텐츠를 표시하는 기능('세계 고정 렌더링')입니다. 불완전한 월드 고정 렌더링은 사용자 경험에 부정적인 영향을 미칠 수 있는 지각적 아티팩트를 발생시킵니다. 한 가지 예는 고정되도록 의도된 가상 객체 위치의 무작위 변화(지터')입니다. 인간의 시각 시스템은 움직이는 물체를 감지하는 데 고도로 적응되어 있으며, 더욱이 물체의 움직임과 자기 움직임으로 인해 각각 발생하는 망막 속도를 명확하게 구분할 수 있습니다. 본 연구에서는 AR 객체 지터의 인지도가 사용자 자체 모션의 함수에 따라 어떻게 달라지는지 조사했습니다. 3D 텍스처 큐브를 표시하기 위해 시중에서 판매되는 AR HMD를 사용하여 우리는 2간격 강제 선택 작업을 사용하여 추가된 지터와 지터가 없는 참조에 대한 민감도를 측정했습니다. 세 가지 사용자 동작 조건(정지, 머리 회전, 걷기)과 세 가지 개체 배치 조건(자유 공간에 떠 있음, 책상 위, 벽에 기대어 있음)이 완전 요인 설계에서 테스트되었습니다. 우리는 (1) 사용자가 자체 모션 중에 머리와 눈을 움직일 때 망막 속도가 추가되어 지터에 대한 민감도가 감소할 것이라는 가설을 세웠습니다. (2) 물리적 표면 근처에 가상 객체를 렌더링하면 근위의 실제 시각적 단서를 제공하여 지터에 대한 민감도가 높아집니다. 심리 측정 임계값은 가설(1)과 일치하여 사용자가 정지해 있을 때보다 자체 모션 중에 불안감에 훨씬 덜 민감하다는 것을 나타냅니다. 또한 사용자는 두 가지 개체 배치 조건 중 하나에서 지터에 더 민감하여 가설(2)을 부분적으로 뒷받침했습니다. 고유한 사용자 동작 및 개체 배치 조건을 넘어 일반화하기 위해 시선 추적 데이터도 분석했습니다. 망막 슬립의 양(즉, 가상 물체를 가로질러 표류하는 시선의 양)은 기록된 머리 움직임만 단독으로 사용하는 것보다 지터 임계값을 더 잘 예측했으며, 이는 자체 모션 중 지터 민감도가 망막으로 인해 감소함을 나타냅니다. 이러한 결과는 AR 세계 고정 렌더링 시스템에 대한 요구 사항과 사용자 머리 및 눈 움직임의 온라인 측정을 사용하여 동적으로 업데이트할 수 있는 방법을 알려주는 데 사용될 수 있습니다.",https://doi.org/10.1109/ISMAR59233.2023.00049,Interaction & Input; Display & Optics,Eye / Gaze Tracking,Quantitative Experiment,System / Framework
404,2023,Using Identification with AR Face Filters to Predict Explicit & Implicit Gender Bias,AR 얼굴 필터와 함께 식별 기능을 사용하여 명시적 및 암묵적 성별 편견 예측,"Augmented Reality (AR) filters, such as those used by social media platforms like Snapchat and Instagram, are perhaps the most commonly used AR technology. As with fully immersive Virtual Reality (VR) systems, individuals can use AR to embody different people. This experience in VR has been able to influence real world biases such as sexism. However, there is little to no comparative research on AR embodiment’s impact on societal biases. This study aims to set groundwork by examining possible connections between using gender changing Snapchat AR face filters and a person’s predicted implicit and explicit gender biases. We discovered that participants who experienced identification with gender manipulated versions of themselves showed both greater and lesser amounts of bias against men and women. These results depended the user’s gender, the filter applied, and the level of identification users reported with their AR manipulated selves. The results were similar to past VR findings but offered unique AR observations that could be useful for future bias intervention efforts.","Snapchat 및 Instagram과 같은 소셜 미디어 플랫폼에서 사용되는 것과 같은 증강 현실(AR) 필터는 아마도 가장 일반적으로 사용되는 AR 기술일 것입니다. 완전 몰입형 가상 현실(VR) 시스템과 마찬가지로 개인은 AR을 사용하여 다양한 사람을 구현할 수 있습니다. VR에서의 이러한 경험은 성차별과 같은 현실 세계의 편견에 영향을 미칠 수 있었습니다. 그러나 AR 구현이 사회적 편견에 미치는 영향에 대한 비교 연구는 거의 또는 전혀 없습니다. 이 연구는 성별을 변경하는 Snapchat AR 얼굴 필터를 사용하는 것과 개인의 예측된 암묵적 및 명시적 성별 편견 사이의 가능한 연관성을 조사하여 토대를 마련하는 것을 목표로 합니다. 우리는 자신의 성별이 조작된 버전에 대한 동일시를 경험한 참가자가 남성과 여성에 대해 더 크거나 더 적은 양의 편견을 보였다는 것을 발견했습니다. 이러한 결과는 사용자의 성별, 적용된 필터, 사용자가 AR 조작 자아로 보고한 식별 수준에 따라 달라졌습니다. 결과는 과거 VR 결과와 유사했지만 향후 편견 개입 노력에 유용할 수 있는 고유한 AR 관찰을 제공했습니다.",https://doi.org/10.1109/ISMAR59233.2023.00019,Perception & Cognition,Other,User Study,System / Framework
405,2023,VRS-NeRF: Accelerating Neural Radiance Field Rendering with Variable Rate Shading,VRS-NeRF: 가변 속도 셰이딩으로 Neural Radiance Field 렌더링 가속화,"Recent advancements in Neural Radiance Fields (NeRF) provide enormous potential for a wide range of Mixed Reality (MR) applications. However, the applicability of NeRF to real-time MR systems is still largely limited by the rendering performance of NeRF. In this paper, we present a novel approach for Variable Rate Shading for Neural Radiance Fields (VRS-NeRF). In contrast to previous techniques, our approach does not require training multiple neural networks or re-training of already existing ones, but instead utilizes the raytracing properties of NeRF. This is achieved by merging rays depending on a variable shading rate, which reduces the overall number of queries to the neural network. We demonstrate the generalizability of our approach by implementing three alternative functions for the determination of the shading rate. The first method uses the gaze of users to effectively implement a foveated rendering technique in NeRF. For the other two techniques, we utilize shading rates based on edges and saliency. Based on a psychophysical experiment and multiple image-based metrics, we suggest a set of parameters for each technique, yielding an optimal tradeoff between rendering performance gain and perceived visual quality.","NeRF(Neural Radiance Fields)의 최근 발전은 다양한 혼합 현실(MR) 애플리케이션에 엄청난 잠재력을 제공합니다. 그러나 NeRF를 실시간 MR 시스템에 적용하는 것은 여전히 ​​NeRF의 렌더링 성능에 의해 크게 제한됩니다. 본 논문에서는 VRS-NeRF(Variable Rate Shading for Neural Radiance Fields)에 대한 새로운 접근 방식을 제시합니다. 이전 기술과 달리 우리의 접근 방식은 여러 신경망을 훈련하거나 기존 신경망을 재훈련할 필요가 없으며 대신 NeRF의 광선 추적 속성을 활용합니다. 이는 가변 음영 비율에 따라 광선을 병합하여 달성되며, 이는 신경망에 대한 전체 쿼리 수를 줄입니다. 우리는 음영율 결정을 위한 세 가지 대체 기능을 구현하여 우리 접근 방식의 일반화 가능성을 입증합니다. 첫 번째 방법은 NeRF에서 포비티드 렌더링 기법을 효과적으로 구현하기 위해 사용자의 시선을 활용하는 방법입니다. 다른 두 기술의 경우 가장자리와 돌출성을 기반으로 음영 비율을 활용합니다. 정신물리학적 실험과 다양한 이미지 기반 지표를 기반으로 각 기술에 대한 일련의 매개변수를 제안하여 렌더링 성능 향상과 인지된 시각적 품질 간의 최적의 균형을 도출합니다.",https://doi.org/10.1109/ISMAR59233.2023.00039,Rendering & Visualization; Interaction & Input,3D Reconstruction,Technical Evaluation,Algorithm / Method
406,2023,Vanishing Point Aided Hash-Frequency Encoding for Neural Radiance Fields (NeRF) from Sparse 360°Input,Sparse 360°Input의 NeRF(Neural Radiance Field)에 대한 소실점 지원 해시 주파수 인코딩,"Neural Radiance Fields (NeRF) enable novel view synthesis of 3D scenes when trained with a set of 2D images. One of the key components of NeRF is the input encoding, i.e. mapping the coordinates to higher dimensions to learn high-frequency details, which has been proven to increase the quality. Among various input mappings, hash encoding is gaining increasing attention for its efficiency. However, its performance on sparse inputs is limited. To address this limitation, we propose a new input encoding scheme that improves hash-based NeRF for sparse inputs, i.e. few and distant cameras, specifically for 360° view synthesis. In this paper, we combine frequency encoding and hash encoding and show that this combination can increase dramatically the quality of hash-based NeRF for sparse inputs. Additionally, we explore scene geometry by estimating vanishing points in omnidirectional images (ODI) of indoor and city scenes in order to align frequency encoding with scene structures. We demonstrate that our vanishing point-aided scene alignment further improves deterministic and non-deterministic encodings on image regression and NeRF tasks where sharper textures and more accurate geometry of scene structures can be reconstructed.","NeRF(Neural Radiance Fields)는 2D 이미지 세트로 훈련할 때 3D 장면의 새로운 뷰 합성을 가능하게 합니다. NeRF의 주요 구성 요소 중 하나는 입력 인코딩입니다. 즉, 고주파 세부 정보를 학습하기 위해 좌표를 더 높은 차원으로 매핑하는 것입니다. 이는 품질을 높이는 것으로 입증되었습니다. 다양한 입력 매핑 중에서 해시 인코딩은 효율성 측면에서 점점 주목을 받고 있습니다. 그러나 희소 입력에 대한 성능은 제한적입니다. 이러한 제한 사항을 해결하기 위해 우리는 특히 360° 뷰 합성을 위해 희소 입력, 즉 소수의 먼 카메라에 대한 해시 기반 NeRF를 개선하는 새로운 입력 인코딩 방식을 제안합니다. 본 논문에서는 주파수 인코딩과 해시 인코딩을 결합하여 이 조합이 희소 입력에 대한 해시 기반 NeRF의 품질을 극적으로 향상시킬 수 있음을 보여줍니다. 또한 주파수 인코딩을 장면 구조에 맞추기 위해 실내 및 도시 장면의 전방향 이미지(ODI)에서 소실점을 추정하여 장면 형상을 탐색합니다. 우리는 소실점 지원 장면 정렬이 이미지 회귀 및 장면 구조의 보다 정확한 텍스처와 보다 정확한 기하학을 재구성할 수 있는 NeRF 작업에서 결정적 및 비결정적 인코딩을 더욱 향상시킨다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR59233.2023.00131,Rendering & Visualization,3D Reconstruction,Technical Evaluation,Other
407,2023,Visual ScanPath Transformer: Guiding Computers to See the World,Visual ScanPath Transformer: 컴퓨터가 세상을 볼 수 있도록 안내,"We propose to exploit the scanpath prediction technology to simulate human visual system to automatically generate gaze scanpaths for VR/AR applications, to alleviate the equipment and computational cost in foveated rendering. Specifically, we propose a novel deep learning-based scanpath prediction model called Visual ScanPath Transformer (VSPT), to predict human gaze scanpaths in both free viewing and task-driven viewing situations, based on which the VR/AR systems can execute foveated rendering rapidly and cheaply. The proposed VSPT first extracts highly task-related image features from the visual scene, and then explores the global dependency relationships among all the image regions to generate each image region a global feature. Next, VSPT simulates the human visual working memory to consider all the previous fixations’ influences when predicting each fixation. Experimental findings confirm that our model exhibits adherence to classical visual principles during saccadic decision-making, surpassing the current state-of-the-art performance in free-viewing and task-driven (goal-driven and question-driven) visual scenarios.",우리는 인간 시각 시스템을 시뮬레이션하여 VR/AR 애플리케이션을 위한 시선 스캔 경로를 자동으로 생성하고 포비티드 렌더링의 장비 및 계산 비용을 완화하기 위해 스캔 경로 예측 기술을 활용할 것을 제안합니다. 구체적으로 우리는 VSPT(Visual ScanPath Transformer)라는 새로운 딥러닝 기반 스캔 경로 예측 모델을 제안합니다. 이를 통해 VR/AR 시스템이 빠르고 저렴하게 포비티드 렌더링을 실행할 수 있는 자유 보기 및 작업 중심 보기 상황 모두에서 인간의 시선 스캔 경로를 예측합니다. 제안된 VSPT는 먼저 시각적 장면에서 작업과 관련된 이미지 특징을 추출한 다음 모든 이미지 영역 간의 전역 종속 관계를 탐색하여 각 이미지 영역을 전역 특징으로 생성합니다. 다음으로 VSPT는 인간의 시각적 작업 기억을 시뮬레이션하여 각 고정을 예측할 때 이전 고정의 영향을 모두 고려합니다. 실험 결과에 따르면 우리 모델은 단속적인 의사 결정 중에 고전적인 시각적 원리를 고수하며 자유 보기 및 작업 중심(목표 중심 및 질문 중심) 시각적 시나리오에서 현재의 최첨단 성능을 능가한다는 것을 확인했습니다.,https://doi.org/10.1109/ISMAR59233.2023.00037,Interaction & Input,Deep Learning / Neural Networks; Eye / Gaze Tracking,Technical Evaluation,Algorithm / Method
408,2023,Well-being in Isolation: Exploring Artistic Immersive Virtual Environments in a Simulated Lunar Habitat to Alleviate Asthenia Symptoms,고립된 웰빙: 무력증 증상을 완화하기 위해 시뮬레이션된 달 서식지에서 예술적 몰입형 가상 환경 탐색,"Revived interest in lunar and planetary exploration is heralding a new era for human spaceflight, characterized by frequent strain on astronaut’s mental well-being, which stems from increased exposure to isolated, confined, and extreme (ICE) conditions. Whilst Immersive Virtual Reality (IVR) has been employed to facilitate self-help interventions to mitigate challenges caused by isolated environments in several domains, its applicability in support of future space expeditions remains largely unexplored. To address this limitation, we administered the use of distinct IVR environments to crew members $(n=5)$ partaking in a simulated lunar habitat study. Utilizing a Bayesian approach to scrutinize small group data, we discovered a significant relationship between IVR usage and a reduction in perceived stress-related symptoms, particularly those associated with asthenia (syndrome often linked to chronic fatigue and weakness; a condition characterized by feelings of energy depletion or exhaustion that can be amplified in ICE conditions). The reductions were most prominent with the use of interactive virtual environments. The ’Aesthetic Realities’ - virtual environments conceived as art exhibits - received exceptional praise from our participants. These environments mark a fascinating convergence of art and science, holding promise to mitigate effects related to isolation in spaceflight training and beyond","달 및 행성 탐사에 대한 관심이 다시 높아짐에 따라 인간 우주 비행의 새로운 시대가 열리게 되었습니다. 이는 우주비행사의 정신 건강에 대한 빈번한 부담을 특징으로 하며, 이는 고립되고, 제한되고, 극한(ICE) 조건에 대한 노출 증가로 인해 발생합니다. IVR(Immersive Virtual Reality)은 여러 영역에서 고립된 환경으로 인해 발생하는 문제를 완화하기 위해 자조 개입을 촉진하기 위해 사용되었지만 향후 우주 탐사를 지원하는 적용 가능성은 아직 대부분 탐구되지 않았습니다. To address this limitation, we administered the use of distinct IVR environments to crew members $(n=5)$ partaking in a simulated lunar habitat study. 소그룹 데이터를 면밀히 조사하기 위해 베이지안 접근 방식을 활용하여 우리는 IVR 사용과 인지된 스트레스 관련 증상, 특히 무력증(종종 만성 피로 및 쇠약과 관련된 증후군, ICE 상태에서 증폭될 수 있는 에너지 고갈 또는 피로감을 특징으로 하는 증상)과 관련된 증상의 감소 사이의 중요한 관계를 발견했습니다. 대화형 가상 환경을 사용할 때 감소가 가장 두드러졌습니다. 예술 전시를 컨셉으로 한 가상 환경인 '미학적 현실'은 참가자들로부터 특별한 찬사를 받았습니다. 이러한 환경은 예술과 과학의 매혹적인 융합을 나타내며 우주 비행 훈련 및 그 이상에서 고립과 관련된 영향을 완화할 수 있는 가능성을 제시합니다.",https://doi.org/10.1109/ISMAR59233.2023.00033,Education & Training,Sensor Fusion,Simulation,Algorithm / Method
409,2023,What And How Together: A Taxonomy On 30 Years Of Collaborative Human-Centered XR Tasks,무엇을 어떻게 함께: 인간 중심의 협업 XR 작업에 대한 30년 분류,"We present a taxonomy of human-centered collaborative XR tasks. XR technologies have extended into the realm of collaboration, improving the quality and accessibility of teamwork. However, after a comprehensive assessment of the literature on the interaction between XR technologies and collaboration, no comprehensive method that emphasizes task actions and properties exists to classify collaborative tasks. Thus, our suggested taxonomy represents a classification system for collaborative tasks. After conducting a thorough literature review across different research venues, we conducted several exhaustive classification and review cycles for over 800 papers collected, which resulted in 148 papers retained to create the taxonomy. We dissected the actions and properties that the collaborative endeavors and tasks of these papers encompass as well as the types of categorizations and relations these papers illustrate. We expand on the design choices and usage of our taxonomy, followed by its limitations and future work. We built this taxonomy in order to reduce ambiguities and confusion regarding the design and comprehension of human-based collaborative tasks that use XR technology, which could prove useful in aiding the development and understanding of these tasks. Our taxonomy reveals a framework for understanding how collaborative tasks are designed and a systematic way of classifying different methods by which people can collaborate and interact in environments that involve XR, while still promoting efficient communication, teamwork, goal achievement and productivity.","인간 중심 협업 XR 작업의 분류 체계를 제시합니다. XR 기술은 협업 영역으로 확장되어 팀워크의 품질과 접근성을 향상시켰습니다. 그러나 XR 기술과 협업 간의 상호작용에 관한 문헌을 종합적으로 평가한 결과, 협업 작업을 분류하기 위한 작업 동작 및 속성을 강조하는 포괄적인 방법은 존재하지 않습니다. 따라서 우리가 제안하는 분류법은 협업 작업을 위한 분류 시스템을 나타냅니다. 다양한 연구 장소에서 철저한 문헌 검토를 수행한 후 수집된 800개 이상의 논문에 대해 여러 차례 철저한 분류 및 검토 주기를 수행했으며 그 결과 분류법을 생성하기 위해 148개의 논문이 보관되었습니다. 우리는 이 문서의 공동 노력과 작업이 포함하는 활동과 속성뿐만 아니라 이 문서가 설명하는 분류 및 관계 유형을 분석했습니다. 우리는 분류법의 디자인 선택과 사용법을 확장하고 그 한계와 향후 작업을 확장합니다. 우리는 XR 기술을 사용하는 인간 기반 협업 작업의 설계 및 이해와 관련된 모호함과 혼란을 줄이기 위해 이 분류 체계를 구축했습니다. 이는 이러한 작업의 개발과 이해를 돕는 데 유용할 수 있습니다. 우리의 분류 체계는 협업 작업이 설계되는 방식을 이해하기 위한 프레임워크와 사람들이 XR과 관련된 환경에서 협업하고 상호 작용할 수 있는 다양한 방법을 분류하는 체계적인 방법을 보여 주며 동시에 효율적인 의사소통, 팀워크, 목표 달성 및 생산성을 촉진합니다.",https://doi.org/10.1109/ISMAR59233.2023.00047,Collaboration & Social; Interaction & Input,Other,Literature Review / Meta-analysis,System / Framework; Survey / Review
410,2023,"Who's Watching Me?: Exploring the Impact of Audience Familiarity on Player Performance, Experience, and Exertion in Virtual Reality Exergames","나를 지켜보는 사람은 누구인가?: 가상 현실 Exergames에서 관객의 친숙함이 플레이어 성능, 경험 및 활동에 미치는 영향 탐색","Familiarity with audiences plays a significant role in shaping individual performance and experience across various activities in everyday life. This study delves into the impact of familiarity with non-playable character (NPC) audiences on player performance and experience in virtual reality (VR) exergames. By manipulating of NPC appearance (face and body shape) and voice familiarity, we explored their effect on game performance, experience, and exertion. The findings reveal that familiar NPC audiences have a positive impact on performance, creating a more enjoyable gaming experience, and leading players to perceive less exertion. Moreover, individuals with higher levels of self-consciousness exhibit heightened sensitivity to the familiarity with NPC audiences. Our results shed light on the role of familiar NPC audiences in enhancing player experiences and provide insights for designing more engaging and personalized VR exergame environments.","관객과의 친숙함은 일상생활의 다양한 활동 전반에 걸쳐 개인의 성과와 경험을 형성하는 데 중요한 역할을 합니다. 본 연구는 NPC(플레이 불가능한 캐릭터) 관객과의 친숙함이 가상 현실(VR) 엑서게임에서 플레이어 성능과 경험에 미치는 영향을 조사합니다. NPC 외모(얼굴 및 체형)와 음성 친숙도를 조작하여 게임 성능, 경험 및 노력에 미치는 영향을 탐색했습니다. 연구 결과에 따르면 친숙한 NPC 청중은 성능에 긍정적인 영향을 미치고 더 즐거운 게임 경험을 창출하며 플레이어가 덜 노력하도록 유도하는 것으로 나타났습니다. 더욱이, 자의식 수준이 높은 개인은 NPC 청중과의 친숙함에 대해 높은 민감성을 나타냅니다. 우리의 결과는 플레이어 경험을 향상시키는 데 있어 친숙한 NPC 청중의 역할을 조명하고 보다 매력적이고 개인화된 VR 엑서게임 환경을 설계하기 위한 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR59233.2023.00077,Other,Natural Language Processing,Other,User Study / Empirical Findings
411,2023,Would You Go to a Virtual Doctor? A Systematic Literature Review on User Preferences for Embodied Virtual Agents in Healthcare,가상 의사에게 가겠습니까? 의료 분야에서 구현된 가상 에이전트에 대한 사용자 선호도에 대한 체계적인 문헌 검토,"Medical virtual agents (VAs) hold great potential to support patients in achieving their health goals, especially at times or in regions where the demand for physiological and psychological therapy exceeds the capacity of medical services. To create an accepted complement to on-site diagnosis, treatment, and counseling, it is critical to understand the impact of factors such as the agent’s visual representation, behavior, and responsibilities on creating a trustworthy human-agent relationship. To gain insights into these factors, we conducted a systematic literature review including 59 papers on embodied VAs in the medical domain. Our review focused on the application fields and the role of VAs in medicine, as well as the technology used to display them. Using thematic analysis, we discuss our findings in terms of user preferences, as well as potentials and barriers faced in the interaction with medical VAs. Concerning the visual representation, the users’ wish for customization in terms of appearance and communication modalities was pointed out. It was also important that the agent’s information builds up on trustworthy sources, that they are motivating and adapted to the users’ knowledge. Finally, our results identify research gaps, in particular regarding the technological implementation and the use of artificial intelligence.","의료 가상 에이전트(VA)는 특히 생리적, 심리적 치료에 대한 수요가 의료 서비스의 역량을 초과하는 시기나 지역에서 환자의 건강 목표 달성을 지원할 수 있는 큰 잠재력을 가지고 있습니다. 현장 진단, 치료, 상담에 대한 수용 가능한 보완책을 만들려면 상담원의 시각적 표현, 행동, 책임과 같은 요소가 신뢰할 수 있는 인간-상담원 관계를 형성하는 데 미치는 영향을 이해하는 것이 중요합니다. 이러한 요인에 대한 통찰력을 얻기 위해 우리는 의료 분야의 구체화된 VA에 관한 59편의 논문을 포함하여 체계적인 문헌 검토를 수행했습니다. 우리의 검토는 응용 분야와 의학에서 VA의 역할, 그리고 이를 표시하는 데 사용되는 기술에 중점을 두었습니다. 주제별 분석을 사용하여 사용자 선호도뿐만 아니라 의료 VA와의 상호 작용에서 직면하는 잠재력과 장벽 측면에서 연구 결과를 논의합니다. 시각적 표현에 있어서는 외형과 의사소통 방식 측면에서 사용자의 커스터마이징에 대한 희망이 지적되었다. 또한 에이전트의 정보가 신뢰할 수 있는 소스를 기반으로 구축되어 사용자의 지식에 동기를 부여하고 적용하는 것도 중요했습니다. 마지막으로, 우리의 결과는 특히 기술 구현 및 인공 지능 사용과 관련된 연구 격차를 식별합니다.",https://doi.org/10.1109/ISMAR59233.2023.00082,Medical & Healthcare; Interaction & Input,Optical / Display Technology,Qualitative Analysis,User Study / Empirical Findings
412,2023,XR Input Error Mediation for Hand-Based Input: Task and Context Influences a User's Preference,수동 기반 입력에 대한 XR 입력 오류 중재: 작업 및 컨텍스트가 사용자의 선호도에 영향을 미침,"Many XR devices use bare-hand gestures to reduce the need for handheld controllers. Such gestures, however, lead to false positive and false negative recognition errors, which detract from the user experience. While mediation techniques enable users to overcome recognition errors by clarifying their intentions via UI elements, little research has explored how mediation techniques should be designed in XR and how a user’s task and context may impact their design preferences. This research presents empirical studies about the impact of user perceived error costs on users’ preferences for three mediation technique designs, under different simulated scenarios that were inspired by real-life tasks. Based on a large-scale crowd-sourced survey and an immersive VR-based user study, our results suggest that the varying contexts within each task type can impact users’ perceived error costs, leading to different preferred mediation techniques. We further discuss the study implications of these results on future XR interaction design.","많은 XR 장치는 휴대용 컨트롤러의 필요성을 줄이기 위해 맨손 제스처를 사용합니다. 그러나 이러한 제스처는 거짓 긍정 및 거짓 부정 인식 오류로 이어져 사용자 경험을 저하시킵니다. 조정 기술을 통해 사용자는 UI 요소를 통해 자신의 의도를 명확히 함으로써 인식 오류를 극복할 수 있지만, XR에서 조정 기술이 어떻게 설계되어야 하는지, 사용자의 작업과 컨텍스트가 디자인 선호도에 어떻게 영향을 미칠 수 있는지에 대한 연구는 거의 없습니다. 이 연구는 실제 작업에서 영감을 얻은 다양한 시뮬레이션 시나리오에서 사용자가 인지한 오류 비용이 세 가지 중재 기술 설계에 대한 사용자 선호도에 미치는 영향에 대한 실증적 연구를 제시합니다. 대규모 크라우드 소싱 설문 조사와 몰입형 VR 기반 사용자 연구를 기반으로 한 결과는 각 작업 유형 내의 다양한 컨텍스트가 사용자가 인지한 오류 비용에 영향을 미쳐 선호하는 중재 기술이 달라질 수 있음을 시사합니다. 우리는 미래의 XR 상호 작용 설계에 대한 이러한 결과의 연구 의미에 대해 더 논의합니다.",https://doi.org/10.1109/ISMAR59233.2023.00117,Interaction & Input,Sensor Fusion,User Study,User Study / Empirical Findings; Algorithm / Method
413,2022,A Literature Review of User Studies in Extended Reality Applications for Archaeology,고고학을 위한 확장 현실 응용 프로그램의 사용자 연구에 대한 문헌 검토,"In the present study we conducted a systematic review on user studies for Archaeology in eXtended Reality of the last 10 years. After a screening and selection process, 52 articles were selected for an in-depth analysis. Their classification follows different axes: devices, location dependency, type of users, interaction and collaboration. We also organised the existing user studies according to tasks, evaluation measurements, number of participants, and how the study was conducted (pre-test and/or post-test, formative and summative evaluation, quantitative and qualitative data). We found an intertwined relation between Archaeology and Cultural Heritage, which is reflected in the vast presence of applications for museum exhibitions and tours on archaeological sites. Similarities between systems developed for archaeologists and for general public were also investigated. Our purpose was to find a common ground between different user studies that could help designers of the next systems have a base on which they can build their system. We also highlighted which would be the preferred and most suitable evaluation techniques, when they are needed, with the type of users to address. The results show a heterogeneity of measurable variables and possible choices, but some guidelines could be derived.","본 연구에서는 지난 10년간 확장현실 고고학에 대한 사용자 연구를 체계적으로 검토했다. 심사 및 선정 과정을 거쳐 심층 분석을 위해 52편의 논문이 선정되었습니다. 분류는 장치, 위치 의존성, 사용자 유형, 상호 작용 및 협업 등 다양한 축을 따릅니다. 또한 기존 사용자 연구를 과제, 평가 측정, 참가자 수, 연구 수행 방법(사전 및/또는 사후 테스트, 형성 및 총괄 평가, 정량 및 정성 데이터)에 따라 구성했습니다. 우리는 고고학과 문화유산이 서로 얽혀 있는 관계를 발견했는데, 이는 박물관 전시회와 고고학 유적지 투어에 대한 광범위한 응용 프로그램이 존재한다는 사실에 반영되어 있습니다. 고고학자를 위해 개발된 시스템과 일반 대중을 위해 개발된 시스템 간의 유사점도 조사되었습니다. 우리의 목적은 차세대 시스템 설계자가 시스템을 구축할 수 있는 기반을 마련하는 데 도움이 될 수 있는 다양한 사용자 연구 간의 공통점을 찾는 것이었습니다. 또한 필요한 경우 처리할 사용자 유형에 따라 선호되고 가장 적합한 평가 기술이 무엇인지 강조했습니다. 결과는 측정 가능한 변수와 가능한 선택의 이질성을 보여 주지만 일부 지침은 도출될 수 있습니다.",https://doi.org/10.1109/ISMAR55827.2022.00023,Interaction & Input,Deep Learning / Neural Networks,Quantitative Experiment; Literature Review / Meta-analysis,Survey / Review
414,2022,ABOVE & BELOW: Investigating Ceiling and Floor for Augmented Reality Content Placement,위와 아래: 증강 현실 콘텐츠 배치를 위한 천장과 바닥 조사,"Augmented Reality (AR) interfaces support users by providing access to digital content within real-world environments. However, displaying content at the users’ eye level might result in the occlusion of the real world. Therefore, it requires finding AR content placement areas that free the users’ field of vision. In this work, we systematically investigate two content placement areas beyond the users’ eye level: the ceiling and floor. To understand how potential users perceive virtual content on the ceiling and floor and how the content should be placed on these areas, we conducted two user studies. While the first exploratory study showed the general usefulness of either area, the second quantitative study allowed us to define optimal placement parameters regarding visibility and comfort. With insights from our studies, we provide design recommendations for future AR applications that support 2D content presentation on the ceiling and the floor.","증강 현실(AR) 인터페이스는 실제 환경 내에서 디지털 콘텐츠에 대한 액세스를 제공하여 사용자를 지원합니다. 그러나 사용자의 눈높이에 콘텐츠를 표시하면 현실 세계가 가려질 수 있습니다. 따라서 사용자의 시야를 자유롭게 해주는 AR 콘텐츠 배치 영역을 찾아야 합니다. 본 작업에서는 사용자의 눈높이를 넘어서는 두 가지 콘텐츠 배치 영역인 천장과 바닥을 체계적으로 조사합니다. 잠재 사용자가 천장과 바닥의 가상 콘텐츠를 어떻게 인식하는지, 그리고 이러한 영역에 콘텐츠가 어떻게 배치되어야 하는지를 이해하기 위해 두 가지 사용자 연구를 수행했습니다. 첫 번째 탐구 연구에서는 두 영역 모두의 일반적인 유용성을 보여주었지만, 두 번째 정량적 연구에서는 가시성과 편안함에 관한 최적의 배치 매개변수를 정의할 수 있었습니다. 연구에서 얻은 통찰력을 바탕으로 천장과 바닥에서 2D 콘텐츠 프레젠테이션을 지원하는 미래 AR 애플리케이션에 대한 디자인 권장 사항을 제공합니다.",https://doi.org/10.1109/ISMAR55827.2022.00068,Interaction & Input,Other,Quantitative Experiment,Design Guidelines
415,2022,"ATOFIS, an AR Training System for Manual Assembly: A Full Comparative Evaluation against Guides",수동 조립을 위한 AR 훈련 시스템 ATOFIS: 가이드와의 전체 비교 평가,"This paper reports on a user study to comparatively evaluate two AR training systems designed for step-by-step manual operations: ATOFIS - recently proposed in the literature, and Microsoft Dynamics 365 Guides (hereinafter Guides) - one of the most relevant state-of-the-art commercial solutions. The user study (N=16) was conducted in two stages- i.e., training and authoring, on a partial replica of a real-world assembly workstation. During training, the participant learns a sequence of manual operations by performing two assembly cycles, guided by each of the two AR training systems. During authoring, the participant creates the two sets of AR work instructions used in the next training session, one set with each of the two authoring systems. We bound the authoring and training procedures during the experiment to comparatively assess the AR systems overall, and address at the same time an evaluation gap observed in the literature. The experimental results demonstrated advantages of the authoring approach proposed by ATOFIS (i.e., low-cost, formalized, in-situ, immersive and on-the-fly), proved the usability and effectiveness of the AR instructions authored with ATOFIS and validated a set of hypotheses formulated by the authors of the system. ATOFIS authoring was $1.72\times$ faster and unanimously preferred by the participants; ATOFIS training reported zero assembly errors and was 13% faster than Guides. ATOFIS reported excellent system usability (i.e., SUS) and mental workload (i.e., NASA-TLX) scores for both authoring and training, outperforming Guides on all dimensions.","이 문서에서는 단계별 수동 작업을 위해 설계된 두 가지 AR 교육 시스템인 ATOFIS(최근 문헌에서 제안됨)와 가장 관련성이 높은 최첨단 상용 솔루션 중 하나인 Microsoft Dynamics 365 Guides(이하 가이드)를 비교 평가하기 위한 사용자 연구에 대해 보고합니다. 사용자 연구(N=16)는 실제 조립 워크스테이션의 부분 복제에 대해 훈련과 저작의 두 단계로 수행되었습니다. 훈련 중에 참가자는 두 가지 AR 훈련 시스템 각각의 안내에 따라 두 가지 조립 주기를 수행하여 일련의 수동 작업을 배웁니다. 작성하는 동안 참가자는 다음 교육 세션에 사용되는 AR 작업 지침의 두 세트(두 저작 시스템 각각에 대해 한 세트)를 생성합니다. 우리는 AR 시스템 전체를 비교 평가하고 동시에 문헌에서 관찰된 평가 격차를 해결하기 위해 실험 중에 작성 및 교육 절차를 묶었습니다. 실험 결과는 ATOFIS가 제안한 저작 접근 방식(예: 저비용, 형식화, 현장, 몰입형 및 즉석)의 장점을 입증했으며, ATOFIS로 작성된 AR 지침의 유용성과 효율성을 입증했으며 시스템 작성자가 공식화한 일련의 가설을 검증했습니다. ATOFIS 작성은 $1.72\times$ 더 빠르고 참가자들이 만장일치로 선호했습니다. ATOFIS 교육에서는 조립 오류가 전혀 보고되지 않았으며 가이드보다 13% 더 빨랐습니다. ATOFIS는 저작 및 교육 모두에서 뛰어난 시스템 유용성(예: SUS) 및 정신적 작업 부하(예: NASA-TLX) 점수를 보고했으며 모든 측면에서 Guide보다 뛰어난 성능을 보였습니다.",https://doi.org/10.1109/ISMAR55827.2022.00072,Education & Training,Other,User Study; Questionnaire / Survey,System / Framework
416,2022,Adaptive Visual Cues for Guiding a Bimanual Unordered Task in Virtual Reality,가상 현실에서 양손으로 정렬되지 않은 작업을 안내하기 위한 적응형 시각적 단서,"Work on cueing performance in AR and VR has focused on sequential tasks in which each step must be completed in order before the user can proceed to the next. However, for unordered tasks such as putting books back on a library shelf, the user may be able to perform multiple steps concurrently without needing to follow a specific order. In such situations, giving the user multiple cues for potentially concurrent steps may improve performance time. To investigate this, we built a bimanual VR testbed in which the user needs to move objects to designated destinations, guided by different numbers of cues. The user can decide the order to perform the cued steps and, in some conditions, can affect which cues are shown.In a formal user study, we found that in most conditions, participants perform fastest with three cues. Dynamically updating the set of displayed cues based on hand proximity improves performance, and updating the set based on eye gaze improves performance even more. Finally, for both the hand-proximity and eye-gaze mechanisms, performance can be further improved by locking the cues for objects predicted to be moved next based on hand distance.","AR 및 VR의 큐 성능에 대한 작업은 사용자가 다음 단계로 진행하기 전에 각 단계를 순서대로 완료해야 하는 순차적 작업에 중점을 두었습니다. 그러나 책을 도서관 선반에 다시 올려놓는 등 순서가 지정되지 않은 작업의 경우 사용자는 특정 순서를 따르지 않고도 여러 단계를 동시에 수행할 수 있습니다. 이러한 상황에서는 잠재적으로 동시 단계에 대한 여러 신호를 사용자에게 제공하면 성능 시간이 향상될 수 있습니다. 이를 조사하기 위해 우리는 사용자가 다양한 수의 단서에 따라 개체를 지정된 대상으로 이동해야 하는 양방향 VR 테스트베드를 구축했습니다. 사용자는 신호 단계를 수행하는 순서를 결정할 수 있으며 일부 조건에서는 표시되는 신호에 영향을 줄 수 있습니다. 공식 사용자 연구에서 우리는 대부분의 조건에서 참가자가 세 가지 신호로 가장 빠르게 수행한다는 것을 발견했습니다. 손 근접성을 기반으로 표시된 큐 세트를 동적으로 업데이트하면 성능이 향상되고, 시선을 기반으로 세트를 업데이트하면 성능이 더욱 향상됩니다. 마지막으로 손 근접 및 눈 응시 메커니즘 모두 손 거리를 기반으로 다음에 이동할 것으로 예측되는 개체에 대한 단서를 잠그면 성능이 더욱 향상될 수 있습니다.",https://doi.org/10.1109/ISMAR55827.2022.00059,Interaction & Input,Other,User Study,System / Framework
417,2022,An Emotionally Responsive Virtual Parent for Pediatric Nursing Education: A Framework for Multimodal Momentary and Accumulated Interventions,소아 간호 교육을 위한 정서적으로 반응하는 가상 부모: 다중 모드 순간 및 누적 중재를 위한 프레임워크,"Immersive virtual reality (VR) simulations become more and more popular for basic nursing skills training in a realistic VR environment. However, there is still a gap of research that specifically focuses on pediatric nursing interventions where nurse trainees should be able to recognize and deal with the patient parent’s emotional responses appropriately. In this paper, we propose a novel nursing intervention analysis framework that evaluates the user’s nursing performance, by analyzing not only their momentary multimodal (verbal and nonverbal) behaviors, but also accumulated intervention behaviors that capture the overall nursing context. Based on the proposed framework, we developed an immersive VR-based nursing education system with an emotionally responsive virtual parent, and designed a realistic pediatric nursing intervention scenario in collaboration with a subject-matter expert (a nursing faculty in our university). An expert-evaluation study with professional nurses was conducted to assess the potential of the developed system—in particular, the effects of the emotionally responsive virtual parent— as an effective education tool, by comparing with an emotionally static/neutral virtual parent. Several factors of learning experience, e.g., immersion, realism, learning efficacy, and usefulness, were examined through a subjective questionnaire, and the results support the effectiveness of our system in clinical practice or educational settings. We discuss the findings and implications of our research with the nurse participants’ qualitative feedback, while also addressing possible limitations and future research directions.","몰입형 가상 현실(VR) 시뮬레이션은 현실적인 VR 환경에서 기본 간호 기술 교육을 위해 점점 더 인기를 얻고 있습니다. 그러나 간호사 연수생이 환자 부모의 정서적 반응을 적절하게 인식하고 처리할 수 있어야 하는 소아 간호 중재에 특별히 초점을 맞춘 연구에는 여전히 공백이 있습니다. 본 논문에서는 사용자의 순간적인 다중 모드(언어적, 비언어적) 행동뿐만 아니라 전반적인 간호 맥락을 포착하는 누적된 개입 행동을 분석하여 사용자의 간호 성과를 평가하는 새로운 간호 중재 분석 프레임워크를 제안합니다. 제안된 프레임워크를 기반으로 감정적으로 반응하는 가상 부모를 갖춘 몰입형 VR 기반 간호 교육 시스템을 개발하고, 해당 분야 전문가(우리 대학 간호학과)와 협력하여 현실적인 소아 간호 중재 시나리오를 설계했습니다. 개발된 시스템의 잠재력, 특히 정서적으로 반응하는 가상부모가 효과적인 교육 도구로서 미치는 영향을 정서적으로 정적/중립적인 가상부모와 비교하여 평가하기 위해 전문 간호사를 대상으로 전문가 평가 연구를 실시하였다. 학습 경험의 여러 요소(예: 몰입감, 현실성, 학습 효능, 유용성)를 주관적 설문지를 통해 조사했으며, 그 결과는 임상 실습이나 교육 환경에서 우리 시스템의 효율성을 뒷받침합니다. 우리는 간호사 참가자의 질적 피드백을 통해 연구 결과와 의미를 논의하는 동시에 가능한 한계와 향후 연구 방향을 다루었습니다.",https://doi.org/10.1109/ISMAR55827.2022.00052,Education & Training; Perception & Cognition,Sensor Fusion,User Study,System / Framework; User Study / Empirical Findings
418,2022,An Exploration of Hands-free Text Selection for Virtual Reality Head-Mounted Displays,가상 현실 헤드 마운트 디스플레이를 위한 핸즈프리 텍스트 선택 탐색,"Hand-based interaction, such as using a handheld controller or making hand gestures, has been widely adopted as the primary method for interacting with both virtual reality (VR) and augmented reality (AR) head-mounted displays (HMDs). In contrast, hands-free interaction avoids the need for users’ hands and although it can afford additional benefits, there has been limited research in exploring and evaluating hands-free techniques for these HMDs. As VR HMDs become ubiquitous, people will need to do text editing, which requires selecting text segments. Similar to hands-free interaction, text selection is underexplored. This research focuses on both, text selection via hands-free interaction. Our exploration involves a user study with 24 participants to investigate the performance, user experience, and workload of three hands-free selection mechanisms (Dwell, Blink, Voice) to complement head-based pointing. Results indicate that Blink outperforms Dwell and Voice in completion time. Users’ subjective feedback also shows that Blink is the preferred technique for text selection. This work is the first to explore handsfree interaction for text selection in VR HMDs. Our results provide a solid platform for further research in this important area.","휴대용 컨트롤러를 사용하거나 손짓을 하는 등의 손 기반 상호 작용은 가상 현실(VR) 및 증강 현실(AR) 헤드 장착 디스플레이(HMD)와 상호 작용하는 기본 방법으로 널리 채택되었습니다. 대조적으로, 핸즈프리 상호 작용은 사용자의 손이 필요하지 않으며 추가적인 이점을 제공할 수 있지만 이러한 HMD에 대한 핸즈프리 기술을 탐색하고 평가하는 연구는 제한적이었습니다. VR HMD가 보편화됨에 따라 사람들은 텍스트 편집을 수행해야 하며 이를 위해서는 텍스트 세그먼트를 선택해야 합니다. 핸즈프리 상호 작용과 유사하게 텍스트 선택에 대한 탐색이 부족합니다. 이 연구는 핸즈프리 상호 작용을 통한 텍스트 선택에 중점을 둡니다. 우리의 탐색에는 머리 기반 포인팅을 보완하기 위해 세 가지 핸즈프리 선택 메커니즘(Dwell, Blink, Voice)의 성능, 사용자 경험 및 작업 부하를 조사하기 위해 24명의 참가자를 대상으로 한 사용자 연구가 포함됩니다. 결과는 Blink가 완료 시간에서 Dwell 및 Voice보다 성능이 우수함을 나타냅니다. 사용자의 주관적인 피드백은 Blink가 텍스트 선택에 선호되는 기술임을 보여줍니다. 이 작업은 VR HMD에서 텍스트 선택을 위한 핸즈프리 상호 작용을 탐구한 최초의 작업입니다. 우리의 결과는 이 중요한 분야에 대한 추가 연구를 위한 견고한 플랫폼을 제공합니다.",https://doi.org/10.1109/ISMAR55827.2022.00021,Interaction & Input; Display & Optics,Hand / Gesture Recognition,User Study,Algorithm / Method; User Study / Empirical Findings
419,2022,An Object Synthesis Method to Enhance Visuo-Haptic Consistency,시각-촉각 일관성을 향상시키는 객체 합성 방법,"The sense of reality is enhanced by presenting appropriate haptic feedback when the user interacts with a virtual object in virtual reality (VR). To present appropriate feedback, we often use a real object that resembles the virtual one to manipulate. However, such a real object is not always available. The user may feel a visuohaptic inconsistency between real and virtual objects when their shapes are different. To alleviate such an inconsistency, we propose a novel object synthesis method that combines the shape of the real object that the user manipulates in reality and the shape of the virtual object which was to be presented to the user in VR. In other words, this synthesized object, a chimera object, is created by transforming the part of the virtual object that the user would touch into a shape that is similar to the corresponding part of the real object while maintaining the other parts of the virtual object intact. The expected haptic sensation from the appearance of our chimera object is more consistent with the one produced by the real object. Therefore, the visuo-haptic inconsistency is expected to alleviate in VR, compared to the original virtual object. In addition, we propose an interactive system to support the design of a chimera object for users. To investigate the effectiveness of our method, we conducted two user studies. The first experiment confirmed that our proposed chimera object helps enhance visuo-haptic consistency. The second experiment confirmed that our system was effective for chimera object creation by users with acceptable system usability.","사용자가 가상현실(VR)에서 가상 객체와 상호작용할 때 적절한 촉각 피드백을 제시함으로써 현실감을 향상시킵니다. 적절한 피드백을 제시하기 위해 우리는 가상 개체와 유사한 실제 개체를 사용하여 조작하는 경우가 많습니다. 그러나 이러한 실제 개체가 항상 사용 가능한 것은 아닙니다. 사용자는 실제 물체와 가상 물체의 모양이 다를 때 사이의 시촉각적 불일치를 느낄 수 있습니다. 이러한 불일치를 완화하기 위해 우리는 사용자가 현실에서 조작하는 실제 객체의 형상과 VR에서 사용자에게 제시하려는 가상 객체의 형상을 결합한 새로운 객체 합성 방법을 제안합니다. 즉, 이 합성된 객체인 키메라 객체는 가상 객체의 다른 부분은 그대로 유지하면서 사용자가 터치하는 가상 객체의 부분을 실제 객체의 해당 부분과 유사한 형태로 변환함으로써 생성된다. 키메라 개체의 모양에서 예상되는 햅틱 감각은 실제 개체에서 생성되는 것과 더 일치합니다. 따라서 VR에서는 원래의 가상 객체에 비해 시촉각 불일치가 완화될 것으로 기대된다. 또한, 사용자를 위한 키메라 개체 디자인을 지원하는 대화형 시스템을 제안합니다. 우리 방법의 효율성을 조사하기 위해 두 가지 사용자 연구를 수행했습니다. 첫 번째 실험에서는 우리가 제안한 키메라 물체가 시촉각 일관성을 향상시키는 데 도움이 된다는 것을 확인했습니다. 두 번째 실험에서는 우리 시스템이 수용 가능한 시스템 유용성을 갖춘 사용자의 키메라 개체 생성에 효과적이라는 것을 확인했습니다.",https://doi.org/10.1109/ISMAR55827.2022.00058,Interaction & Input,Haptic / Tactile Feedback,Other,System / Framework
420,2022,"Arrow, Bézier Curve, or Halos? - Comparing 3D Out-of-View Object Visualization Techniques for Handheld Augmented Reality","화살표, 베지어 곡선 또는 후광? - 휴대용 증강현실을 위한 3D 시야외 객체 시각화 기법 비교","Handheld augmented reality (AR) applications allow users to interact with their virtually augmented environment on the screen of their tablet or smartphone by simply pointing its camera at nearby objects or “points of interest” (POIs). However, this often requires users to carefully scan their surroundings in search of POIs that are out of view. Proposed 2D guides for out-of-view POIs can, unfortunately, be ambiguous due to the projection of a 3D position to 2D screen space. We address this by using 3D visualizations that directly encode the POI’s 3D direction and distance. Based on related work, we implemented three such visualization techniques: (1) 3D Arrow, (2) 3D Bézier Curve, and (3) 3D Halos. We confirmed the applicability of these three techniques in a case study and then compared them in a user study, evaluating performance, workload, and user experience. Participants performed best using 3D Arrow, while surprisingly, 3D Halos led to poor results. We discuss the design implications of these results that can inform future 3D out-of-view object visualization techniques.","휴대용 증강 현실(AR) 애플리케이션을 사용하면 사용자는 카메라를 근처 개체나 ""관심 지점""(POI)으로 향하기만 하면 태블릿이나 스마트폰 화면에서 가상으로 증강된 환경과 상호 작용할 수 있습니다. 그러나 이를 위해서는 사용자가 시야 밖에 있는 POI를 찾기 위해 주변을 주의 깊게 살펴보아야 하는 경우가 많습니다. 안타깝게도 시야 밖 POI에 대해 제안된 2D 가이드는 3D 위치를 2D 화면 공간에 투영하기 때문에 모호할 수 있습니다. POI의 3D 방향과 거리를 직접 인코딩하는 3D 시각화를 사용하여 이 문제를 해결합니다. 관련 작업을 기반으로 (1) 3D Arrow, (2) 3D Bézier Curve, (3) 3D Halos 등 세 가지 시각화 기술을 구현했습니다. 우리는 사례 연구에서 이 세 가지 기술의 적용 가능성을 확인한 후 사용자 연구에서 이를 비교하여 성능, 작업 부하 및 사용자 경험을 평가했습니다. 참가자들은 3D Arrow를 사용하여 가장 좋은 결과를 얻었지만 놀랍게도 3D Halos는 좋지 않은 결과를 가져왔습니다. 우리는 미래의 3D 시야 밖 객체 시각화 기술을 알릴 수 있는 이러한 결과의 설계 의미에 대해 논의합니다.",https://doi.org/10.1109/ISMAR55827.2022.00098,Interaction & Input,Optical / Display Technology,User Study,Algorithm / Method
421,2022,Assessing the Effect of Interactivity Design In VR Based Second Language Learning Tool,VR 기반 제2언어 학습 도구의 인터랙티브 디자인 효과 평가,"Virtual Reality (VR), as a helpful tool in language education, is widely supported by the current literature. VR can provide a variety of stimulating scenarios that keep learner engagement high so its use in the language classroom has increased rapidly. This makes it necessary for further research to be conducted in the field to determine ways to maximize its potential. This research aims to determine if the level of interactivity presented in an Immersive VR Environment is a factor that will impact a user’s capability to successfully learn a second language, particularly when facing subjects with different age, gender, and previous VR experience. To satisfy these aims, 3 versions of a Virtual Reality Language Learning Application were created with varying levels of interactivity. Participants of this study were administered pre and post evaluations to analyze the efficiency of their second language learning experience. Our results show that the level of interactivity may not be a factor that impacts a user’s capability of learning and provide insight into the factors needed for successful language learning in Virtual Reality.","가상 현실(VR)은 언어 교육에 유용한 도구로서 현재 문헌에서 널리 뒷받침되고 있습니다. VR은 학습자의 참여도를 높게 유지하는 다양한 자극 시나리오를 제공할 수 있으므로 언어 ​​수업에서의 사용이 빠르게 증가했습니다. 따라서 잠재력을 극대화하는 방법을 결정하기 위해 현장에서 추가 연구가 수행되어야 합니다. 이 연구의 목표는 몰입형 VR 환경에서 나타나는 상호작용 수준이 특히 다양한 연령, 성별 및 이전 VR 경험을 가진 대상을 대면할 때 제2 언어를 성공적으로 배우는 사용자의 능력에 영향을 미치는 요소인지 확인하는 것입니다. 이러한 목표를 충족하기 위해 다양한 수준의 상호 작용 기능을 갖춘 3가지 버전의 가상 현실 언어 학습 애플리케이션이 만들어졌습니다. 본 연구의 참가자들은 제2외국어 학습 경험의 효율성을 분석하기 위해 사전 및 사후 평가를 받았습니다. 우리의 결과는 상호작용 수준이 사용자의 학습 능력에 영향을 미치는 요소가 아닐 수 있으며 가상 현실에서 성공적인 언어 학습에 필요한 요소에 대한 통찰력을 제공한다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR55827.2022.00015,Education & Training,Sensor Fusion,User Study,Design Guidelines
422,2022,Auditory Feedback to Make Walking in Virtual Reality More Accessible,가상 현실에서의 보행을 더욱 쉽게 만들기 위한 청각 피드백,"The objective of this study is to investigate the impact of several auditory feedback modalities on gait (i.e., walking patterns) in virtual reality (VR). Prior research has substantiated gait disturbances in VR users as one of the primary obstacles to VR usability. However, minimal research has been done to mitigate this issue. We recruited 39 participants (with mobility impairments: 18, without mobility impairments: 21) who completed timed walking tasks in a real-world environment and the same tasks in a VR environment with various types of auditory feedback. Within-subject results showed that each auditory condition significantly improved gait performance while in VR $(p \lt$.001) compared to the no auditory condition in VR for both groups of participants with and without mobility impairments. Moreover, spatial audio improved gait performance significantly $(p \lt$.001) compared to other auditory conditions for both groups of participants. This research could help to make walking in VR more accessible for people with and without mobility impairments.","이 연구의 목적은 가상 현실(VR)에서 보행(즉, 걷기 패턴)에 대한 여러 청각 피드백 양식의 영향을 조사하는 것입니다. 이전 연구에서는 VR 사용자의 보행 장애가 VR 사용성에 대한 주요 장애물 중 하나로 입증되었습니다. 그러나 이 문제를 완화하기 위해 최소한의 연구가 수행되었습니다. 우리는 실제 환경에서 시간 제한 걷기 작업을 완료하고 다양한 유형의 청각 피드백이 있는 VR 환경에서 동일한 작업을 완료한 39명의 참가자(이동 장애가 있는 사람: 18명, 이동 장애가 없는 사람: 21명)를 모집했습니다. 피험자 내 결과는 이동성 장애가 있는 참가자 그룹과 없는 참가자 그룹 모두에 대해 VR에서 청각 조건이 없는 경우와 비교하여 VR에서 보행 성능이 크게 향상되었음을 보여주었습니다. 또한, 공간 오디오는 두 그룹의 참가자 모두에서 다른 청각 조건에 비해 보행 성능을 크게 향상시켰습니다. 이 연구는 이동 장애가 있거나 없는 사람들이 VR 걷기에 더 쉽게 접근할 수 있도록 하는 데 도움이 될 수 있습니다.",https://doi.org/10.1109/ISMAR55827.2022.00103,Audio & Sound,Other,User Study,User Study / Empirical Findings
423,2022,Augmented Scale Models: Presenting Multivariate Data Around Physical Scale Models in Augmented Reality,증강 축소 모델: 증강 현실에서 물리적 규모 모델에 대한 다변량 데이터 제시,"Recent research in immersive visualisations has explored the use of physical 3D models together with virtual data visualisations, with a simple data encoding. However, little is known about how this technique could be extended towards more complex multivariate data with multiple charts. We present augmented scale models, immersive visualisations that place charts of multivariate data via Augmented Reality (AR) registered to physical 3D models. We identified two main factors for presenting AR charts in the limited display space around the models: 1) how charts are laid out (Slides vs Dashboard), and 2) how the chart views are arranged in the 3D space (On Scale Model, On Table, On Virtual Board). In a within-subject user study, we evaluated these two design considerations. We found that chart layout and view arrangement do not affect task error but do vary in response time. Dashboard and Slides perform equally well on simple tasks that require comparison of a single chart across scale models. However, in more complex tasks such as comparing multiple charts within a scale model or across several scale models, Dashboard shows no decrease in time performance, while Slides’s time performance decreases significantly. We also found that On Scale Model has the fastest performance, has good chart-scale model integration, and supports charts comparison well. On Table and On Virtual Board on the other hand, show a trade-off between the ability to support charts comparison across models and the chart-scale model integration.","몰입형 시각화에 대한 최근 연구에서는 간단한 데이터 인코딩을 통해 가상 데이터 시각화와 함께 실제 3D 모델을 사용하는 방법을 탐구했습니다. 그러나 이 기술이 여러 차트를 사용하는 보다 복잡한 다변량 데이터로 어떻게 확장될 수 있는지에 대해서는 알려진 바가 거의 없습니다. 우리는 실제 3D 모델에 등록된 증강 현실(AR)을 통해 다변량 데이터의 차트를 배치하는 증강 규모 모델, 몰입형 시각화를 제시합니다. 우리는 모델 주변의 제한된 디스플레이 공간에서 AR 차트를 표시하는 두 가지 주요 요소를 식별했습니다. 1) 차트가 배치되는 방식(슬라이드 대 대시보드), 2) 차트 보기가 3D 공간에서 정렬되는 방식(규모 모델, 테이블, 가상 보드). 피험자 내 사용자 연구에서 우리는 이러한 두 가지 설계 고려 사항을 평가했습니다. 차트 레이아웃과 보기 배열은 작업 오류에 영향을 미치지 않지만 응답 시간에는 변화가 있다는 것을 발견했습니다. 대시보드와 슬라이드는 전체 규모 모델에서 단일 차트를 비교해야 하는 간단한 작업에서 동일하게 잘 수행됩니다. 그러나 규모 모델 내의 여러 차트 또는 여러 규모 모델 간의 비교와 같은 보다 복잡한 작업에서는 Dashboard의 시간 성능 저하가 나타나지 않는 반면 Slides의 시간 성능은 크게 감소합니다. 또한 On Scale Model의 성능이 가장 빠르고, 차트 규모 모델 통합이 뛰어나며, 차트 비교를 잘 지원한다는 사실도 발견했습니다. 반면에 On Table 및 On Virtual Board는 모델 간 차트 비교를 지원하는 기능과 차트 규모 모델 통합 간의 절충점을 보여줍니다.",https://doi.org/10.1109/ISMAR55827.2022.00019,Interaction & Input,Other,User Study,Algorithm / Method; User Study / Empirical Findings
424,2022,Augmenting Feature Importance Analysis: How Color and Size Can Affect Context-Aware AR Explanation Visualizations?,기능 중요도 분석 강화: 색상과 크기가 상황 인식 AR 설명 시각화에 어떤 영향을 미칠 수 있습니까?,"Augmented Reality (AR) has shown significant potential in supporting in-situ decision-making in various application areas. Many prior works have shown how AR can visualize the decision support data in various contexts. However, prior research about AR-based decision support systems rarely explored how the explanations were visualized. Providing context-aware explanations within AR-based recommendation systems may help users instantly understand the recommendations they have been given. Therefore, this paper presents the world-first user study exploring AR explanation visualization designs. Three feature importance analysis visualizations that apply different color-coding and size-scaling strategies were designed to explain the recommendations provided by a context-aware AR shopping assistant system. Twenty-four participants were recruited to evaluate these three explanations in a shopping scenario. The results revealed novel findings that could help guide the appropriate utilization of descriptive parameters when designing AR explanation artifacts. The results also show the potential of providing intuitive visualization to explain recommendations in AR.",증강 현실(AR)은 다양한 응용 분야에서 현장 의사 결정을 지원하는 데 상당한 잠재력을 보여주었습니다. 이전의 많은 연구에서는 AR이 다양한 맥락에서 의사결정 지원 데이터를 시각화하는 방법을 보여주었습니다. 그러나 AR 기반 의사결정 지원 시스템에 대한 선행 연구에서는 설명이 어떻게 시각화되는지 탐구한 경우가 거의 없었습니다. AR 기반 추천 시스템 내에서 상황 인식 설명을 제공하면 사용자가 제공된 추천을 즉시 이해하는 데 도움이 될 수 있습니다. 따라서 본 논문에서는 AR 설명 시각화 디자인을 탐구하는 세계 최초의 사용자 연구를 제시합니다. 다양한 색상 코딩 및 크기 조정 전략을 적용하는 세 가지 기능 중요도 분석 시각화는 상황 인식 AR 쇼핑 도우미 시스템에서 제공하는 권장 사항을 설명하기 위해 설계되었습니다. 쇼핑 시나리오에서 이러한 세 가지 설명을 평가하기 위해 24명의 참가자가 모집되었습니다. 그 결과 AR 설명 아티팩트를 디자인할 때 설명 매개변수의 적절한 활용을 안내하는 데 도움이 될 수 있는 새로운 발견이 밝혀졌습니다. 결과는 또한 AR에서 추천 사항을 설명하기 위해 직관적인 시각화를 제공할 수 있는 가능성을 보여줍니다.,https://doi.org/10.1109/ISMAR55827.2022.00067,Rendering & Visualization,Optical / Display Technology,User Study,User Study / Empirical Findings
425,2022,Biophilic Enriched Virtual Environments for Industrial Training: a User Study,산업 교육을 위한 생체친화성이 강화된 가상 환경: 사용자 연구,"Immersive Vimial Reality (IVR) training offers the capability to industrial workers to acquire skills and address complex tasks by immersing them in a safe and controlled virtual environment (VE). However, in the literature, IVR training is mainly based on principles of standardization and efficiency without considering the operators’ well-being. A novel design approach consists of the introduction in the VE of Positive Computing to improve workers’ well-being by applying the Biophilia hypothesis. In this work, we explored the possibility of introducing biophilic elements in a VE training scenario that would support psychological well-being and human potential. However, the introduction of virtual elements not related to the training task may distract operators, impairing their performance. We selected as a training scenario the assembly of a real truck engine. It is accomplished in a workstation, and operators do not interact with the surrounding VE. Therefore, we placed the training area into four different types of VEs: 3D Minimal (MIN), 3D Minimal Biophilic enriched (MIN+BIO), 3D Realistic (REAL), and 3D Realistic Biophilic enriched (REAL+BIO). We compared the MIN and REAL scenarios with the respective biophilic enriched scenarios. The performance of 40 participants was evaluated in terms of completion time, object fixation time, training task accuracy, knowledge accuracy, cognitive load, and user experience. The results revealed that introducing biophilic elements in a VR training environment attracts users’ attention in the idle phase of the training. In contrast, they keep concentrating on the task without worsening their performance during the task accomplishment.","IVR(Immersive Vimial Reality) 교육은 산업 근로자가 안전하고 통제된 가상 환경(VE)에 몰입하여 기술을 습득하고 복잡한 작업을 처리할 수 있는 기능을 제공합니다. 그러나 문헌에서 IVR 교육은 주로 운영자의 복지를 고려하지 않고 표준화 및 효율성 원칙을 기반으로 합니다. 새로운 설계 접근 방식은 바이오필리아 가설을 적용하여 작업자의 웰빙을 향상시키기 위해 포지티브 컴퓨팅을 VE에 도입하는 것으로 구성됩니다. 이 작업에서 우리는 심리적 웰빙과 인간 잠재력을 지원하는 VE 훈련 시나리오에 생체친화적 요소를 도입할 가능성을 탐구했습니다. 그러나 훈련 작업과 관련되지 않은 가상 요소를 도입하면 작업자의 주의가 산만해지며 성능이 저하될 수 있습니다. 우리는 실제 트럭 엔진 조립을 교육 시나리오로 선택했습니다. 이는 워크스테이션에서 수행되며 운영자는 주변 VE와 상호 작용하지 않습니다. 따라서 우리는 훈련 영역을 3D Minimal(MIN), 3D Minimal Biophilic Enriched(MIN+BIO), 3D Realistic(REAL) 및 3D Realistic Biophilic Enriched(REAL+BIO)의 네 가지 유형의 VE에 배치했습니다. 우리는 MIN 및 REAL 시나리오를 각각의 친생물성 농축 시나리오와 비교했습니다. 참가자 40명의 성과는 완료 시간, 물체 고정 시간, 훈련 작업 정확도, 지식 정확도, 인지 부하 및 사용자 경험 측면에서 평가되었습니다. 그 결과 VR 훈련 환경에 생체친화적 요소를 도입하면 훈련의 유휴 단계에서 사용자의 관심을 끄는 것으로 나타났습니다. 반면에 그들은 과제를 수행하는 동안 성과를 악화시키지 않고 계속해서 과제에 집중합니다.",https://doi.org/10.1109/ISMAR55827.2022.00035,Education & Training; Perception & Cognition,Eye / Gaze Tracking,User Study; Quantitative Experiment,Algorithm / Method
426,2022,Blending On-Body and Mid-Air Interaction in Virtual Reality,가상 현실에서 신체 및 공중 상호 작용 혼합,"On-body interfaces, which leverage the human body’s surface as an input or output platform, can provide new opportunities for designing VR interaction. However, it remains unclear how on-body interfaces can best support current VR systems that mainly rely on mid-air interaction. We propose BodyOn, a collection of six design patterns that leverage combined on-body and mid-air interfaces to achieve more effective 3D interaction. Specifically, a user may use thumb-on-finger gestures, finger-on-arm gestures, or on-body displays with mid-air input, including hand movement and orientation, to complete an interaction task. To test our design concepts, we implemented example interaction techniques based on BodyOn that can assist users in various 3D interaction tasks. We further conducted an expert evaluation using the techniques as probes to elicit immediate design issues that emerge from the novel combination of on-body and midair interaction. We provide insights that can inspire and inform the design of future 3D user interfaces.","인체 표면을 입력 또는 출력 플랫폼으로 활용하는 온바디 인터페이스는 VR 상호작용을 디자인하는 데 새로운 기회를 제공할 수 있습니다. 그러나 신체 내 인터페이스가 주로 공중 상호 작용에 의존하는 현재 VR 시스템을 가장 잘 지원할 수 있는 방법은 불분명합니다. 우리는 보다 효과적인 3D 상호 작용을 달성하기 위해 결합된 신체 및 공중 인터페이스를 활용하는 6가지 디자인 패턴 모음인 BodyOn을 제안합니다. 구체적으로, 사용자는 상호 작용 작업을 완료하기 위해 손가락에 손가락을 대는 제스처, 팔에 손가락을 대는 제스처 또는 손 움직임 및 방향을 포함한 공중 입력이 있는 신체 디스플레이를 사용할 수 있습니다. 디자인 개념을 테스트하기 위해 다양한 3D 상호 작용 작업에서 사용자를 지원할 수 있는 BodyOn을 기반으로 하는 상호 작용 기술 예시를 구현했습니다. 우리는 또한 신체 및 공중 상호 작용의 새로운 조합에서 나타나는 즉각적인 설계 문제를 도출하기 위해 이 기술을 프로브로 사용하여 전문가 평가를 수행했습니다. 우리는 미래의 3D 사용자 인터페이스 디자인에 영감을 주고 정보를 제공할 수 있는 통찰력을 제공합니다.",https://doi.org/10.1109/ISMAR55827.2022.00081,Interaction & Input,Other,Other,System / Framework; Interaction Technique
427,2022,Blending Spaces: Cross-Reality Interaction Techniques for Object Transitions Between Distinct Virtual and Augmented Realities,공간 혼합: 별개의 가상 현실과 증강 현실 사이의 객체 전환을 위한 현실 간 상호 작용 기술,"Cross-Reality (CR) involves interaction between different modalities and levels of immersion such as Virtual and Augmented Reality, as we explore in this paper. Whereas previous work assumed similarity between their respective Virtual and Augmented Environment (VE and AE), we explore the case in which VE and AE are distinct. This gives rise to novel and critical problems, such as how to visualise and interact with the other environment. In this context we investigate the fundamental interaction of transitioning an object across environments, to which we contribute five interaction techniques. Two are inspired by literature: Virtual Magic Lens and Binary Transition; while the other three are entirely novel: Auto Blended Space, Manual Blended Space - Button Transition and Manual Blended Space - Touch Transition. In a study evaluating the first four techniques, we found that participants (N=20) performed a CR object manipulation and transition task significantly faster using our Auto Blended Space technique. We then modified Manual Blended Space - Button Transition into Manual Blended Space - Touch Transition in response to these results, and reassessed the four techniques in a more complex object manipulation task (N=16). We found that this type of task was better suited to manual transition methods rather than automatic methods. Taken together, our final contribution are five blended space design factors, and timely Cross-Reality transition design guidelines.","CR(Cross-Reality)은 본 백서에서 살펴본 것처럼 가상 현실과 증강 현실과 같은 다양한 양식과 몰입 수준 간의 상호 작용을 포함합니다. 이전 연구에서는 각각의 가상 환경과 증강 환경(VE 및 AE) 간의 유사성을 가정한 반면, 우리는 VE와 AE가 구별되는 사례를 탐색합니다. 이로 인해 다른 환경을 시각화하고 상호 작용하는 방법과 같은 새롭고 중요한 문제가 발생합니다. 이러한 맥락에서 우리는 환경 전반에 걸쳐 객체를 전환하는 기본적인 상호 작용을 조사하고 여기에 5가지 상호 작용 기술을 제공합니다. 두 가지는 문헌에서 영감을 얻었습니다: Virtual Magic Lens와 Binary Transition; 나머지 세 가지는 완전히 새로운 것입니다. 자동 혼합 공간, 수동 혼합 공간 - 버튼 전환 및 수동 혼합 공간 - 터치 전환. 처음 4가지 기술을 평가한 연구에서 우리는 참가자(N=20)가 Auto Blended Space 기술을 사용하여 CR 개체 조작 및 전환 작업을 훨씬 더 빠르게 수행한다는 사실을 발견했습니다. 그런 다음 이러한 결과에 따라 수동 혼합 공간 - 버튼 전환을 수동 혼합 공간 - 터치 전환으로 수정하고 보다 복잡한 개체 조작 작업(N=16)에서 네 가지 기술을 재평가했습니다. 우리는 이러한 유형의 작업이 자동 방법보다는 수동 전환 방법에 더 적합하다는 것을 발견했습니다. 종합하면, 우리의 최종 기여는 다섯 가지 혼합 공간 디자인 요소와 시기적절한 교차 현실 전환 디자인 지침입니다.",https://doi.org/10.1109/ISMAR55827.2022.00069,Interaction & Input,Other,User Study,Algorithm / Method
428,2022,Bridging the Gap Across Realities: Visual Transitions Between Virtual and Augmented Reality,현실 간의 격차 해소: 가상 현실과 증강 현실 간의 시각적 전환,"Cross-Virtuality applications enabling users to move between different stages of Milgram’s reality-virtuality continuum are a rapidly growing field of research. Modern video see-through head-mounted displays allow users to switch between augmented and virtual reality without removing the headset. This enables for the first time a fluent transition between augmented and virtual reality. Based on insights from literature and preliminary experiments we designed and implemented four transitions: Fade, SimpleCut, TeleportBeam and Portal. These techniques were expected to represent the best suitable concepts for transitioning seamlessly between augmented and virtual reality. After incorporating results from a pre-study, the transition techniques were evaluated in a qualitative user study regarding user experience, simulator sickness, continuity and applicability. Participants were able to freely move between both realities during the study in an immersive analytics scenario for logistics data. In the user study, users preferred Fade in a workplace setting due to its efficiency and simplicity when transitioning frequently between realities. The Portal technique was deemed visually exciting and suitable for infrequent transitions between realities that differ greatly.","사용자가 Milgram의 현실-가상 연속체의 여러 단계 사이를 이동할 수 있도록 하는 교차 가상 애플리케이션은 빠르게 성장하는 연구 분야입니다. 최신 비디오 시스루 헤드 장착형 디스플레이를 통해 사용자는 헤드셋을 벗지 않고도 증강 현실과 가상 현실 사이를 전환할 수 있습니다. 이를 통해 처음으로 증강 현실과 가상 현실 간의 원활한 전환이 가능해졌습니다. 문헌과 예비 실험에서 얻은 통찰력을 바탕으로 우리는 Fade, SimpleCut, TeleportBeam 및 Portal의 네 가지 전환을 설계하고 구현했습니다. 이러한 기술은 증강 현실과 가상 현실 사이를 원활하게 전환하는 데 가장 적합한 개념을 나타낼 것으로 예상되었습니다. 사전 연구 결과를 통합한 후 사용자 경험, 시뮬레이터 멀미, 연속성 및 적용 가능성에 관한 질적 사용자 연구에서 전환 기술을 평가했습니다. 참가자들은 물류 데이터에 대한 몰입형 분석 시나리오에서 연구 중에 두 현실 사이를 자유롭게 이동할 수 있었습니다. 사용자 연구에서 사용자는 현실 사이를 자주 전환할 때 효율성과 단순성으로 인해 작업장 설정에서 페이드를 선호했습니다. 포털 기술은 시각적으로 흥미롭고 크게 다른 현실 간의 간헐적인 전환에 적합한 것으로 간주되었습니다.",https://doi.org/10.1109/ISMAR55827.2022.00101,Display & Optics,Sensor Fusion,User Study,Algorithm / Method
429,2022,CardsVR: A Two-Person VR Experience with Passive Haptic Feedback from a Deck of Playing Cards,CardsVR: 카드 놀이에서 나오는 수동적 햅틱 피드백을 통한 2인용 VR 경험,"Presence in virtual reality (VR) is meaningful for remotely connecting with others and facilitating social interactions despite great distance while providing a sense of “being there.” This work presents CardsVR, a two-person VR experience that allows remote participants to play a game of cards together. An entire deck of tracked cards are used to recreate the sense of playing cards in-person. Prior work in VR commonly provides passive haptic feedback either through a single object or through static objects in the environment. CardsVR is novel in providing passive haptic feedback through multiple cards that are individually tracked and represented in the virtual environment. Participants interact with the physical cards by picking them up, holding them, playing them, or moving them on the physical table. Our participant study (N=23) shows that passive haptic feedback provides significant improvement in three standard measures of presence: Possibility to Act, Realism, and Haptics.","가상 현실(VR)에서의 존재감은 먼 거리에도 불구하고 다른 사람들과 원격으로 연결하고 사회적 상호 작용을 촉진하는 동시에 '그 곳에 있는' 느낌을 제공한다는 점에서 의미가 있습니다. 이 작품은 원격 참가자들이 함께 카드 게임을 할 수 있는 2인 VR 경험인 CardsVR을 선보입니다. 추적된 카드의 전체 덱은 직접 카드를 사용하는 느낌을 재현하는 데 사용됩니다. VR의 이전 작업은 일반적으로 단일 객체 또는 환경의 정적 객체를 통해 수동적인 햅틱 피드백을 제공합니다. CardsVR은 가상 환경에서 개별적으로 추적되고 표시되는 여러 카드를 통해 수동적 촉각 피드백을 제공한다는 점에서 참신합니다. 참가자는 실제 카드를 집어 들고, 들고, 플레이하고, 실제 테이블 위에서 이동함으로써 실제 카드와 상호 작용합니다. 우리의 참가자 연구(N=23)는 수동적 햅틱 피드백이 존재에 대한 세 가지 표준 측정, 즉 행동 가능성, 현실감, 햅틱에서 상당한 개선을 제공한다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR55827.2022.00070,Interaction & Input; Perception & Cognition,Haptic / Tactile Feedback,User Study,Other
430,2022,CleAR Sight: Exploring the Potential of Interacting with Transparent Tablets in Augmented Reality,Clear Sight: 증강 현실에서 투명 태블릿과 상호 작용할 수 있는 가능성 탐색,"In this paper, we examine the potential of incorporating transparent, handheld devices into head-mounted Augmented Reality (AR). Additional mobile devices have long been successfully used in head-mounted AR, but they obscure the visual context and real world objects during interaction. Transparent tangible displays can address this problem, using either transparent OLED screens or rendering by the head-mounted display itself. However, so far, there is no systematic analysis of the use of such transparent tablets in combination with AR head-mounted displays (HMDs), with respect to their benefits and arising challenges. We address this gap by introducing a research platform based on a touch-enabled, transparent interaction panel, for which we present our custom hardware design and software stack in detail. Furthermore, we developed a series of interaction concepts for this platform and demonstrate them in the context of three use case scenarios: the exploration of 3D volumetric data, collaborative visual data analysis, and the control of smart home appliances. We validate the feasibility of our concepts with interactive prototypes that we used to elicit feedback from HCI experts. As a result, we contribute to a better understanding of how transparent tablets can be integrated into future AR environments.","본 논문에서는 투명한 휴대용 장치를 머리에 장착하는 증강 현실(AR)에 통합할 수 있는 가능성을 조사합니다. 추가 모바일 장치는 오랫동안 머리 장착형 AR에 성공적으로 사용되어 왔지만 상호 작용 중에 시각적 맥락과 실제 개체를 모호하게 만듭니다. 투명한 유형 디스플레이는 투명한 OLED 화면을 사용하거나 헤드 마운트 디스플레이 자체에 의한 렌더링을 사용하여 이 문제를 해결할 수 있습니다. 그러나 지금까지 이러한 투명 태블릿을 AR 헤드마운트디스플레이(HMD)와 함께 사용하는 것에 대한 이점과 문제점에 대한 체계적인 분석은 없습니다. 우리는 맞춤형 하드웨어 디자인과 소프트웨어 스택을 자세히 제시하는 터치 지원 투명 상호 작용 패널을 기반으로 한 연구 플랫폼을 도입하여 이러한 격차를 해소합니다. 또한 우리는 이 플랫폼에 대한 일련의 상호 작용 개념을 개발하고 3D 체적 데이터 탐색, 협업적 시각적 데이터 분석 및 스마트 가전 제품 제어라는 세 가지 사용 사례 시나리오의 맥락에서 이를 시연합니다. 우리는 HCI 전문가로부터 피드백을 이끌어내기 위해 사용한 대화형 프로토타입을 통해 우리 개념의 타당성을 검증합니다. 결과적으로 우리는 투명한 태블릿이 미래 AR 환경에 어떻게 통합될 수 있는지 더 잘 이해하는 데 기여합니다.",https://doi.org/10.1109/ISMAR55827.2022.00034,Interaction & Input; Display & Optics,Optical / Display Technology,Case Study / Application Demo,System / Framework; User Study / Empirical Findings
431,2022,Cognitive load Classification with a Stroop task in Virtual Reality based on Physiological data,생리학적 데이터를 기반으로 한 가상 현실의 Stroop 작업을 통한 인지 부하 분류,"Cognitive load (CL) when using Virtual Reality (VR) requires more experimental inputs, especially to determine how VR affects human psychophysiology depending on the task. Classifying humans’ physiological variations in a controlled setup is essential. We randomly assigned 92 participants to three experimental conditions: control, stereoscopy, and dual-task. Participants fulfilled a rest-baseline period, a Stroop task (25 congruent, 25 incongruent words), and a NASA-TLX questionnaire. We recorded behavioral and physiological data from eye-tracking(ET), electrocardiogram(ECG), and electrodermal activity (EDA). NASA-TLX scores of control and stereoscopy were statistically different with dual-task conditions. We used NASA-TLX scores to create three classes and train a CL classifier based on physiological variations. We deployed linear models penalized with the L1 norm to select the most relevant features correlated with subjective CL levels. The ECG sensor provided the most selected features compared to EDA and ET. We compared SVM, Logistic Regression, and Gradient boosting classifier models. The Gradient boosting method with 87.23% accuracy and an 87.13% F1 score is the most performant. Future works will try to compare such an approach with stressful stimuli.","가상 현실(VR)을 사용할 때 인지 부하(CL)에는 특히 VR이 작업에 따라 인간의 정신 생리학에 어떤 영향을 미치는지 확인하기 위해 더 많은 실험적 입력이 필요합니다. 통제된 설정에서 인간의 생리학적 변화를 분류하는 것은 필수적입니다. 우리는 92명의 참가자를 제어, 입체 관찰 및 이중 작업의 세 가지 실험 조건에 무작위로 할당했습니다. 참가자들은 휴식 기준 기간, Stroop 작업(합치 단어 25개, 부조화 단어 25개) 및 NASA-TLX 설문지를 완료했습니다. 우리는 안구 추적(ET), 심전도(ECG) 및 피부 전기 활동(EDA)을 통해 행동 및 생리학적 데이터를 기록했습니다. NASA-TLX의 제어 및 입체경 점수는 이중 작업 조건에 따라 통계적으로 달랐습니다. 우리는 NASA-TLX 점수를 사용하여 세 가지 클래스를 만들고 생리학적 변화를 기반으로 CL 분류기를 훈련했습니다. 우리는 주관적 CL 수준과 상관관계가 있는 가장 관련성이 높은 기능을 선택하기 위해 L1 표준으로 불이익을 받은 선형 모델을 배포했습니다. ECG 센서는 EDA 및 ET에 비해 가장 많이 선택된 기능을 제공했습니다. 우리는 SVM, 로지스틱 회귀, Gradient Boosting Classifier 모델을 비교했습니다. 87.23%의 정확도와 87.13%의 F1 점수를 지닌 Gradient Boosting 방법이 가장 성능이 좋습니다. 향후 연구에서는 이러한 접근 방식을 스트레스 자극과 비교하려고 노력할 것입니다.",https://doi.org/10.1109/ISMAR55827.2022.00083,Interaction & Input,Sensor Fusion,Questionnaire / Survey; Technical Evaluation,Algorithm / Method
432,2022,"ComforTable User Interfaces: Surfaces Reduce Input Error, Time, and Exertion for Tabletop and Mid-air User Interfaces","ComforTable 사용자 인터페이스: 표면은 테이블탑 및 공중 사용자 인터페이스의 입력 오류, 시간 및 노력을 줄입니다.","Real-world work-spaces typically revolve around tables, which enable knowledge workers to comfortably perform tasks over an extended period of time during productivity tasks. Tables afford more ergonomic postures and provide opportunities for rest, which raises the question of whether they may also benefit prolonged interaction in Virtual Reality (VR). In this paper, we investigate the effects of tabletop surface presence in situated VR settings on task performance, behavior, and subjective experience. In an empirical study, 24 participants performed two tasks (selection, docking) on virtual interfaces placed at two distances and two orientations. Our results show that a physical tabletop inside VR improves comfort, agency, and task performance while decreasing physical exertion and strain of the neck, shoulder, elbow, and wrist, assessed through objective metrics and subjective reporting. Notably, we found that these benefits apply when the UI is placed on and aligned with the table itself as well as when it is positioned vertically in mid-air above it. Our experiment therefore provides empirical evidence for integrating physical table surfaces into VR scenarios to enable and support prolonged interaction. We conclude by discussing the effective usage of surfaces in situated VR experiences and provide initial guidelines.","실제 작업 공간은 일반적으로 테이블을 중심으로 회전하므로 지식 근로자가 생산성 작업 중에 장시간 편안하게 작업을 수행할 수 있습니다. 테이블은 보다 인체공학적인 자세를 제공하고 휴식 기회를 제공하므로 가상 현실(VR)에서 장기간 상호 작용하는 데 도움이 될 수 있는지에 대한 의문이 제기됩니다. 본 논문에서는 상황에 맞는 VR 설정에서 탁상 표면 존재가 작업 수행, 행동 및 주관적 경험에 미치는 영향을 조사합니다. 실증적 연구에서 24명의 참가자는 두 가지 거리와 두 가지 방향에 배치된 가상 인터페이스에서 두 가지 작업(선택, 도킹)을 수행했습니다. 우리의 결과는 객관적인 지표와 주관적인 보고를 통해 평가된 VR 내부의 물리적 테이블탑이 목, 어깨, 팔꿈치 및 손목의 신체 활동과 긴장을 줄이는 동시에 편안함, 선택 의지 및 작업 성능을 향상시키는 것으로 나타났습니다. 특히 UI가 테이블 자체에 배치되고 정렬될 때뿐만 아니라 UI가 테이블 위 공중에 수직으로 배치될 때에도 이러한 이점이 적용된다는 사실을 발견했습니다. 따라서 우리의 실험은 물리적 테이블 표면을 VR 시나리오에 통합하여 장기적인 상호 작용을 가능하게 하고 지원하는 경험적 증거를 제공합니다. 우리는 상황에 맞는 VR 경험에서 표면의 효과적인 사용에 대해 논의하고 초기 지침을 제공함으로써 결론을 내립니다.",https://doi.org/10.1109/ISMAR55827.2022.00029,Interaction & Input,Optical / Display Technology,User Study,User Study / Empirical Findings
433,2022,Comparing Gaze-Supported Modalities with Empathic Mixed Reality Interfaces in Remote Collaboration,원격 협업에서 시선 지원 방식과 공감 혼합 현실 인터페이스 비교,"In this paper, we share real-time collaborative gaze behaviours, hand pointing, gesturing, and heart rate visualisations between remote collaborators using a live 360 ° panoramic-video based Mixed Reality (MR) system. We first ran a pilot study to explore visual designs to combine communication cues with biofeedback (heart rate), aiming to understand user perceptions of empathic collaboration. We then conducted a formal study to investigate the effect of modality (Gaze+Hand, Hand-only) and interface (Near-Gaze, Embodied). The results show that the Gaze+Hand modality in a Near-Gaze interface is significantly better at reducing task load, improving co-presence, enhancing understanding and tightening collaborative behaviours compared to the conventional Embodied hand-only experience. Ranked as the most preferred condition, the Gaze+Hand in Near-Gaze condition is perceived to reduce the need for dividing attention to the collaborator’s physical location, although it feels slightly less natural compared to the embodied visualisations. In addition, the Gaze+Hand conditions also led to more joint attention and less hand pointing to align mutual understanding. Lastly, we provide a design guideline to summarize what we have learned from the studies on the representation between modality, interface, and biofeedback.","이 논문에서는 라이브 360° 파노라마 비디오 기반 혼합 현실(MR) 시스템을 사용하여 원격 공동 작업자 간의 실시간 공동 시선 동작, 손 포인팅, 몸짓 및 심박수 시각화를 공유합니다. 우리는 먼저 공감적 협업에 대한 사용자 인식을 이해하기 위해 의사소통 단서와 바이오피드백(심박수)을 결합하는 시각적 디자인을 탐색하기 위한 파일럿 연구를 실행했습니다. 그런 다음 양식(Gaze+Hand, Hand-only)과 인터페이스(Near-Gaze, Embodied)의 효과를 조사하기 위해 정식 연구를 수행했습니다. 결과는 Near-Gaze ​​인터페이스의 Gaze+Hand 양식이 기존의 Embodied Hand Only 경험에 비해 작업 부하를 줄이고, 공동 존재를 개선하고, 이해를 향상하고, 협업 동작을 강화하는 데 훨씬 더 우수하다는 것을 보여줍니다. 가장 선호되는 조건으로 평가되는 Near-Gaze ​​조건의 Gaze+Hand는 구현된 시각화에 비해 약간 덜 자연스럽게 느껴지지만 공동 작업자의 물리적 위치에 주의를 분산할 필요성을 줄이는 것으로 인식됩니다. 또한 Gaze+Hand 조건은 상호 이해를 일치시키기 위해 더 많은 공동 관심과 더 적은 손 가리키기로 이어졌습니다. 마지막으로, 양식, 인터페이스 및 바이오피드백 간의 표현에 대한 연구를 통해 배운 내용을 요약하는 설계 지침을 제공합니다.",https://doi.org/10.1109/ISMAR55827.2022.00102,Collaboration & Social; Interaction & Input,Optical / Display Technology,Case Study / Application Demo,User Study / Empirical Findings
434,2022,Comparing the Fidelity of Contemporary Pointing with Controller Interactions on Performance of Personal Space Target Selection,개인 공간 대상 선택 성능에 대한 컨트롤러 상호 작용과 현대 포인팅의 충실도 비교,"The goal of this research is to provide much needed empirical data on how the fidelity of popular hand gesture tracked based pointing metaphors versus commodity controller based input affects the efficiency and speed-accuracy tradeoff in users’ spatial selection in personal space interactions in VR. We conduct two experiments in which participants select spherical targets arranged in a circle in personal space, or near-field within their maximum arms reach distance, in VR. Both experiments required participants to select the targets with either a VR controller or with their dominant hand’s index finger, which was tracked with one of two popular contemporary tracking methods. In the first experiment, the targets are arranged in a flat circle in accordance with the ISO 9241-9 Fitts’ law standard, and the simulation selected random combinations of 3 target amplitudes and 3 target widths. Targets were placed centered around the users’ eye level, and the arrangement was placed at either 60%, 75%, or 90% depth plane of the users’ maximum arm’s reach. In experiment 2, the targets varied in depth randomly from one depth plane to another within the same configuration of 13 targets within a trial set, which resembled button selection task in hierarchical menus in differing depth planes in the near-field. The study was conducted using the HTC Vive head-mounted display, and used either a VR controller (HTC Vive), low-fidelity virtual pointing (Leap Motion), or a high-fidelity virtual pointing (tracked VR glove) conditions. Our results revealed that low-fidelity pointing performed worse than both high-fidelity pointing and the VR controller. Overall, target selection performance was found to be worse in depth planes closer to the maximum arms reach, as compared to middle and nearer distances.","이 연구의 목표는 인기 있는 손 제스처 추적 기반 포인팅 은유 대 상품 컨트롤러 기반 입력의 충실도가 VR의 개인 공간 상호 작용에서 사용자 공간 선택의 효율성과 속도-정확도 균형에 어떻게 영향을 미치는지에 대해 매우 필요한 경험적 데이터를 제공하는 것입니다. 우리는 참가자들이 VR에서 개인 공간 또는 최대 팔 도달 거리 내의 근거리에서 원형으로 배열된 구형 타겟을 선택하는 두 가지 실험을 수행합니다. 두 실험 모두 참가자는 VR 컨트롤러 또는 주로 사용하는 손의 검지로 대상을 선택해야 했으며, 이는 두 가지 인기 있는 현대 추적 방법 중 하나로 추적되었습니다. 첫 번째 실험에서는 ISO 9241-9 Fitts의 법칙 표준에 따라 타겟을 평평한 원으로 배열하고 시뮬레이션에서는 3개의 타겟 진폭과 3개의 타겟 너비의 무작위 조합을 선택했습니다. 타겟은 사용자의 눈높이를 중심으로 배치되었으며, 배치는 사용자의 팔 최대 도달 거리의 60%, 75%, 90% 깊이 평면에 배치되었습니다. 실험 2에서 타겟은 시험 세트 내의 13개 타겟의 동일한 구성 내에서 한 깊이 평면에서 다른 깊이 평면으로 무작위로 깊이가 다양했으며 이는 근거리의 서로 다른 깊이 평면의 계층적 메뉴에서 버튼 선택 작업과 유사했습니다. 이 연구는 HTC Vive 헤드 장착 디스플레이를 사용하여 수행되었으며 VR 컨트롤러(HTC Vive), 저충실도 가상 포인팅(Leap Motion) 또는 고충실도 가상 포인팅(추적 VR 장갑) 조건을 사용했습니다. 우리의 결과는 저충실도 포인팅이 고충실도 포인팅과 VR 컨트롤러보다 성능이 떨어지는 것으로 나타났습니다. 전반적으로 목표 선택 성능은 중거리 및 근거리 거리에 비해 최대 무기 도달 범위에 가까운 깊이 평면에서 더 나쁜 것으로 나타났습니다.",https://doi.org/10.1109/ISMAR55827.2022.00056,Interaction & Input; Display & Optics,Sensor Fusion,User Study,Algorithm / Method
435,2022,Complex Virtual Environments on Thin VR Systems Through Continuous Near-Far Partitioning,지속적인 근거리-원거리 분할을 통한 얇은 VR 시스템의 복잡한 가상 환경,"This paper describes a method for reducing rendering load such that complex virtual environments (VEs) can be deployed on “thin” VR systems with limited rendering power. The method partitions the VE into four regions: a near region, an intermediate region, a stationary far region, and a dynamic far region. The stationary far region is replaced with an environment map, which brings a substantial rendering load reduction. The other three regions are rendered from geometry: the near region is rendered from the user viewpoint, the dynamic far geometry is rendered from the center of the environment map, and the intermediate region is rendered with a morph that switches viewpoint gradually from the user viewpoint to the center of the environment map. The intermediate region connects the near and far regions seamlessly. Furthermore, the environment map is enhanced with per pixel range which allows depth compositing the dynamic and stationary far geometry. An IRB approved user study (N = 22) found significant advantages for our method over conventional near-far partitioning.","이 문서에서는 렌더링 성능이 제한된 ""얇은"" VR 시스템에 복잡한 가상 환경(VE)을 배포할 수 있도록 렌더링 부하를 줄이는 방법을 설명합니다. 이 방법은 VE를 근거리 영역, 중간 영역, 고정 원거리 영역 및 동적 원거리 영역의 4개 영역으로 분할합니다. 고정된 원거리 영역은 환경 맵으로 대체되어 렌더링 부하가 크게 줄어듭니다. 다른 세 영역은 기하학에서 렌더링됩니다. 가까운 영역은 사용자 관점에서 렌더링되고, 동적 원거리 기하학은 환경 맵의 중심에서 렌더링되며, 중간 영역은 사용자 관점에서 환경 맵의 중심으로 시점을 점진적으로 전환하는 모프를 사용하여 렌더링됩니다. 중간 영역은 근거리 영역과 원거리 영역을 원활하게 연결합니다. 또한 환경 맵은 픽셀당 범위로 향상되어 동적 및 고정 원거리 형상을 깊이 합성할 수 있습니다. IRB가 ​​승인한 사용자 연구(N = 22)에서는 기존의 근거리 분할에 비해 우리 방법이 상당한 이점을 가지고 있음을 발견했습니다.",https://doi.org/10.1109/ISMAR55827.2022.00017,Rendering & Visualization,Other,User Study,System / Framework
436,2022,Defuse the Training of Risky Tasks: Collaborative Training in XR,위험한 작업 교육 완화: XR의 협업 교육,"Extensive training is crucial but challenging in certain areas such as explosive ordnance disposal. Past conflicts have shown that not only military personal but also civilians have to learn how to disarm unexploded ordnance. The preparation for dangerous situations is difficult and limited in the real world. Extended reality (XR) offers new possibilities to enhance the training of explosive ordnance disposal experts due to its immersive capabilities. This paper presents a comparative study (n = 75) of three distinct training methods: 1) Real-world (Real)-Training with a tangible replica object, 2) Virtual Reality (VR)-Training with a non-see-through Head-Mounted-Display (HMD), and 3) Mixed Reality (MR)-Training in a Cave Automatic Virtual Environment (CAVE). All training methods are collaborative, i.e., an instructor teaches the training content to a participant in the real or virtual world. We evaluate the suitability of these approaches in terms of usability, cognitive workload, training motivation, and training success. Our results indicate that the virtual methods, VR-Training and MR-Training, provide significantly superior results in the evaluated aspects compared to the real-world training. These results can also be applied to other collaborative training methods, as the training concept of this use case was non-specific. Therefore, these virtual technologies can increase the safety of explosive ordnance disposal personnel, and we recommend establishing this in future training.","광범위한 훈련은 중요하지만 폭발물 처리와 같은 특정 분야에서는 어려운 일입니다. 과거의 분쟁을 통해 군인뿐만 아니라 민간인도 불발탄을 해제하는 방법을 배워야 한다는 사실이 드러났습니다. 현실 세계에서는 위험한 상황에 대비하는 것이 어렵고 제한적입니다. 확장 현실(XR)은 몰입형 기능으로 인해 폭발물 처리 전문가의 교육을 향상할 수 있는 새로운 가능성을 제공합니다. 이 논문에서는 1) 실제 세계(실제)-유형 복제 개체를 사용한 훈련, 2) 가상 현실(VR)-투시형 헤드 장착 디스플레이(HMD)를 사용한 훈련, 3) 혼합 현실(MR)-동굴 자동 가상 환경(CAVE)에서의 훈련의 세 가지 개별 훈련 방법에 대한 비교 연구(n = 75)를 제시합니다. 모든 교육 방법은 협력적입니다. 즉, 강사가 실제 또는 가상 세계의 참가자에게 교육 콘텐츠를 가르칩니다. 우리는 유용성, 인지 작업량, 훈련 동기 부여 및 훈련 성공 측면에서 이러한 접근 방식의 적합성을 평가합니다. 우리의 결과는 가상 방법인 VR-Training과 MR-Training이 실제 훈련과 비교하여 평가 측면에서 훨씬 우수한 결과를 제공한다는 것을 나타냅니다. 이 사용 사례의 훈련 개념은 구체적이지 않았기 때문에 이러한 결과는 다른 협업 훈련 방법에도 적용될 수 있습니다. 따라서 이러한 가상 기술은 폭발물 처리 인력의 안전성을 높일 수 있으며 향후 교육에서 이를 구축하는 것이 좋습니다.",https://doi.org/10.1109/ISMAR55827.2022.00087,Display & Optics; Interaction & Input,Other,Technical Evaluation,Algorithm / Method
437,2022,Demographic and Behavioral Correlates of Cybersickness: A Large Lab-in-the-Field Study of 837 Participants,사이버 질병의 인구통계학적 및 행동적 상관관계: 837명의 참가자를 대상으로 한 대규모 현장 연구실,"ybersickness has been one of the main impediments to the widespread adoption of Virtual Reality for decades. It has been argued that several factors can influence the occurrence of cybersickness, such as technical factors, interaction design, but also users’ demographics and their perceived presence. Yet, previous studies had comparably small sample sizes and demographically homogeneous samples; comparisons across studies (e.g., regarding demographic factors) are challenging due to the large variation in the studied virtual environments. In this paper, we address these limitations and report the results of a lab-in-the-field experiment on cybersickness with a large and heterogeneous sample of $N =837$ participants who navigated and interacted inside a virtual environment (ages 18–80, $M = 29.34, SD = 9.50$, 431 males, 400 females, 6 non-binaries and other). We found that female participants and participants with lower VR experience were more susceptible to experiencing higher levels of cybersickness. Participants’ cybersickness levels increased with the time spent in VR and with the distance traversed in the virtual world up to a point, above which reported levels declined. We also found a link between higher levels of cybersickness and reduced head motion, as well as between lower levels of cybersickness and more head motion, which led them to explore more of the virtual environment. In contrast to past studies, we did not find any evidence suggesting an effect of age on cybersickness, nor a negative correlation between presence and cybersickness. Based on our results, we derived a model that achieves a mean classification accuracy of 67.1% for two levels of cybersickness using demographic, user experience, and behavioral data in VR.","ybersickness는 수십 년 동안 가상 현실의 광범위한 채택을 가로막는 주요 장애물 중 하나였습니다. 기술적인 요인, 상호 작용 디자인뿐만 아니라 사용자의 인구 통계 및 인지된 존재와 같은 여러 요인이 사이버 멀미의 발생에 영향을 미칠 수 있다고 주장되어 왔습니다. 그러나 이전 연구에서는 표본 크기가 비교적 작고 인구통계학적으로 동질적인 표본이 있었습니다. 연구된 가상 환경의 큰 변화로 인해 연구 간 비교(예: 인구통계학적 요인 관련)가 어렵습니다. 본 논문에서는 이러한 제한 사항을 해결하고 가상 환경(18~80세, $M = 29.34, SD = 9.50$, 남성 431명, 여성 400명, 비바이너리 6명 등) 내에서 탐색하고 상호 작용한 $N =837$ 참가자의 대규모 이질적 샘플을 사용하여 사이버 멀미에 대한 현장 실험 결과를 보고합니다. 우리는 여성 참가자와 VR 경험이 낮은 참가자가 더 높은 수준의 사이버 멀미를 경험할 가능성이 더 높다는 것을 발견했습니다. 참가자의 사이버 멀미 수준은 VR에서 보낸 시간과 가상 세계에서 이동한 거리가 특정 지점까지 증가함에 따라 증가했으며, 그 이상에서는 보고된 수준이 감소했습니다. 우리는 또한 높은 수준의 사이버 멀미와 감소된 머리 움직임 사이의 연관성뿐만 아니라 낮은 수준의 사이버 멀미와 더 많은 머리 움직임 사이의 연관성을 발견하여 가상 환경을 더 많이 탐색하게 되었습니다. 과거 연구와 달리 우리는 나이가 사이버 멀미에 미치는 영향이나 현재 상태와 사이버 멀미 사이에 부정적인 상관 관계를 암시하는 증거를 찾지 못했습니다. 우리의 결과를 바탕으로 VR의 인구 통계, 사용자 경험 및 행동 데이터를 사용하여 두 가지 수준의 사이버 멀미에 대해 평균 분류 정확도 67.1%를 달성하는 모델을 도출했습니다.",https://doi.org/10.1109/ISMAR55827.2022.00046,Perception & Cognition,Optical / Display Technology,User Study,Algorithm / Method
438,2022,Distant Object Manipulation with Adaptive Gains in Virtual Reality,가상 현실에서 적응형 이득을 이용한 원거리 객체 조작,"Object Manipulation is a fundamental interaction in virtual reality (VR). The efficiency and accuracy of object manipulation are important to provide immersion to users. We propose a manipulation method with adaptive gains to improve the efficiency and accuracy of object manipulation in VR applications. First, we introduce manipulation gains. We then design an experiment to collect user behavior during manipulation to determine fitting functions for calculating manipulation gain. At last, we design a user study to evaluate the performance of our distant object manipulation method with adaptive gains. The results show that, compared with the state of the art methods, our method has a significant improvement in the completion time, and the manipulation accuracy of the tasks. Moreover, our method significantly increases usability and reduces task load.","객체 조작(Object Manipulation)은 가상 현실(VR)의 기본적인 상호 작용입니다. 사용자에게 몰입감을 주기 위해서는 객체 조작의 효율성과 정확성이 중요합니다. VR 애플리케이션에서 객체 조작의 효율성과 정확성을 향상시키기 위해 적응형 이득을 갖춘 조작 방법을 제안합니다. 먼저 조작 이득을 소개합니다. 그런 다음 조작 이득 계산을 위한 피팅 함수를 결정하기 위해 조작 중 사용자 행동을 수집하는 실험을 설계합니다. 마지막으로 적응형 이득을 사용하여 원거리 객체 조작 방법의 성능을 평가하기 위한 사용자 연구를 설계합니다. 결과는 최신 방법과 비교하여 우리 방법이 완료 시간과 작업 조작 정확도가 크게 향상되었음을 보여줍니다. 또한, 우리의 방법은 유용성을 크게 높이고 작업 부하를 줄입니다.",https://doi.org/10.1109/ISMAR55827.2022.00092,Interaction & Input,Other,Quantitative Experiment,Algorithm / Method
439,2022,DroneARchery: Human-Drone Interaction through Augmented Reality with Haptic Feedback and Multi-UAV Collision Avoidance Driven by Deep Reinforcement Learning,DroneARchery: 심층 강화 학습을 통해 구동되는 햅틱 피드백 및 다중 UAV 충돌 방지 기능을 갖춘 증강 현실을 통한 인간-드론 상호 작용,"We propose a novel concept of augmented reality (AR) human-drone interaction driven by RL-based swarm behavior to achieve intuitive and immersive control of a swarm formation of unmanned aerial vehicles. The DroneARchery system developed by us allows the user to quickly deploy a swarm of drones, generating flight paths simulating archery. The haptic interface LinkGlide delivers a tactile stimulus of the bowstring tension to the forearm to increase the precision of aiming. The swarm of released drones dynamically avoids collisions between each other, the drone following the user, and external obstacles with behavior control based on deep reinforcement learning. The developed concept was tested in the scenario with a human, where the user shoots from a virtual bow with a real drone to hit the target. The human operator observes the ballistic trajectory of the drone in an AR and achieves a realistic and highly recognizable experience of the bowstring tension through the haptic display. The experimental results revealed that the system improves trajectory prediction accuracy by 63.3% through applying AR technology and conveying haptic feedback of pulling force. DroneARchery users highlighted the naturalness (4.3 out of 5 point Likert scale) and increased confidence (4.7 out of 5) when controlling the drone. We have designed the tactile patterns to present four sliding distances (tension) and three applied force levels (stiffness) of the haptic display. Users demonstrated the ability to distinguish tactile patterns produced by the haptic display representing varying bowstring tension(average recognition rate is of 72.8%) and stiffness (average recognition rate is of 94.2%). The novelty of the research is the development of an AR-based approach for drone control that does not require special skills and training from the operator. In the future, the proposed interaction can be applied in various fields, for example, for fast swarm deployment in search and rescue missions, crop monitoring, inspection and maintenance.","우리는 무인 항공기의 군집 형성에 대한 직관적이고 몰입적인 제어를 달성하기 위해 RL 기반 군집 행동에 의해 구동되는 증강 현실(AR) 인간-드론 상호 작용의 새로운 개념을 제안합니다. 우리가 개발한 DroneARchery 시스템을 사용하면 사용자는 드론 떼를 빠르게 배치하여 양궁을 시뮬레이션하는 비행 경로를 생성할 수 있습니다. 햅틱 인터페이스 LinkGlide는 활현 장력의 촉각 자극을 팔뚝에 전달하여 조준 정확도를 높입니다. 출시된 드론 떼는 심층 강화 학습 기반의 행동 제어를 통해 서로, 사용자를 따라가는 드론, 외부 장애물 간의 충돌을 동적으로 회피합니다. 개발된 개념은 사용자가 실제 드론으로 가상 활을 쏘아 목표물을 맞추는 인간 시나리오에서 테스트되었습니다. 인간 조작자는 AR에서 드론의 탄도 궤적을 관찰하고 햅틱 디스플레이를 통해 현의 장력을 현실적이고 인지도가 높은 경험으로 구현합니다. 실험 결과, AR 기술을 적용하고 당기는 힘에 대한 햅틱 피드백을 전달함으로써 궤적 예측 정확도가 63.3% 향상되는 것으로 나타났다. DroneARchery 사용자는 드론 제어 시 자연스러움(5점 만점 중 4.3점 Likert 척도)과 향상된 자신감(5점 만점 중 4.7점)을 강조했습니다. 우리는 촉각 디스플레이의 4가지 슬라이딩 거리(장력)와 3가지 적용 힘 수준(강성)을 나타내기 위해 촉각 패턴을 설계했습니다. 사용자는 다양한 현 장력(평균 인식률 72.8%)과 강성(평균 인식률 94.2%)을 나타내는 햅틱 디스플레이에서 생성된 촉각 패턴을 구별하는 능력을 시연했습니다. 연구의 참신함은 운영자의 특별한 기술과 교육이 필요하지 않은 드론 제어를 위한 AR 기반 접근 방식을 개발했다는 ​​것입니다. 앞으로 제안된 상호 작용은 수색 및 구조 임무에서 빠른 떼 배치, 작물 모니터링, 검사 및 유지 관리 등 다양한 분야에 적용될 수 있습니다.",https://doi.org/10.1109/ISMAR55827.2022.00042,Education & Training; Interaction & Input,Haptic / Tactile Feedback,Quantitative Experiment,Algorithm / Method
440,2022,EditAR: A Digital Twin Authoring Environment for Creation of AR/VR and Video Instructions from a Single Demonstration,EditAR: 단일 데모에서 AR/VR 및 비디오 지침을 생성하기 위한 디지털 트윈 제작 환경,"Augmented/Virtual reality and video-based media play a vital role in the digital learning revolution to train novices in spatial tasks. However, creating content for these different media requires expertise in several fields. We present EditAR, a unified authoring, and editing environment to create content for AR, VR, and video based on a single demonstration. EditAR captures the user’s interaction within an environment and creates a digital twin, enabling users without programming backgrounds to develop content. We conducted formative interviews with both subject and media experts to design the system. The prototype was developed and reviewed by experts. We also performed a user study comparing traditional video creation with 2D video creation from 3D recordings, via a 3D editor, which uses freehand interaction for in-headset editing. Users took 5 times less time to record instructions and preferred EditAR, along with giving significantly higher usability scores.","증강/가상 현실과 비디오 기반 미디어는 초보자에게 공간 작업을 교육하는 디지털 학습 혁명에서 중요한 역할을 합니다. 그러나 이러한 다양한 미디어에 대한 콘텐츠를 제작하려면 여러 분야의 전문 지식이 필요합니다. 단일 데모를 기반으로 AR, VR, 영상용 콘텐츠를 제작할 수 있는 통합 저작 및 편집 환경인 EditAR을 선보입니다. EditAR은 환경 내에서 사용자의 상호 작용을 캡처하고 디지털 트윈을 생성하여 프로그래밍 배경이 없는 사용자가 콘텐츠를 개발할 수 있도록 합니다. 우리는 시스템 설계를 위해 주제 전문가와 미디어 전문가 모두와 형성 인터뷰를 실시했습니다. 프로토타입은 전문가에 의해 개발되고 검토되었습니다. 또한 헤드셋 내 편집을 위해 자유로운 상호 작용을 사용하는 3D 편집기를 통해 기존 비디오 제작과 3D 녹화를 통한 2D 비디오 제작을 비교하는 사용자 연구를 수행했습니다. 사용자는 지침을 기록하는 데 5배 더 적은 시간을 소비했으며 EditAR을 선호했으며 사용성 점수도 상당히 높았습니다.",https://doi.org/10.1109/ISMAR55827.2022.00048,Interaction & Input; Education & Training,Other,Case Study / Application Demo,System / Framework
441,2022,Effects of User Construction Behavior on User Experience in a Virtual Indoor Environment,가상 실내 환경에서 사용자 구성 행위가 사용자 경험에 미치는 영향,"The metaverse elicits an extensive range of user curiosity by attractive virtual indoor environments, in which users can solve different tasks and conduct enormous collaborations. However, limited interaction methods are slowing down this development process, and the understanding of how such methods impact user experience is limited. To address this problem, we conducted a lab experiment with 150 valid participants to analyze the effect of user construction behavior on user satisfaction, perceived performance, and recommendation intention. By using the PROCESS 3.3 model, the results show that a higher level of participation facilitates satisfaction, which is a complete mediator that has a significant positive effect on perceived performance. Additionally, recommendation intention is positively affected by both satisfaction and perceived performance. Demographic factors could also generate significant impacts on user experience. We discuss the theoretical explanations, as well as design implications. Taken together, this study represents the first inquiry into the relationship between user construction behavior and user experience, thereby laying the groundwork for future interaction methods.","메타버스는 사용자가 다양한 작업을 해결하고 엄청난 협업을 수행할 수 있는 매력적인 가상 실내 환경을 통해 광범위한 사용자 호기심을 불러일으킵니다. 그러나 제한된 상호 작용 방법으로 인해 개발 프로세스가 느려지고 이러한 방법이 사용자 경험에 미치는 영향에 대한 이해가 제한됩니다. 이 문제를 해결하기 위해 우리는 150명의 유효 참가자를 대상으로 사용자 구성 행동이 사용자 만족도, 인지된 성능 및 추천 의도에 미치는 영향을 분석하기 위해 랩 실험을 수행했습니다. PROCESS 3.3 모델을 사용하면 참여 수준이 높을수록 만족도가 높아지는 것으로 나타났습니다. 이는 인지된 성과에 상당한 긍정적인 영향을 미치는 완전한 중재자입니다. 또한, 추천의도는 만족도와 인지된 성과 모두에 긍정적인 영향을 미칩니다. 인구통계학적 요인도 사용자 경험에 상당한 영향을 미칠 수 있습니다. 이론적인 설명과 디자인의 의미에 대해 논의합니다. 종합해보면, 이 연구는 사용자 구성 행동과 사용자 경험 사이의 관계에 대한 첫 번째 탐구를 의미하며, 이를 통해 미래 상호 작용 방법의 토대를 마련합니다.",https://doi.org/10.1109/ISMAR55827.2022.00073,Interaction & Input,Other,User Study,User Study / Empirical Findings; Algorithm / Method
442,2022,Efficient Special Character Entry on a Virtual Keyboard by Hand Gesture-Based Mode Switching,손 제스처 기반 모드 전환을 통한 가상 키보드의 효율적인 특수 문자 입력,"The need to support efficient text input in virtual reality continues to attract significant research attention. However, much of this research understandably focuses exclusively on core text input tasks involving the entry of standard alphabetic characters. The less common though still critical task of entering special characters is often ignored. In this paper we focus on this niche use case as chiefly encountered when entering passwords. Current commercial virtual keyboards allow users to switch between different layers of the keyboard in order to access capital letters, numerals and special characters by pressing an explicit mode-switch button. We propose a new method of switching between layers of a virtual keyboard using hand gestures. Critically, these hand gestures are seamlessly performed in conjunction with key selections to deliver an efficient and intuitive interaction. We report on a user study with 16 participants entering standard passwords comparing our gesture-based mode switching approach to a conventional button-based baseline. We find that with practice our proposed method results in significantly faster entry rates without any deterioration in accuracy. Feedback from users also indicates that our technique is considered efficient and comfortable to use.","가상 현실에서 효율적인 텍스트 입력을 지원해야 할 필요성은 계속해서 상당한 연구 관심을 끌고 있습니다. 그러나 이 연구의 대부분은 표준 알파벳 문자 입력과 관련된 핵심 텍스트 입력 작업에만 전적으로 초점을 맞추고 있습니다. 특수 문자를 입력하는 덜 일반적이지만 여전히 중요한 작업은 종종 무시됩니다. 이 백서에서는 비밀번호를 입력할 때 주로 발생하는 틈새 사용 사례에 중점을 둡니다. 현재 상용 가상 키보드를 사용하면 사용자는 명시적인 모드 전환 버튼을 눌러 대문자, 숫자 및 특수 문자에 액세스하기 위해 키보드의 여러 레이어 간에 전환할 수 있습니다. 손 제스처를 이용하여 가상 키보드의 레이어 간을 전환하는 새로운 방법을 제안합니다. 중요한 점은 이러한 손 제스처가 주요 선택과 함께 원활하게 수행되어 효율적이고 직관적인 상호 작용을 제공한다는 것입니다. 우리는 제스처 기반 모드 전환 접근 방식을 기존 버튼 기반 기준과 비교하여 표준 비밀번호를 입력하는 16명의 참가자를 대상으로 한 사용자 연구에 대해 보고합니다. 제안된 방법을 실제로 실행하면 정확도가 저하되지 않고 진입 속도가 훨씬 빨라진다는 것을 알 수 있습니다. 사용자의 피드백은 또한 우리 기술이 효율적이고 사용하기 편한 것으로 간주된다는 것을 나타냅니다.",https://doi.org/10.1109/ISMAR55827.2022.00105,Interaction & Input,Hand / Gesture Recognition,User Study,Algorithm / Method
443,2022,Enabling Customizable Workflows for Industrial AR Applications,산업용 AR 애플리케이션을 위한 맞춤형 워크플로 활성화,"Augmented Reality (AR) is increasingly considered for use in real industrial applications [8]. In our industrial research lab at Siemens Technology, we are continuously discussing suitable AR scenarios in business sectors involving power plants, wind turbine plants, and oil and gas factories. We have developed a company-internal AR system architecture, Hololayer, which provides AR facilities in a reusable manner to development engineers such that they can build their own AR applications for their specialized use cases. The integration of AR technology into industrial processes encounters many complex issues: we need to adhere to many safety, security and quality guarantees while also adapting quickly to the rapidly changing state of the art of AR devices (HMDs, tablets). To increase flexibility, it might be good to integrate Hololayer with one of the open frameworks that have recently been proposed [11], [29], [38]. Yet, care must be taken when converting an already existing, large company-owned framework like Hololayer to such platforms. Some of the proposed standardized APIs and communication protocols may be difficult to abide by, without requiring significant rewriting efforts or even violating company-internal regulations. In this paper, we report on our efforts to combine Hololayer with one such platform, Ubi-Interact [38]. This is a collaboration between Siemens technology and the FAR group at TU Munich. After exemplary descriptions of typical application scenarios, we present the underlying principles of Hololayer to support this range of applications. We then describe rudimentary concepts of Ubi-Interact, followed by an elaboration on the efforts to combine both systems for a selected application scenario and a discussion of the results achieved thus far.","증강 현실(AR)은 실제 산업 응용 분야에서의 사용이 점점 더 고려되고 있습니다[8]. Siemens Technology의 산업 연구소에서는 발전소, 풍력 터빈 플랜트, 석유 및 가스 공장과 같은 비즈니스 부문에 적합한 AR 시나리오를 지속적으로 논의하고 있습니다. 우리는 회사 내부 AR 시스템 아키텍처인 Hololayer를 개발했습니다. 이 아키텍처는 개발 엔지니어가 특수한 사용 사례에 맞게 자체 AR 애플리케이션을 구축할 수 있도록 재사용 가능한 방식으로 AR 시설을 제공합니다. AR 기술을 산업 프로세스에 통합하면 많은 복잡한 문제에 직면하게 됩니다. 우리는 많은 안전, 보안 및 품질 보장을 준수하는 동시에 급변하는 AR 장치(HMD, 태블릿)의 최신 기술에 빠르게 적응해야 합니다. 유연성을 높이려면 Hololayer를 최근 제안된 개방형 프레임워크 중 하나와 통합하는 것이 좋을 수 있습니다[11, 29, 38]. 그러나 Hololayer와 같은 기존의 대규모 회사 소유 프레임워크를 이러한 플랫폼으로 변환할 때는 주의를 기울여야 합니다. 제안된 표준화된 API 및 통신 프로토콜 중 일부는 상당한 다시 작성 노력이 필요하지 않거나 회사 내부 규정을 위반하지 않고도 준수하기 어려울 수 있습니다. 이 논문에서는 Hololayer를 Ubi-Interact 플랫폼 중 하나와 결합하려는 노력에 대해 보고합니다[38]. 이는 Siemens 기술과 TU 뮌헨의 FAR 그룹 간의 협력입니다. 일반적인 애플리케이션 시나리오에 대한 예시적인 설명을 마친 후 이러한 범위의 애플리케이션을 지원하는 Hololayer의 기본 원리를 제시합니다. 그런 다음 Ubi-Interact의 기본 개념을 설명하고 선택한 애플리케이션 시나리오에 대해 두 시스템을 결합하려는 노력에 대해 자세히 설명하고 지금까지 달성한 ​​결과에 대해 논의합니다.",https://doi.org/10.1109/ISMAR55827.2022.00079,Collaboration & Social,Other,Case Study / Application Demo,System / Framework
444,2022,Enhancing the Sense of Agency by Transitional Weight Control in Virtual Co-Embodiment,가상 공동 구현에서 과도기적 체중 제어를 통한 주체성 강화,"Virtual reality helps us learn complex motor skills by providing a situation in which we observe or follow a teacher’s movements from a first-person perspective. However, it has been suggested that if the learners themselves do not behave actively, their body schemes will not be updated and motor skills will be acquired temporarily, but will not be retained in the long term. As a solution to this problem, “co-embodiment” in which two people embody an avatar that reflects the weighted average of their movements was proposed, and it is shown that the user can feel an excessive sense of agency (SoA) even when their control weight is small. From the perspective of motor skill learning, the learner must feel as strong a SoA as possible while performing the exercise as close to the teacher as possible. Therefore, in this study, we propose a method to transitionally change the weights in a situation where co-embodiment is used, such that a strong SoA is felt despite the high weights of control by others. Considering the two-step account of the agency model, which states that the SoA is influenced by context, we tested the hypothesis that an initially strong SoA can maintain the SoA despite a gradually decreased control weight. The experimental results support this hypothesis, and it is expected that the proposed method will enhance the effectiveness of motor skill learning using co-embodiment.","가상현실은 교사의 움직임을 1인칭 시점에서 관찰하거나 따라가는 상황을 제공함으로써 우리가 복잡한 운동 능력을 배울 수 있도록 도와줍니다. 그러나 학습자 자신이 적극적으로 행동하지 않으면 신체 체계가 업데이트되지 않고 운동 기술이 일시적으로 습득되지만 장기적으로 유지되지 않을 것이라고 제안되었습니다. 이에 대한 해결책으로 두 사람의 움직임에 대한 가중평균이 반영된 아바타를 구현하는 '공동 구현'이 제안되었으며, 사용자는 자신의 제어 가중치가 작더라도 과도한 대리권(SoA)을 느낄 수 있는 것으로 나타났다. 운동 기술 학습의 관점에서 볼 때, 학습자는 교사와 최대한 가깝게 운동을 수행하면서 가능한 한 강한 SoA를 느껴야 합니다. 따라서 본 연구에서는 타인에 의한 통제의 가중치가 높음에도 불구하고 강한 SoA가 느껴지도록 공동 구현을 사용하는 상황에서 가중치를 과도적으로 변경하는 방법을 제안한다. SoA가 컨텍스트의 영향을 받는다는 에이전시 모델의 2단계 설명을 고려하여 처음에 강력한 SoA가 점차 감소하는 제어 가중치에도 불구하고 SoA를 유지할 수 있다는 가설을 테스트했습니다. 실험 결과는 이러한 가설을 뒷받침하며, 제안하는 방법이 공동 구현을 활용한 운동 기술 학습의 효율성을 높일 것으로 기대된다.",https://doi.org/10.1109/ISMAR55827.2022.00043,Perception & Cognition; Education & Training,Other,Other,Algorithm / Method
445,2022,Estimating the Just Noticeable Difference of Tactile Feedback in Oculus Quest 2 Controllers,Oculus Quest 2 컨트롤러에서 촉각 피드백의 눈에 띄는 차이 추정,"In virtual reality (VR) applications, humans experience the provided content not only through the visual and auditory systems but also through the somatosensory system. Thus, we decided to conduct a VR study to further explore the just noticeable difference (JND) of tactile feedback to understand humans’ perceptions of tactile stimuli. Our VR study examined the JND in terms of the intensity, duration, and frequency of tactile feedback provided through commercially available vibrotactile motion controllers, the Oculus Quest 2 controllers. We instructed participants to report whether they perceived a difference between a reference (variation) and a testing stimulus at each point in the experiment for a different property (intensity, duration, and frequency) of tactile feedback. We report both positive and negative JND values for the three properties of tactile feedback. We discuss our findings and limitations and provide directions for future studies regarding tactile perception for commercially available tactile feedback devices.","가상현실(VR) 애플리케이션에서 인간은 제공된 콘텐츠를 시각, 청각 시스템뿐만 아니라 체성감각 시스템을 통해서도 경험하게 됩니다. 따라서 우리는 촉각 자극에 대한 인간의 인식을 이해하기 위해 촉각 피드백의 JND(Just Notable Difference)를 더 자세히 조사하기 위해 VR 연구를 수행하기로 결정했습니다. 우리의 VR 연구에서는 상업적으로 이용 가능한 진동촉각 모션 컨트롤러인 Oculus Quest 2 컨트롤러를 통해 제공되는 촉각 피드백의 강도, 지속 시간 및 빈도 측면에서 JND를 조사했습니다. 우리는 참가자들에게 촉각 피드백의 다양한 속성(강도, 지속 시간 및 빈도)에 대한 실험의 각 지점에서 참조(변이)와 테스트 자극 간의 차이를 인식했는지 보고하도록 지시했습니다. 우리는 촉각 피드백의 세 가지 속성에 대해 양수 및 음수 JND 값을 모두 보고합니다. 우리는 우리의 연구 결과와 한계를 논의하고 상업적으로 이용 가능한 촉각 피드백 장치에 대한 촉각 인식에 관한 향후 연구에 대한 방향을 제공합니다.",https://doi.org/10.1109/ISMAR55827.2022.00013,Interaction & Input,Haptic / Tactile Feedback,User Study,Hardware / Device
446,2022,Evaluating the Benefits of Explicit and Semi-Automated Clusters for Immersive Sensemaking,몰입형 센스메이킹을 위한 명시적 및 반자동 클러스터의 이점 평가,"Immersive spaces have great potential to support analysts in complex sensemaking tasks, but the use of only manual interactions for organizing data elements can become tedious. We analyzed the user interactions to support cluster formation in an immersive sensemaking system, and we designed a semi-automated cluster creation technique that determines the user’s intent to create a cluster based on object proximity. We present the results of a user study comparing this proximity-based technique with a manual clustering technique and a baseline immersive workspace with no explicit clustering support. We found that semi-automated clustering was faster and preferred, while manual clustering gave greater control to users. These results provide support for the approach of adding intelligent semantic interactions to aid the users of immersive analytics systems.","몰입형 공간은 복잡한 감지 작업에서 분석가를 지원할 수 있는 큰 잠재력을 가지고 있지만 데이터 요소를 구성하기 위해 수동 상호 작용만 사용하는 것은 지루할 수 있습니다. 몰입형 센스메이킹 시스템에서 클러스터 형성을 지원하기 위해 사용자 상호 작용을 분석하고, 객체 근접성을 기반으로 클러스터를 생성하려는 사용자의 의도를 결정하는 반자동 클러스터 생성 기술을 설계했습니다. 우리는 이 근접성 기반 기술을 수동 클러스터링 기술 및 명시적인 클러스터링 지원이 없는 기본 몰입형 작업 공간과 비교하는 사용자 연구 결과를 제시합니다. 우리는 반자동 클러스터링이 더 빠르고 선호되는 반면, 수동 클러스터링은 사용자에게 더 큰 제어권을 제공한다는 사실을 발견했습니다. 이러한 결과는 몰입형 분석 시스템 사용자를 돕기 위해 지능형 의미론적 상호 작용을 추가하는 접근 방식을 지원합니다.",https://doi.org/10.1109/ISMAR55827.2022.00064,Interaction & Input,Deep Learning / Neural Networks,User Study,Algorithm / Method
447,2022,Evaluating the Object-Centered User Interface in Head-Worn Mixed Reality Environment,머리에 착용하는 혼합 현실 환경에서 객체 중심 사용자 인터페이스 평가,"User interface (UI) in head-worn mixed reality (MR) can be divided into two categories: user-centered and object-centered. Many studies have focused on user-centered UI, but few have explored object-centered UI. In this research, we explored the design of object-centered UI in head-worn MR environment. First, (b) we proposed three design considerations for object-centered UI in head-worn MR environment, and then verified them through two user studies with four UIs, where one is pure object-centered UI, one is pure user-centered UI, the others are hybrid UIs. In the first study, we conducted a user experience (UX) research on the access comfort, convenience and unobstructed sight degree of each UI, and demonstrated users’ preferences for the object-centered UI. In the second study, we designed a real application scenario to further evaluate the design considerations in a complex MR environment, and assessed its usability. We found positive user experience qualities of object-centered UI. Finally, we summarized four design recommendations, which could guide the UI design for the interaction with real-world objects in the future of everyday life with head-worn MR.","머리에 착용하는 혼합 현실(MR)의 사용자 인터페이스(UI)는 사용자 중심과 객체 중심의 두 가지 범주로 나눌 수 있습니다. 사용자 중심 UI에 초점을 맞춘 연구는 많지만 객체 중심 UI를 탐구한 연구는 거의 없습니다. 본 연구에서는 머리 착용형 MR 환경에서 객체 중심 UI 디자인을 탐색했습니다. 먼저, (b) 우리는 머리 착용형 MR 환경에서 객체 중심 UI에 대한 세 가지 설계 고려 사항을 제안한 후 순수 객체 중심 UI, 순수 사용자 중심 UI, 나머지는 하이브리드 UI인 4가지 UI에 대한 두 가지 사용자 연구를 통해 이를 검증했습니다. 첫 번째 연구에서는 각 UI의 접근 편의성, 편의성, 시야 방해 없는 정도에 대한 사용자 경험(UX) 연구를 수행하고, 객체 중심 UI에 대한 사용자 선호도를 입증했습니다. 두 번째 연구에서는 복잡한 MR 환경에서 설계 고려 사항을 추가로 평가하기 위해 실제 적용 시나리오를 설계하고 그 유용성을 평가했습니다. 우리는 객체 중심 UI의 긍정적인 사용자 경험 특성을 발견했습니다. 마지막으로, 머리 착용형 MR을 통해 미래의 일상 생활에서 실제 사물과의 상호 작용을 위한 UI 디자인을 안내할 수 있는 네 가지 디자인 권장 사항을 요약했습니다.",https://doi.org/10.1109/ISMAR55827.2022.00057,Interaction & Input,Other,Case Study / Application Demo,Design Guidelines
448,2022,Evaluation of Text Selection Techniques in Virtual Reality Head-Mounted Displays,가상 현실 헤드 마운트 디스플레이의 텍스트 선택 기술 평가,"Text selection is an essential activity in interactive systems, including virtual reality (VR) head-mounted displays (HMDs). It is useful for: sharing information across apps or platforms, highlighting and making notes while reading articles, and text editing tasks. Despite its usefulness, the space of text selection interaction is underexplored in VR HMDs. In this research, we performed a user study with 24 participants to investigate the performance and user preference of six text selection techniques (Controller+Dwell, Controller+Click, Head+Dwell, Head+Click, Hand+Dwell, Hand+Pinch). Results reveal that Head+Click is ranked first since it has excellent speedaccuracy performance (2nd fastest task completion speed with 3rd lowest total error rate), provides the best user experience, and produces a very low workload—followed by Controller+Click, which has the fastest speed and comparable experience with Head+Click, but much higher total error rate. Other methods can also be useful depending on the goals of the system or the users. As a first systematic evaluation of pointing $\times$ selection techniques for text selection in VR, the results of this work provide a strong foundation for further research in this area of growing importance to the future of VR to help it become a more ubiquitous and pervasive platform.","텍스트 선택은 가상 현실(VR) 헤드 마운트 디스플레이(HMD)를 포함한 대화형 시스템에서 필수적인 활동입니다. 앱이나 플랫폼 간 정보 공유, 기사를 읽는 동안 강조 표시 및 메모 작성, 텍스트 편집 작업에 유용합니다. 유용성에도 불구하고 VR HMD에서는 텍스트 선택 상호 작용 공간이 충분히 탐색되지 않습니다. 본 연구에서는 6가지 텍스트 선택 기술(Controller+Dwell, Controller+Click, Head+Dwell, Head+Click, Hand+Dwell, Hand+Pinch)의 성능과 사용자 선호도를 조사하기 위해 24명의 참가자를 대상으로 사용자 연구를 수행했습니다. 결과에 따르면 Head+Click은 뛰어난 속도 정확성 성능(두 번째로 빠른 작업 완료 속도, 세 번째로 낮은 총 오류율)을 갖고 최고의 사용자 경험을 제공하며 작업 부하가 매우 낮기 때문에 1위를 차지했으며, 그 다음으로 Head+Click과 비교할 수 있는 가장 빠른 속도와 비슷한 경험을 제공하지만 총 오류율은 훨씬 더 높은 Controller+Click이 그 뒤를 이었습니다. 시스템이나 사용자의 목표에 따라 다른 방법도 유용할 수 있습니다. VR에서 텍스트 선택을 위한 $\times$ 선택 기술에 대한 최초의 체계적인 평가로서, 이 작업의 결과는 VR의 미래에 중요성이 커지고 있는 이 분야에 대한 추가 연구를 위한 강력한 기반을 제공하여 VR이 보다 유비쿼터스적이고 널리 퍼진 플랫폼이 되도록 돕습니다.",https://doi.org/10.1109/ISMAR55827.2022.00027,Interaction & Input; Display & Optics,Other,User Study; Quantitative Experiment,System / Framework; Algorithm / Method
449,2022,Exploring Efficiency of Vision Transformers for Self-Supervised Monocular Depth Estimation,자기 감독 단안 깊이 추정을 위한 비전 변환기의 효율성 탐색,"Depth estimation is a crucial task for the creation of depth maps, one of the most important components for augmented reality (AR) and other applications. However, the most widely used hardware for AR and smartphones has only sparse depth sensors with different ground truth depth acquisition methods. Thus, depth estimation models that are robust for downstream AR tasks performance can only be trained reliably using self-supervised learning based on camera information. Previous works in the field mostly focus on self-supervised models with pure convolutional architectures, without taking global spatial context into account.In this paper, we utilize vision transformer architectures for self-supervised monocular depth estimation and propose VTDepth, a vision transformer-based model, which provides a solution to the problem of the global spatial context. We compare various combinations of convolutional and transformer architectures for self-supervised depth estimation and show that the best combination of models is an encoder with a transformer basis and convolutional decoder. Our experiments demonstrate the efficiency of VTDepth for self-supervised depth estimation. Our set of models achieves state-of-the-art performance for self-supervised learning on NYUv2 and KITTI datasets. Our code is available at https://github.com/ahbpp/VTDepth.",깊이 추정은 증강 현실(AR) 및 기타 애플리케이션의 가장 중요한 구성 요소 중 하나인 깊이 맵을 생성하는 데 중요한 작업입니다. 그러나 AR 및 스마트폰에 가장 널리 사용되는 하드웨어에는 다양한 지상 진실 깊이 획득 방법을 갖춘 희박한 깊이 센서만 있습니다. 따라서 다운스트림 AR 작업 성능에 강력한 깊이 추정 모델은 카메라 정보를 기반으로 한 자기 지도 학습을 통해서만 안정적으로 훈련될 수 있습니다. 해당 분야의 이전 연구는 전역 공간 컨텍스트를 고려하지 않고 순수 컨볼루셔널 아키텍처를 사용하는 자기 감독 모델에 주로 중점을 둡니다. 본 논문에서는 자기 감독 단안 깊이 추정을 위해 비전 변환기 아키텍처를 활용하고 전역 공간 컨텍스트 문제에 대한 솔루션을 제공하는 비전 변환기 기반 모델인 VTDepth를 제안합니다. 우리는 자기지도 깊이 추정을 위해 컨벌루션 및 변환기 아키텍처의 다양한 조합을 비교하고 모델의 가장 좋은 조합은 변환기 기반 인코더와 컨벌루션 디코더임을 보여줍니다. 우리의 실험은 자기 지도 깊이 추정을 위한 VTDepth의 효율성을 보여줍니다. 우리의 모델 세트는 NYUv2 및 KITTI 데이터 세트에 대한 자기 지도 학습을 위한 최첨단 성능을 달성합니다. 우리 코드는 https://github.com/ahbpp/VTDepth에서 확인할 수 있습니다.,https://doi.org/10.1109/ISMAR55827.2022.00089,Interaction & Input,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method; Hardware / Device
450,2022,Exploring the Impact of Visual Information on Intermittent Typing in Virtual Reality,가상 현실에서 간헐적인 입력에 대한 시각적 정보의 영향 탐색,"For touch typists, using a physical keyboard ensures optimal text entry task performance in immersive virtual environments. However, successful typing depends on the user’s ability to accurately position their hands on the keyboard after performing other, non-keyboard tasks. Finding the correct hand position depends on sensory feedback, including visual information. We designed and conducted a user study where we investigated the impact of visual representations of the keyboard and users’ hands on the time required to place hands on the homing bars of a keyboard after performing other tasks. We found that this keyboard homing time decreased as the fidelity of visual representations of the keyboard and hands increased, with a video pass-through condition providing the best performance. We discuss additional impacts of visual representations of a user’s hands and the keyboard on typing performance and user experience in virtual reality.",터치 타이피스트의 경우 실제 키보드를 사용하면 몰입형 가상 환경에서 최적의 텍스트 입력 작업 성능이 보장됩니다. 그러나 성공적인 타이핑은 키보드가 아닌 다른 작업을 수행한 후 키보드 위에 손을 정확하게 위치시키는 사용자의 능력에 달려 있습니다. 올바른 손 위치를 찾는 것은 시각적 정보를 포함한 감각 피드백에 따라 달라집니다. 우리는 키보드와 사용자 손의 시각적 표현이 다른 작업을 수행한 후 키보드의 원점 조정 막대에 손을 얹는 데 필요한 시간에 미치는 영향을 조사하는 사용자 연구를 설계하고 수행했습니다. 우리는 키보드와 손의 시각적 표현의 충실도가 높아짐에 따라 이 키보드 원점 복귀 시간이 감소하고 비디오 통과 조건이 최고의 성능을 제공한다는 것을 발견했습니다. 우리는 사용자의 손과 키보드의 시각적 표현이 가상 현실에서의 타이핑 성능과 사용자 경험에 미치는 추가적인 영향에 대해 논의합니다.,https://doi.org/10.1109/ISMAR55827.2022.00014,Interaction & Input,Other,User Study,User Study / Empirical Findings
451,2022,Gait Differences in the Real World and Virtual Reality: The Effect of Prior Virtual Reality Experience,현실세계와 가상현실의 보행 차이: 사전 가상현실 경험의 효과,"Walking through immersive virtual environments is one of the important parts of Virtual Reality (VR) applications. Prior research has established that users’ gait in virtual and real environments differs; however, little research has evaluated how users’ gait differs as users gain more experience with VR. We conducted experiments measuring novice and experienced subjects’ gait parameters in VR and real environments. Results showed that subjects’ performance in VR and Real World was more similar in the last trials than in the first trials; their walking dissimilarity in the start trials diminished by walking more trials. We found trial as a significant variable affecting the walking speed, step length, and trunk angle for both groups of users. While the main effect of expertise was not observed, an interaction effect between expertise and the trial number was shown. Trunk angle increased over time for novices but decreased for experts.","몰입형 가상 환경을 걷는 것은 가상 현실(VR) 애플리케이션의 중요한 부분 중 하나입니다. 이전 연구에서는 가상 환경과 실제 환경에서 사용자의 보행이 서로 다르다는 사실이 밝혀졌습니다. 그러나 사용자가 VR을 더 많이 경험함에 따라 사용자의 보행이 어떻게 달라지는지 평가한 연구는 거의 없습니다. VR 및 실제 환경에서 초보자와 경험자의 보행 매개변수를 측정하는 실험을 수행했습니다. 결과는 VR과 실제 세계에서의 피험자의 성과가 첫 번째 시도보다 마지막 시도에서 더 유사하다는 것을 보여주었습니다. 시작 시도에서 걷기의 차이는 더 많은 시도를 걸으면서 감소했습니다. 우리는 두 그룹의 사용자 모두에 대해 보행 속도, 보폭 및 몸통 각도에 영향을 미치는 중요한 변수로서 실험을 발견했습니다. 전문성의 주효과는 관찰되지 않았으나, 전문성과 시행횟수 사이의 상호작용 효과가 나타났다. 초보자의 경우 몸통 각도가 시간이 지남에 따라 증가했지만 전문가의 경우 감소했습니다.",https://doi.org/10.1109/ISMAR55827.2022.00080,Interaction & Input,Other,User Study,User Study / Empirical Findings
452,2022,Gestalt Driven Augmented Collimator Widget for Precise 5 DOF Dental Drill Tool Positioning in 3D Space,3D 공간에서 정확한 5 DOF 치과 드릴 도구 위치 지정을 위한 게슈탈트 기반 증강 콜리메이터 위젯,"Drill tool positioning in dental implantology is a challenging task requiring 5DOF precision as the rotation around the tool axis is not influential. This work improves the quasi-static visual elements of the state-of-the-art with a novel Augmented Collimation Widget (ACW), an interactive tool of position and angle error visualization based on the gestalt reification, the human ability to group geometric elements. The user can seek in a quick, pre-attentive way the collimation of five (three positional and two rotational) error component widgets (ECWs), taking advantage of three key aspects: component separation and reification, error visual amplification, and dynamic hiding of the collimated components. We compared the ACW with the golden standard in a within-subjects (N=30) user test using 32 implant targets, measuring the time, error, and usability. ACW performed significantly better in positional (+19%) and angular (+47%) precision accuracy and with less mental demand (-6%) and frustration (-13%), but with an expected increase in task time (+59%) and physical demand (+64%). The interview indicated the ACW as the main preference and aesthetically more pleasant than GSW, candidating it as the new golden standard for implantology, but also for other applications where 5DOF positioning is key.","치과 임플란트학에서 드릴 도구 위치 지정은 도구 축을 중심으로 한 회전이 영향을 미치지 않기 때문에 5DOF 정밀도가 필요한 어려운 작업입니다. 이 작업은 기하학적 요소를 그룹화하는 인간의 능력인 게슈탈트 구체화를 기반으로 한 위치 및 각도 오류 시각화의 대화형 도구인 새로운 ACW(Augmented Collimation Widget)를 사용하여 최첨단 준정적 시각적 요소를 개선합니다. 사용자는 구성 요소 분리 및 구체화, 오류 시각적 증폭, 시준된 구성 요소의 동적 숨기기라는 세 가지 주요 측면을 활용하여 5개(3개 위치 및 2개 회전) 오류 구성 요소 위젯(ECW)의 시준을 사전 주의 깊은 방식으로 신속하게 찾을 수 있습니다. 우리는 32개의 임플란트 타겟을 사용하여 피험자 내(N=30) 사용자 테스트에서 ACW를 황금 표준과 비교하고 시간, 오류 및 유용성을 측정했습니다. ACW는 위치(+19%) 및 각도(+47%) 정밀도 정확도에서 훨씬 더 나은 성능을 발휘했으며 정신적 요구(-6%)와 좌절(-13%)이 적었지만 작업 시간(+59%)과 물리적 요구(+64%)가 증가할 것으로 예상됩니다. 인터뷰에서는 ACW가 GSW보다 주요 선호도이고 심미적으로 더 쾌적한 것으로 나타났으며 이를 임플란트학의 새로운 황금 표준으로 후보로 제시할 뿐만 아니라 5DOF 포지셔닝이 핵심인 기타 응용 분야에서도 사용할 수 있습니다.",https://doi.org/10.1109/ISMAR55827.2022.00033,Rendering & Visualization,Other,User Study; Quantitative Experiment,Interaction Technique
453,2022,How bright should a virtual object be to appear opaque in optical see-through AR?,광학 투명 AR에서 가상 물체가 불투명하게 보이려면 얼마나 밝아야 합니까?,"Reproduction of occlusions and opaque surfaces are the major challenges of additive optical see-through (OST) displays. This is because the user of an OST display sees a linear mixture of display and environment light, which creates an impression of transparency unless the displayed color is sufficiently bright. The primary goal of this work is to determine how bright a displayed surface needs to be in relation to environment light to be perceived as opaque. We test multiple factors that could affect the perception of opacity: background luminance, contrast, spatial frequency, and accommodation depth in foveal vision. The subjective results, collected on a high-dynamic-range multi-focal stereo display, indicate that a virtual object needs to be, on average, 60 times brighter than the background environment light to be perceived as opaque. A higher contrast of the texture of the virtual object and a background that is out of focus can reduce the required luminance ratio. We demonstrate that a model of visual perception based on Weber’s law and accounting for contrast masking and defocus blur can predict the experimental data with an averaged prediction error of 8.29%. Existing perceptual image difference metrics (PSNR, FovVideoVDP and HDR-VDP-3) can also predict the effect of major factors, but with lower accuracy (e.g. prediction error of 34% for PSNR with PU21 encoding).","폐색 및 불투명 표면의 재현은 OST(적층 광학 투명) 디스플레이의 주요 과제입니다. 이는 OST 디스플레이 사용자가 디스플레이와 환경 조명의 선형 혼합을 보게 되기 때문입니다. 이는 표시되는 색상이 충분히 밝지 않으면 투명한 느낌을 줍니다. 이 작업의 주요 목표는 불투명하게 인식되기 위해 환경 조명과 관련하여 표시된 표면이 얼마나 밝아야 하는지 결정하는 것입니다. 우리는 불투명도 인식에 영향을 미칠 수 있는 여러 요소(배경 휘도, 대비, 공간 주파수 및 중심와 시야의 조절 깊이)를 테스트합니다. 높은 동적 범위(high-dynamic-range) 다초점 스테레오 디스플레이에서 수집된 주관적인 결과는 가상 물체가 불투명한 것으로 인식되려면 배경 환경 조명보다 평균 60배 더 밝아야 함을 나타냅니다. 가상 물체의 질감과 초점이 ​​맞지 않는 배경의 대비가 높을수록 필요한 휘도 비율이 줄어들 수 있습니다. 우리는 Weber의 법칙을 기반으로 하고 대비 마스킹 및 디포커스 흐림을 고려한 시각적 인식 모델이 8.29%의 평균 예측 오류로 실험 ​​데이터를 예측할 수 있음을 보여줍니다. 기존 지각 이미지 차이 측정항목(PSNR, FovVideoVDP 및 HDR-VDP-3)도 주요 요인의 영향을 예측할 수 있지만 정확도는 낮습니다(예: PU21 인코딩을 사용하는 PSNR의 예측 오류 34%).",https://doi.org/10.1109/ISMAR55827.2022.00085,Display & Optics; Diminished Reality,Other,Quantitative Experiment,Algorithm / Method
454,2022,How can the Additional Motion Parallax along the y and z-axis Affect Viewer's 3D Perception?: A Generic Approach and Evaluation,y축과 z축을 따른 추가 모션 시차가 뷰어의 3D 인식에 어떤 영향을 미칠 수 있습니까?: 일반적인 접근 방식 및 평가,"Light Field Displays (LFD) offer the potential for a true window into a virtual world without any form of headset. However, the vast majority of LFD only provide motion parallax along the x-axis (horizontal) due to its domination of human 3D perception. The additional motion parallax along y and the z-axis are rarely or never achieved and let alone evaluated. This paper proposed the first approach that provided the real full-motion parallax covering the z-axis. Moreover, this generic approach enables on-demand y and z-axis motion parallax on any off-the-shelf LFD. A prototype was created according to the proposed approach, and with the use of it, an experiment with two tasks was carried out among 24 participants. This pioneering study explored the effect of the motion parallax along the additional axes. Subjective and objective metrics were collected to measure participants’ 3D perception on four viewing conditions (LFD with motion parallax along the x-axis, x-and-y-axis, x-and-z-axis, and x-y-and-z-axis). The study manifested three main findings: 1) The additional y-axis and z-axis motion parallax increased viewers’ engagement with the LFD. 2) LFD with full-motion parallax provided the optimal user experience. 3) The additional motion parallax along with the y-axis increased viewers’ user experience more than the z-axis.","LFD(라이트 필드 디스플레이)는 어떤 형태의 헤드셋 없이도 가상 세계를 실제로 볼 수 있는 가능성을 제공합니다. 그러나 대부분의 LFD는 인간의 3D 인식이 지배적이기 때문에 x축(수평)을 따라 모션 시차만 제공합니다. y축과 z축을 따른 추가 모션 시차는 거의 또는 전혀 달성되지 않으며 평가는 물론입니다. 본 논문에서는 z축을 포괄하는 실제 풀모션 시차를 제공하는 첫 번째 접근 방식을 제안했습니다. 게다가 이 일반적인 접근 방식은 모든 기성품 LFD에서 주문형 y축 및 z축 모션 시차를 가능하게 합니다. 제안된 접근 방식에 따라 프로토타입을 제작하고 이를 활용하여 24명의 참가자를 대상으로 두 가지 작업에 대한 실험을 수행했습니다. 이 선구적인 연구는 추가 축을 따라 동작 시차의 효과를 탐구했습니다. 네 가지 보기 조건(x축, x 및 y축, x 및 z축 및 xy 및 z축을 따라 동작 시차가 있는 LFD)에서 참가자의 3D 인식을 측정하기 위해 주관적 및 객관적인 측정항목이 수집되었습니다. 이 연구에서는 세 가지 주요 결과가 나타났습니다. 1) 추가된 y축 및 z축 모션 시차가 LFD에 대한 시청자의 참여를 증가시켰습니다. 2) 풀모션 시차를 적용한 LFD는 최적의 사용자 경험을 제공했습니다. 3) Y축에 추가된 모션 시차는 Z축보다 시청자의 사용자 경험을 더 증가시켰습니다.",https://doi.org/10.1109/ISMAR55827.2022.00040,Display & Optics,Optical / Display Technology,User Study,User Study / Empirical Findings
455,2022,In-Place Gestures Classification via Long-term Memory Augmented Network,장기 기억 증강 네트워크를 통한 현장 제스처 분류,"In-place gesture-based virtual locomotion techniques enable users to control their viewpoint and intuitively move in the 3D virtual environment. A key research problem is to accurately and quickly recognize in-place gestures, since they can trigger specific movements of virtual viewpoints and enhance user experience. However, to achieve real-time experience, only short-term sensor sequence data (up to about 300ms, 6 to 10 frames) can be taken as input, which actually affects the classification performance due to limited spatiotemporal information. In this paper, we propose a novel long-term memory augmented network for in-place gestures classification. It takes as input both short-term gesture sequence samples and their corresponding long-term sequence samples that provide extra relevant spatio-temporal information in the training phase. We store long-term sequence features with an external memory queue. In addition, we design a memory augmented loss to help cluster features of the same class and push apart features from different classes, thus enabling our memory queue to memorize more relevant long-term sequence features. In the inference phase, we input only short-term sequence samples to recall the stored features accordingly, and fuse them together to predict the gesture class. We create a large-scale in-place gestures dataset from 25 participants with 11 gestures. Our method achieves a promising accuracy of 95.1% with a latency of 192ms, and an accuracy of 97.3% with a latency of 312ms, and is demonstrated to be superior to recent in-place gesture classification techniques. User study also validates our approach. Our source code and dataset will be made available to the community.","In-Place 제스처 기반 가상 이동 기술을 통해 사용자는 3D 가상 환경에서 자신의 시점을 제어하고 직관적으로 이동할 수 있습니다. 주요 연구 문제는 가상 시점의 특정 움직임을 유발하고 사용자 경험을 향상시킬 수 있기 때문에 제자리 제스처를 정확하고 빠르게 인식하는 것입니다. 그러나 실시간 경험을 달성하기 위해서는 단기 센서 시퀀스 데이터(최대 약 300ms, 6~10프레임)만 입력할 수 있으며, 이는 제한된 시공간 정보로 인해 실제로 분류 성능에 영향을 미칩니다. 본 논문에서는 제자리 제스처 분류를 위한 새로운 장기 기억 증강 네트워크를 제안합니다. 이는 훈련 단계에서 추가적인 관련 시공간 정보를 제공하는 단기 제스처 시퀀스 샘플과 해당 장기 시퀀스 샘플을 모두 입력으로 사용합니다. 우리는 외부 메모리 대기열을 사용하여 장기 시퀀스 기능을 저장합니다. 또한 동일한 클래스의 기능을 클러스터링하고 다른 클래스의 기능을 분리하는 데 도움이 되는 메모리 증가 손실을 설계하여 메모리 대기열이 더 관련성이 높은 장기 시퀀스 기능을 기억할 수 있도록 합니다. 추론 단계에서는 단기 시퀀스 샘플만 입력하여 그에 따라 저장된 특징을 호출하고 이를 융합하여 제스처 클래스를 예측합니다. 우리는 11가지 제스처가 포함된 25명의 참가자로부터 대규모 내부 제스처 데이터세트를 만듭니다. 우리의 방법은 192ms의 대기 시간으로 95.1%의 유망한 정확도와 312ms의 대기 시간으로 97.3%의 정확도를 달성하며 최근의 내부 제스처 분류 기술보다 우수한 것으로 입증되었습니다. 사용자 연구는 또한 우리의 접근 방식을 검증합니다. 우리의 소스 코드와 데이터 세트는 커뮤니티에 공개될 것입니다.",https://doi.org/10.1109/ISMAR55827.2022.00037,Interaction & Input,Redirected Walking / Locomotion,Technical Evaluation; User Study,Algorithm / Method
456,2022,Infinite Virtual Space Exploration Using Space Tiling and Perceivable Reset at Fixed Positions,고정된 위치에서 공간 타일링과 인지 가능한 재설정을 사용한 무한 가상 공간 탐색,"A simultaneous walking experience in virtual and real spaces can provide a high sense of presence. However, users may face challenges when walking within a large virtual space while walking in a small and complex real space. Several methods such as Redirected Walking (RDW) and Substitutional Reality (SR) have been proposed as different approaches to this problem. However, the users must “reset” their movement direction at unpredictable moments to avoid collision in a small and complex real space when using subtle RDW that does not maintain the correspondence between virtual and real space. Contrarily, exploration through the SR has a limitation in that the VR scene is restricted to a controlled area. In this paper, we propose Reset at Fixed Positions (RFP), a method that combines RDW with the advantage of the SR and matches walkable real space with walkable virtual space. To utilize RFP, we defined Guaranteed Space Block (GSB), a unit space that constitutes a walkable virtual space. This space is obtained through the point reflection of the GSB utilizing the reset position within the GSB. RFPs can be implemented by two methods: Generating Virtual Space Using RFP (G-RFP) and Implementing Given Virtual Space Using RFP (I-RFP). G-RFP can create an infinitely large virtual space for exploration. On the other hand, I-RFP can conFigure a given virtual environment to make users walk. We observed that G-RFP provides higher presence, immersion and a higher mean distance traveled between resets compared to the existing RDW method in a complex real space through a user study. In addition, exploration through I-RFP provided a higher immersion, a comparable presence, and a similar number of resets.","가상공간과 현실공간을 동시에 걷는 경험은 높은 현장감을 제공할 수 있다. 그러나 사용자는 작고 복잡한 실제 공간을 걷는 동안 큰 가상 공간을 걷는 데 어려움을 겪을 수 있습니다. 이 문제에 대한 다른 접근 방식으로 RDW(Redirected Walking) 및 SR(Substitutional Reality)과 같은 여러 가지 방법이 제안되었습니다. 그러나 가상과 현실 공간의 대응을 유지하지 못하는 미묘한 RDW를 사용할 경우, 작고 복잡한 현실 공간에서 충돌을 피하기 위해 사용자는 예측할 수 없는 순간에 이동 방향을 '재설정'해야 합니다. 반면, SR을 통한 탐색은 VR 장면이 통제된 구역으로 제한된다는 한계가 있습니다. 본 논문에서는 RDW와 SR의 장점을 결합하고 보행 가능한 실제 공간과 보행 가능한 가상 공간을 일치시키는 방법인 Reset at 고정 위치(RFP)를 제안합니다. RFP를 활용하기 위해 보행 가능한 가상 공간을 구성하는 단위 공간인 GSB(Guaranteed Space Block)를 정의했습니다. 이 공간은 GSB 내의 재설정 위치를 활용하는 GSB의 점 반사를 통해 확보됩니다. RFP는 RFP를 사용하여 가상 공간 생성(G-RFP)과 RFP를 사용하여 주어진 가상 공간 구현(I-RFP)의 두 가지 방법으로 구현할 수 있습니다. G-RFP는 탐색을 위해 무한히 큰 가상 공간을 만들 수 있습니다. 반면에 I-RFP는 사용자가 걷도록 가상 환경을 구성할 수 있습니다. 우리는 사용자 연구를 통해 G-RFP가 복잡한 실제 공간에서 기존 RDW 방법에 비해 더 높은 존재감, 몰입도 및 더 높은 재설정 간 평균 이동 거리를 제공한다는 것을 관찰했습니다. 또한 I-RFP를 통한 탐색은 더 높은 몰입도, 비슷한 존재감, 비슷한 횟수의 재설정을 제공했습니다.",https://doi.org/10.1109/ISMAR55827.2022.00094,Perception & Cognition,Sensor Fusion,User Study,Algorithm / Method
457,2022,Integrated Design of Augmented Reality Spaces Using Virtual Environments,가상환경을 활용한 증강현실 공간의 통합설계,"Demand is growing for markerless augmented reality (AR) experiences, but designers of the real-world spaces that host them still have to rely on inexact, qualitative guidelines on the visual environment to try and facilitate accurate pose tracking. Furthermore, the need for visual texture to support markerless AR is often at odds with human aesthetic preferences, and understanding how to balance these competing requirements is challenging due to the siloed nature of the relevant research areas. To address this, we present an integrated design methodology for AR spaces, that incorporates both tracking and human factors into the design process. On the tracking side, we develop the first VI-SLAM evaluation technique that combines the flexibility and control of virtual environments with real inertial data. We use it to perform systematic, quantitative experiments on the effect of visual texture on pose estimation accuracy; through 2000 trials in 20 environments, we reveal the impact of both texture complexity and edge strength. On the human side, we show how virtual reality (VR) can be used to evaluate user satisfaction with environments, and highlight how this can be tailored to AR research and use cases. Finally, we demonstrate our integrated design methodology with a case study on AR museum design, in which we conduct both VI-SLAM evaluations and a VR-based user study of four different museum environments.","마커 없는 증강 현실(AR) 경험에 대한 수요가 증가하고 있지만 이를 호스팅하는 실제 공간의 디자이너는 정확한 포즈 추적을 시도하고 촉진하기 위해 여전히 시각적 환경에 대한 부정확하고 질적인 지침에 의존해야 합니다. 더욱이, 마커 없는 AR을 지원하기 위한 시각적 질감의 필요성은 종종 인간의 미적 선호도와 상충되며, 관련 연구 분야의 고립된 특성으로 인해 이러한 경쟁 요구 사항의 균형을 맞추는 방법을 이해하는 것이 어렵습니다. 이를 해결하기 위해 우리는 추적 및 인적 요소를 디자인 프로세스에 통합하는 AR 공간의 통합 디자인 방법론을 제시합니다. 추적 측면에서 우리는 가상 환경의 유연성과 제어를 실제 관성 데이터와 결합한 최초의 VI-SLAM 평가 기술을 개발합니다. 우리는 이를 사용하여 시각적 질감이 포즈 추정 정확도에 미치는 영향에 대한 체계적이고 정량적인 실험을 수행합니다. 20가지 환경에서 2000번의 시험을 통해 텍스처 복잡성과 가장자리 강도의 영향을 밝혀냈습니다. 인간 측면에서는 가상 현실(VR)을 사용하여 환경에 대한 사용자 만족도를 평가하는 방법을 보여주고 이것이 AR 연구 및 사용 사례에 어떻게 맞춤화될 수 있는지 강조합니다. 마지막으로, 우리는 AR 박물관 디자인에 대한 사례 연구를 통해 통합 디자인 방법론을 보여줍니다. 여기서는 네 가지 박물관 환경에 대한 VI-SLAM 평가와 VR 기반 사용자 연구를 모두 수행합니다.",https://doi.org/10.1109/ISMAR55827.2022.00045,Tracking & Localization,SLAM / Spatial Mapping,Quantitative Experiment; Case Study / Application Demo,User Study / Empirical Findings; Algorithm / Method
458,2022,Inverse Kinematics Assistance for the Creation of Redirected Walking Paths,리디렉션된 보행 경로 생성을 위한 역기구학 지원,"Virtual reality interactions that require a specific relationship between the virtual and physical coordinate systems, such as passive haptic interactions, are not possible with locomotion techniques using redirected walking. To address this limitation, recent research has introduced environmental alignment, which is the notion of using redirected walking techniques to align the virtual and physical coordinate systems such that these interactions are possible. However, the previous research has only implemented environmental alignment in a reactive manner, and the authors posited that better results could be achieved if a predictive algorithm was instead used. In this work, we introduce a novel way to model the environmental alignment problem as a version of the inverse kinematics problem which can be incorporated into several existing predictive algorithms, as well as a simple proof-of-concept implementation. An exploratory human subject study (N=17) was conducted to evaluate this implementation’s usability as a tool for authoring planned path redirected walking scenarios that incorporate physical interactivity. To our knowledge, this is the first study to evaluate redirected walking experience design tools and provides a possible framework for future studies. Our qualitative analysis of the results generated both guidance for integrating automatic solvers and broad recommendations for designing redirected path authoring tools.","수동적 촉각 상호작용과 같이 가상 좌표계와 물리적 좌표계 사이의 특정 관계가 필요한 가상 현실 상호작용은 방향 전환된 걷기를 사용하는 운동 기술로는 불가능합니다. 이러한 한계를 해결하기 위해 최근 연구에서는 방향 전환된 걷기 기술을 사용하여 이러한 상호 작용이 가능하도록 가상 및 물리적 좌표계를 정렬하는 개념인 환경 정렬을 도입했습니다. 그러나 이전 연구에서는 반응적인 방식으로만 환경 정렬을 구현했으며, 대신 예측 알고리즘을 사용하면 더 나은 결과를 얻을 수 있다고 저자는 가정했습니다. 이 연구에서는 간단한 개념 증명 구현뿐만 아니라 기존의 여러 예측 알고리즘에 통합될 수 있는 역운동학 문제의 버전으로 환경 정렬 문제를 모델링하는 새로운 방법을 소개합니다. 탐색적 인간 대상 연구(N=17)는 물리적 상호 작용을 통합하는 계획된 경로 리디렉션 걷기 시나리오를 작성하기 위한 도구로서 이 구현의 유용성을 평가하기 위해 수행되었습니다. 우리가 아는 한, 이것은 방향 전환된 보행 경험 디자인 도구를 평가하고 향후 연구를 위한 가능한 프레임워크를 제공하는 최초의 연구입니다. 결과에 대한 질적 분석을 통해 자동 솔버 통합을 위한 지침과 리디렉션된 경로 작성 도구 설계를 위한 광범위한 권장 사항이 모두 생성되었습니다.",https://doi.org/10.1109/ISMAR55827.2022.00076,Interaction & Input; Content Authoring,Redirected Walking / Locomotion,User Study,Algorithm / Method; System / Framework
459,2022,Investigating Input Modality and Task Geometry on Precision-first 3D Drawing in Virtual Reality,가상 현실의 정밀 우선 3D 드로잉에 대한 입력 양식 및 작업 형상 조사,"Accurately drawing non-planar 3D curves in immersive Virtual Reality (VR) is indispensable for many precise 3D tasks. However, due to lack of physical support, limited depth perception, and the non-planar nature of 3D curves, it is challenging to adjust mid-air strokes to achieve high precision. Instead of creating new interaction techniques, we investigated how task geometric shapes and input modalities affect precision-first drawing performance in a within-subject study (n=12) focusing on 3D target tracing in commercially available VR headsets. We found that compared to using bare hands, VR controllers and pens yield nearly 30% of precision gain, and that the tasks with large curvature, forward-backward or left-right orientations perform best. We finally discuss opportunities for designing novel interaction techniques for precise 3D drawing. We believe that our work will benefit future research aiming to create usable toolboxes for precise 3D drawing.","몰입형 가상 현실(VR)에서 비평면 3D 곡선을 정확하게 그리는 것은 많은 정밀 3D 작업에 필수적입니다. 그러나 물리적 지지력 부족, 제한된 깊이 인식, 3D 곡선의 비평면 특성으로 인해 높은 정밀도를 달성하기 위해 공중 스트로크를 조정하는 것은 어렵습니다. 새로운 상호 작용 기술을 만드는 대신, 우리는 상용 VR 헤드셋의 3D 대상 추적에 초점을 맞춘 주제 내 연구(n=12)에서 작업 기하학적 모양과 입력 양식이 정밀도 우선 그리기 성능에 어떻게 영향을 미치는지 조사했습니다. 맨손을 사용하는 경우에 비해 VR 컨트롤러와 펜을 사용하면 정밀도가 거의 30% 향상되고 곡률이 크거나 앞으로-뒤로 또는 왼쪽-오른쪽 방향으로 작업하는 것이 가장 잘 수행되는 것으로 나타났습니다. 마지막으로 정밀한 3D 드로잉을 위한 새로운 상호 작용 기술을 설계할 수 있는 기회에 대해 논의합니다. 우리는 우리의 작업이 정밀한 3D 도면을 위한 유용한 도구 상자를 만드는 것을 목표로 하는 향후 연구에 도움이 될 것이라고 믿습니다.",https://doi.org/10.1109/ISMAR55827.2022.00054,Interaction & Input; Perception & Cognition,Other,User Study,System / Framework
460,2022,Investigating The Effect of Direction on The Limits of Haptic Retargeting,햅틱 리타겟팅의 한계에 대한 방향의 영향 조사,"Haptic Retargeting enables spatially decoupled physical objects to provide haptic feedback for multiple virtual objects in Virtual Reality (VR). By decoupling the virtual hand from its real position, through Hand Redirection, multiple virtual objects can be mapped to a single physical proxy. However, redirection beyond a detectable level is disruptive to the user experience. The limits of haptic retargeting have mainly been explored in one primary direction—the user reaching forwards. We designed an experiment with participants performing reaching movements across 8 reaching directions in the horizontal plane, with a hand redirection of up to 30°. We identify an overall haptic retargeting limit and find that a physical proxy can be remapped to virtual objects of up to 16.14° away. We find a significant effect of reaching direction on the limit. In practice, however, these differences are small, measuring only a couple of degrees, translating to approximately 1cm across a 30cm reach. We argue that, while the psychology literature might suggest the need for specific directional limits and while we do find an effect of direction on retargeting limits, interaction designers can mitigate these requirements by applying slightly conservative global retargeting limits. Our contributions further the community’s knowledge of both how to deploy haptic retargeting in interaction without compromising the user’s experience and how visual and proprioceptive cues interact in peripersonal space in VR.",햅틱 리타겟팅(Haptic Retargeting)은 공간적으로 분리된 물리적 객체를 활성화하여 가상 현실(VR)의 여러 가상 객체에 대한 햅틱 피드백을 제공합니다. 손 리디렉션을 통해 가상 손을 실제 위치에서 분리함으로써 여러 가상 객체를 단일 물리적 프록시에 매핑할 수 있습니다. 그러나 감지 가능한 수준을 넘어서는 리디렉션은 사용자 경험을 방해합니다. 햅틱 리타겟팅의 한계는 주로 사용자가 앞으로 도달하는 한 가지 주요 방향에서 탐구되었습니다. 우리는 참가자들이 수평면에서 8개의 도달 방향에 걸쳐 최대 30°의 손 방향 전환을 통해 도달 동작을 수행하는 실험을 설계했습니다. 우리는 전반적인 햅틱 리타게팅 제한을 식별하고 물리적 프록시가 최대 16.14° 떨어진 가상 개체에 다시 매핑될 수 있음을 확인했습니다. 우리는 한계에 대한 방향 도달의 중요한 효과를 발견합니다. 그러나 실제로 이러한 차이는 2도 정도만 측정되어 30cm 도달 범위에서 약 1cm에 불과할 정도로 작습니다. 우리는 심리학 문헌이 특정 방향 제한의 필요성을 제안할 수 있고 방향이 리타겟팅 제한에 미치는 영향을 찾는 동안 상호 작용 디자이너는 약간 보수적인 글로벌 리타겟팅 제한을 적용하여 이러한 요구 사항을 완화할 수 있다고 주장합니다. 우리의 기여는 사용자 경험을 손상시키지 않고 상호 작용에 햅틱 리타겟팅을 배포하는 방법과 VR의 개인 주변 공간에서 시각적 및 고유 감각 신호가 상호 작용하는 방법에 대한 커뮤니티의 지식을 더욱 발전시킵니다.,https://doi.org/10.1109/ISMAR55827.2022.00078,Interaction & Input,Haptic / Tactile Feedback,User Study,User Study / Empirical Findings
461,2022,Investigating User Embodiment of Inverse-Kinematic Avatars in Smartphone Augmented Reality,스마트폰 증강 현실에서 역운동학적 아바타의 사용자 구현 조사,"Smartphone Augmented Reality (AR) has already provided us with a plethora of social applications such as Pokemon Go or Harry Potter Wizards Unite. However, to enable smartphone AR for social applications similar to VRChat or AltspaceVR, proper user tracking is necessary to accurately animate the avatars. In Virtual Reality (VR), avatar tracking is rather easy due to the availability of hand-tracking, controllers, and HMD whereas smartphone AR has only the back-(and front) camera and IMUs available for this task. In this paper we propose ARIKA, a tracking solution for avatars in smartphone AR. ARIKA uses tracking information from ARCore to track the users hand position and to calculate a pose using Inverse Kinematics (IK). We compare the accuracy of our system against a commercial motion tracking system and compare both systems with respect to sense of agency, self-location, and body-ownership. For this, 20 participants observed their avatars in an augmented virtual mirror and executed a navigation and a pointing task. Our results show that participants felt a higher sense of agency and self location when using the full body tracked avatar as opposed to IK avatars. Interestingly and in favor of ARIKA, there were no significant differences in body-ownership between our solution and the full-body tracked avatars. Thus, ARIKA and it’s single-camera approach is valid solution for smartphone AR applications where body-ownership is essential.","스마트폰 증강 현실(AR)은 이미 Pokemon Go 또는 Harry Potter Wizards Unite와 같은 수많은 소셜 애플리케이션을 제공했습니다. 그러나 VRChat 또는 AltspaceVR과 유사한 소셜 애플리케이션에서 스마트폰 AR을 활성화하려면 아바타를 정확하게 애니메이션화하기 위해 적절한 사용자 추적이 필요합니다. 가상 현실(VR)에서는 핸드 트래킹, 컨트롤러 및 HMD를 사용할 수 있기 때문에 아바타 추적이 다소 쉬운 반면, 스마트폰 AR에는 이 작업에 사용할 수 있는 후면(및 전면) 카메라와 IMU만 있습니다. 본 논문에서는 스마트폰 AR의 아바타 추적 솔루션인 ARIKA를 제안한다. ARIKA는 ARCore의 추적 정보를 사용하여 사용자의 손 위치를 추적하고 IK(역운동학)를 사용하여 포즈를 계산합니다. 우리는 우리 시스템의 정확성을 상업용 모션 추적 시스템과 비교하고, 주체성, 자기 위치 및 신체 소유권과 관련하여 두 시스템을 비교합니다. 이를 위해 20명의 참가자는 증강된 가상 거울에서 자신의 아바타를 관찰하고 탐색 및 포인팅 작업을 실행했습니다. 우리의 결과는 참가자들이 IK 아바타와 달리 전신 추적 아바타를 사용할 때 주체성과 자기 위치에 대한 더 높은 감각을 느꼈다는 것을 보여줍니다. 흥미롭게도 ARIKA에 유리한 점은 우리 솔루션과 전신 추적 아바타 사이의 신체 소유권에 큰 차이가 없었습니다. 따라서 ARIKA와 ARIKA의 단일 카메라 접근 방식은 본체 소유권이 필수적인 스마트폰 AR 애플리케이션에 유효한 솔루션입니다.",https://doi.org/10.1109/ISMAR55827.2022.00084,Interaction & Input; Perception & Cognition,Sensor Fusion,User Study,System / Framework
462,2022,Label Guidance based Object Locating in Virtual Reality,가상 현실에서 라벨 안내 기반 객체 찾기,"Object locating in virtual reality (VR) has been widely used in many VR applications, such as virtual assembly, virtual repair, virtual remote coaching. However, when there are a large number of objects in the virtual environment(VE), the user cannot locate the target object efficiently and comfortably. In this paper, we propose a label guidance based object locating method for locating the target object efficiently in VR. Firstly, we introduce the label guidance based object locating pipeline to improve the efficiency of the object locating. It arranges the labels of all objects on the same screen, lets the user select the target labels first, and then uses the flying labels to guide the user to the target object. Then we summarize five principles for constructing the label layout for object locating and propose a two-level hierarchical sorted and orientated label layout based on the five principles for the user to select the candidate labels efficiently and comfortably. After that, we propose the view and gaze based label guidance method for guiding the user to locate the target object based on the selected candidate labels. It generates specific flying trajectories for candidate labels, updates the flying speed of candidate labels, keeps valid candidate labels, and removes the invalid candidate labels in real time during object locating with the guidance of the candidate labels. Compared with the traditional method, the user study results show that our method significantly improves efficiency and reduces task load for object locating.","가상 현실(VR)에서 객체 위치를 찾는 것은 가상 조립, 가상 수리, 가상 원격 코칭과 같은 많은 VR 애플리케이션에서 널리 사용되었습니다. 그러나 가상환경(VE)에 객체의 수가 많으면 사용자는 대상 객체를 효율적이고 편안하게 찾을 수 없습니다. 본 논문에서는 VR에서 효율적으로 대상 객체를 찾기 위한 라벨 안내 기반의 객체 탐색 방법을 제안한다. 먼저, 객체 찾기의 효율성을 높이기 위해 라벨 안내 기반 객체 찾기 파이프라인을 소개합니다. 모든 객체의 라벨을 동일한 화면에 배열하고 사용자가 대상 라벨을 먼저 선택한 다음 플라잉 라벨을 사용하여 사용자를 대상 객체로 안내합니다. 그런 다음 객체 찾기를 위한 라벨 레이아웃 구성의 5가지 원칙을 요약하고, 사용자가 효율적이고 편안하게 후보 라벨을 선택할 수 있도록 5가지 원칙을 기반으로 2단계 계층적 정렬 및 지향 라벨 레이아웃을 제안합니다. 이후, 선택된 후보 라벨을 기반으로 사용자에게 대상 객체의 위치를 ​​안내하는 뷰 및 시선 기반 라벨 안내 방법을 제안한다. 후보 라벨의 특정 비행 궤적을 생성하고, 후보 라벨의 비행 속도를 업데이트하며, 유효한 후보 라벨을 유지하고, 후보 라벨의 안내에 따라 물체를 찾는 동안 실시간으로 유효하지 않은 후보 라벨을 제거합니다. 기존 방법과 비교하여 사용자 연구 결과에 따르면 우리 방법은 효율성을 크게 향상시키고 개체 찾기 작업 부하를 줄이는 것으로 나타났습니다.",https://doi.org/10.1109/ISMAR55827.2022.00060,Interaction & Input; Content Authoring,Other,User Study,System / Framework
463,2022,Layerable Apps: Comparing Concurrent and Exclusive Display of Augmented Reality Applications,계층화 가능한 앱: 증강 현실 애플리케이션의 동시 및 독점 디스플레이 비교,"Current augmented reality (AR) interfaces are often designed for interacting with one application at a time, significantly limiting a user’s ability to concurrently interact with and switch between multiple applications or modalities that could run in parallel. In this work, we introduce an application model called Layerable Apps, which supports a variety of AR application types while enabling multitasking through concurrent execution, fast application switching, and the ability to layer application views to adjust the degree of augmentation to the user’s preference. We evaluated Layerable Apps through a within-subjects user study (n=44), compared against a traditional single-focus application model on a split-information task involving the simultaneous use of multiple applications. We report the results of our study, where we found differences in quantitative task performance, favoring Layerable mode. We also analyzed app usage patterns, spatial awareness, and overall preferences between both modes as well as between experienced and novice AR users.","현재 증강 현실(AR) 인터페이스는 한 번에 하나의 애플리케이션과 상호 작용하도록 설계되는 경우가 많기 때문에 병렬로 실행될 수 있는 여러 애플리케이션 또는 양식 간에 동시에 상호 작용하고 전환하는 사용자의 능력이 크게 제한됩니다. 이 작업에서는 다양한 AR 애플리케이션 유형을 지원하는 동시에 동시 실행, 빠른 애플리케이션 전환, 애플리케이션 보기 계층화 기능을 통해 멀티태스킹을 지원하여 사용자 선호도에 따라 증강 정도를 조정하는 Layerable Apps라는 애플리케이션 모델을 소개합니다. 우리는 여러 애플리케이션을 동시에 사용하는 분할 정보 작업에 대한 기존 단일 초점 애플리케이션 모델과 비교하여 주제 내 사용자 연구(n=44)를 통해 계층화 가능한 앱을 평가했습니다. 우리는 Layerable 모드를 선호하는 정량적 작업 성능의 차이를 발견한 연구 결과를 보고합니다. 또한 두 모드 간, 경험이 풍부한 AR 사용자와 초보 AR 사용자 간의 앱 사용 패턴, 공간 인식, 전반적인 선호도를 분석했습니다.",https://doi.org/10.1109/ISMAR55827.2022.00104,Perception & Cognition,Sensor Fusion,User Study,Algorithm / Method
464,2022,Leaning-Based Control of an Immersive-Telepresence Robot,몰입형 텔레프레즌스 로봇의 기대어 기반 제어,"In this paper, we present an implementation of a leaning-based control of a differential drive telepresence robot and a user study in simulation, with the goal of bringing the same functionality to a real telepresence robot. The participants used a balance board to control the robot and viewed the virtual environment through a head-mounted display. The main motivation for using a balance board as the control device stems from Virtual Reality (VR) sickness; even small movements of your own body matching the motions seen on the screen decrease the sensory conflict between vision and vestibular organs, which lies at the heart of most theories regarding the onset of VR sickness. To test the hypothesis that the balance board as a control method would be less sickening than using joysticks, we designed a user study (N=32, 15 women) in which the participants drove a simulated differential drive robot in a virtual environment with either a Nintendo Wii Balance Board or joysticks. However, our pre-registered main hypotheses were not supported; the joystick did not cause any more VR sickness on the participants than the balance board, and the board proved to be statistically significantly more difficult to use, both subjectively and objectively. Analyzing the open-ended questions revealed these results to be likely connected, meaning that the difficulty of use seemed to affect sickness; even unlimited training time before the test did not make the use as easy as the familiar joystick. Thus, making the board easier to use is a key to enable its potential; we present a few possibilities towards this goal.","본 논문에서는 실제 텔레프레즌스 로봇에 동일한 기능을 적용하는 것을 목표로 차동 구동 텔레프레즌스 로봇의 기울기 기반 제어 구현과 시뮬레이션 사용자 연구를 제시합니다. 참가자들은 균형판을 이용해 로봇을 제어하고, 머리에 장착된 디스플레이를 통해 가상 환경을 관람했다. 밸런스 보드를 제어 장치로 사용하는 주된 동기는 가상 현실(VR) 멀미에서 비롯됩니다. 화면에 보이는 움직임과 일치하는 자신의 몸의 작은 움직임조차도 VR 질병 발병에 관한 대부분의 이론의 핵심인 시력과 전정 기관 사이의 감각 충돌을 감소시킵니다. 제어 방법으로서 밸런스 보드가 조이스틱을 사용하는 것보다 덜 아프다는 가설을 테스트하기 위해 참가자가 Nintendo Wii 밸런스 보드 또는 조이스틱을 사용하여 가상 환경에서 시뮬레이션된 차동 구동 로봇을 운전하는 사용자 연구(N=32, 15명의 여성)를 설계했습니다. 그러나 사전 등록된 주요 가설은 지지되지 않았습니다. 조이스틱은 밸런스 보드보다 참가자에게 더 이상 VR 멀미를 유발하지 않았으며 보드는 주관적으로나 객관적으로 사용하기가 통계적으로 훨씬 더 어려운 것으로 입증되었습니다. 개방형 질문을 분석한 결과 이러한 결과는 서로 연관되어 있을 가능성이 있는 것으로 나타났습니다. 이는 사용의 어려움이 질병에 영향을 미치는 것으로 보인다는 것을 의미합니다. 테스트 전의 무제한 교육 시간에도 익숙한 조이스틱만큼 쉽게 사용할 수는 없었습니다. 따라서 보드를 사용하기 쉽게 만드는 것이 보드의 잠재력을 활성화하는 열쇠입니다. 우리는 이 목표를 향해 몇 가지 가능성을 제시합니다.",https://doi.org/10.1109/ISMAR55827.2022.00074,Display & Optics; Education & Training,Deep Learning / Neural Networks,Simulation; User Study,Hardware / Device
465,2022,MFF-PR: Point Cloud and Image Multi-modal Feature Fusion for Place Recognition,MFF-PR: 장소 인식을 위한 포인트 클라우드 및 이미지 다중 모드 기능 융합,"Place recognition technology can eliminate cumulative errors and thus plays a vital role in autonomous driving. In this paper, the composite feature of point cloud and image data is obtained by multi-modal feature fusion, thereby improving positioning accuracy. Semantic features, instance features, topological features, and image texture features are integrated to obtain comprehensive features, presenting strong robustness and complex scene expression abilities. Topological features consist of intra-class features and inter-instance features, which allow users to obtain more comprehensive scene structure information. The place recognition methods of data-level fusion and feature-level fusion based on point cloud and image are compared. This paper verifies the proposed method on SemanticKitti and nuScenes datasets. The results show that it outperforms state-of-the-art place recognition methods.","장소 인식 기술은 누적 오류를 제거할 수 있어 자율주행에 중요한 역할을 합니다. 본 논문에서는 다중 모드 특징 융합을 통해 포인트 클라우드와 이미지 데이터의 복합 특징을 획득하여 위치 정확도를 향상시킵니다. 시맨틱 특징, 인스턴스 특징, 토폴로지 특징, 이미지 텍스처 특징이 통합되어 포괄적인 특징을 얻으며 강력한 견고성과 복잡한 장면 표현 능력을 제공합니다. 토폴로지 기능은 클래스 내 기능과 인스턴스 간 기능으로 구성되어 있어 사용자가 보다 포괄적인 장면 구조 정보를 얻을 수 있습니다. 포인트 클라우드와 이미지 기반의 데이터 수준 융합과 특징 수준 융합의 장소 인식 방법을 비교한다. 본 논문에서는 제안된 방법을 SemanticKitti와 nuScenes 데이터셋에 검증한다. 결과는 최첨단 장소 인식 방법보다 성능이 우수하다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR55827.2022.00082,Rendering & Visualization,Cloud / Edge Computing,Technical Evaluation,Algorithm / Method
466,2022,Mixed Reality Communication for Medical Procedures: Teaching the Placement of a Central Venous Catheter,의료 절차를 위한 혼합 현실 통신: 중심 정맥 카테터 배치 교육,"Medical procedures are an essential part of healthcare delivery, and the acquisition of procedural skills is a critical component of medical education. Unfortunately, procedural skill is not evenly distributed among medical providers. Skills may vary within departments or institutions, and across geographic regions, depending on the provider’s training and ongoing experience. We present a mixed reality real-time communication system to increase access to procedural skill training and to improve remote emergency assistance. Our system allows a remote expert to guide a local operator through a medical procedure. RGBD cameras capture a volumetric view of the local scene including the patient, the operator, and the medical equipment. The volumetric capture is augmented onto the remote expert’s view to allow the expert to spatially guide the local operator using visual and verbal instructions. We evaluated our mixed reality communication system in a study in which experts teach the ultrasound-guided placement of a central venous catheter (CVC) to students in a simulation setting. The study compares state-of-the-art video communication against our system. The results indicate that our system enhances and offers new possibilities for visual communication compared to video teleconference-based training.","의료 절차는 의료 서비스 제공의 필수적인 부분이며, 절차적 기술의 습득은 의학 교육의 중요한 구성 요소입니다. 불행하게도 절차적 기술은 의료 제공자 사이에 균등하게 분배되지 않습니다. 기술은 제공자의 교육 및 지속적인 경험에 따라 부서 또는 기관 내에서, 그리고 지역에 따라 달라질 수 있습니다. 절차적 기술 훈련에 대한 접근성을 높이고 원격 응급 지원을 향상시키기 위해 혼합 현실 실시간 통신 시스템을 제시합니다. 우리 시스템을 통해 원격 전문가가 현지 운영자에게 의료 절차를 안내할 수 있습니다. RGBD 카메라는 환자, 조작자, 의료 장비를 포함한 로컬 장면의 체적 뷰를 캡처합니다. 전문가가 시각적 및 언어적 지침을 사용하여 로컬 운영자를 공간적으로 안내할 수 있도록 체적 캡처가 원격 전문가의 시야에 확대됩니다. 우리는 전문가들이 시뮬레이션 환경에서 학생들에게 중심정맥 카테터(CVC)의 초음파 유도 배치를 가르치는 연구에서 혼합 현실 통신 시스템을 평가했습니다. 이 연구는 최첨단 비디오 커뮤니케이션을 우리 시스템과 비교합니다. 결과는 우리 시스템이 화상 원격 회의 기반 교육에 비해 시각적 의사소통의 새로운 가능성을 향상시키고 제공한다는 것을 나타냅니다.",https://doi.org/10.1109/ISMAR55827.2022.00050,Education & Training; Medical & Healthcare,Sensor Fusion,Technical Evaluation,System / Framework
467,2022,Mixed Reality Tunneling Effects for Stereoscopic Untethered Video-See-Through Head-Mounted Displays,입체형 무선 비디오 투시형 헤드 장착형 디스플레이에 대한 혼합 현실 터널링 효과,"We present mixed reality (MR) tunneling, a novel method to balance the trade-off between limited render performance and high visual quality of video see-through head-mounted displays (VST-HMDs) through fusing images of two types of camera sensors with different resolutions and frame rates. By merging a color video stream from an external stereoscopic camera with the grayscale VST commonly integrated into today’s standalone virtual reality (VR) headsets, we create a perceptually high-resolution and wide field of view VSTHMD prototype. The external high-resolution VST displayed at the central foveal to the para-peripheral region of the human visual field complements the low-resolution, low-latency grayscale VST at the far peripheral region, producing a tunneling effect, which simulates the human foveal and peripheral vision, with the potential to reduce cybersickness as in the tunneling effect in immersive VR. We propose two extensions to the MR tunneling method. The first one accommodates the user’s head movement speed by fading out the external VST when fast head movements are detected, thus potentially compensating for video streaming latency. The second one is a foveated MR tunneling effect, which displays the center of the external VST based on the tracked user eye movements. We evaluated the three MR tunneling methods in a within-subject study with 24 participants. The user study demonstrates the potential of our prototype and techniques based on the example of an assembly task that requires hand-eye coordination, untethered locomotion, and fine motor skills. The results demonstrate that, although not significant, the MR tunneling effects lead to higher overall usability, less perceived motion sickness, and a better sense of presence.1","우리는 해상도와 프레임 속도가 서로 다른 두 가지 유형의 카메라 센서의 이미지를 융합하여 제한된 렌더링 성능과 비디오 투명 머리 장착형 디스플레이(VST-HMD)의 높은 시각적 품질 사이의 균형을 맞추는 새로운 방법인 혼합 현실(MR) 터널링을 제시합니다. 외부 입체 카메라의 컬러 비디오 스트림을 오늘날의 독립형 가상 현실(VR) 헤드셋에 일반적으로 통합된 그레이스케일 VST와 병합하여 지각적으로 고해상도와 넓은 시야의 VSTHMD 프로토타입을 만듭니다. 인간 시야의 주변 주변 영역에 대한 중앙 중심와에 표시되는 외부 고해상도 VST는 먼 주변 영역의 저해상도, 저지연 그레이스케일 VST를 보완하여 인간 중심와 및 주변 시야를 시뮬레이션하는 터널링 효과를 생성하며 몰입형 VR의 터널링 효과에서와 같이 사이버 멀미를 줄일 수 있는 가능성이 있습니다. 우리는 MR 터널링 방법에 대한 두 가지 확장을 제안합니다. 첫 번째는 빠른 머리 움직임이 감지될 때 외부 VST를 페이드 아웃하여 사용자의 머리 움직임 속도를 수용함으로써 잠재적으로 비디오 스트리밍 지연 시간을 보상합니다. 두 번째는 추적된 사용자의 눈 움직임을 기반으로 외부 VST의 중심을 표시하는 포비티드 MR 터널링 효과입니다. 우리는 24명의 참가자를 대상으로 한 피험자 내 연구에서 세 가지 MR 터널링 방법을 평가했습니다. 사용자 연구는 손과 눈의 협응, 자유로운 이동 및 미세한 운동 기술이 필요한 조립 작업의 예를 기반으로 프로토타입과 기술의 잠재력을 보여줍니다. 결과는 중요하지는 않지만 MR 터널링 효과가 전반적인 사용성을 높이고, 멀미를 덜 느끼며, 존재감이 더 좋아지는 것으로 나타났습니다.1",https://doi.org/10.1109/ISMAR55827.2022.00018,Display & Optics; Interaction & Input,Cloud / Edge Computing,User Study; Quantitative Experiment,Algorithm / Method; Hardware / Device
468,2022,Multimodal Volume Data Exploration through Mid-Air Haptics,공중 햅틱을 통한 다중 모드 볼륨 데이터 탐색,"We present a mid-air haptic rendering method to explore volumetric data in augmented reality (AR) environments. Users directly interact with a volume-rendered hologram using their bare hands. Since the volume rendering method accumulates color along with its transparency, the depth perception of the user’s hand within the hologram is vague when used with only visual feedback. Therefore, to enhance the localization of internal structures within the volume data, we propose an AR system that provides a tactile presence for boundaries and associated information (e.g., texture). Leveraging Low-High (LH) histograms, our system provides boundary information to the user both visually and haptically. Specifically, when the user interacts with the volume data using their hands, the system computes a set of focal points or a set of tactile patterns using a GPGPU-based estimation. Additionally, we developed mid-air haptic rendering methods using amplitude modulation (AM) and spatiotemporal modulation (STM) techniques. Both methods were implemented on HoloLens 2 and could run in real-time. The effectiveness of each proposed method was evaluated with respect to various volume data sets, including synthetic and computed tomography (CT) scan data. Our results show that while both haptic rendering methods produce tangible experiences from the hologram interaction, our spatiotemporal modulation method generates better shape discrimination performance with respect to the amplitude modulation method in the case of multiple region of interests.","증강현실(AR) 환경에서 체적 데이터를 탐색하기 위한 공중 햅틱 렌더링 방법을 제시합니다. 사용자는 맨손으로 볼륨 렌더링된 홀로그램과 직접 상호 작용합니다. 볼륨렌더링 방식은 투명도와 함께 색상을 축적하기 때문에 시각적 피드백만으로 홀로그램 내 사용자 손의 깊이 인식이 모호하다. 따라서 볼륨 데이터 내 내부 구조의 위치 파악을 향상시키기 위해 경계 및 관련 정보(예: 질감)에 대한 촉각적 존재감을 제공하는 AR 시스템을 제안합니다. LH(Low-High) 히스토그램을 활용하는 당사 시스템은 시각적, 촉각적으로 사용자에게 경계 정보를 제공합니다. 특히 사용자가 손을 사용하여 볼륨 데이터와 상호 작용할 때 시스템은 GPGPU 기반 추정을 사용하여 초점 집합 또는 촉각 패턴 집합을 계산합니다. 또한 진폭 변조(AM) 및 시공간 변조(STM) 기술을 사용하여 공중 햅틱 렌더링 방법을 개발했습니다. 두 방법 모두 HoloLens 2에서 구현되었으며 실시간으로 실행될 수 있습니다. 제안된 각 방법의 효율성은 합성 및 컴퓨터 단층촬영(CT) 스캔 데이터를 포함한 다양한 볼륨 데이터 세트에 대해 평가되었습니다. 우리의 결과는 두 가지 햅틱 렌더링 방법이 홀로그램 상호 작용을 통해 실질적인 경험을 생성하는 반면, 시공간 변조 방법은 다중 관심 영역의 경우 진폭 변조 방법과 관련하여 더 나은 모양 식별 성능을 생성한다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR55827.2022.00039,Rendering & Visualization; Interaction & Input,Haptic / Tactile Feedback,Other,Algorithm / Method
469,2022,NailRing: An Intelligent Ring for Recognizing Micro-gestures in Mixed Reality,NailRing: 혼합 현실에서 마이크로 제스처를 인식하는 지능형 링,"Gesture interaction is currently a main interaction technology in the field of mixed reality. However, long-term and large-scale gesture in mid-air will lead to muscle fatigue and privacy problems, which cannot meet the comfort requirements of continuous interaction and inevitably hinder the development of mixed reality systems. To solve this problem, we propose NailRing, an intelligent ring to recognize fingertip micro-gestures using a micro-close-focus camera on a fingertip bracket. Such fingertip physiological characteristics as the changes in fingertip color distribution and muscle shape changes caused by fingertip pressure have been studied. According to the recognition principle, ten types of micro-gestures have been designed and used for contact interaction and one-hand interaction respectively. The accuracy of gesture recognition (cross-session $ F_{Macro}=98.3\%$; cross-person $ F_{Macro}=86.4\%$) in user studies verifies the performances of NailRing under different interaction conditions. Finally, the capability of NailRing in a series of potential application scenarios has also been discussed and analyzed.","제스처 상호작용은 현재 혼합현실 분야의 주요 상호작용 기술이다. 그러나 공중에서 장기간 및 대규모 제스처를 수행하면 근육 피로 및 개인 정보 보호 문제가 발생하여 지속적인 상호 작용에 대한 편안함 요구 사항을 충족할 수 없으며 필연적으로 혼합 현실 시스템의 개발을 방해하게 됩니다. 이러한 문제를 해결하기 위해 우리는 손가락 끝 브라켓에 장착된 마이크로 근접 초점 카메라를 이용하여 손가락 끝의 미세한 동작을 인식하는 지능형 링인 NailRing을 제안합니다. 손가락 끝의 압력에 따른 손가락 끝의 색상 분포 변화, 근육 모양의 변화 등 손가락 끝의 생리적 특성이 연구되었습니다. 인식 원리에 따라 접촉 상호 작용과 한 손 상호 작용에 각각 10가지 유형의 마이크로 제스처가 설계되어 사용되었습니다. 사용자 연구에서 제스처 인식의 정확도(교차 세션 $ F_{매크로}=98.3\%$; 교차 개인 $ F_{매크로}=86.4\%$)는 다양한 상호 작용 조건에서 NailRing의 성능을 검증합니다. 마지막으로, 일련의 잠재적인 응용 시나리오에서 NailRing의 기능에 대해서도 논의하고 분석했습니다.",https://doi.org/10.1109/ISMAR55827.2022.00032,Interaction & Input,Hand / Gesture Recognition,Quantitative Experiment,System / Framework
470,2022,Neural 3D Gaze: 3D Pupil Localization and Gaze Tracking based on Anatomical Eye Model and Neural Refraction Correction,신경 3D 시선: 해부학적 눈 모델과 신경 굴절 교정을 기반으로 한 3D 동공 위치 파악 및 시선 추적,"Eye tracking has already made its way to current commercial wearable display devices, and is becoming increasingly important for virtual and augmented reality applications. However, the existing model-based eye tracking solutions are not capable of conducting very accurate gaze angle measurements, and may not be sufficient to solve challenging display problems such as pupil steering or eyebox expansion. In this paper, we argue that accurate detection and localization of pupil in 3D space is a necessary intermediate step in model-based eye tracking. Existing methods and datasets either ignore evaluating the accuracy of 3D pupil localization or evaluate it only on synthetic data. To this end, we capture the first 3D pupilgaze-measurement dataset using a high precision setup with head stabilization and release it as the first benchmark dataset to evaluate both 3D pupil localization and gaze tracking methods. Furthermore, we utilize an advanced eye model to replace the commonly used oversimplified eye model. Leveraging the eye model, we propose a novel 3D pupil localization method with a deep learning-based corneal refraction correction. We demonstrate that our method outperforms the state-of-the-art works by reducing the 3D pupil localization error by 47.5% and the gaze estimation error by 18.7%. Our dataset and codes can be found here: link.","시선 추적은 이미 현재 상용 웨어러블 디스플레이 장치에 적용되었으며 가상 및 증강 현실 애플리케이션에서 점점 더 중요해지고 있습니다. 그러나 기존 모델 기반 안구 추적 솔루션은 매우 정확한 시선 각도 측정을 수행할 수 없으며, 동공 조정 또는 안구 상자 확장과 같은 까다로운 디스플레이 문제를 해결하는 데 충분하지 않을 수 있습니다. 본 논문에서는 3D 공간에서 동공의 정확한 검출과 위치 파악이 모델 기반 시선 추적의 필수 중간 단계라고 주장합니다. 기존 방법과 데이터 세트는 3D 동공 위치 파악의 정확성 평가를 무시하거나 합성 데이터에서만 평가합니다. 이를 위해 우리는 머리 안정화 기능을 갖춘 고정밀 설정을 사용하여 첫 번째 3D 동공 시선 측정 데이터 세트를 캡처하고 이를 3D 동공 위치 파악 및 시선 추적 방법을 모두 평가하기 위한 첫 번째 벤치마크 데이터 세트로 출시합니다. 또한 일반적으로 사용되는 지나치게 단순화된 눈 모델을 대체하기 위해 고급 눈 모델을 활용합니다. 눈 모델을 활용하여 딥러닝 기반 각막 굴절 보정을 적용한 새로운 3D 동공 위치 파악 방법을 제안합니다. 우리는 우리의 방법이 3D 동공 위치 파악 오류를 47.5%, 시선 추정 오류를 18.7% 줄임으로써 최첨단 작업보다 성능이 우수하다는 것을 입증합니다. 우리의 데이터 세트와 코드는 여기에서 찾을 수 있습니다: 링크.",https://doi.org/10.1109/ISMAR55827.2022.00053,Tracking & Localization; Interaction & Input,Eye / Gaze Tracking,Quantitative Experiment; Technical Evaluation,Algorithm / Method; Hardware / Device
471,2022,NeuroLens: Augmented Reality-based Contextual Guidance through Surgical Tool Tracking in Neurosurgery,NeuroLens: 신경외과 수술 도구 추적을 통한 증강 현실 기반 상황별 안내,"External ventricular drain (EVD) is a common, yet challenging neurosurgical procedure of placing a catheter into the brain ventricular system that requires prolonged training for surgeons to improve the catheter placement accuracy. In this paper, we introduce NeuroLens, an Augmented Reality (AR) system that provides neurosurgeons with guidance that aides them in completing an EVD catheter placement. NeuroLens builds on prior work in AR-assisted EVD to present a registered hologram of a patient’s ventricles to the surgeons, and uniquely incorporates guidance on the EVD catheter’s trajectory, angle of insertion, and distance to the target. The guidance is enabled by tracking the EVD catheter. We evaluate NeuroLens via a study with 33 medical students, in which we analyzed students’ EVD catheter insertion accuracy and completion time, eye gaze patterns, and qualitative responses. Our study, in which NeuroLens was used to aid students in inserting an EVD catheter into a realistic phantom model of a human head, demonstrated the potential of NeuroLens as a tool that will aid and educate novice neurosurgeons. On average, the use of NeuroLens improved the EVD placement accuracy of year 1 students by 39.4% and of the year 2–4 students by 45.7%. Furthermore, students who focused more on NeuroLens-provided contextual guidance achieved better results.","외부 심실 배수 장치(EVD)는 카테터 배치 정확도를 향상시키기 위해 외과 의사에게 장기간의 훈련이 필요한 뇌 심실 시스템에 카테터를 배치하는 일반적이면서도 어려운 신경외과적 절차입니다. 본 논문에서는 신경외과 의사에게 EVD 카테터 배치를 완료하는 데 도움이 되는 지침을 제공하는 증강 현실(AR) 시스템인 NeuroLens를 소개합니다. NeuroLens는 AR 지원 EVD의 이전 작업을 기반으로 환자의 심실 등록 홀로그램을 외과 의사에게 제공하고 EVD 카테터의 궤적, 삽입 각도 및 대상까지의 거리에 대한 지침을 고유하게 통합합니다. 안내는 EVD 카테터를 추적하여 활성화됩니다. 우리는 33명의 의대생을 대상으로 학생들의 EVD 카테터 삽입 정확도 및 완료 시간, 시선 패턴 및 정성적 반응을 분석한 연구를 통해 NeuroLens를 평가합니다. 학생들이 EVD 카테터를 인간 머리의 실제 팬텀 모델에 삽입하는 데 도움을 주기 위해 NeuroLens를 사용한 우리 연구에서는 초보 신경외과 의사를 지원하고 교육하는 도구로서 NeuroLens의 잠재력을 보여주었습니다. 평균적으로 NeuroLens를 사용하면 1학년 학생의 EVD 배치 정확도가 39.4%, 2~4학년 학생의 경우 45.7% 향상되었습니다. 또한 NeuroLens가 제공하는 상황별 안내에 더 집중한 학생들이 더 나은 결과를 얻었습니다.",https://doi.org/10.1109/ISMAR55827.2022.00051,Education & Training; Medical & Healthcare,Optical / Display Technology,Quantitative Experiment,System / Framework
472,2022,OA-SLAM: Leveraging Objects for Camera Relocalization in Visual SLAM,OA-SLAM: Visual SLAM에서 카메라 재지역화를 위해 객체 활용,"In this work, we explore the use of objects in Simultaneous Localization and Mapping in unseen worlds and propose an object-aided system (OA-SLAM). More precisely, we show that, compared to low-level points, the major benefit of objects lies in their higher-level semantic and discriminating power. Points, on the contrary, have a better spatial localization accuracy than the generic coarse models used to represent objects (cuboid or ellipsoid). We show that combining points and objects is of great interest to address the problem of camera pose recovery. Our main contributions are: (1) we improve the relocalization ability of a SLAM system using high-level object landmarks; (2) we build an automatic system, capable of identifying, tracking and reconstructing objects with 3D ellipsoids; (3) we show that object-based localization can be used to reinitialize or resume camera tracking. Our fully automatic system allows on-the-fly object mapping and enhanced pose tracking recovery, which we think, can significantly benefit to the AR community. Our experiments show that the camera can be relocalized from viewpoints where classical methods fail. We demonstrate that this localization allows a SLAM system to continue working despite a tracking loss, which can happen frequently with an uninitiated user. Our code and test data are released at gitlab.inria.fr/tangram/oa-slam.","본 연구에서는 보이지 않는 세계의 동시 위치 파악 및 매핑에서 객체의 활용을 탐구하고 객체 지원 시스템(OA-SLAM)을 제안합니다. 보다 정확하게는 낮은 수준의 지점과 비교하여 객체의 주요 이점은 더 높은 수준의 의미 및 식별력에 있다는 것을 보여줍니다. 반대로 점은 객체(직육면체 또는 타원체)를 나타내는 데 사용되는 일반적인 거친 모델보다 더 나은 공간 위치 파악 정확도를 갖습니다. 우리는 점과 객체를 결합하는 것이 카메라 포즈 복구 문제를 해결하는 데 큰 관심을 갖고 있음을 보여줍니다. 우리의 주요 기여는 다음과 같습니다: (1) 우리는 높은 수준의 객체 랜드마크를 사용하여 SLAM 시스템의 재위치화 능력을 향상시킵니다. (2) 3D 타원체로 물체를 식별, 추적 및 재구성할 수 있는 자동 시스템을 구축합니다. (3) 객체 기반 위치 파악을 사용하여 카메라 추적을 다시 초기화하거나 재개할 수 있음을 보여줍니다. 우리의 완전 자동 시스템은 즉각적인 객체 매핑과 향상된 자세 추적 복구를 가능하게 하며, 이는 AR 커뮤니티에 상당한 이점을 줄 수 있다고 생각합니다. 우리의 실험은 고전적인 방법이 실패하는 관점에서 카메라가 다시 위치화될 수 있음을 보여줍니다. 우리는 이 현지화를 통해 시작하지 않은 사용자에게 자주 발생할 수 있는 추적 손실에도 불구하고 SLAM 시스템이 계속 작동할 수 있음을 보여줍니다. 우리의 코드와 테스트 데이터는 gitlab.inria.fr/tangram/oa-slam에 공개되어 있습니다.",https://doi.org/10.1109/ISMAR55827.2022.00090,Tracking & Localization,SLAM / Spatial Mapping,Quantitative Experiment,Algorithm / Method
473,2022,PanoSynthVR: Toward Light-weight 360-Degree View Synthesis from a Single Panoramic Input,PanoSynthVR: 단일 파노라마 입력으로 가벼운 360도 뷰 합성을 향하여,"We investigate how real-time, 360° view synthesis can be achieved on current virtual reality hardware from a single panoramic image input. We introduce a light-weight method to automatically convert a single panoramic input into a multi-cylinder image representation that supports real-time, free-viewpoint view synthesis rendering for virtual reality. We apply an existing convolutional neural network trained on pinhole images to a cylindrical panorama with wrap padding to ensure agreement between the left and right edges. The network outputs a stack of semi-transparent panoramas at varying depths which can be easily rendered and composited with over blending. Quantitative experiments and a user study show that the method produces convincing parallax and fewer artifacts than a textured mesh representation.",우리는 단일 파노라마 이미지 입력을 통해 현재 가상 현실 하드웨어에서 실시간 360° 뷰 합성을 어떻게 달성할 수 있는지 조사합니다. 단일 파노라마 입력을 가상 현실을 위한 실시간 자유 시점 뷰 합성 렌더링을 지원하는 다중 실린더 이미지 표현으로 자동 변환하는 경량 방법을 소개합니다. 핀홀 이미지에 대해 훈련된 기존 컨볼루션 신경망을 랩 패딩이 있는 원통형 파노라마에 적용하여 왼쪽과 오른쪽 가장자리 간의 일치를 보장합니다. 네트워크는 오버 블렌딩을 통해 쉽게 렌더링하고 합성할 수 있는 다양한 깊이의 반투명 파노라마 스택을 출력합니다. 정량적 실험과 사용자 연구에 따르면 이 방법은 질감 있는 메시 표현보다 확실한 시차와 더 적은 아티팩트를 생성하는 것으로 나타났습니다.,https://doi.org/10.1109/ISMAR55827.2022.00075,Rendering & Visualization,Deep Learning / Neural Networks,User Study,Algorithm / Method
474,2022,Parallel Adaptation: Switching between Two Virtual Bodies with Different Perspectives Enables Dual Motor Adaptation,병렬 적응: 서로 다른 관점을 가진 두 개의 가상 몸체 간 전환으로 이중 모터 적응 가능,"Virtual Reality (VR) lets us experiment embodiment. Here we investigate dual embodiment under the prism of dual motor adaptation. We asked participants (N=21) to perform reaching motions in VR with opposite gradual visuomotor perturbations. The participants sequentially switched between 2 VR bodies and had to adapt to the VR body’s perturbation (up to +15° for the VR bodyA, and-15° for the VR bodyB). We then designed a 2$\times$2 within-subject study: 1 factor being the perspective (1st person or 3rd person), and 1 factor being the head rotation (without head rotation before the reaching motion or with head rotation before the reaching motion). We found that by providing strong visual cues between bodies (alternating symmetric perspective and/or symmetric head rotation), participants had little awareness of the perturbations, good adaptation, and large aftereffects in both VR bodies. Those elements are consistent with implicit dual adaptation. In contrast, a naive 1st person perspective resulted in little to no adaptation with a high cognitive load and no aftereffects.","가상 현실(VR)을 통해 구현을 실험할 수 있습니다. 여기서 우리는 이중 모터 적응이라는 프리즘 아래에서 이중 구현을 조사합니다. 우리는 참가자들(N=21)에게 점진적인 시력 운동 교란이 반대되는 VR에서 도달 동작을 수행하도록 요청했습니다. 참가자들은 2개의 VR 몸체 사이를 순차적으로 전환하고 VR 몸체의 섭동에 적응해야 했습니다(VR 몸체 A의 경우 최대 +15°, VR 몸체 B의 경우 -15°). 그런 다음 우리는 2$\times$2 주제 내 연구를 설계했습니다. 1개 요소는 관점(1인칭 또는 3인칭)이고, 1개 요소는 머리 회전(손 뻗기 동작 전 머리 회전 없음 또는 뻗기 동작 전 머리 회전 포함)입니다. 우리는 신체 간에 강력한 시각적 신호(대칭 관점 및/또는 대칭 머리 회전 교대)를 제공함으로써 참가자들이 두 VR 신체의 교란, 좋은 적응 및 큰 후유증에 대해 거의 인식하지 못한다는 것을 발견했습니다. 이러한 요소는 암묵적인 이중 적응과 일치합니다. 대조적으로, 순진한 1인칭 관점은 인지 부하가 ​​높고 후유증이 없는 적응을 거의 또는 전혀 하지 못했습니다.",https://doi.org/10.1109/ISMAR55827.2022.00031,Perception & Cognition,Other,User Study,Other
475,2022,Perceptibility of Jitter in Augmented Reality Head-Mounted Displays,증강 현실 헤드 마운트 디스플레이의 지터 인지도,"When using a see-through augmented reality head-mounted display system (AR HMD), a user’s perception of virtual content may be degraded by a variety of perceptual artifacts resulting from the architecture of rendering and display pipelines. In particular, virtual content that is rendered to appear stationary in the real world (worldlocked) can be susceptible to spatial and temporal 3D position errors. A subset of these errors, termed jitter, result from mismatches between the spatial localization, rendering, and display pipelines, and can manifest as perceived motion of intended-to-be stationary content. Here, we employ psychophysical methods to quantify the perceptibility of jitter artifacts in an AR HMD. For some viewing conditions, participants perceived jitter that was smaller than the pixel pitch of the testbed (i.e., subpixel jitter). In general, we found that jitter perceptibility increased as viewing distance increased and decreased as background luminance increased. We did not find that the contrast ratio of virtual content, age, or experience with AR/VR modulatedjitter perceptibility. Taken together, this study quantifies the degree of jitter that a user can perceive in an AR HMD and demonstrates that it is critical to consider the capabilities and limits of the human visual system when designing the next generation of spatial computing platforms.","투명한 증강 현실 헤드 마운트 디스플레이 시스템(AR HMD)을 사용할 때 가상 콘텐츠에 대한 사용자의 인식은 렌더링 및 디스플레이 파이프라인의 아키텍처로 인해 발생하는 다양한 인식 아티팩트로 인해 저하될 수 있습니다. 특히 현실 세계(세계 잠금)에서 정지된 것처럼 보이도록 렌더링된 가상 콘텐츠는 공간적 및 시간적 3D 위치 오류에 취약할 수 있습니다. 지터라고 하는 이러한 오류의 하위 집합은 공간 위치 파악, 렌더링 및 디스플레이 파이프라인 간의 불일치로 인해 발생하며 의도된 고정 콘텐츠의 인식된 모션으로 나타날 수 있습니다. 여기서는 AR HMD에서 지터 아티팩트의 인지도를 정량화하기 위해 정신물리학적 방법을 사용합니다. 일부 보기 조건에서 참가자는 테스트베드의 픽셀 피치보다 작은 지터(즉, 하위 픽셀 지터)를 인식했습니다. 일반적으로 지터 인지도는 시청 거리가 증가할수록 증가하고, 배경 휘도가 증가함에 따라 감소하는 것으로 나타났습니다. AR/VR 변조된 지터 인식에 대한 가상 콘텐츠, 연령 또는 경험의 명암비를 찾지 못했습니다. 종합하면, 이 연구는 사용자가 AR HMD에서 인지할 수 있는 지터의 정도를 정량화하고 차세대 공간 컴퓨팅 플랫폼을 설계할 때 인간 시각 시스템의 기능과 한계를 고려하는 것이 중요하다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR55827.2022.00063,Display & Optics; Perception & Cognition,Other,User Study,System / Framework
476,2022,Personalization of a Mid-Air Gesture Keyboard using Multi-Objective Bayesian Optimization,다중 목표 베이지안 최적화를 사용한 공중 제스처 키보드의 개인화,"We present AdaptiKeyboard, a mid-air gesture keyboard that uses multi-objective Bayesian optimization to adaptively change layout size to simultaneously optimize speed and accuracy. Gesture keyboards are well suited for enabling mid-air text entry in augmented reality (AR) due to their relative robustness to articulation inaccuracy. However, transplanting gesture keyboards to AR involves a larger design and operational space compared to touchscreen interactions. One potential advantage of this larger design and operational space is that mid-air keyboards presented in AR can be more versatile than their touchscreen equivalents. A key component of a mid-air gesture keyboard is the layout size, which can be made adaptive in order to optimize text entry speed and accuracy at the individual user level. This adaptive personalization can refine the keyboard design to reflect the differences users exhibit in motor behaviors and personal preferences. In this paper, we propose a multi-objective Bayesian optimization approach for adapting the layout size of a mid-air gesture keyboard to individual users. We show that this process can deliver a 14.4% improvement in speed and a 13.8% improvement in accuracy relative to a baseline design with a constant size derived from the default system keyboard on the HoloLens 2.",다목적 베이지안 최적화를 사용하여 레이아웃 크기를 적응적으로 변경하여 속도와 정확성을 동시에 최적화하는 공중 제스처 키보드인 AdaptiKeyboard를 소개합니다. 제스처 키보드는 발음 부정확성에 대한 상대적 견고성으로 인해 증강 현실(AR)에서 공중 텍스트 입력을 구현하는 데 매우 적합합니다. 하지만 제스처 키보드를 AR로 이식하려면 터치스크린 상호작용에 비해 디자인과 작동 공간이 더 커야 합니다. 이러한 더 넓은 디자인과 작동 공간의 잠재적인 이점 중 하나는 AR에 제공되는 공중 키보드가 터치스크린 키보드보다 더 다양할 수 있다는 것입니다. 공중 제스처 키보드의 핵심 구성 요소는 개별 사용자 수준에서 텍스트 입력 속도와 정확성을 최적화하기 위해 적응형으로 만들 수 있는 레이아웃 크기입니다. 이러한 적응형 개인화는 사용자가 운동 행동과 개인 선호도에서 나타내는 차이를 반영하여 키보드 디자인을 개선할 수 있습니다. 본 논문에서는 공중 제스처 키보드의 레이아웃 크기를 개별 사용자에 맞게 조정하기 위한 다목적 베이지안 최적화 접근 방식을 제안합니다. 이 프로세스는 HoloLens 2의 기본 시스템 키보드에서 파생된 일정한 크기를 사용하는 기본 디자인에 비해 속도가 14.4% 향상되고 정확도가 13.8% 향상될 수 있음을 보여줍니다.,https://doi.org/10.1109/ISMAR55827.2022.00088,Interaction & Input,Sensor Fusion,Quantitative Experiment,Algorithm / Method
477,2022,Petting a cat helps you incarnate the avatar: Influence of the emotions over embodiment in VR,고양이를 쓰다듬으면 아바타를 구현하는 데 도움이 됩니다. VR 구현에 대한 감정의 영향,"Thanks to its strong capacity to immerse users in virtual worlds, virtual reality can elicit various emotions with diverse environments. This aspect of virtual reality makes it an interesting and powerful tool in many fields, such as entertainment with scenarios based on a strong emotional implication, training in particular for social or communication skills, or even medical therapy with phobia or addiction treatment. However, in virtual reality the participant lives the experience through an avatar, and feels the emotion according to what happens to this avatar.This paper discusses the link between embodiment and emotional implication in virtual reality. In particular, we looked at how emotions and the sense of embodiment are correlated in virtual reality. Through an experiment, we demonstrate that the sense of embodiment is strongly correlated with the emotional experience of the virtual environment. The sense of embodiment is increased when the virtual scenarios make the participants feel strong emotions, whether those emotions are positive or negative. We also show that emotions mainly affect two sub-components of embodiment: the appearance and response sub-scales.","가상 현실은 사용자를 가상 세계에 몰입시키는 강력한 능력 덕분에 다양한 환경에서 다양한 감정을 이끌어 낼 수 있습니다. 가상 현실의 이러한 측면은 강한 감정적 암시를 기반으로 한 시나리오를 통한 엔터테인먼트, 특히 사회적 또는 의사소통 기술을 위한 훈련, 심지어 공포증이나 중독 치료를 포함한 의료 치료와 같은 다양한 분야에서 흥미롭고 강력한 도구가 됩니다. 그러나 가상 현실에서 참가자는 아바타를 통해 경험을 하고, 이 아바타에게 일어나는 일에 따라 감정을 느낍니다. 본 논문은 가상 현실에서의 구체화와 정서적 함의 사이의 연관성을 논의합니다. 특히 가상현실에서 감정과 체현감이 어떤 상관관계가 있는지 살펴보았다. 실험을 통해 우리는 체현감이 가상 환경의 정서적 경험과 밀접한 상관관계가 있음을 보여줍니다. 가상 시나리오를 통해 참가자가 긍정적이든 부정적이든 강한 감정을 느끼게 되면 구체화 감각이 높아집니다. 우리는 또한 감정이 주로 구체화의 두 가지 하위 구성 요소인 출현 및 반응 하위 척도에 영향을 미친다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR55827.2022.00028,Perception & Cognition; Collaboration & Social,Other,User Study,User Study / Empirical Findings
478,2022,Plausibility and Perception of Personalized Virtual Humans between Virtual and Augmented Reality,가상현실과 증강현실 간 개인화된 가상인간의 타당성과 인식,"This article investigates the effects of different XR displays on the perception and plausibility of personalized virtual humans. We compared immersive virtual reality (VR), video see-through augmented reality (VST AR), and optical see-through AR (OST AR). The personalized virtual alter egos were generated by state-of-the-art photogrammetry methods. 42 participants were repeatedly exposed to animated versions of their 3D-reconstructed virtual alter egos in each of the three XR display conditions. The reconstructed virtual alter egos were additionally modified in body weight for each repetition. We show that the display types lead to different degrees of incongruence between the renderings of the virtual humans and the presentation of the respective environmental backgrounds, leading to significant effects of perceived mismatches as part of a plausibility measurement. The device-related effects were further partly confirmed by subjective misestimations of the modified body weight and the measured spatial presence. Here, the exceedingly incongruent OST AR condition leads to the significantly highest weight misestimations as well as to the lowest perceived spatial presence. However, similar effects could not be confirmed for the affective appraisal (i.e., humanness, eeriness, or attractiveness) of the virtual humans, giving rise to the assumption that these factors might be unrelated to each other.","이 기사에서는 다양한 XR 디스플레이가 개인화된 가상 인간의 인식과 타당성에 미치는 영향을 조사합니다. 몰입형 가상 현실(VR), 비디오 투시 증강 현실(VST AR), 광학 투시 AR(OST AR)을 비교했습니다. 개인화된 가상 분신은 최첨단 사진 측량 방법을 통해 생성되었습니다. 42명의 참가자는 세 가지 XR 디스플레이 조건 각각에서 3D로 재구성된 가상 분신의 애니메이션 버전에 반복적으로 노출되었습니다. 재구성된 가상 분신은 각 반복마다 체중이 추가로 수정되었습니다. 우리는 디스플레이 유형이 가상 인간의 렌더링과 각 환경 배경의 표현 사이에 서로 다른 정도의 부조화를 초래하여 타당성 측정의 일부로 인식된 불일치의 중요한 영향을 초래한다는 것을 보여줍니다. 장치 관련 효과는 수정된 체중과 측정된 공간 존재에 대한 주관적인 잘못된 추정에 의해 부분적으로 확인되었습니다. 여기서, 매우 부적합한 OST AR 조건은 상당히 높은 중량 잘못된 추정과 가장 낮은 인식 공간 존재로 이어집니다. 그러나 가상인간에 대한 정서적 평가(인간다움, 으스스함, 매력 등)에 대해서는 유사한 효과를 확인할 수 없어 이들 요소가 서로 무관할 수 있다는 가정이 제기되었다.",https://doi.org/10.1109/ISMAR55827.2022.00065,Perception & Cognition; Display & Optics,3D Reconstruction,User Study,Algorithm / Method; Hardware / Device
479,2022,Portal Rendering and Creation Interactions in Virtual Reality,가상 현실에서의 포털 렌더링 및 생성 상호 작용,"Transformative portals are a novel technique for supporting visualizations and interactions between locations that are not co-located in virtual reality (VR). We are interested in improving upon current portal rendering for VR, and supporting VR users to create new portals quickly and accurately. In this paper, we introduce a new high-level algorithm for rendering transformative portals in VR and present the benefits of our approach. By leveraging single-pass stereo and stencil portal rendering, we developed our algorithm to support multiple portals with “infinite” recursion, while running performant on mobile VR devices. We then benchmarked our performance against other common portal rendering implementations. In addition, we focused our research on VR interactions for handheld portals, in which the initial placement of the portal’s destination is imperative to its effectiveness. In this paper, we present four new portal creation interactions for handheld portals: fishing reel, raycast with fishing reel, marker, and projectile curve. Based on existing research, we also established standard controls for fine-grained portal manipulation. In a user study, we compared and evaluated how well the creation interactions performed and argue that projectile curve is the most suitable general-purpose technique.","변형 포털은 가상 현실(VR)에서 같은 위치에 있지 않은 위치 간의 시각화 및 상호 작용을 지원하는 새로운 기술입니다. 우리는 현재 VR용 포털 렌더링을 개선하고 VR 사용자가 새로운 포털을 빠르고 정확하게 만들 수 있도록 지원하는 데 관심이 있습니다. 본 논문에서는 VR에서 변형 포털을 렌더링하기 위한 새로운 고급 알고리즘을 소개하고 우리 접근 방식의 이점을 제시합니다. 단일 패스 스테레오 및 스텐실 포털 렌더링을 활용하여 우리는 모바일 VR 장치에서 성능을 실행하면서 ""무한"" 재귀로 여러 포털을 지원하는 알고리즘을 개발했습니다. 그런 다음 다른 일반적인 포털 렌더링 구현과 비교하여 성능을 벤치마킹했습니다. 또한 우리는 포털 대상의 초기 배치가 효율성에 필수적인 휴대용 포털의 VR 상호 작용에 대한 연구에 중점을 두었습니다. 이 문서에서는 낚시 릴, 낚시 릴을 사용한 레이캐스트, 마커, 발사체 곡선 등 휴대용 포털을 위한 네 가지 새로운 포털 생성 상호 작용을 제시합니다. 기존 연구를 바탕으로 세분화된 포털 조작을 위한 표준 제어도 확립했습니다. 사용자 연구에서 우리는 생성 상호 작용이 얼마나 잘 수행되는지 비교 및 ​​평가했으며 발사체 곡선이 가장 적합한 범용 기술이라고 주장했습니다.",https://doi.org/10.1109/ISMAR55827.2022.00030,Interaction & Input; Rendering & Visualization,Other,User Study,Algorithm / Method
480,2022,Real-time Gaze Tracking with Head-eye Coordination for Head-mounted Displays,머리 장착형 디스플레이를 위한 머리-눈 조정을 통한 실시간 시선 추적,"High-accuracy, low-latency gaze tracking is becoming one of the indispensable features in augmented reality (AR) head-mounted devices (HMDs). Researchers have proposed different approaches to predict gaze positions from eye images. However, since only the eye modality is focused, these appearance-based algorithms are still struggle to trade off the accuracy and running speed in HMDs. In this paper, we propose a lightweight multi-modal network (HE-Tracker) to regress gaze positions. By fusing head-movement features with eye features, HE-Tracker achieves comparable accuracy (3.655° in all subjects) and $27 \times$ speedup (48 fps in the specialized AR HMD) compared to the state-of-the-art gaze tracking algorithm. We further demonstrate that when applying our head-eye coordination strategy to other baseline models, all these models achieve at least 6.36% performance improvement without a pronounced effect on running speed. Moreover, we construct HE-Gaze, the first multi-modal dataset with eye images and head-movement data for near-eye gaze tracking. This dataset is currently made of 757,360 frames and 15 persons, providing an opportunity to foster research in multi-modal gaze tracking approaches. Our dataset is available at DOWNLOAD LINK 1.","고정밀, 저지연 시선 추적은 증강 현실(AR) 헤드 장착 장치(HMD)에서 없어서는 안 될 기능 중 하나가 되고 있습니다. 연구자들은 눈 이미지로부터 시선 위치를 예측하기 위한 다양한 접근법을 제안했습니다. 그러나 눈 양식에만 초점이 맞춰지기 때문에 이러한 모양 기반 알고리즘은 HMD의 정확도와 실행 속도를 절충하는 데 여전히 어려움을 겪고 있습니다. 본 논문에서는 시선 위치 회귀를 위한 경량 다중 모드 네트워크(HE-Tracker)를 제안합니다. 머리 움직임 기능과 눈 기능을 융합함으로써 HE-Tracker는 최첨단 시선 추적 알고리즘과 비교하여 비슷한 정확도(모든 피사체에서 3.655°)와 $27 \times$ 속도 향상(특수 AR HMD의 경우 48fps)을 달성합니다. 우리는 머리-눈 조정 전략을 다른 기본 모델에 적용할 때 이러한 모든 모델이 실행 속도에 큰 영향을 주지 않고 최소 6.36%의 성능 향상을 달성한다는 것을 추가로 보여줍니다. 또한, 근거리 시선 추적을 위한 눈 이미지와 머리 움직임 데이터를 갖춘 최초의 다중 모드 데이터 세트인 HE-Gaze를 구성합니다. 이 데이터 세트는 현재 757,360개의 프레임과 15명의 사람으로 구성되어 있어 다중 모드 시선 추적 접근 방식에 대한 연구를 촉진할 수 있는 기회를 제공합니다. 우리의 데이터세트는 DOWNLOAD LINK 1에서 이용 가능합니다.",https://doi.org/10.1109/ISMAR55827.2022.00022,Display & Optics,Eye / Gaze Tracking,Quantitative Experiment; Technical Evaluation,Algorithm / Method
481,2022,Real-time Shadow-aware Portrait Relighting in Virtual Backgrounds for Realistic Telepresence,사실적인 텔레프레즌스를 위한 가상 배경의 실시간 그림자 인식 인물 사진 재조명,"While using virtual backgrounds has recently become a very popular feature in videoconferencing, there often exists a jarring mismatch between the lighting of the user and the illumination condition of the virtual background. Existing portrait relighting methods can alleviate the problem, but do not have the capacity to deal with difficult shadow effects. In this paper, we present a new shadow-aware portrait relighting system that can relight an input portrait to be consistent with a given desired background image with shadow effects. Our system consists of four major components: portrait neutralization, illumination estimation, shadow generation and hierarchical neural rendering, which are all based on deep neural networks, and the whole system is end-to-end trainable. In addition, we created a large-scale photorealistic synthetic dataset with shadow, illumination and depth annotations for training, which allows our model to generalize well to real images. The extensive experiments demonstrate that our shadow-aware relight system outperforms the state-of-the-art portrait relighting solutions in terms of producing more lighting-consistent relighted images with shadow effects.","가상 배경을 사용하는 것이 최근 화상 회의에서 매우 인기 있는 기능이 되었지만, 사용자의 조명과 가상 배경의 조명 조건 사이에 불일치가 발생하는 경우가 종종 있습니다. 기존 인물 조명 재조명 방법은 문제를 완화할 수 있지만 어려운 그림자 효과를 처리할 수 있는 능력이 없습니다. 본 논문에서는 그림자 효과를 사용하여 주어진 원하는 배경 이미지와 일치하도록 입력 인물 사진을 재조명할 수 있는 새로운 그림자 인식 인물 재조명 시스템을 제시합니다. 우리 시스템은 인물 중립화, 조명 추정, 그림자 생성 및 계층적 신경 렌더링의 네 가지 주요 구성 요소로 구성되어 있으며 모두 심층 신경망을 기반으로 하며 전체 시스템은 엔드투엔드 학습이 가능합니다. 또한 훈련을 위한 그림자, 조명 및 깊이 주석이 포함된 대규모의 사실적인 합성 데이터세트를 만들었으므로 모델이 실제 이미지에 잘 일반화될 수 있습니다. 광범위한 실험을 통해 우리의 그림자 인식 재조명 시스템이 그림자 효과를 사용하여 조명이 더욱 일관적으로 재조명된 이미지를 생성한다는 측면에서 최첨단 인물 사진 재조명 솔루션보다 성능이 뛰어남을 보여줍니다.",https://doi.org/10.1109/ISMAR55827.2022.00091,Rendering & Visualization,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method; Dataset / Benchmark
482,2022,Selection Techniques for 3D Extended Desktop Workstation with AR HMD,AR HMD를 탑재한 3D 확장 데스크탑 워크스테이션 선택 기술,"Extending a standard desktop workstation (i.e. a screen, a mouse, a keyboard) with virtual scenes displayed on an Augmented Reality Head-Mounted Display (AR HMD) offers many identified advantages including limited physical space requirements, very large and flexible display spaces, and 3D stereoscopic views. While the technologies become more mainstream, the remaining open question is how to interact with such hybrid workstations that combine 2D views displayed on a physical monitor and 3D views displayed on a HoloLens. For a selection task, we compared mouse-based interaction (standard for 2D desktop workstations) and direct touch interaction in mid-air (standard for 3D AR) while considering different positions of the 3D scene according to a physical monitor. To extend mouse-based selection to 3D views, we experimentally explored different interaction metaphors where the mouse cursor moves either on a horizontal or a vertical plane in a 3D virtual scene. To check for ecological validity of our results, we conducted an additional study focusing on interaction with a 2D/3D Gapminder dataset visualization. The results show 1) that the mouse-based interaction, as compared to direct touch interaction in mid-air, is easy and efficient, 2) that using a vertical plane placed in front of the 3D virtual scene to mimic the double screen metaphor outperforms other interaction techniques and 3) that flexibility is required to allow users to choose the selection techniques and to position the 3D virtual scene relative to the physical monitor. Based on these results, we derive interaction design guidelines for hybrid workstations.","증강 현실 헤드 마운트 디스플레이(AR HMD)에 표시되는 가상 장면으로 표준 데스크톱 워크스테이션(예: 화면, 마우스, 키보드)을 확장하면 제한된 물리적 공간 요구 사항, 매우 크고 유연한 디스플레이 공간, 3D 입체 보기를 포함하여 확인된 많은 이점을 제공합니다. 기술이 더욱 주류가 되면서 남은 질문은 실제 모니터에 표시되는 2D 보기와 HoloLens에 표시되는 3D 보기를 결합하는 하이브리드 워크스테이션과 상호 작용하는 방법입니다. 선택 작업을 위해 물리적 모니터에 따른 3D 장면의 다양한 위치를 고려하면서 마우스 기반 상호 작용(2D 데스크톱 워크스테이션의 표준)과 공중에서의 직접 터치 상호 작용(3D AR의 표준)을 비교했습니다. 마우스 기반 선택을 3D 보기로 확장하기 위해 우리는 3D 가상 장면에서 마우스 커서가 수평 또는 수직 평면에서 움직이는 다양한 상호 작용 비유를 실험적으로 탐색했습니다. 결과의 생태학적 타당성을 확인하기 위해 2D/3D Gapminder 데이터 세트 시각화와의 상호 작용에 초점을 맞춘 추가 연구를 수행했습니다. 결과는 1) 공중에서 직접 터치하는 상호작용에 비해 마우스 기반 상호작용이 쉽고 효율적이라는 점, 2) 이중 화면 비유를 모방하기 위해 3D 가상 장면 앞에 수직면을 배치하는 것이 다른 상호작용 기술보다 성능이 우수하다는 점, 3) 사용자가 선택 기술을 선택하고 물리적 모니터를 기준으로 3D 가상 장면을 배치할 수 있도록 유연성이 필요하다는 점을 보여줍니다. 이러한 결과를 바탕으로 하이브리드 워크스테이션에 대한 인터랙션 디자인 지침을 도출합니다.",https://doi.org/10.1109/ISMAR55827.2022.00062,Interaction & Input; Display & Optics,Other,Technical Evaluation,Interaction Technique
483,2022,Sensorimotor Realities: Formalizing Ability-Mediating Design for Computer-Mediated Reality Environments,감각운동 현실: 컴퓨터 중재 현실 환경을 위한 능력 중재 설계 공식화,"We introduce “Sensorimotor Realities,” a new concept in the XR landscape and corresponding technology-agnostic framework for computer-mediated perception and motor action. Sensorimotor Realities capitalize on the heterogeneity of human sensorimotor abilities to support conceptualization, characterization, and design of computer technology that leverages existing abilities in new, computer-mediated worlds. We introduce a conceptual space for Sensorimotor Realities with six dimensions, discuss examples of interactive systems in this space, and show how Sensorimotor Realities are distinct in nature and goal from Augmented, Mixed, Virtual, and Mediated Reality. We capitalize on Sensorimotor Realities to propose “ability-mediating design,” an approach to designing accessible interactive computer systems that complements ability-based design. We discuss how Sensorimotor Realities offer new opportunities for research and development at the intersection of XR, wearable computing, ambient intelligence, and accessible computing, and draw a research roadmap with three major directions of scientific investigation.","우리는 XR 환경의 새로운 개념인 ""Sensorimotor Realities""와 컴퓨터 매개 인식 및 운동 동작을 위한 기술에 구애받지 않는 프레임워크를 소개합니다. 감각 운동 현실은 새로운 컴퓨터 매개 세계에서 기존 능력을 활용하는 컴퓨터 기술의 개념화, 특성화 및 설계를 지원하기 위해 인간 감각 운동 능력의 이질성을 활용합니다. 우리는 6차원으로 구성된 감각 운동 현실의 개념적 공간을 소개하고, 이 공간에서 상호 작용 시스템의 예를 논의하며, 감각 운동 현실이 증강 현실, 혼합 현실, 가상 현실, 매개 현실과 성격 및 목표 면에서 어떻게 구별되는지 보여줍니다. 우리는 감각 운동 현실을 활용하여 능력 기반 디자인을 보완하는 접근 가능한 대화형 컴퓨터 시스템을 디자인하는 접근 방식인 ""능력 중재 디자인""을 제안합니다. 우리는 Sensorimotor Realities가 어떻게 XR, 웨어러블 컴퓨팅, 주변 지능, 접근 가능한 컴퓨팅의 교차점에서 연구 개발을 위한 새로운 기회를 제공하는지 논의하고 과학 조사의 세 가지 주요 방향으로 연구 로드맵을 그려냅니다.",https://doi.org/10.1109/ISMAR55827.2022.00086,Perception & Cognition,Other,Other,System / Framework; Hardware / Device
484,2022,Stereoscopic Video See-Through Head-Mounted Displays for Laser Safety: An Empirical Evaluation at Advanced Optics Laboratories,레이저 안전을 위한 입체 비디오 투명 헤드 장착형 디스플레이: 고급 광학 연구소의 실증적 평가,"Nowadays, high-power and multi-spectral lasers are used in many scientific experiments and industrial processes. Those laser sources can rapidly cause permanent damage to human eyes. Research and development work with those laser sources requires typically wearing personal protective equipment (PPE), such as laser safety goggles as eye protectors. Currently, laser safety goggles are based on optical spectral filters, which block spectral bands where hazardous laser radiation is emitted. Such laser safety goggles can filter up to 99% of the visible spectrum, rendering researchers working in hazardous and complex laboratory environments visually impaired. Video see-through head-mounted displays (VST-HMD) could be used as eye protectors without reducing users’ visibility of the environment since they can be constructed such that all laser and ambient light is blocked from the human eye. To date, this application domain is still largely unexplored in the MR community. To our best knowledge, there has been no comprehensive work that investigates the human factors of such an eye protection method at an advanced optics laboratory. In this work, we present the results of an empirical study where we evaluate the usability, perceived safety, advantages, and limitations of using VST-HMDs as laser safety goggles. We use a stereoscopic VST-HMD developed through a human-centered design approach at one of the most advanced optics laboratories in the world. 18 participants including 14 laser experts evaluated the current prototype. Our user evaluation and field studies confirm that the complex and hazardous working conditions at high-energy laser laboratories could be significantly improved with MR technology.","요즘에는 많은 과학 실험과 산업 공정에서 고출력 다중 스펙트럼 레이저가 사용됩니다. 이러한 레이저 소스는 사람의 눈에 급속히 영구적인 손상을 줄 수 있습니다. 이러한 레이저 소스를 사용한 연구 및 개발 작업에는 일반적으로 눈 보호용 레이저 안전 고글과 같은 개인 보호 장비(PPE)를 착용해야 합니다. 현재 레이저 안전 고글은 위험한 레이저 방사선이 방출되는 스펙트럼 대역을 차단하는 광학 스펙트럼 필터를 기반으로 합니다. 이러한 레이저 안전 고글은 가시 스펙트럼의 최대 99%를 필터링할 수 있어 위험하고 복잡한 실험실 환경에서 일하는 연구자에게 시각 장애를 줄 수 있습니다. VST-HMD(Video see-through Head Mounted Display)는 사람의 눈에서 모든 레이저 및 주변광을 차단하도록 구성할 수 있으므로 사용자의 환경 가시성을 줄이지 않고도 눈 보호 장치로 사용할 수 있습니다. 현재까지 이 응용 프로그램 도메인은 MR 커뮤니티에서 아직 대부분 탐색되지 않았습니다. 우리가 아는 한, 첨단 광학 연구실에서 이러한 눈 보호 방법의 인적 요소를 조사하는 포괄적인 작업은 없었습니다. 본 연구에서는 VST-HMD를 레이저 안전 고글로 사용하는 경우의 유용성, 인지된 안전성, 장점 및 한계를 평가하는 실증적 연구 결과를 제시합니다. 우리는 세계에서 가장 진보된 광학 연구소 중 하나에서 인간 중심 설계 접근 방식을 통해 개발된 입체 VST-HMD를 사용합니다. 레이저 전문가 14명을 포함한 18명의 참가자가 현재 프로토타입을 평가했습니다. 우리의 사용자 평가와 현장 연구를 통해 고에너지 레이저 실험실의 복잡하고 위험한 작업 조건이 MR 기술을 통해 크게 개선될 수 있음이 확인되었습니다.",https://doi.org/10.1109/ISMAR55827.2022.00025,Display & Optics,Optical / Display Technology,User Study,Algorithm / Method
485,2022,Strafing Gain: Redirecting Users One Diagonal Step at a Time,Strafing Gain: 사용자를 한 번에 대각선으로 한 단계씩 리디렉션,"Redirected walking can effectively utilize a user’s physical space when traversing larger virtual environments by using virtual self-motion gains for a user’s physical motions. In particular, curvature gain presents unique advantages in redirection but can lead to suboptimal orientations. To prevent this and add additional utility in redirected walking, we formally present strafing gain. Strafing gain seeks to add incremental lateral movements to a user’s position causing the user to walk along a diagonal trajectory while maintaining the original orientation of the user. In a study with 27 participants, we tested 11 values to determine the detection thresholds of strafing gain. The study, which was modeled on prior detection threshold studies, found that strafing gain could successfully redirect participants to walk along a 5.57° diagonal to the right and a 4.68° diagonal to the left. Furthermore, a supplementary study with 10 participants was conducted, verifying that orientation was maintained throughout redirection and validating the obtained detection thresholds. We discuss the implications of these findings and potential ways of improving these quantities in real-world applications.","방향 전환 걷기는 사용자의 물리적 동작에 대한 가상 자체 동작 이득을 사용하여 더 큰 가상 환경을 횡단할 때 사용자의 물리적 공간을 효과적으로 활용할 수 있습니다. 특히 곡률 이득은 방향 전환 시 고유한 이점을 제공하지만 최적이 아닌 방향으로 이어질 수 있습니다. 이를 방지하고 방향 전환에 추가적인 유용성을 추가하기 위해 공식적으로 Strafing Gain을 제시합니다. Strafing Gain은 사용자의 원래 방향을 유지하면서 사용자가 대각선 궤적을 따라 걸을 수 있도록 사용자의 위치에 점진적인 측면 이동을 추가하려고 합니다. 27명의 참가자를 대상으로 한 연구에서 우리는 스트래핑 게인의 감지 임계값을 결정하기 위해 11개의 값을 테스트했습니다. 이전 감지 임계값 연구를 모델로 한 이 연구에서는 기총 이득이 참가자들을 성공적으로 오른쪽 대각선 5.57°, 왼쪽 대각선 4.68°를 따라 걷도록 방향을 바꿀 수 있음을 발견했습니다. 또한 10명의 참가자를 대상으로 한 보충 연구를 수행하여 리디렉션 전반에 걸쳐 방향이 유지되는지 확인하고 획득된 감지 임계값을 검증했습니다. 우리는 이러한 발견의 의미와 실제 적용에서 이러한 양을 향상시킬 수 있는 잠재적인 방법에 대해 논의합니다.",https://doi.org/10.1109/ISMAR55827.2022.00077,Other,Redirected Walking / Locomotion,User Study,Algorithm / Method
486,2022,Studying the Effects of Network Latency on Audio-Visual Perception During an AR Musical Task,AR 음악 작업 중 시청각 인식에 대한 네트워크 지연 시간의 영향 연구,"Augmented Reality (AR), with its ability to make people feel like they are in the same space as friends from across the world, is an ideal medium for the purpose of Networked Musical Collaboration. Most conventional systems that enable networked musical collaboration minimize network latency by focusing on the transfer of auditory information at the expense of visual feedback. Studies into human perception have shown that sensory integration of audio and visual stimuli can take place even when there is a slight delay between the two signals. We studied the way changes in network latency effect participants’ auditory and visual perception in latency detection, latency tolerance and attention focus; in this paper, we explore the trade-off between the presence of AR visuals and the minimization of latency. Twenty-four participants were asked to play a hand drum and collaborate with a prerecorded remote musician rendered as an avatar in AR. Multiple trials involving different levels of audio-visual latency were conducted. We then analyzed the subjective responses of the participants together with the recorded musical information from each session. Results indicate a minimum noticeable delay value–defined as the highest amount of delay that can be experienced before two stimulated senses are perceived as separate events–between 160 milliseconds (ms) and 320 ms. Players also reported that a delay between sound and an accompanying avatar animation became less tolerable at 320 ms of delay, but was never completely intolerable, even up to 1200 ms of delay. We conclude that players begin to notice delay at about 320 ms and most players can tolerate large delays between sound and animation.","사람들이 전 세계 친구들과 같은 공간에 있는 것처럼 느끼게 하는 증강 현실(AR)은 네트워크 음악 협업 목적에 이상적인 매체입니다. 네트워크로 연결된 음악 협업을 가능하게 하는 대부분의 기존 시스템은 시각적 피드백을 희생하면서 청각 정보 전송에 중점을 두어 네트워크 대기 시간을 최소화합니다. 인간의 지각에 대한 연구에 따르면 두 신호 사이에 약간의 지연이 있는 경우에도 오디오와 시각적 자극의 감각 통합이 발생할 수 있는 것으로 나타났습니다. 우리는 네트워크 대기 시간의 변화가 대기 시간 감지, 대기 시간 내성 및 주의 집중에 있어 참가자의 청각 및 시각적 인식에 미치는 영향을 연구했습니다. 본 논문에서는 AR 비주얼의 존재와 지연 시간 최소화 간의 균형을 탐구합니다. 24명의 참가자는 손으로 드럼을 연주하고 AR에서 아바타로 렌더링된 사전 녹음된 원격 음악가와 협력하도록 요청 받았습니다. 다양한 수준의 시청각 대기 시간과 관련된 여러 시험이 수행되었습니다. 그런 다음 각 세션에서 녹음된 음악 정보와 함께 참가자의 주관적인 반응을 분석했습니다. 결과는 두 개의 자극된 감각이 별도의 이벤트로 인식되기 전에 경험할 수 있는 최대 지연 양으로 정의된 160밀리초(ms)에서 320ms 사이의 눈에 띄는 최소 지연 값을 나타냅니다. 플레이어들은 또한 사운드와 그에 수반되는 아바타 애니메이션 사이의 지연이 320ms 지연에서는 견딜 수 없게 되었지만 최대 1200ms 지연에서도 완전히 견딜 수 없었던 적은 없었다고 보고했습니다. 우리는 플레이어가 약 320ms부터 지연을 느끼기 시작하며 대부분의 플레이어가 사운드와 애니메이션 사이의 큰 지연을 견딜 수 있다고 결론을 내렸습니다.",https://doi.org/10.1109/ISMAR55827.2022.00016,Perception & Cognition; Audio & Sound,Sensor Fusion,User Study,Algorithm / Method
487,2022,Studying the Role of Self and External Touch in the Appropriation of Dysmorphic Hands,변형된 손의 활용에서 자기와 외부 접촉의 역할 연구,"In Virtual Reality, self-touch (ST) stimulation is a promising method of sense of body ownership (SoBO) induction that does not require an external effector. However, its applicability to dysmorphic bodies has not been explored yet and remains uncertain due to the requirement to provide incongruent visuomotor sensations. In this, paper, we studied the effect of ST stimulation on dysmorphic hands via haptic retargeting, as compared to a classical external-touch (ET) stimulation, on the SoBO. Our results indicate that ST can induce similar levels of dysmorphic SoBO than ET stimulation, but that some types of dysmorphism might decrease the ST stimulation accuracy due to the nature of the re-targeting that they induce.",가상 현실에서 셀프 터치(ST) 자극은 외부 이펙터가 필요하지 않은 신체 소유권(SoBO) 유도의 유망한 방법입니다. 그러나 이형체에 대한 적용 가능성은 아직 연구되지 않았으며 부적합한 시력 운동 감각을 제공해야 한다는 요구 사항으로 인해 불확실한 상태로 남아 있습니다. 본 논문에서는 SoBO에서 전통적인 외부 터치(ET) 자극과 비교하여 햅틱 리타겟팅을 통해 변형된 손에 대한 ST 자극의 효과를 연구했습니다. 우리의 결과는 ST가 ET 자극과 유사한 수준의 이형 SoBO를 유도할 수 있지만 일부 유형의 이형은 유도하는 재타겟팅의 특성으로 인해 ST 자극 정확도를 감소시킬 수 있음을 나타냅니다.,https://doi.org/10.1109/ISMAR55827.2022.00024,Interaction & Input,Sensor Fusion,Quantitative Experiment,Algorithm / Method
488,2022,Temporal View Synthesis of Dynamic Scenes through 3D Object Motion Estimation with Multi-Plane Images,다중 평면 이미지를 이용한 3D 객체 모션 추정을 통한 동적 장면의 시간적 뷰 합성,"The challenge of graphically rendering high frame-rate videos on low compute devices can be addressed through periodic prediction of future frames to enhance the user experience in virtual reality applications. This is studied through the problem of temporal view synthesis (TVS), where the goal is to predict the next frames of a video given the previous frames and the head poses of the previous and the next frames. In this work, we consider the TVS of dynamic scenes in which both the user and objects are moving. We design a framework that decouples the motion into user and object motion to effectively use the available user motion while predicting the next frames. We predict the motion of objects by isolating and estimating the 3D object motion in the past frames and then extrapolating it. We employ multi-plane images (MPI) as a 3D representation of the scenes and model the object motion as the 3D displacement between the corresponding points in the MPI representation. In order to handle the sparsity in MPIs while estimating the motion, we incorporate partial convolutions and masked correlation layers to estimate corresponding points. The predicted object motion is then integrated with the given user or camera motion to generate the next frame. Using a disocclusion infilling module, we synthesize the regions uncovered due to the camera and object motion. We develop a new synthetic dataset for TVS of dynamic scenes consisting of 800 videos at full HD resolution. We show through experiments on our dataset and the MPI Sintel dataset that our model outperforms all the competing methods in the literature.",낮은 컴퓨팅 장치에서 높은 프레임 속도의 비디오를 그래픽으로 렌더링하는 문제는 가상 현실 애플리케이션에서 사용자 경험을 향상시키기 위해 미래 프레임을 주기적으로 예측함으로써 해결할 수 있습니다. 이는 이전 프레임과 이전 및 다음 프레임의 머리 자세를 바탕으로 비디오의 다음 프레임을 예측하는 것이 목표인 시간적 뷰 합성(TVS) 문제를 통해 연구됩니다. 본 작업에서는 사용자와 객체가 모두 움직이는 동적 장면의 TVS를 고려합니다. 우리는 모션을 사용자와 객체 모션으로 분리하여 다음 프레임을 예측하는 동안 사용 가능한 사용자 모션을 효과적으로 사용하는 프레임워크를 설계합니다. 과거 프레임의 3차원 객체 움직임을 분리하고 추정한 후 이를 외삽하여 객체의 움직임을 예측합니다. 우리는 장면의 3D 표현으로 다중 평면 이미지(MPI)를 사용하고 MPI 표현의 해당 지점 사이의 3D 변위로 객체 모션을 모델링합니다. 모션을 추정하는 동안 MPI의 희소성을 처리하기 위해 부분 컨볼루션과 마스크된 상관 레이어를 통합하여 해당 지점을 추정합니다. 예측된 객체 모션은 주어진 사용자 또는 카메라 모션과 통합되어 다음 프레임을 생성합니다. Disocclusion infilling 모듈을 사용하여 카메라와 물체의 움직임으로 인해 발견되지 않은 영역을 합성합니다. 우리는 풀 HD 해상도의 800개 비디오로 구성된 동적 장면의 TVS용 새로운 합성 데이터세트를 개발합니다. 우리는 데이터 세트와 MPI Sintel 데이터 세트에 대한 실험을 통해 우리 모델이 문헌의 모든 경쟁 방법보다 성능이 우수하다는 것을 보여줍니다.,https://doi.org/10.1109/ISMAR55827.2022.00100,Rendering & Visualization,Other,Technical Evaluation,Algorithm / Method
489,2022,"The Effects of Avatar and Environment Design on Embodiment, Presence, Activation, and Task Load in a Virtual Reality Exercise Application","가상현실 연습 애플리케이션에서 아바타와 환경 디자인이 체현, 존재감, 활성화 및 작업 부하에 미치는 영향","The development of embodied Virtual Reality (VR) systems involves multiple central design choices. These design choices affect the user perception and therefore require thorough consideration. This article reports on two user studies investigating the influence of common design choices on relevant intermediate factors (sense of embodiment, presence, motivation, activation, and task load) in a VR application for physical exercises. The first study manipulated the avatar fidelity (abstract, partial body vs. anthropomorphic, full-body) and the environment (with vs. without mirror). The second study manipulated the avatar type (healthy vs. injured) and the environment type (beach vs. hospital) and, hence, the avatar-environment congruence. The full-body avatar significantly increased the sense of embodiment and decreased mental demand. Interestingly, the mirror did not influence the dependent variables. The injured avatar significantly increased the temporal demand. The beach environment significantly reduced the tense activation. On the beach, participants felt more present in the incongruent condition embodying the injured avatar.","구현된 가상 현실(VR) 시스템의 개발에는 여러 가지 중심 설계 선택이 포함됩니다. 이러한 디자인 선택은 사용자 인식에 영향을 미치므로 철저한 고려가 필요합니다. 이 기사에서는 신체 운동을 위한 VR 애플리케이션의 관련 중간 요소(실체감, 존재감, 동기 부여, 활성화 및 작업 부하)에 대한 일반적인 디자인 선택의 영향을 조사하는 두 가지 사용자 연구에 대해 보고합니다. 첫 번째 연구에서는 아바타 충실도(추상, 신체 일부 vs. 의인화, 전신)와 환경(거울 있음 vs. 거울 없음)을 조작했습니다. 두 번째 연구에서는 아바타 유형(건강한 대 부상자)과 환경 유형(해변 대 병원)을 조작하여 아바타-환경 일치를 조정했습니다. 전신 아바타는 체화감을 크게 증가시키고 정신적 요구를 감소시켰습니다. 흥미롭게도 거울은 종속변수에 영향을 미치지 않았습니다. 부상당한 아바타는 시간적 요구를 크게 증가시켰습니다. 해변 환경은 긴장된 활성화를 크게 감소시켰습니다. 해변에서 참가자들은 부상당한 아바타를 구현하는 부조화한 상태에 더 많은 존재감을 느꼈습니다.",https://doi.org/10.1109/ISMAR55827.2022.00041,Perception & Cognition,Other,User Study,User Study / Empirical Findings
490,2022,The Effects of Device and Spatial Layout on Social Presence During a Dynamic Remote Collaboration Task in Mixed Reality,혼합 현실에서 동적 원격 협업 작업 중 사회적 존재에 대한 장치 및 공간 레이아웃의 효과,"This paper evaluates factors of social presence during a dynamic remote collaboration task in a technologically asymmetric Mixed Reality (MR) setting for two spatial layouts. While active movement during MR remote collaboration is afforded by how the shared 3D space is mediated and configured, studies investigating the impact of these conditions on user experience have been scarce. In a between-group study $(\mathrm{n}=48)$, a host user in Augmented Reality (AR) and a remote user in Virtual Reality (VR), both wearing Head Mounted Displays (HMDs), simultaneously moved around the shared space to find and assemble parts of a Mars exploration rover together, one group in a Peripheral layout and the other in a Scattered layout with disparate levels of spatial affordance. Results show that while VR facilitates higher co-presence and spatial presence than AR through HMDs, the Peripheral layout enables users to pay more attention to one another than the Scattered. We analyze the results and derive implications aimed at bridging the AR-VR gap in social presence for dynamic MR remote collaboration through the adaptive placement of virtual content in shared spaces.","본 논문에서는 두 가지 공간 레이아웃에 대한 기술적으로 비대칭인 혼합 현실(MR) 설정에서 동적 원격 협업 작업 중 사회적 존재의 요소를 평가합니다. MR 원격 협업 중 활발한 움직임은 공유된 3D 공간이 중재되고 구성되는 방식에 따라 제공되지만 이러한 조건이 사용자 경험에 미치는 영향을 조사하는 연구는 거의 없습니다. 그룹 간 연구 $(\mathrm{n}=48)$에서 증강 현실(AR)의 호스트 사용자와 가상 현실(VR)의 원격 사용자는 둘 다 머리 장착형 디스플레이(HMD)를 착용하고 동시에 공유 공간을 돌아다니며 화성 탐사 로버의 부품을 찾아 조립했습니다. 한 그룹은 주변 장치 레이아웃으로, 다른 그룹은 서로 다른 수준의 공간 여유가 있는 분산형 레이아웃으로 구성했습니다. 결과는 VR이 HMD를 통해 AR보다 더 높은 공동 존재감과 공간적 존재감을 촉진하는 반면, 주변 장치 레이아웃을 사용하면 사용자가 Scattered보다 서로에게 더 많은 주의를 기울일 수 있음을 보여줍니다. 우리는 공유 공간에 가상 콘텐츠를 적응형으로 배치하여 역동적인 MR 원격 협업을 위해 사회적 존재감에서 AR-VR 격차를 해소하는 것을 목표로 결과를 분석하고 시사점을 도출합니다.",https://doi.org/10.1109/ISMAR55827.2022.00055,Collaboration & Social; Perception & Cognition,Sensor Fusion,Other,User Study / Empirical Findings
491,2022,The Effects of Hand Tracking on User Performance: an experimental study of an object selection based memory game,손 추적이 사용자 성능에 미치는 영향: 객체 선택 기반 메모리 게임에 대한 실험적 연구,"Until recently, Virtual Reality (VR) applications relied on controllers to enable user interaction in virtual environments. With advances in tracking technology, HMDs are now able to track the movements of users’ hands in real-time with significantly greater accuracy, allowing us to interact with the digital world directly with our hands. However, it is not entirely clear how hand tracking affects users’ performance. In this study, we investigate user performance using an in-game analytics-based assessment methodology for a VR memory puzzle task. We conducted a within-subjects experiment with 30 participants in three conditions: 1- Hand-tracking, 2-Controller Without Haptics, and 3- Controller With Haptics. In all our measurements (correct order and pattern, correct pattern only, and trial completion) except for the initial selection time, participants performed best with hand tracking. The use of controllers with haptics did not outperform controllers without haptics in most measures, possibly because other feedback cues compensated for the lack of haptics. This study helps us better understand the three selected interactivity methods when used in VR, as well as the importance of naturalistic experience in interaction design.","최근까지 가상 현실(VR) 애플리케이션은 컨트롤러를 사용하여 가상 환경에서 사용자 상호 작용을 가능하게 했습니다. 추적 기술의 발전으로 HMD는 이제 훨씬 더 정확하게 사용자의 손 움직임을 실시간으로 추적할 수 있게 되었으며, 이를 통해 우리는 손으로 직접 디지털 세계와 상호 작용할 수 있게 되었습니다. 그러나 핸드 트래킹이 사용자의 성능에 어떤 영향을 미치는지는 완전히 명확하지 않습니다. 본 연구에서는 VR 메모리 퍼즐 작업에 대한 게임 내 분석 기반 평가 방법론을 사용하여 사용자 성능을 조사합니다. 우리는 1- 손 추적, 2- 햅틱 없는 컨트롤러, 3- 햅틱 있는 컨트롤러의 세 가지 조건에서 30 명의 참가자를 대상으로 피험자 내 실험을 수행했습니다. 초기 선택 시간을 제외한 모든 측정(올바른 순서 및 패턴, 올바른 패턴만, 시험 완료)에서 참가자는 핸드 트래킹을 통해 가장 좋은 성과를 거두었습니다. 햅틱이 있는 컨트롤러를 사용하면 대부분의 측면에서 햅틱이 없는 컨트롤러보다 성능이 뛰어나지 않았는데, 이는 아마도 다른 피드백 큐가 햅틱의 부족을 보상했기 때문일 수 있습니다. 이 연구는 VR에서 사용될 때 선택된 세 가지 상호 작용 방법과 상호 작용 디자인에서 자연스러운 경험의 중요성을 더 잘 이해하는 데 도움이 됩니다.",https://doi.org/10.1109/ISMAR55827.2022.00095,Interaction & Input,Hand / Gesture Recognition,User Study; Quantitative Experiment,Algorithm / Method
492,2022,Touching The Droid: Understanding and Improving Touch Precision With Mobile Devices in Virtual Reality,Touching The Droid: 가상 현실에서 모바일 장치의 터치 정밀도 이해 및 개선,"Touch interaction with physical smartphones and tablets in Virtual Reality offers interesting opportunities for cross-device input. Unfortunately, any imprecision in the alignment of the visual representation of either the hand or device can impact the precision of touch and the realism of the experience. We first study a user’s ability to rely solely on preoperative feedback to perform touch interaction in VR, where no rendering of the hand is provided. Results indicate that touch in VR is possible without a visual representation of the hand, but accuracy is influenced by how the device is held and the distance traveled to the target. We then introduce a dynamic calibration algorithm to minimize the offset between the physical hand and its virtual representation. In a second study, we show that this algorithm can increase touch accuracy by 43%, and minimize depth-based “screen penetration” or “floating touch” errors.",가상 현실에서 실제 스마트폰 및 태블릿과의 터치 상호 작용은 교차 장치 입력에 대한 흥미로운 기회를 제공합니다. 불행하게도 손이나 장치의 시각적 표현 정렬이 부정확하면 터치의 정확성과 경험의 현실성에 영향을 미칠 수 있습니다. 우리는 먼저 손 렌더링이 제공되지 않는 VR에서 터치 상호 작용을 수행하기 위해 수술 전 피드백에만 의존하는 사용자의 능력을 연구합니다. 결과는 손을 시각적으로 표현하지 않고도 VR에서 터치가 가능하지만 정확도는 장치를 잡는 방법과 대상까지 이동한 거리에 따라 영향을 받는다는 것을 나타냅니다. 그런 다음 실제 손과 가상 표현 사이의 오프셋을 최소화하기 위해 동적 보정 알고리즘을 도입합니다. 두 번째 연구에서는 이 알고리즘이 터치 정확도를 43% 높이고 깊이 기반 '화면 침투' 또는 '플로팅 터치' 오류를 최소화할 수 있음을 보여줍니다.,https://doi.org/10.1109/ISMAR55827.2022.00099,Interaction & Input,Optical / Display Technology,Quantitative Experiment,Algorithm / Method
493,2022,"Towards Forecasting the Onset of Cybersickness by Fusing Physiological, Head-tracking and Eye-tracking with Multimodal Deep Fusion Network","멀티모달 딥 퓨전 네트워크(Multimodal Deep Fusion Network)와 생리적, 머리 추적 및 시선 추적을 융합하여 사이버 멀미 발병 예측을 위해","A plethora of studies has been conducted to detect and reduce cybersickness in real-time. However, prior attempts to detect and minimize cybersickness after its onset may be ineffective as the onset tends to persist beyond its first occurrence. By forecasting the onset of cybersickness, it may be possible to mitigate the severity of cybersickness through earlier interventions. This research proposed a multimodal deep fusion approach to forecast cybersickness from the user’s physiological, head-tracking, and eye-tracking data. We proposed several hybrid multimodal deep fusion neural networks with Long short-term memory (LSTMs), Neural basis expansion analysis for interpretable time series forecasting(NBEATs) and Deep Temporal Convolutional Networks(DeepTCN) neural models to forecast cybersickness 30-60s in advance to its onset. To validate our proposed approach, we recruited 30 participants who were immersed in five virtual reality simulations. We collected eye-tracking, head-tracking, heart rate, and galvanic skin response data and used the fast-motion scale as ground truth. Our results suggest that the DeepTCN model with our proposed multimodal fusion network can forecast cybersickness onset 60 seconds in advance with a root-mean-square error of 0.49 (on a scale from 0-10). Furthermore, our results demonstrated that fusing eye tracking, heart rate, and galvanic skin response data outperformed other data fusion approaches. This research clarifies how early cybersickness can be forecast, paving the way for future research on early cybersickness mitigation approaches.","사이버 멀미를 실시간으로 감지하고 줄이기 위한 수많은 연구가 수행되었습니다. 그러나 사이버 멀미가 발생한 후 사이버 멀미를 감지하고 최소화하려는 사전 시도는 발병이 처음 발생한 이후에도 지속되는 경향이 있기 때문에 효과적이지 않을 수 있습니다. 사이버 멀미의 시작을 예측함으로써 조기 개입을 통해 사이버 멀미의 심각도를 완화하는 것이 가능할 수도 있습니다. 본 연구에서는 사용자의 생리적, 머리 추적, 시선 추적 데이터로부터 사이버 멀미를 예측하는 다중 모드 딥 퓨전 접근 방식을 제안했습니다. 우리는 사이버 멀미가 발병하기 30~60년대를 미리 예측하기 위해 장단기 기억(LSTM), 해석 가능한 시계열 예측을 위한 신경 기반 확장 분석(NBEAT) 및 Deep Temporal Convolutional Networks(DeepTCN) 신경 모델을 갖춘 여러 가지 하이브리드 멀티모달 심층 융합 신경망을 제안했습니다. 제안된 접근 방식을 검증하기 위해 우리는 5개의 가상 현실 시뮬레이션에 몰입한 30명의 참가자를 모집했습니다. 우리는 시선 추적, 머리 추적, 심박수 및 피부 전기 반응 데이터를 수집하고 빠른 동작 척도를 기준 진실로 사용했습니다. 우리의 결과는 우리가 제안한 다중 모드 융합 네트워크를 갖춘 DeepTCN 모델이 0.49의 제곱 평균 오차(0-10 범위)로 사이버 멀미 발병을 60초 전에 미리 예측할 수 있음을 시사합니다. 또한, 우리의 결과는 안구 추적, 심박수 및 피부 갈바닉 반응 데이터를 융합하는 것이 다른 데이터 융합 접근 방식보다 성능이 우수하다는 것을 보여주었습니다. 이 연구는 초기 사이버 멀미를 예측할 수 있는 방법을 명확히 하여 초기 사이버 멀미 완화 접근 방식에 대한 향후 연구의 기반을 마련합니다.",https://doi.org/10.1109/ISMAR55827.2022.00026,Interaction & Input,Deep Learning / Neural Networks,User Study,Algorithm / Method
494,2022,Towards Spatial Airflow Interaction: Schlieren Imaging for Augmented Reality,공간적 공기 흐름 상호 작용을 향하여: 증강 현실을 위한 Schlieren 이미징,"This work integrates Schlieren Imaging, a unique sensing modality, into Augmented Reality (AR) to explore ways to utilize invisible airflows for AR. Schlieren imaging is an imaging technique that visualizes the flow of fluids, which is normally invisible to the eyes. Theoretically, the technique can calculate the motion, pressure, temperature, and density of the airflow in our physical world. This unique, but less applied modality may expand interaction paradigms in AR and VR. We build a proof-of-concept AR system combined with Schlieren imaging that allows real airflow to affect virtual objects. The results of quantitative analyses show that our system can integrate different types of airflow with pressure values ranging from weak breathing actions to a heat gun up to 10m/s or 0.25m3/min airflow. We also showcase AR use cases including blowing out a virtual candle and a heat gun.","이 작업은 독특한 감지 방식인 Schlieren Imaging을 증강 현실(AR)에 통합하여 AR에 보이지 않는 공기 흐름을 활용하는 방법을 모색합니다. Schlieren 이미징은 일반적으로 눈에 보이지 않는 유체의 흐름을 시각화하는 이미징 기술입니다. 이론적으로 이 기술은 물리적 세계의 공기 흐름의 움직임, 압력, 온도 및 밀도를 계산할 수 있습니다. 이 독특하지만 덜 적용되는 방식은 AR 및 VR의 상호 작용 패러다임을 확장할 수 있습니다. 우리는 실제 공기 흐름이 가상 물체에 영향을 미칠 수 있도록 하는 Schlieren 이미징과 결합된 개념 증명 AR 시스템을 구축합니다. 정량적 분석 결과에 따르면 당사 시스템은 약한 호흡 작용부터 최대 10m/s 또는 0.25m3/min 기류의 히트건에 이르는 압력 값과 다양한 유형의 기류를 통합할 수 있는 것으로 나타났습니다. 가상 촛불 끄기, 히트건 등 AR 활용 사례도 선보입니다.",https://doi.org/10.1109/ISMAR55827.2022.00036,Interaction & Input,Other,Quantitative Experiment,System / Framework
495,2022,TruVR: Trustworthy Cybersickness Detection using Explainable Machine Learning,TruVR: 설명 가능한 기계 학습을 사용한 신뢰할 수 있는 사이버 질병 감지,"Cybersickness can be characterized by nausea, vertigo, headache, eye strain, and other discomforts when using virtual reality (VR) systems. The previously reported machine learning (ML) and deep learning (DL) algorithms for detecting (classification) and predicting (regression) VR cybersickness use black-box models; thus, they lack explainability. Moreover, VR sensors generate a massive amount of data, resulting in complex and large models. Therefore, having inherent explainability in cybersickness detection models can significantly improve the model’s trustworthiness and provide insight into why and how the ML/DL model amved at a specific decision. To address this issue, we present three explainable machine learning (xML) models to detect and predict cybersickness: 1) explainable boosting machine (EBM), 2) decision tree (DT), and 3) logistic regression (LR). We evaluate xML-based models with publicly available physiological and gameplay datasets for cybersickness. The results show that the EBM can detect cybersickness with an accuracy of 99.75% and 94.10% for the physiological and gameplay datasets, respectively. On the other hand, while predicting the cybersickness, EBM resulted in a Root Mean Square Error (RMSE) of 0.071 for the physiological dataset and 0.27 for the gameplay dataset. Furthermore, the EBM-based global explanation reveals exposure length, rotation, and acceleration as key features causing cybersickness in the gameplay dataset. In contrast, galvanic skin responses and heart rate are most significant in the physiological dataset. Our results also suggest that EBM-based local explanation can identify cybersickness-causing factors for individual samples. We believe the proposed xML-based cybersickness detection method can help future researchers understand, analyze, and design simpler cybersickness detection and reduction models.","사이버멀미는 가상 현실(VR) 시스템을 사용할 때 메스꺼움, 현기증, 두통, 눈의 피로 및 기타 불편함을 특징으로 할 수 있습니다. 이전에 보고된 VR 사이버 멀미를 감지(분류)하고 예측(회귀)하기 위한 머신러닝(ML) 및 딥러닝(DL) 알고리즘은 블랙박스 모델을 사용합니다. 따라서 설명 가능성이 부족합니다. 게다가 VR 센서는 엄청난 양의 데이터를 생성하므로 복잡하고 큰 모델이 생성됩니다. 따라서 사이버 멀미 탐지 모델에 고유한 설명 기능이 있으면 모델의 신뢰성이 크게 향상되고 ML/DL 모델이 특정 결정에서 왜, 어떻게 반응했는지에 대한 통찰력을 제공할 수 있습니다. 이 문제를 해결하기 위해 우리는 사이버 멀미를 탐지하고 예측하기 위한 세 가지 설명 가능한 기계 학습(xML) 모델, 즉 1) 설명 가능한 부스팅 머신(EBM), 2) 의사 결정 트리(DT), 3) 로지스틱 회귀(LR)를 제시합니다. 우리는 사이버 멀미에 대해 공개적으로 사용 가능한 생리학적 및 게임플레이 데이터 세트를 사용하여 xML 기반 모델을 평가합니다. 결과는 EBM이 생리학적 데이터 세트와 게임 플레이 데이터 세트에 대해 각각 99.75%와 94.10%의 정확도로 사이버 멀미를 감지할 수 있음을 보여줍니다. 반면, EBM은 사이버 멀미를 예측하는 동안 생리학적 데이터 세트의 경우 RMSE(Root Mean Square Error)가 0.071, 게임 플레이 데이터 세트의 경우 0.27로 나타났습니다. 또한 EBM 기반 글로벌 설명은 게임 플레이 데이터 세트에서 사이버 멀미를 유발하는 주요 기능으로 노출 길이, 회전 및 가속도를 나타냅니다. 대조적으로, 전기적 피부 반응과 심박수는 생리학적 데이터 세트에서 가장 중요합니다. 우리의 결과는 또한 EBM 기반 로컬 설명이 개별 샘플의 사이버 멀미 유발 요인을 식별할 수 있음을 시사합니다. 우리는 제안된 xML 기반 사이버 멀미 탐지 방법이 미래의 연구자들이 더 간단한 사이버 멀미 탐지 및 감소 모델을 이해, 분석 및 설계하는 데 도움이 될 수 있다고 믿습니다.",https://doi.org/10.1109/ISMAR55827.2022.00096,Perception & Cognition,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method
496,2022,User-Centered Design and Evaluation of ARTTS: an Augmented Reality Triage Tool Suite for Mass Casualty Incidents,ARTTS의 사용자 중심 설계 및 평가: 대량 사상자 사고를 위한 증강 현실 분류 도구 모음,"In this work we present ARTTS: a head-worn Augmented Reality (AR) Triage Tool Suite containing an initial sorting tool, virtual assessment tool, and virtual triage tag to assist emergency responders in mass casualty incidents. The initial sorting tool can prompt novice responders through first-wave tasks to aid recalibration from shock to triage. The virtual assessment tool provides novice responders, potentially confused by the chaos, with a walkthrough of the SALT triage flowchart. Finally, current emergency medical triage processes leverage static paper tags susceptible to loss or illegible damage. ARTTS’ virtual triage tags are dynamic, can be updated through responder interaction, and employ user interface emergent features based on individual patient conditions. This paper describes ARTTS’ capabilities, as well as the applied user-centered design process including review of existing triage material, subject-matter expert interview transcripts, wireframing, application of usability and user-centered design principles, as well as iterative usability subject-matter expert assessments and design walkthroughs. The ARTTS user experience aims to enhance, not upend, existing triage processes. Finally, this paper provides a usability evaluation comparing ARTTS’ virtual triage tag to a physical paper triage tag. Our tag achieved requisite System Usability Scale (SUS) scores and showed negligible differences to the paper triage tag on usability and mental workload.","이 작업에서 우리는 대규모 사상자 ​​발생 시 응급 구조원을 지원하기 위한 초기 분류 도구, 가상 평가 도구 및 가상 분류 태그가 포함된 머리에 착용하는 증강 현실(AR) 분류 도구 제품군인 ARTTS를 제시합니다. 초기 분류 도구는 초심자에게 1차 작업을 통해 쇼크부터 분류까지 재보정을 지원하도록 유도할 수 있습니다. 가상 평가 도구는 혼란으로 인해 혼란스러울 수 있는 초보 대응자에게 SALT 분류 흐름도를 안내합니다. 마지막으로, 현재의 응급 의료 분류 프로세스에서는 손실이나 읽을 수 없는 손상이 발생할 수 있는 정적 종이 태그를 활용합니다. ARTTS의 가상 분류 태그는 ​​동적이며 응답자 상호 작용을 통해 업데이트될 수 있으며 개별 환자 상태에 따라 사용자 인터페이스 응급 기능을 사용합니다. 이 문서에서는 ARTTS의 기능뿐만 아니라 기존 분류 자료 검토, 주제 전문가 인터뷰 기록, 와이어프레임 작성, 유용성 적용 및 사용자 중심 디자인 원칙, 반복적 유용성 주제 전문가 평가 및 디자인 연습을 포함한 응용 사용자 중심 디자인 프로세스에 대해 설명합니다. ARTTS 사용자 경험은 기존 분류 프로세스를 뒤집는 것이 아니라 향상시키는 것을 목표로 합니다. 마지막으로, 본 논문에서는 ARTTS의 가상 분류 태그와 실제 종이 분류 태그를 비교한 사용성 평가를 제공합니다. 우리 태그는 필수 SUS(시스템 사용성 척도) 점수를 달성했으며 유용성과 정신적 작업량 측면에서 종이 분류 태그와 무시할 만한 차이를 보여주었습니다.",https://doi.org/10.1109/ISMAR55827.2022.00049,Interaction & Input; Medical & Healthcare,Other,Questionnaire / Survey,System / Framework
497,2022,Using HMD-based Hand Tracking Virtual Reality in Canine Anatomy Summative Assessment: a User Study,개 해부학 종합 평가에서 HMD 기반 손 추적 가상 현실 사용: 사용자 연구,"Due to the global pandemic, heavily practical based teaching in veterinary education has been hugely impacted. Even before the crisis, an innovative way to provide an assessment that could simulate the examination of a cadaver has been a long-standing dream for veterinary schools and faculty. We designed and implemented a summative assessment system using touchless interaction head-mounted display (HMD) Virtual Reality (VR), and presented a user study that was an initial exploration of the proposed system designed to replace traditional spatial multiple choices questions (MCQs) for veterinary students. The performance relating to the score and time spent of 24 veterinary students was measured in online MCQ and VR MCQ. Questionnaires were sent out to gather demographic data and opinions. Manipulation time in the VR environment was tracked. The results showed that the time spent was significantly shorter using VR as an assessment tool compared to online MCQ. Participants revealed that the VR assessment outperformed the traditional method in terms of satisfaction, distinctiveness, concentration, and nervousness levels. However, they underperformed in the VR examination significantly. The summative assessment in VR has the potential to further support veterinary education, and the usage of HMDs and touchless interaction will boost the process. This evaluation illustrates the gap between students’ perception of VR examination and their MCQ performance that will require future research to untangle if this technology is to be adopted in the anatomy classroom.","세계적인 유행병으로 인해 수의학 교육에서 실용성을 기반으로 한 교육이 큰 영향을 받았습니다. 위기 이전에도 시체 검사를 시뮬레이션할 수 있는 평가를 제공하는 혁신적인 방법은 수의과 대학과 교수진의 오랜 꿈이었습니다. 우리는 터치리스 상호 작용 헤드 장착 디스플레이(HMD) 가상 현실(VR)을 사용하여 총괄 평가 시스템을 설계 및 구현했으며, 수의과 학생들을 위한 전통적인 공간 다중 선택 질문(MCQ)을 대체하도록 설계된 제안된 시스템의 초기 탐색인 사용자 연구를 발표했습니다. 온라인 MCQ와 VR MCQ를 통해 수의대생 24명의 점수와 소요시간에 대한 성과를 측정하였다. 인구통계학적 데이터와 의견을 수집하기 위해 설문지를 발송했습니다. VR 환경에서의 조작 시간을 추적했습니다. 그 결과, 온라인 MCQ에 비해 VR을 평가 도구로 사용하는 데 소요되는 시간이 현저히 짧은 것으로 나타났습니다. 참가자들은 VR 평가가 만족도, 특이성, 집중력, 초조함 수준 측면에서 기존 방법보다 우수한 것으로 나타났습니다. 그러나 VR 시험에서는 크게 저조한 성과를 보였습니다. VR의 총괄 평가는 수의학 교육을 더욱 지원할 수 있는 잠재력을 갖고 있으며, HMD의 사용과 비접촉식 상호작용은 프로세스를 향상시킬 것입니다. 이 평가는 VR 시험에 대한 학생들의 인식과 이 기술을 해부학 수업에 채택하려면 향후 연구가 필요한 MCQ 수행 간의 격차를 보여줍니다.",https://doi.org/10.1109/ISMAR55827.2022.00044,Interaction & Input; Education & Training,Sensor Fusion,User Study,System / Framework
498,2022,VRContour: Bringing Contour Delineations of Medical Structures Into Virtual Reality,VRContour: 의료 구조의 윤곽 묘사를 가상 현실로 가져오기,"Contouring is an indispensable step in Radiotherapy (RT) treatment planning. However, today’s contouring software is constrained to only work with a 2D display, which is less intuitive and requires high task loads. Virtual Reality (VR) has shown great potential in various specialties of healthcare and health sciences education due to the unique advantages of intuitive and natural interactions in immersive spaces. VR-based radiation oncology integration has also been advocated as a target healthcare application, allowing providers to directly interact with 3D medical structures. We present VRContour and investigate how to effectively bring contouring for radiation oncology into VR. Through an autobiographical iterative design, we defined three design spaces focused on contouring in VR with the support of a tracked tablet and VR stylus, and investigating dimensionality for information consumption and input (either 2D or 2D + 3D). Through a within-subject study (n = 8), we found that visualizations of 3D medical structures significantly increase precision, and reduce mental load, frustration, as well as overall contouring effort. Participants also agreed with the benefits of using such metaphors for learning purposes.","컨투어링은 방사선치료(RT) 치료 계획에서 없어서는 안 될 단계입니다. 그러나 오늘날의 윤곽 소프트웨어는 2D 디스플레이에서만 작동하도록 제한되어 있어 덜 직관적이고 높은 작업 부하가 필요합니다. 가상 현실(VR)은 몰입형 공간에서 직관적이고 자연스러운 상호 작용이 가능하다는 고유한 장점으로 인해 의료 및 건강 과학 교육의 다양한 전문 분야에서 큰 잠재력을 보여왔습니다. VR 기반 방사선 종양학 통합은 의료 제공자가 3D 의료 구조와 직접 상호 작용할 수 있도록 하는 대상 의료 애플리케이션으로도 옹호되었습니다. 우리는 VRContour를 제시하고 방사선 종양학에 대한 윤곽을 VR로 효과적으로 가져오는 방법을 조사합니다. 자서전적 반복 디자인을 통해 우리는 추적된 태블릿과 VR 스타일러스의 지원을 통해 VR의 윤곽에 초점을 맞추고 정보 소비 및 입력(2D 또는 2D + 3D)에 대한 차원성을 조사하는 데 초점을 맞춘 세 가지 디자인 공간을 정의했습니다. 피험자 내 연구(n = 8)를 통해 3D 의료 구조의 시각화가 정밀도를 크게 높이고 정신적 부담, 좌절감 및 전체적인 윤곽 형성 노력을 줄이는 것으로 나타났습니다. 참가자들은 또한 학습 목적으로 그러한 은유를 사용하는 것의 이점에 동의했습니다.",https://doi.org/10.1109/ISMAR55827.2022.00020,Interaction & Input; Medical & Healthcare,Other,User Study,Design Guidelines
499,2022,VRDoc: Gaze-based Interactions for VR Reading Experience,VRDoc: VR 독서 경험을 위한 시선 기반 상호 작용,"Virtual reality (VR) offers the promise of an infinite office and remote collaboration, however, existing interactions in VR do not strongly support one of the most essential tasks for most knowledge workers, reading. This paper presents VRDoc, a set of gaze-based interaction methods designed to improve the reading experience in VR. We introduce three key components: Gaze Select-and-Snap for document selection, Gaze MagGlass for enhanced text legibility, and Gaze Scroll for ease of document traversal. We implemented each of these tools using a commodity VR headset with eye-tracking. In a series of user studies with 13 participants, we show that VRDoc makes VR reading both more efficient (p < 0.01) and less demanding (p < 0.01), and when given a choice, users preferred to use our tools over the current VR reading methods.","가상 현실(VR)은 무한한 사무실과 원격 협업을 약속하지만, VR의 기존 상호 작용은 대부분의 지식 근로자에게 가장 필수적인 작업 중 하나인 독서를 강력하게 지원하지 않습니다. 본 논문은 VR에서 독서 경험을 향상시키기 위해 고안된 시선 기반 상호 작용 방법 세트인 VRDoc을 제시합니다. 문서 선택을 위한 Gaze Select-and-Snap, 향상된 텍스트 가독성을 위한 Gaze MagGlass, 간편한 문서 탐색을 위한 Gaze Scroll의 세 가지 주요 구성 요소를 소개합니다. 우리는 시선 추적 기능이 있는 상용 VR 헤드셋을 사용하여 이러한 각 도구를 구현했습니다. 13명의 참가자를 대상으로 한 일련의 사용자 연구에서 우리는 VRDoc이 VR 읽기를 더 효율적이고(p < 0.01) 덜 까다롭게 만들고(p < 0.01) 선택이 주어졌을 때 사용자가 현재 VR 읽기 방법보다 우리 도구를 사용하는 것을 선호한다는 것을 보여주었습니다.",https://doi.org/10.1109/ISMAR55827.2022.00097,Interaction & Input; Collaboration & Social,Optical / Display Technology,User Study,Algorithm / Method
500,2022,VTONShoes: Virtual Try-on of Shoes in Augmented Reality on a Mobile Device,VTONShoes: 모바일 장치의 증강 현실에서 신발 가상 시착,"The virtual try-on (VTON) system in augmented reality (AR) has attracted significant research interest. This paper presents a novel real-time AR virtual shoe try-on system (VTONShoes). Users can see the virtual shoes with full degrees of freedom on the mobile device. In particular, we propose an efficient framework to detect, classify, and recover 6-DoF pose of shoes from the captured images and then accurately render the 3D shoe model on the screen in realtime and in full degrees of freedom. For accurate pose recovery, dense keypoints are designed on the 3D shoe model. An efficient joint 2D keypoint localization and leg silhouette segmentation module (KeyPointLoc) is designed to predict keypoint projections on 2D images and the shoe-leg occlusion relationship. In order to reduce jitter between frames, an optimization-based framework with the longest continuous invariant subarray constraints is proposed to minimize classification errors caused by model switching, and a smoothing module with Exponential Weights Decay is presented to post-process the rendered results. We also developed a large-scale dataset named Diverse-Shoes which contains images extracted from 80K videos, annotated with the shoe bounding box, transformation matrices, and silhouettes of legs. Our system has achieved a smooth and stable try-on effect on mainstream devices with a real-time speed of around 25 to 45 FPS on mainstream mobile phones, which significantly outperforms state-of-the-art methods for real-time performance and rendering accuracy.","증강 현실(AR)의 가상 체험(VTON) 시스템은 상당한 연구 관심을 끌었습니다. 본 논문에서는 새로운 실시간 AR 가상 신발 시착 시스템(VTONShoes)을 제시한다. 사용자는 모바일 장치에서 완전한 자유도로 가상 신발을 볼 수 있습니다. 특히, 우리는 캡처된 이미지에서 신발의 6-DoF 자세를 감지, 분류 및 복구한 후 실시간으로 완전한 자유도로 화면에 3D 신발 모델을 정확하게 렌더링하는 효율적인 프레임워크를 제안합니다. 정확한 포즈 복구를 위해 3D 신발 모델에 조밀한 키포인트를 설계했습니다. 효율적인 관절 2D 키포인트 위치 파악 및 다리 실루엣 분할 모듈(KeyPointLoc)은 2D 이미지의 키포인트 투영과 신발-다리 폐색 관계를 예측하도록 설계되었습니다. 프레임 간 지터를 줄이기 위해 모델 전환으로 인한 분류 오류를 최소화하기 위해 가장 긴 연속 불변 하위 배열 제약 조건을 갖춘 최적화 기반 프레임워크를 제안하고, 렌더링된 결과를 후처리하기 위해 Exponential Weights Decay가 포함된 평활화 모듈을 제시합니다. 우리는 또한 신발 경계 상자, 변환 행렬 및 다리 실루엣으로 주석이 달린 80K 비디오에서 추출된 이미지를 포함하는 Diverse-Shoes라는 대규모 데이터 세트를 개발했습니다. 우리 시스템은 주류 휴대폰에서 약 25~45FPS의 실시간 속도로 주류 장치에서 부드럽고 안정적인 시험 효과를 달성했으며, 이는 실시간 성능 및 렌더링 정확도에 대한 최첨단 방법보다 훨씬 뛰어납니다.",https://doi.org/10.1109/ISMAR55827.2022.00038,Tracking & Localization,Computer Vision,Technical Evaluation; Quantitative Experiment,Algorithm / Method; System / Framework
501,2022,Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural Implicit Representation,Vox-Fusion: 복셀 기반 신경 암시적 표현을 사용한 고밀도 추적 및 매핑,"In this work, we present a dense tracking and mapping system named Vox-Fusion, which seamlessly fuses neural implicit representations with traditional volumetric fusion methods. Our approach is inspired by the recently developed implicit mapping and positioning system and further extends the idea so that it can be freely applied to practical scenarios. Specifically, we leverage a voxel-based neural implicit surface representation to encode and optimize the scene inside each voxel. Furthermore, we adopt an octree-based structure to divide the scene and support dynamic expansion, enabling our system to track and map arbitrary scenes without knowing the environment like in previous works. Moreover, we proposed a high-performance multi-process framework to speed up the method, thus supporting some applications that require real-time performance. The evaluation results show that our methods can achieve better accuracy and completeness than previous methods. We also show that our Vox-Fusion can be used in augmented reality and virtual reality applications. Our source code is publicly available at https://github.com/zju3dv/Vox-Fusion.","이 작업에서 우리는 Vox-Fusion이라는 조밀한 추적 및 매핑 시스템을 제시합니다. 이 시스템은 신경 암시적 표현을 기존의 체적 융합 방법과 원활하게 융합합니다. 우리의 접근 방식은 최근 개발된 암시적 매핑 및 위치 확인 시스템에서 영감을 얻었으며 실제 시나리오에 자유롭게 적용할 수 있도록 아이디어를 더욱 확장했습니다. 특히, 우리는 복셀 기반 신경 암시적 표면 표현을 활용하여 각 복셀 내부의 장면을 인코딩하고 최적화합니다. 또한 장면을 분할하고 동적 확장을 지원하기 위해 옥트리 기반 구조를 채택하여 이전 작품처럼 환경을 알지 않고도 임의의 장면을 추적하고 매핑할 수 있는 시스템이 가능합니다. 또한, 우리는 방법의 속도를 높이기 위해 고성능 다중 프로세스 프레임워크를 제안하여 실시간 성능이 필요한 일부 응용 프로그램을 지원했습니다. 평가 결과는 우리의 방법이 이전 방법보다 더 나은 정확성과 완전성을 달성할 수 있음을 보여줍니다. 또한 Vox-Fusion이 증강 현실 및 가상 현실 애플리케이션에 사용될 수 있음을 보여줍니다. 우리의 소스 코드는 https://github.com/zju3dv/Vox-Fusion에서 공개적으로 제공됩니다.",https://doi.org/10.1109/ISMAR55827.2022.00066,Tracking & Localization,3D Reconstruction,Quantitative Experiment,System / Framework; Algorithm / Method
502,2022,What Can I Do There? Controlling AR Self-Avatars to Better Perceive Affordances of the Real World,거기서 무엇을 할 수 있나요? 현실 세계의 여유를 더 잘 인식하기 위해 AR 셀프 아바타 제어,"This work explores a new usage of Augmented Reality (AR) to extend perception and interaction within physical areas ahead of ourselves. To do so, we propose to detach ourselves from our physical position by creating a controllable “digital copy”; of our body that can be used to navigate in local space from a third-person perspective. With such a viewpoint, we aim to improve our mental representation of distant space and understanding of action possibilities (called affordances), without requiring us to physically enter this space. Our approach relies on AR to virtually integrate the user’s body in remote areas in the form of an avatar. We discuss concrete application scenarios and propose several techniques to manipulate avatars in the third person as a part of a larger conceptual framework. Finally, through a user study employing one of the proposed techniques (puppeteering), we evaluate the validity of using third-person embodiment to extend our perception of the real world to areas outside of our proximal zone. We found that this approach succeeded in enhancing the user’s accuracy and confidence when estimating their action capabilities at distant locations.",이 작업은 증강 현실(AR)의 새로운 사용법을 탐구하여 우리보다 앞서 있는 물리적 영역 내에서 인식과 상호 작용을 확장합니다. 그렇게 하기 위해 우리는 통제 가능한 “디지털 사본”을 생성함으로써 우리의 물리적 위치로부터 우리 자신을 분리할 것을 제안합니다. 제3자의 관점에서 로컬 공간을 탐색하는 데 사용할 수 있는 우리 몸의 정보입니다. 이러한 관점을 통해 우리는 물리적으로 이 공간에 들어가지 않고도 먼 공간에 대한 정신적 표현과 행동 가능성(어포던스라고 함)에 대한 이해를 향상시키는 것을 목표로 합니다. 우리의 접근 방식은 AR을 사용하여 원격 지역에 있는 사용자의 신체를 아바타 형태로 가상으로 통합합니다. 우리는 구체적인 적용 시나리오를 논의하고 더 큰 개념 프레임워크의 일부로 제3자 아바타를 조작하는 여러 기술을 제안합니다. 마지막으로 제안된 기술 중 하나(인형 조종)를 사용한 사용자 연구를 통해 현실 세계에 대한 인식을 근위 영역 외부 영역으로 확장하기 위해 3인칭 구현을 사용하는 것의 타당성을 평가합니다. 우리는 이 접근법이 먼 위치에서 사용자의 행동 능력을 평가할 때 사용자의 정확성과 자신감을 높이는 데 성공했다는 것을 발견했습니다.,https://doi.org/10.1109/ISMAR55827.2022.00061,Perception & Cognition; Interaction & Input,Optical / Display Technology,User Study,Algorithm / Method; User Study / Empirical Findings
503,2022,Wormholes in VR: Teleporting Hands for Flexible Passive Haptics,VR의 웜홀: 유연한 수동적 햅틱을 위한 손 순간 이동,"Presenting haptic feedback in virtual reality (VR) is a long-standing challenge, with passive haptics being one way of presenting haptic feedback inexpensively. However, passive haptics requires props in physical environments that are co-located with their virtual counterparts, which is often not the case in the real world. Although redirected hands and other methods have previously been proposed to solve this problem, significant differences between the displayed and actual hand positions can cause the degradation of presence and sense of embodiment, limiting the range of presentable environments. In this study, we present a new hand displacement method called wormholes, in which the virtual hand is teleported discontinuously as the user inserts their hand into the hole. The experiment showed that the wormhole could maintain the sense of embodiment, presence, and task performance even with large hand displacements. Our method enables to apply passive haptics even when the actual and virtual environments are quite different, contributing to the realization of inexpensive and flexible haptic presentation in VR applications.","가상 현실(VR)에서 햅틱 피드백을 제공하는 것은 오랜 과제였으며, 수동적 햅틱은 햅틱 피드백을 저렴하게 제공하는 한 가지 방법입니다. 그러나 수동적 햅틱에는 가상 환경과 같은 위치에 있는 물리적 환경의 소품이 필요하지만 현실 세계에서는 그렇지 않은 경우가 많습니다. 이 문제를 해결하기 위해 이전에 방향을 바꾸는 손 및 기타 방법이 제안되었지만, 표시된 손 위치와 실제 손 위치 사이의 상당한 차이로 인해 현장감과 구현 감각이 저하되어 표현 가능한 환경의 범위가 제한될 수 있습니다. 본 연구에서는 사용자가 구멍에 손을 삽입할 때 가상 손이 불연속적으로 순간 이동되는 웜홀이라는 새로운 손 변위 방법을 제시합니다. 실험은 웜홀이 손을 크게 움직여도 구체화 감각, 존재감 및 작업 수행 능력을 유지할 수 있음을 보여주었습니다. 우리의 방법은 실제 환경과 가상 환경이 상당히 다른 경우에도 수동적 햅틱을 적용할 수 있어 VR 애플리케이션에서 저렴하고 유연한 햅틱 표현을 실현하는 데 기여합니다.",https://doi.org/10.1109/ISMAR55827.2022.00093,Interaction & Input; Perception & Cognition,Haptic / Tactile Feedback,Simulation,Algorithm / Method
504,2022,WriArm: Leveraging Wrist Movement to Design Wrist+Arm Based Teleportation in VR,WriArm: 손목 움직임을 활용하여 VR에서 손목+팔 기반 순간이동 설계,"Teleportation, a widely used locomotion technique in Virtual Reality (VR), is used to move users through a virtual environment. Until recently, handheld controllers have been used for teleportation, where users use controllers to point to a location and perform an action (e.g., button press) to be instantly moved to the targeted location. Recent advancements in VR hand tracking enable users to move through and interact with the virtual world without controllers. This opens the opportunity for compelling alternatives to explore hand tracking-based teleportation techniques for more natural, intuitive and immersive interactions. Prior work mostly explores using arm movement for teleportation as an alternative to using the controller. In this paper, we design and evaluate WriArm, a VR locomotion technique that leverages both wrist and arm movement for VR teleportation. We first conduct a design study to find suitable hand gesture sets that can be mapped to teleportation activities such as activation, pointing, confirmation and cancellation for WriArm and arm-based techniques. Based on the results, we conduct a study comparing users’ performance while navigating tasks with the two techniques and three gesture sets. Results show that WriArm improves navigation efficiency by allowing users to navigate the environment quickly. We conclude with design guidelines for arm and wrist-based teleportation in VR.","가상현실(VR)에서 널리 사용되는 이동 기술인 순간이동(Teleportation)은 사용자를 가상 환경에서 이동시키는 데 사용됩니다. 최근까지 휴대용 컨트롤러는 사용자가 컨트롤러를 사용하여 위치를 가리키고 동작(예: 버튼 누르기)을 수행하여 목표 위치로 즉시 이동하는 순간 이동에 사용되었습니다. VR 핸드 트래킹의 최근 발전으로 사용자는 컨트롤러 없이 가상 세계를 이동하고 상호 작용할 수 있습니다. 이는 보다 자연스럽고 직관적이며 몰입감 있는 상호 작용을 위해 손 추적 기반 순간 이동 기술을 탐색할 수 있는 강력한 대안의 기회를 열어줍니다. 이전 작업에서는 주로 컨트롤러를 사용하는 대신 순간 이동을 위해 팔 움직임을 사용하는 방법을 탐구했습니다. 본 논문에서는 VR 순간이동을 위해 손목과 팔의 움직임을 모두 활용하는 VR 이동 기술인 WriArm을 설계하고 평가합니다. 우리는 먼저 WriArm 및 팔 기반 기술에 대한 활성화, 포인팅, 확인 및 취소와 같은 순간이동 활동에 매핑할 수 있는 적합한 손 제스처 세트를 찾기 위해 설계 연구를 수행합니다. 결과를 바탕으로 두 가지 기술과 세 가지 제스처 세트를 사용하여 작업을 탐색하는 동안 사용자의 성능을 비교하는 연구를 수행합니다. 결과에 따르면 WriArm은 사용자가 환경을 빠르게 탐색할 수 있도록 하여 탐색 효율성을 향상시키는 것으로 나타났습니다. VR에서 팔과 손목 기반 순간이동을 위한 디자인 지침으로 결론을 내립니다.",https://doi.org/10.1109/ISMAR55827.2022.00047,Interaction & Input,Hand / Gesture Recognition; Redirected Walking / Locomotion,Simulation,Algorithm / Method
505,2022,XRtic: A Prototyping Toolkit for XR Applications using Cloth Deformation,XRtic: 옷감 변형을 사용하는 XR 애플리케이션용 프로토타이핑 툴킷,"This paper presents XRtic, a prototyping toolkit enabling real-world cloth deformations to be used in novel ways in eXtended Reality (XR) applications. XRtic was developed based on the insights gathered from semi-structured interviews with XR developers. It consists of custom-made actuators that can be attached to regular clothing, a controller bus system, and a controller interface. Using our toolkit, users can design and integrate different cloth deformation types synchronised with virtual content in a plug-and-play manner. Along with a technical analysis of the actuation behaviour of the XRtic actuators, we present the findings gathered from a user study with eight XR developers, focusing on the usability of the system and creative support. Overall, participants found it an easy-to-use toolkit that supports iterative and rapid prototyping, and enables cloth to be deformed in unique ways in synchronisation with XR applications. Based on the findings, we also report limitations and future work relating to our system.","이 문서에서는 XR(eXtended Reality) 애플리케이션에서 실제 옷감 변형을 새로운 방식으로 사용할 수 있는 프로토타입 제작 툴킷인 XRtic을 소개합니다. XRtic은 XR 개발자와의 반구조적 인터뷰를 통해 수집된 통찰력을 기반으로 개발되었습니다. 일반 의류에 부착할 수 있는 맞춤형 액추에이터, 컨트롤러 버스 시스템, 컨트롤러 인터페이스로 구성됩니다. 우리의 툴킷을 사용하여 사용자는 플러그 앤 플레이 방식으로 가상 콘텐츠와 동기화된 다양한 천 변형 유형을 디자인하고 통합할 수 있습니다. XRtic 액추에이터의 작동 동작에 대한 기술적 분석과 함께 시스템의 유용성과 창의적인 지원에 중점을 두고 8명의 XR 개발자와 함께 사용자 연구에서 수집한 결과를 제시합니다. 전반적으로 참가자들은 반복적이고 신속한 프로토타이핑을 지원하고 XR 애플리케이션과 동기화하여 천을 독특한 방식으로 변형할 수 있는 사용하기 쉬운 툴킷이라는 점을 확인했습니다. 조사 결과를 바탕으로 시스템과 관련된 제한 사항과 향후 작업도 보고합니다.",https://doi.org/10.1109/ISMAR55827.2022.00071,Interaction & Input,Other,User Study,System / Framework; User Study / Empirical Findings
506,2021,A Comparison of the Fatigue Progression of Eye-Tracked and Motion-Controlled Interaction in Immersive Space,몰입형 공간에서 시선 추적 및 동작 제어 상호 작용의 피로 진행 비교,"Eye-tracking enabled virtual reality (VR) headsets have recently become more widely available. This opens up opportunities to incorporate eye gaze interaction methods in VR applications. However, studies on the fatigue-induced performance fluctuations of these new input modalities are scarce and rarely provide a direct comparison with established interaction methods. We conduct a study to compare the selection-interaction performance between commonly used handheld motion control devices and emerging eye interaction technology in VR. We investigate each interaction’s unique fatigue progression pattern in study sessions with ten minutes of continuous engagement. The results support and extend previous findings regarding the progression of fatigue in eye-tracked interaction over prolonged periods. By directly comparing gaze-with motion-controlled interaction, we put the emerging eye-trackers into perspective with the state-of-the-art interaction method for immersive space. We then discuss potential implications for future extended reality (XR) interaction design based on our findings.",시선 추적이 가능한 가상 현실(VR) 헤드셋은 최근 더욱 널리 보급되었습니다. 이는 VR 애플리케이션에 시선 상호 작용 방법을 통합할 수 있는 기회를 열어줍니다. 그러나 이러한 새로운 입력 양식의 피로로 인한 성능 변동에 대한 연구는 거의 없으며 확립된 상호 작용 방법과 직접적인 비교를 거의 제공하지 않습니다. 우리는 일반적으로 사용되는 휴대용 모션 제어 장치와 VR에서 새롭게 떠오르는 눈 상호 작용 기술 간의 선택-상호 작용 성능을 비교하기 위한 연구를 수행합니다. 우리는 10분 동안 지속적으로 참여하는 학습 세션에서 각 상호 작용의 고유한 피로 진행 패턴을 조사합니다. 이번 결과는 장기간에 걸친 눈 추적 상호 작용의 피로 진행에 관한 이전 연구 결과를 뒷받침하고 확장합니다. 시선과 동작 제어 상호 작용을 직접 비교함으로써 우리는 몰입형 공간을 위한 최첨단 상호 작용 방법을 사용하여 신흥 시선 추적기를 관점에 놓았습니다. 그런 다음 연구 결과를 바탕으로 미래의 확장 현실(XR) 상호 작용 디자인에 대한 잠재적인 영향을 논의합니다.,https://doi.org/10.1109/ISMAR52148.2021.00063,Interaction & Input,Optical / Display Technology,Technical Evaluation,User Study / Empirical Findings; Hardware / Device
507,2021,A Predictive Performance Model for Immersive Interactions in Mixed Reality,혼합 현실의 몰입형 상호 작용을 위한 예측 성능 모델,"The design of immersive interaction for mixed reality based on head-mounted displays (HMDs), hereafter referred to as Mixed Reality (MR), is still a tedious task which can hinder the advent of such devices. Indeed, the effects of the interface design on task performance are difficult to anticipate during the design phase: the spatial layout of virtual objects and the interaction techniques used to select those objects can have an impact on task completion time. Besides, testing such interfaces with users in controlled experiments requires considerable time and efforts. To overcome this problem, predictive models, such as the Keystroke-Level Model (KLM), can be used to predict the time required to complete an interactive task at an early stage of the design process. However, so far these models have not been properly extended to address the specific interaction techniques of MR environments. In this paper we propose an extension of the KLM model to interaction performed in MR. First, we propose new operators and experimentally determine the unit times for each of them with a HoloLens v1. Then, we perform experiments based on realistic interaction scenarios to consolidate our model. These experiments confirm the validity of our extension of KLM to predict interaction time in mixed reality environments..",HMD(헤드 마운트 디스플레이)를 기반으로 하는 혼합 현실(이하 MR)을 위한 몰입형 상호 작용 설계는 여전히 이러한 장치의 출현을 방해할 수 있는 지루한 작업입니다. 실제로 인터페이스 디자인이 작업 성능에 미치는 영향은 디자인 단계에서는 예측하기 어렵습니다. 가상 개체의 공간 레이아웃과 해당 개체를 선택하는 데 사용되는 상호 작용 기술은 작업 완료 시간에 영향을 미칠 수 있습니다. 게다가 통제된 실험을 통해 사용자와 이러한 인터페이스를 테스트하려면 상당한 시간과 노력이 필요합니다. 이 문제를 극복하기 위해 KLM(키 입력 수준 모델)과 같은 예측 모델을 사용하여 설계 프로세스의 초기 단계에서 대화형 작업을 완료하는 데 필요한 시간을 예측할 수 있습니다. 그러나 지금까지 이러한 모델은 MR 환경의 특정 상호 작용 기술을 다루기 위해 적절하게 확장되지 않았습니다. 본 논문에서는 MR에서 수행되는 상호 작용에 대한 KLM 모델의 확장을 제안합니다. 먼저 새로운 연산자를 제안하고 HoloLens v1을 사용하여 각 연산자의 단위 시간을 실험적으로 결정합니다. 그런 다음 현실적인 상호 작용 시나리오를 기반으로 실험을 수행하여 모델을 통합합니다. 이러한 실험은 혼합 현실 환경에서 상호 작용 시간을 예측하기 위한 KLM 확장의 타당성을 확인합니다.,https://doi.org/10.1109/ISMAR52148.2021.00035,Display & Optics; Interaction & Input,Optical / Display Technology,User Study,Algorithm / Method; Hardware / Device
508,2021,A Reinforcement Learning Approach to Redirected Walking with Passive Haptic Feedback,수동적 햅틱 피드백을 통한 방향 전환에 대한 강화 학습 접근 방식,"Various redirected walking (RDW) techniques have been proposed, which unwittingly manipulate the mapping from the user’s physical locomotion to motions of the virtual camera. Thereby, RDW techniques guide users on physical paths with the goal to keep them inside a limited tracking area, whereas users perceive the illusion of being able to walk infinitely in the virtual environment. However, the inconsistency between the user’s virtual and physical location hinders passive haptic feedback when the user interacts with virtual objects, which are represented by physical props in the real environment.In this paper, we present a novel reinforcement learning approach towards RDW with passive haptics. With a novel dense reward function, our method learns to jointly consider physical boundary avoidance and consistency of user-object positioning between virtual and physical spaces. The weights of reward and penalty terms in the reward function are dynamically adjusted to adaptively balance term impacts during the walking process. Experimental results demonstrate the advantages of our technique in comparison to previous approaches. Finally, the code of our technique is provided as an open-source solution.","사용자의 물리적 이동에서 가상 카메라의 모션으로의 매핑을 무의식적으로 조작하는 다양한 리디렉션 걷기(RDW) 기술이 제안되었습니다. 따라서 RDW 기술은 사용자를 제한된 추적 영역 내에 유지한다는 목표를 가지고 물리적 경로로 사용자를 안내하는 반면, 사용자는 가상 환경에서 무한히 걸을 수 있다는 환상을 인식합니다. 그러나 사용자의 가상 위치와 물리적 위치 간의 불일치는 사용자가 실제 환경에서 물리적 소품으로 표현되는 가상 객체와 상호 작용할 때 수동적 햅틱 피드백을 방해합니다. 본 논문에서는 수동적 햅틱을 사용하여 RDW에 대한 새로운 강화 학습 접근 방식을 제시합니다. 새로운 조밀한 보상 기능을 통해 우리의 방법은 물리적 경계 회피와 가상 공간과 물리적 공간 사이의 사용자 개체 위치 일관성을 공동으로 고려하는 방법을 학습합니다. 보상 함수의 보상 및 페널티 조건의 가중치는 걷기 과정 중 기간 영향의 균형을 적응적으로 조정하기 위해 동적으로 조정됩니다. 실험 결과는 이전 접근 방식과 비교하여 우리 기술의 장점을 보여줍니다. 마지막으로 우리 기술의 코드는 오픈 소스 솔루션으로 제공됩니다.",https://doi.org/10.1109/ISMAR52148.2021.00033,Interaction & Input,Redirected Walking / Locomotion,Technical Evaluation,Algorithm / Method
509,2021,A Taxonomy of Interaction Techniques for Immersive Augmented Reality based on an Iterative Literature Review,반복 문헌 검토를 기반으로 한 몰입형 증강 현실을 위한 상호 작용 기술 분류,"Developers of interactive systems have a variety of interaction techniques to choose from, each with individual strengths and limitations in terms of the considered task, context, and users. While there are taxonomies for desktop, mobile, and virtual reality applications, augmented reality (AR) taxonomies have not been established yet. However, recent advances in immersive AR technology (i.e., head-worn or projection-based AR), such as the emergence of untethered headsets with integrated gesture and speech sensors, have enabled the inclusion of additional input modalities and, therefore, novel multimodal interaction methods have been introduced. To provide an overview of interaction techniques for current immersive AR systems, we conducted a literature review of publications between 2016 and 2021. Based on 44 relevant papers, we developed a comprehensive taxonomy focusing on two identified dimensions – task and modality. We further present an adaptation of an iterative taxonomy development method to the field of human-computer interaction. Finally, we discuss observed trends and implications for future work.","대화형 시스템의 개발자는 선택할 수 있는 다양한 상호 작용 기술을 가지고 있으며 각 기술은 고려되는 작업, 컨텍스트 및 사용자 측면에서 개별적인 장점과 한계를 가지고 있습니다. 데스크톱, 모바일 및 가상 현실 애플리케이션에 대한 분류법이 있지만 증강 현실(AR) 분류법은 아직 확립되지 않았습니다. 그러나 제스처 및 음성 센서가 통합된 무선 헤드셋의 출현과 같은 몰입형 AR 기술(예: 머리 착용 또는 프로젝션 기반 AR)의 최근 발전으로 인해 추가 입력 양식을 포함할 수 있게 되었고 이에 따라 새로운 다중 모드 상호 작용 방법이 도입되었습니다. 현재 몰입형 AR 시스템의 상호 작용 기술에 대한 개요를 제공하기 위해 우리는 2016년부터 2021년 사이의 출판물에 대한 문헌 검토를 수행했습니다. 44개의 관련 논문을 기반으로 우리는 확인된 두 가지 차원, 즉 작업과 양식에 초점을 맞춘 포괄적인 분류법을 개발했습니다. 우리는 또한 인간-컴퓨터 상호 작용 분야에 대한 반복적 분류 개발 방법의 적용을 제시합니다. 마지막으로 관찰된 추세와 향후 작업에 대한 시사점을 논의합니다.",https://doi.org/10.1109/ISMAR52148.2021.00060,Interaction & Input,Optical / Display Technology,Literature Review / Meta-analysis,Survey / Review; Algorithm / Method
510,2021,ARENA: The Augmented Reality Edge Networking Architecture,ARENA: 증강 현실 엣지 네트워킹 아키텍처,"Many have predicted the future of the Web to be the integration of Web content with the real-world through technologies such as Augmented Reality (AR). This has led to the rise of Extended Reality (XR) Web Browsers used to shorten the long AR application development and deployment cycle of native applications especially across different platforms. As XR Browsers mature, we face new challenges related to collaborative and multi-user applications that span users, devices, and machines. These collaborative XR applications require: (1) networking support for scaling to many users, (2) mechanisms for content access control and application isolation, and (3) the ability to host application logic near clients or data sources to reduce application latency. In this paper, we present the design and evaluation of the AR Edge Networking Architecture (ARENA) which is a platform that simplifies building and hosting collaborative XR applications on WebXR capable browsers. ARENA provides a number of critical components including: a hierarchical geospatial directory service that connects users to nearby servers and content, a token-based authentication system for controlling user access to content, and an application/service runtime supervisor that can dispatch programs across any network connected device. All of the content within ARENA exists as endpoints in a PubSub scene graph model that is synchronized across all users. We evaluate ARENA in terms of client performance as well as benchmark end-to-end response-time as load on the system scales. We show the ability to horizontally scale the system to Internet-scale with scenes containing hundreds of users and latencies on the order of tens of milliseconds. Finally, we highlight projects built using ARENA and showcase how our approach dramatically simplifies collaborative multi-user XR development compared to monolithic approaches.","많은 사람들이 웹의 미래는 증강 현실(AR)과 같은 기술을 통해 웹 콘텐츠와 현실 세계의 통합이 될 것이라고 예측했습니다. 이로 인해 특히 다양한 플랫폼에서 긴 AR 애플리케이션 개발 및 기본 애플리케이션 배포 주기를 단축하는 데 사용되는 확장 현실(XR) 웹 브라우저가 등장하게 되었습니다. XR 브라우저가 성숙해짐에 따라 우리는 사용자, 장치 및 시스템을 포괄하는 협업 및 다중 사용자 애플리케이션과 관련된 새로운 과제에 직면하게 되었습니다. 이러한 협업 XR 애플리케이션에는 (1) 많은 사용자로 확장하기 위한 네트워킹 지원, (2) 콘텐츠 액세스 제어 및 애플리케이션 격리를 위한 메커니즘, (3) 애플리케이션 대기 시간을 줄이기 위해 클라이언트 또는 데이터 소스 근처에서 애플리케이션 로직을 호스팅하는 기능이 필요합니다. 본 논문에서는 WebXR 지원 브라우저에서 협업 XR 애플리케이션 구축 및 호스팅을 단순화하는 플랫폼인 AR Edge Networking Architecture(ARENA)의 설계 및 평가를 제시합니다. ARENA는 사용자를 근처 서버 및 콘텐츠에 연결하는 계층적 지리공간 디렉터리 서비스, 콘텐츠에 대한 사용자 액세스를 제어하기 위한 토큰 기반 인증 시스템, 네트워크에 연결된 모든 장치에 프로그램을 파견할 수 있는 애플리케이션/서비스 런타임 감독자를 포함한 여러 가지 중요한 구성 요소를 제공합니다. ARENA 내의 모든 콘텐츠는 모든 사용자 간에 동기화되는 PubSub 장면 그래프 모델의 엔드포인트로 존재합니다. 우리는 클라이언트 성능 측면에서 ARENA를 평가하고 시스템 부하가 증가함에 따라 엔드투엔드 응답 시간을 벤치마킹합니다. 우리는 수백 명의 사용자와 수십 밀리초 정도의 대기 시간을 포함하는 장면을 통해 시스템을 인터넷 규모로 수평 확장할 수 있는 능력을 보여줍니다. 마지막으로 ARENA를 사용하여 구축된 프로젝트를 강조하고 우리의 접근 방식이 모놀리식 접근 방식에 비해 어떻게 공동 다중 사용자 XR 개발을 획기적으로 단순화하는지 보여줍니다.",https://doi.org/10.1109/ISMAR52148.2021.00065,Collaboration & Social,Cloud / Edge Computing,Quantitative Experiment,System / Framework; Algorithm / Method
511,2021,AlterEcho: Loose Avatar-Streamer Coupling for Expressive VTubing,AlterEcho: 표현력이 풍부한 VTubing을 위한 느슨한 아바타-스트리머 결합,"VTubers are live streamers who embody computer animation virtual avatars. VTubing is a rapidly rising form of online entertainment in East Asia, most notably in Japan and China, and it has been more recently introduced in the West. However, animating an expressive VTuber avatar remains a challenge due to budget and usability limitations of current solutions, i.e., high-fidelity motion capture is expensive, while keyboard-based VTubing interfaces impose a cognitive burden on the streamer. This paper proposes a novel approach for VTubing animation based on the key principle of loosening the coupling between the VTuber and their avatar, and it describes a first implementation of the approach in the AlterEcho VTubing animation system. AlterEcho generates expressive VTuber avatar animation automatically, without the streamer’s explicit intervention; it breaks the strict tethering of the avatar from the streamer, allowing the avatar’s nonverbal behavior to deviate from that of the streamer. Without the complete independence of a true alter ego, but also without the constraint of mirroring the streamer with the fidelity of an echo, AlterEcho produces avatar animations that have been rated significantly higher by VTubers and viewers (N = 315) compared to animations created using simple motion capture, or using VMagicMirror, a state-of-the-art keyboard-based VTubing system. Our work also opens the door to personalizing the avatar persona for individual viewers.","VTuber는 컴퓨터 애니메이션 가상 아바타를 구현하는 라이브 스트리머입니다. VTubing은 동아시아, 특히 일본과 중국에서 빠르게 성장하고 있는 온라인 엔터테인먼트 형태이며, 서구에서는 최근에 소개되었습니다. 그러나 표현력이 풍부한 VTuber 아바타를 애니메이션화하는 것은 현재 솔루션의 예산 및 유용성 제한으로 인해 여전히 어려운 과제로 남아 있습니다. 즉, 고품질 모션 캡처는 비용이 많이 들고 키보드 기반 VTubing 인터페이스는 스트리머에 인지적 부담을 가합니다. 본 논문은 VTuber와 아바타 사이의 연결을 느슨하게 하는 핵심 원리를 기반으로 하는 VTubing 애니메이션에 대한 새로운 접근 방식을 제안하고 AlterEcho VTubing 애니메이션 시스템에서 접근 방식의 첫 번째 구현을 설명합니다. AlterEcho는 스트리머의 명시적인 개입 없이 자동으로 표현력이 풍부한 VTuber 아바타 애니메이션을 생성합니다. 이는 스트리머로부터 아바타의 엄격한 테더링을 깨뜨려 아바타의 비언어적 행동이 스트리머의 행동에서 벗어날 수 있도록 합니다. 진정한 분신의 완전한 독립성 없이, 또한 에코의 충실도로 스트리머를 미러링해야 하는 제약 없이 AlterEcho는 간단한 모션 캡처를 사용하거나 최첨단 키보드 기반 VTubing 시스템인 VMagicMirror를 사용하여 생성된 애니메이션에 비해 VTuber와 시청자(N = 315)로부터 훨씬 더 높은 평가를 받은 아바타 애니메이션을 생성합니다. 우리의 작업은 또한 개별 시청자를 위해 아바타 페르소나를 개인화할 수 있는 문을 열어줍니다.",https://doi.org/10.1109/ISMAR52148.2021.00027,Perception & Cognition,Other,Technical Evaluation,System / Framework
512,2021,BDLoc: Global Localization from 2.5D Building Map,BDLoc: 2.5D 건물 지도의 글로벌 현지화,"Robust and accurate global 6DoF localization is essential for many applications, i.e., augmented reality and autonomous driving. Most existing 6DoF visual localization approaches need to build a dense texture model in advance, which is computationally extensive and almost infeasible in the global range. In this work, we propose BDLoc, a hierarchical global localization framework via the 2.5D building map, which is able to estimate the accurate pose of the query street-view image without using detailed dense 3D model and texture information. Specifically speaking, we first extract the 3D building information from the street-view image and surrounding 2.5D building map, and then solve a coarse relative pose by local to global registration. In order to improve the feature extraction, we propose a novel SPG-Net which is able to capture both local and global features. Finally, an iterative semantic alignment is applied to obtain a finner result with the differentiable rendering and the cross-view semantic constraint. Except for a coarse longitude and latitude from GPS, BDLoc doesn’t need any additional information like altitude and orientation that are necessary for many previous works. We also create a large dataset to explore the performance of the 2.5D map-based localization task. Extensive experiments demonstrate the superior performance of our method.","강력하고 정확한 전역 6DoF 위치 파악은 증강 현실 및 자율 주행과 같은 많은 애플리케이션에 필수적입니다. 대부분의 기존 6DoF 시각적 위치 파악 접근 방식은 조밀한 텍스처 모델을 미리 구축해야 하는데, 이는 계산적으로 광범위하고 전역 범위에서 거의 실행 불가능합니다. 본 연구에서는 상세한 조밀한 3D 모델과 텍스처 정보를 사용하지 않고도 쿼리 스트리트 뷰 이미지의 정확한 자세를 추정할 수 있는 2.5D 건물 지도를 통한 계층적 글로벌 위치 파악 프레임워크인 BDLoc을 제안합니다. 구체적으로 말하면 먼저 스트리트 뷰 이미지와 주변 2.5D 건물 지도에서 3D 건물 정보를 추출한 다음 로컬에서 글로벌 등록을 통해 대략적인 상대 포즈를 해결합니다. 특징 추출을 개선하기 위해 우리는 로컬 및 글로벌 특징을 모두 캡처할 수 있는 새로운 SPG-Net을 제안합니다. 마지막으로, 미분 가능한 렌더링과 크로스 뷰 의미 제약 조건을 사용하여 더 정밀한 결과를 얻기 위해 반복적인 의미 체계 정렬이 적용됩니다. BDLoc은 GPS에서 얻은 대략적인 경도와 위도를 제외하면 이전의 많은 작업에서 필요했던 고도, 방향과 같은 추가 정보가 필요하지 않습니다. 또한 2.5D 지도 기반 현지화 작업의 성능을 탐색하기 위해 대규모 데이터 세트를 생성합니다. 광범위한 실험을 통해 우리 방법의 탁월한 성능이 입증되었습니다.",https://doi.org/10.1109/ISMAR52148.2021.00022,Tracking & Localization; Rendering & Visualization,Computer Vision,Technical Evaluation,Algorithm / Method
513,2021,Blending Shadows: Casting Shadows in Virtual and Real using Occlusion-Capable Augmented Reality Near-Eye Displays,그림자 블렌딩: 폐색 가능 증강 현실 근안 디스플레이를 사용하여 가상과 현실에 그림자를 드리우다,"The fundamental goal of augmented reality (AR) is to integrate virtual objects into the user’s perceived reality seamlessly. However, various issues hinder this integration. In particular, Optical See Through (OST) AR is hampered by the need for light subtraction due to its see-through nature, making some basic rendering harder to realize. In this paper, we realize mutual shadows between real and virtual objects in OST AR to improve this virtual-real integration. Shadows are a classic problem in computer graphics, virtual reality, and video see-through AR, yet they have not been fully explored in OST AR due to the light subtraction requirement. We build a proof-of-concept system that combines a custom occlusion-capable OST display, global light source estimation, 3D registration, and ray-tracing-based rendering. We will demonstrate mutual shadows using a prototype and demonstrate its effectiveness by quantitatively evaluating shadows with the real environment using a perceptual visual metric.","증강 현실(AR)의 기본 목표는 가상 객체를 사용자가 인식하는 현실에 원활하게 통합하는 것입니다. 그러나 다양한 문제가 이러한 통합을 방해합니다. 특히 OST(Optical See Through) AR은 투명 특성으로 인해 빛을 빼야 하는 필요성으로 인해 방해를 받아 일부 기본 렌더링을 구현하기가 더 어렵습니다. 본 논문에서는 가상-현실 통합을 개선하기 위해 OST AR에서 실제 객체와 가상 객체 간의 상호 그림자를 실현합니다. 그림자는 컴퓨터 그래픽, 가상 현실 및 비디오 투명 AR의 고전적인 문제이지만 빛 빼기 요구 사항으로 인해 OST AR에서는 완전히 탐구되지 않았습니다. 우리는 맞춤형 폐색 가능 OST 디스플레이, 전역 광원 추정, 3D 등록 및 광선 추적 기반 렌더링을 결합하는 개념 증명 시스템을 구축합니다. 프로토타입을 사용하여 상호 그림자를 시연하고, 지각적 시각적 지표를 사용하여 실제 환경과 그림자를 정량적으로 평가하여 그 효과를 입증합니다.",https://doi.org/10.1109/ISMAR52148.2021.00059,Display & Optics; Diminished Reality,Other,Quantitative Experiment,System / Framework
514,2021,BuildingSketch: Freehand Mid-Air Sketching for Building Modeling,BuildingSketch: 건물 모델링을 위한 프리핸드 공중 스케치,"Advancements in virtual reality (VR) technology enable us to rethink the way of interactive 3D modeling - intuitively creating 3D content directly in 3D space. However, conventional VR-based modeling is laborious and tedious to generate a detailed 3D model in full manual mode since users need to carefully draw almost the entire surface. In this paper, we present a freehand mid-air sketching system with the aid of deep learning techniques for modeling structured buildings, where the user freely draws a few key strokes in mid-air using his/her fingers to represent the desired shapes and our system automatically interprets the strokes using a deep neural network and generates a detailed building model based on a procedural modeling method. After creating several building blocks one by one, the user can freely move, rotate, and combine the blocks to form a complex building model. We demonstrate the ease of use for novice users, effectiveness, and efficiency of our sketching system, BuildingSketch, by presenting a variety of building models.","가상 현실(VR) 기술의 발전으로 우리는 대화형 3D 모델링 방식, 즉 3D 공간에서 직접 3D 콘텐츠를 직관적으로 생성하는 방식을 다시 생각할 수 있게 되었습니다. 그러나 기존의 VR 기반 모델링은 사용자가 거의 전체 표면을 세심하게 그려야 하기 때문에 전체 수동 모드에서 상세한 3D 모델을 생성하는 것은 힘들고 지루합니다. 본 논문에서는 구조화된 건물을 모델링하기 위한 딥러닝 기법을 활용한 프리핸드 공중 스케치 시스템을 제시합니다. 사용자는 원하는 모양을 표현하기 위해 손가락을 사용하여 공중에서 몇 가지 키 스트로크를 자유롭게 그리고 우리 시스템은 심층 신경망을 사용하여 스트로크를 자동으로 해석하고 절차적 모델링 방법을 기반으로 상세한 건물 모델을 생성합니다. 여러 개의 빌딩 블록을 하나씩 생성한 후 사용자는 블록을 자유롭게 이동하고 회전하고 결합하여 복잡한 빌딩 모델을 형성할 수 있습니다. 다양한 건물 모델을 제시하여 초보 사용자를 위한 사용 용이성, 스케치 시스템인 BuildingSketch의 효과 및 효율성을 보여줍니다.",https://doi.org/10.1109/ISMAR52148.2021.00049,Interaction & Input; Content Authoring,Deep Learning / Neural Networks,Other,Algorithm / Method
515,2021,Classifying In-Place Gestures with End-to-End Point Cloud Learning,엔드투엔드 포인트 클라우드 학습을 통한 내부 제스처 분류,"Walking in place for moving through virtual environments has attracted noticeable attention recently. Recent attempts focused on training a classifier to recognize certain patterns of gestures (e.g., standing, walking, etc) with the use of neural networks like CNN or LSTM. Nevertheless, they often consider very few types of gestures and/or induce less desired latency in virtual environments. In this paper, we propose a novel framework for accurate and efficient classification of in-place gestures. Our key idea is to treat several consecutive frames as a “point cloud”. The HMD and two VIVE trackers provide three points in each frame, with each point consisting of 12-dimensional features (i.e., three-dimensional position coordinates, velocity, rotation, angular velocity). We create a dataset consisting of 9 gesture classes for virtual in-place locomotion. In addition to the supervised point-based network, we also take unsupervised domain adaptation into account due to inter-person variations. To this end, we develop an end-to-end joint framework involving both a supervised loss for supervised point learning and an unsupervised loss for unsupervised domain adaptation. Experiments demonstrate that our approach generates very promising outcomes, in terms of high overall classification accuracy (95.0%) and real-time performance (192ms latency). We will release our dataset and source code to the community.","최근 가상환경을 이동하기 위한 제자리 걷기가 주목을 받고 있다. 최근 시도는 CNN이나 LSTM과 같은 신경망을 사용하여 특정 동작 패턴(예: 서기, 걷기 등)을 인식하도록 분류기를 훈련하는 데 중점을 두었습니다. 그럼에도 불구하고 그들은 가상 환경에서 매우 적은 유형의 제스처를 고려하거나 원하는 지연 시간을 적게 유도하는 경우가 많습니다. 본 논문에서는 제자리 제스처를 정확하고 효율적으로 분류하기 위한 새로운 프레임워크를 제안합니다. 우리의 핵심 아이디어는 여러 연속 프레임을 ""포인트 클라우드""로 처리하는 것입니다. HMD와 2개의 VIVE 트래커는 각 프레임에 3개의 포인트를 제공하며, 각 포인트는 12차원 특징(예: 3차원 위치 좌표, 속도, 회전, 각속도)으로 구성됩니다. 가상 제자리 이동을 위한 9개의 제스처 클래스로 구성된 데이터세트를 만듭니다. 감독되는 포인트 기반 네트워크 외에도 사람 간 변화로 인한 감독되지 않는 도메인 적응도 고려합니다. 이를 위해 우리는 지도 포인트 학습을 위한 지도 손실과 비지도 도메인 적응을 위한 비지도 손실을 모두 포함하는 엔드투엔드 조인트 프레임워크를 개발합니다. 실험은 우리의 접근 방식이 높은 전체 분류 정확도(95.0%)와 실시간 성능(192ms 대기 시간) 측면에서 매우 유망한 결과를 생성한다는 것을 보여줍니다. 데이터 세트와 소스 코드를 커뮤니티에 공개할 예정입니다.",https://doi.org/10.1109/ISMAR52148.2021.00038,Interaction & Input; Education & Training,Deep Learning / Neural Networks; Cloud / Edge Computing,Quantitative Experiment; Technical Evaluation,Algorithm / Method
516,2021,CrowdXR - Pitfalls and Potentials of Experiments with Remote Participants,CrowdXR - 원격 참가자 실험의 함정과 가능성,"Although the COVID-19 pandemic has made the need for remote data collection more apparent than ever, progress has been slow in the virtual reality (VR) research community, and little is known about the quality of the data acquired from crowdsourced participants who own a head-mounted display (HMD), which we call crowdXR. To investigate this problem, we report on a VR spatial cognition experiment that was conducted both in-lab and out-of-lab. The in-lab study was administered as a traditional experiment with undergraduate students and dedicated VR equipment. The out-of-lab study was carried out remotely by recruiting HMD owners from VR-related research mailing lists, VR subreddits in Reddit, and crowdsourcing platforms. Demographic comparisons show that our out-of-lab sample was older, included more males, and had a higher sense of direction than our in-lab sample. The results of the involved spatial memory tasks indicate that the reliability of the data from out-of-lab participants was as good as or better than their in-lab counterparts. Additionally, the data for testing our research hypotheses were comparable between in- and out-of-lab studies. We conclude that crowdsourcing is a feasible and effective alternative to the use of university participant pools for collecting survey and performance data for VR research, despite potential design issues that may affect the generalizability of study results. We discuss the implications and future directions of running VR studies outside the laboratory and provide a set of practical recommendations.","코로나19 팬데믹으로 인해 원격 데이터 수집의 필요성이 그 어느 때보다 분명해졌음에도 불구하고, 가상 현실(VR) 연구 커뮤니티의 발전은 더디었으며, CrowdXR이라고 부르는 헤드 마운트 디스플레이(HMD)를 소유한 크라우드소싱 참가자로부터 획득한 데이터의 품질에 대해서는 알려진 바가 거의 없습니다. 이 문제를 조사하기 위해 우리는 실험실 내 및 실험실 밖에서 수행된 VR 공간 인식 실험에 대해 보고합니다. 연구실 내 연구는 학부생과 전용 VR 장비를 이용한 전통적인 실험으로 진행되었습니다. 연구실 외부 연구는 VR 관련 연구 메일링 리스트, Reddit의 VR 하위 레딧, 크라우드소싱 플랫폼에서 HMD 소유자를 모집하여 원격으로 수행되었습니다. 인구통계학적 비교에 따르면 실험실 외부 표본은 실험실 내 표본보다 나이가 많고 남성이 더 많으며 방향 감각이 더 높은 것으로 나타났습니다. 관련된 공간 기억 작업의 결과는 실험실 외부 참가자의 데이터 신뢰성이 실험실 참가자보다 좋거나 더 좋았음을 나타냅니다. 또한, 우리의 연구 가설을 테스트하기 위한 데이터는 실험실 내 연구와 외부 연구 간에 비교 가능했습니다. 우리는 크라우드소싱이 연구 결과의 일반화에 영향을 미칠 수 있는 잠재적인 설계 문제에도 불구하고 VR 연구를 위한 설문 조사 및 성과 데이터를 수집하기 위해 대학 참가자 풀을 사용하는 것에 대한 실현 가능하고 효과적인 대안이라고 결론지었습니다. 우리는 실험실 외부에서 VR 연구를 실행하는 것의 의미와 향후 방향에 대해 논의하고 일련의 실용적인 권장 사항을 제공합니다.",https://doi.org/10.1109/ISMAR52148.2021.00062,Display & Optics,Other,User Study,System / Framework
517,2021,Cybersickness Prediction from Integrated HMD's Sensors: A Multimodal Deep Fusion Approach using Eye-tracking and Head-tracking Data,통합 HMD 센서를 통한 사이버 멀미 예측: 시선 추적 및 머리 추적 데이터를 사용한 다중 모드 심층 융합 접근 방식,"Cybersickness prediction is one of the significant research challenges for real-time cybersickness reduction. Researchers have proposed different approaches for predicting cybersickness from bio-physiological data (e.g., heart rate, breathing rate, electroencephalogram). However, collecting bio-physiological data often requires external sensors, limiting locomotion and 3D-object manipulation during the virtual reality (VR) experience. Limited research has been done to predict cybersickness from the data readily available from the integrated sensors in head-mounted displays (HMDs) (e.g., head-tracking, eye-tracking, motion features), allowing free locomotion and 3D-object manipulation. This research proposes a novel deep fusion network to predict cybersickness severity from heterogeneous data readily available from the integrated HMD sensors. We extracted 1755 stereoscopic videos, eye-tracking, and head-tracking data along with the corresponding self-reported cybersickness severity collected from 30 participants during their VR gameplay. We applied several deep fusion approaches with the heterogeneous data collected from the participants. Our results suggest that cybersickness can be predicted with an accuracy of 87.77% and a root-mean-square error of 0.51 when using only eye-tracking and head-tracking data. We concluded that eye-tracking and head-tracking data are well suited for a standalone cybersickness prediction framework.","사이버 멀미 예측은 실시간 사이버 멀미 감소를 위한 중요한 연구 과제 중 하나입니다. 연구자들은 생체 생리학적 데이터(예: 심박수, 호흡수, 뇌전도)로부터 사이버멀미를 예측하기 위한 다양한 접근 방식을 제안했습니다. 그러나 생체 생리학적 데이터를 수집하려면 가상 현실(VR) 경험 중에 운동 및 3D 객체 조작을 제한하는 외부 센서가 필요한 경우가 많습니다. 머리 장착형 디스플레이(HMD)의 통합 센서에서 쉽게 사용할 수 있는 데이터(예: 머리 추적, 시선 추적, 모션 기능)를 통해 사이버 멀미를 예측하기 위한 제한된 연구가 수행되어 자유로운 이동과 3D 객체 조작이 가능해졌습니다. 본 연구에서는 통합 HMD 센서에서 쉽게 사용할 수 있는 이기종 데이터로부터 사이버 멀미의 심각도를 예측하기 위한 새로운 심층 융합 네트워크를 제안합니다. 우리는 VR 게임 플레이 중에 30명의 참가자로부터 수집한 해당 자체 보고 사이버 멀미 심각도와 함께 1755개의 입체 비디오, 시선 추적 및 머리 추적 데이터를 추출했습니다. 우리는 참가자로부터 수집된 이질적인 데이터를 사용하여 여러 가지 심층 융합 접근 방식을 적용했습니다. 우리의 결과는 시선 추적 및 머리 추적 데이터만 사용할 때 87.77%의 정확도와 0.51의 평균 제곱근 오차로 사이버 멀미를 예측할 수 있음을 시사합니다. 우리는 시선 추적 및 머리 추적 데이터가 독립형 사이버 멀미 예측 프레임워크에 매우 적합하다는 결론을 내렸습니다.",https://doi.org/10.1109/ISMAR52148.2021.00017,Display & Optics; Interaction & Input,Redirected Walking / Locomotion,User Study,Algorithm / Method
518,2021,DVIO: Depth-Aided Visual Inertial Odometry for RGBD Sensors,DVIO: RGBD 센서를 위한 심도 보조 시각적 관성 주행 거리계,"In past few years we have observed an increase in the usage of RGBD sensors in mobile devices. These sensors provide a good estimate of the depth map for the camera frame, which can be used in numerous augmented reality applications. This paper presents a new visual inertial odometry (VIO) system, which uses measurements from a RGBD sensor and an inertial measurement unit (IMU) sensor for estimating the motion state of the mobile device. The resulting system is called the depth-aided VIO (DVIO) system. In this system we add the depth measurement as part of the nonlinear optimization process. Specifically, we propose methods to use the depth measurement using one-dimensional (1D) feature parameterization as well as three-dimensional (3D) feature parameterization. In addition, we propose to utilize the depth measurement for estimating time offset between the unsynchronized IMU and the RGBD sensors. Last but not least, we propose a novel block-based marginalization approach to speed up the marginalization processes and maintain the real-time performance of the overall system. Experimental results validate that the proposed DVIO system outperforms the other state-of-the-art VIO systems in terms of trajectory accuracy as well as processing time.","지난 몇 년 동안 우리는 모바일 장치에서 RGBD 센서의 사용이 증가하는 것을 관찰했습니다. 이러한 센서는 다양한 증강 현실 애플리케이션에 사용할 수 있는 카메라 프레임의 깊이 맵에 대한 좋은 추정치를 제공합니다. 본 논문에서는 모바일 장치의 동작 상태를 추정하기 위해 RGBD 센서와 관성 측정 장치(IMU) 센서의 측정값을 사용하는 새로운 VIO(시각적 관성 주행 거리계) 시스템을 제시합니다. 결과 시스템을 깊이 지원 VIO(DVIO) 시스템이라고 합니다. 이 시스템에서는 비선형 최적화 프로세스의 일부로 깊이 측정을 추가합니다. 구체적으로, 1차원(1D) 특징 매개변수화와 3차원(3D) 특징 매개변수화를 이용한 깊이 측정 방법을 제안한다. 또한, 비동기화된 IMU와 RGBD 센서 사이의 시간 오프셋을 추정하기 위해 깊이 측정을 활용할 것을 제안합니다. 마지막으로, 우리는 소외 프로세스의 속도를 높이고 전체 시스템의 실시간 성능을 유지하기 위해 새로운 블록 기반 소외화 접근 방식을 제안합니다. 실험 결과는 제안된 DVIO 시스템이 궤적 정확도 및 처리 시간 측면에서 다른 최첨단 VIO 시스템보다 성능이 우수하다는 것을 검증했습니다.",https://doi.org/10.1109/ISMAR52148.2021.00034,Other,Sensor Fusion,Quantitative Experiment,Algorithm / Method; Hardware / Device
519,2021,Detection-Guided 3D Hand Tracking for Mobile AR Applications,모바일 AR 애플리케이션을 위한 감지 기반 3D 손 추적,"Interaction using bare hands is experiencing a growing interest in mobile-based Augmented Reality (AR). Existing RGB-based works fail to provide a practical solution to identifying rich details of the hand. In this paper, we present a detection-guided method capable of recovery 3D hand posture with a color camera. The proposed method consists of key-point detectors and 3D pose optimizer. The detectors first locate the 2D hand bounding box and then apply a lightweight network on the hand region to provide a pixel-wise like-hood of hand joints. The optimizer lifts the 3D pose from the estimated 2D joints in a model-fitting manner. To ensure the result plausibly, we encode the hand shape into the objective function. The estimated 3D posture allows flexible hand-to-mobile interaction in AR applications. We extensively evaluate the proposed approach on several challenging public datasets. The experimental results indicate the efficiency and effectiveness of the proposed method.",맨손을 이용한 상호작용이 모바일 기반의 증강현실(AR)에 대한 관심이 높아지고 있습니다. 기존 RGB 기반 작업은 손의 풍부한 세부 사항을 식별하는 실용적인 솔루션을 제공하지 못합니다. 본 논문에서는 컬러 카메라를 이용하여 3차원 손 자세를 복구할 수 있는 감지 유도 방법을 제시합니다. 제안하는 방법은 키포인트 검출기와 3D 포즈 최적화기로 구성된다. 감지기는 먼저 2D 손 경계 상자를 찾은 다음 손 영역에 경량 네트워크를 적용하여 손 관절의 픽셀 단위 유사 후드를 제공합니다. 옵티마이저는 모델 피팅 방식으로 추정된 2D 관절에서 3D 포즈를 들어 올립니다. 결과를 그럴듯하게 보장하기 위해 손 모양을 목적 함수로 인코딩합니다. 추정된 3D 자세를 통해 AR 애플리케이션에서 손과 모바일 간 유연한 상호 작용이 가능합니다. 우리는 여러 가지 까다로운 공개 데이터 세트에 대해 제안된 접근 방식을 광범위하게 평가합니다. 실험 결과는 제안된 방법의 효율성과 유효성을 나타냅니다.,https://doi.org/10.1109/ISMAR52148.2021.00055,Interaction & Input,Hand / Gesture Recognition,Technical Evaluation,Algorithm / Method
520,2021,Diegetic Representations for Seamless Cross-Reality Interruptions,원활한 교차 현실 중단을 위한 다이에제틱 표현,"The closed design of virtual reality (VR) head-mounted displays substantially limits users’ awareness of their real-world surroundings. This presents challenges when another person in the same physical space needs to interrupt the VR user for a brief conversation. Such interruptions, e.g., tapping a VR user on the shoulder, can cause a disruptive break in presence (BIP), which affects their place and plausibility illusions, and may cause a drop in performance of their virtual activity. Recent findings related to the concept of diegesis, which denotes the internal consistency of an experience/story, suggest potential benefits of integrating registered virtual representations for physical interactors, especially when these appear internally consistent in VR. In this paper, we present a human-subject study we conducted to compare and evaluate five different diegetic and non-diegetic methods to facilitate cross-reality interruptions in a virtual office environment, where a user’s task was briefly interrupted by a physical person. We created a Cross-Reality Interaction Questionnaire (CRIQ) to capture the quality of the interaction from the VR user’s perspective. Our results show that the diegetic representations afforded reasonably high senses of co-presence, the highest quality interactions, the highest place illusions, and caused the least disruption of the participants’ virtual experiences. We discuss our findings as well as implications for practical applications that aim to leverage virtual representations to ease cross-reality interruptions.","가상 현실(VR) 헤드 마운트 디스플레이의 폐쇄형 디자인은 실제 주변 환경에 대한 사용자의 인식을 크게 제한합니다. 이는 동일한 물리적 공간에 있는 다른 사람이 짧은 대화를 위해 VR 사용자를 방해해야 할 때 문제가 됩니다. 예를 들어 VR 사용자의 어깨를 두드리는 등의 방해는 BIP(방해 중단)를 유발하여 사용자의 위치 및 타당성 환상에 영향을 미치고 가상 활동 성능을 저하시킬 수 있습니다. 경험/스토리의 내부 일관성을 나타내는 디제시스 개념과 관련된 최근 연구 결과는 특히 VR에서 내부적으로 일관성이 있는 경우 물리적 상호작용자에 대해 등록된 가상 표현을 통합하는 것의 잠재적인 이점을 제시합니다. 이 논문에서는 사용자의 작업이 물리적인 사람에 의해 잠시 중단되는 가상 사무실 환경에서 교차 현실 중단을 촉진하기 위해 다섯 가지 서로 다른 디제시스 및 비디제시스 방법을 비교하고 평가하기 위해 수행한 인간 대상 연구를 제시합니다. 우리는 VR 사용자의 관점에서 상호 작용의 품질을 파악하기 위해 CRIQ(Cross-Reality Interaction Questionnaire)를 만들었습니다. 우리의 결과는 다이제시스적 표현이 합리적으로 높은 공동 현존감, 최고 품질의 상호 작용, 가장 높은 장소 환상을 제공하고 참가자의 가상 경험에 최소한의 혼란을 초래한다는 것을 보여줍니다. 우리는 가상 표현을 활용하여 현실 간 중단을 완화하는 것을 목표로 하는 실제 응용 프로그램에 대한 시사점과 연구 결과를 논의합니다.",https://doi.org/10.1109/ISMAR52148.2021.00047,Perception & Cognition; Display & Optics,Other,User Study,Algorithm / Method
521,2021,Distortion-Aware Room Layout Estimation from A Single Fisheye Image,단일 어안 이미지로부터 왜곡 인식 방 레이아웃 추정,"Omnidirectional images of 180° or 360° field of view provide the entire visual content around the capture cameras, giving rise to more sophisticated scene understanding and reasoning and bringing broad application prospects for VR/AR/MR. As a result, researches on omnidirectional image layout estimation have sprung up in recent years. However, existing layout estimation methods designed for panorama images cannot perform well on fisheye images, mainly due to lack of public fisheye dataset as well as the significantly differences in the positions and degree of distortions caused by different projection models. To fill theses gaps, in this work we first reuse the released large-scale panorama datasets and reproduce them to fisheye images via projection conversion, thereby circumventing the challenge of obtaining high-quality fisheye datasets with ground truth layout annotations. Then, we propose a distortion-aware module according to the distortion of the orthographic projection (i.e., OrthConv) to perform effective features extraction from fisheye images. Additionally, we exploit bidirectional LSTM with two-dimensional step mode for horizontal and vertical prediction to capture the long-range geometric pattern of the object for the global coherent predictions even with occlusion and cluttered scenes. We extensively evaluate our deformable convolution for room layout estimation task. In comparison with state-of-the-art approaches, our approach produces considerable performance gains in real-world dataset as well as in synthetic dataset. This technology provides high-efficiency and low-cost technical implementations for VR house viewing and MR video surveillance. We present an MR-based building video surveillance scene equipped with nine fisheye lens can achieve an immersive hybrid display experience, which can be used for intelligent building management in the future.","180° 또는 360° 시야각의 전방향 이미지는 캡처 카메라 주변의 전체 시각적 콘텐츠를 제공하여 보다 정교한 장면 이해 및 추론을 제공하고 VR/AR/MR에 대한 광범위한 응용 가능성을 제공합니다. 결과적으로 최근 몇 년 동안 전방향 이미지 레이아웃 추정에 대한 연구가 생겨났습니다. 그러나 파노라마 이미지용으로 설계된 기존 레이아웃 추정 방법은 공개 어안 데이터 세트가 부족하고 다양한 투영 모델로 인한 위치 및 왜곡 정도의 상당한 차이로 인해 어안 이미지에서 제대로 수행되지 않습니다. 이러한 격차를 메우기 위해 이 작업에서는 먼저 공개된 대규모 파노라마 데이터 세트를 재사용하고 투영 변환을 통해 이를 어안 이미지로 재현함으로써 실제 레이아웃 주석을 사용하여 고품질 어안 데이터 세트를 얻는 문제를 피합니다. 그런 다음 어안 영상에서 효과적인 특징 추출을 수행하기 위해 정사영 왜곡(OrthConv)에 따른 왜곡 인식 모듈을 제안합니다. 또한 수평 및 수직 예측을 위한 2차원 단계 모드를 갖춘 양방향 LSTM을 활용하여 폐색 및 어수선한 장면에서도 전역적으로 일관된 예측을 위해 객체의 장거리 기하학적 패턴을 캡처합니다. 우리는 방 레이아웃 추정 작업을 위한 변형 가능한 컨볼루션을 광범위하게 평가합니다. 최첨단 접근 방식과 비교하여, 우리의 접근 방식은 실제 데이터 집합과 합성 데이터 집합에서 상당한 성능 향상을 가져옵니다. 이 기술은 VR 주택 관람 및 MR 비디오 감시를 위한 고효율 및 저비용 기술 구현을 제공합니다. 우리는 향후 지능형 건물 관리에 사용할 수 있는 몰입형 하이브리드 디스플레이 경험을 달성할 수 있는 9개의 어안 렌즈가 장착된 MR 기반 건물 영상 감시 장면을 제시합니다.",https://doi.org/10.1109/ISMAR52148.2021.00061,Content Authoring; Display & Optics,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method; Dataset / Benchmark
522,2021,Edge-Guided Near-Eye Image Analysis for Head Mounted Displays,헤드 마운트 디스플레이에 대한 에지 유도 근안 이미지 분석,"Eye tracking provides an effective way for interaction in Augmented Reality (AR) Head Mounted Displays (HMDs). Current eye tracking techniques for AR HMDs require eye segmentation and ellipse fitting under near-infrared illumination. However, due to the low contrast between sclera and iris regions and unpredictable reflections, it is still challenging to accomplish accurate iris/pupil segmentation and the corresponding ellipse fitting tasks. In this paper, inspired by the fact that most essential information is encoded in the edge areas, we propose a novel near-eye image analysis method with edge maps as guidance. Specifically, we first utilize an Edge Extraction Network ($E^{2}-$Net) to predict high-quality edge maps, which only contain eyelids and iris/pupil contours without other undesired edges. Then we feed the edge maps into an Edge-Guided Segmentation and Fitting Network (ESF-Net) for accurate segmentation and ellipse fitting. Extensive experimental results demonstrate that our method outperforms current state-of-the-art methods in near-eye image segmentation and ellipse fitting tasks, based on which we present applications of eye tracking with AR HMD.",시선 추적은 증강 현실(AR) 헤드 마운트 디스플레이(HMD)에서 상호 작용을 위한 효과적인 방법을 제공합니다. AR HMD를 위한 현재 시선 추적 기술에는 근적외선 조명 하에서 눈 분할과 타원 피팅이 필요합니다. 그러나 공막과 홍채 영역 사이의 낮은 대비와 예측할 수 없는 반사로 인해 정확한 홍채/동공 분할과 해당 타원 피팅 작업을 수행하는 것은 여전히 ​​어렵습니다. 본 논문에서는 대부분의 필수 정보가 가장자리 영역에 인코딩된다는 사실에서 영감을 받아 가장자리 맵을 지침으로 사용하는 새로운 근안 이미지 분석 방법을 제안합니다. 특히 우리는 먼저 Edge Extraction Network($E^{2}-$Net)를 활용하여 다른 원하지 않는 가장자리 없이 눈꺼풀과 홍채/동공 윤곽만 포함하는 고품질 가장자리 맵을 예측합니다. 그런 다음 정확한 분할 및 타원 피팅을 위해 가장자리 맵을 ESF-Net(Edge-Guided Segmentation and Fitting Network)에 공급합니다. 광범위한 실험 결과는 우리의 방법이 AR HMD를 이용한 시선 추적 애플리케이션을 제시하는 근거리 눈 이미지 분할 및 타원 피팅 작업에서 현재의 최첨단 방법보다 성능이 우수하다는 것을 보여줍니다.,https://doi.org/10.1109/ISMAR52148.2021.00015,Interaction & Input; Display & Optics,Eye / Gaze Tracking,Technical Evaluation,Algorithm / Method
523,2021,Evaluating the user Experience of a Photorealistic Social VR Movie,사실적인 소셜 VR 영화의 사용자 경험 평가,"We all enjoy watching movies together. However, this is not always possible if we live apart. While we can remotely share our screens, the experience differs from being together. We present a social Virtual Reality (VR) system that captures, reconstructs, and transmits multiple users’ volumetric representations into a commercially produced 3D virtual movie, so they have the feeling of “being there” together. We conducted a 48-user experiment where we invited users to experience the virtual movie either using a Head Mounted Display (HMD) or using a 2D screen with a game controller. In addition, we invited 14 VR experts to experience both the HMD and the screen version of the movie and discussed their experiences in two focus groups. Our results showed that both end-users and VR experts found that the way they navigated and interacted inside a 3D virtual movie was novel. They also found that the photorealistic volumetric representations enhanced feelings of co-presence. Our study lays the groundwork for future interactive and immersive VR movie co-watching experiences.","우리는 모두 함께 영화를 보는 것을 즐깁니다. 그러나 우리가 떨어져 산다면 이것이 항상 가능한 것은 아닙니다. 원격으로 화면을 공유할 수 있지만, 함께 있는 것과는 다른 경험을 합니다. 우리는 여러 사용자의 체적 표현을 캡처, 재구성 및 상업적으로 제작된 3D 가상 영화로 전송하여 함께 ""그곳에 있는"" 느낌을 갖게 하는 소셜 가상 현실(VR) 시스템을 제시합니다. 우리는 HMD(Head Mounted Display)를 사용하거나 게임 컨트롤러가 있는 2D 화면을 사용하여 가상 영화를 경험하도록 사용자를 초대하는 48명의 사용자 실험을 수행했습니다. 또한 VR 전문가 14명을 초대하여 HMD와 영화 스크린 버전을 모두 경험하고 두 포커스 그룹에서 그들의 경험에 대해 논의했습니다. 우리의 결과는 최종 사용자와 VR 전문가 모두 3D 가상 영화 내에서 탐색하고 상호 작용하는 방식이 참신하다는 것을 발견했습니다. 그들은 또한 사실적인 체적 표현이 공동 존재감을 강화한다는 것을 발견했습니다. 우리의 연구는 미래의 대화형 몰입형 VR 영화 공동 시청 경험을 위한 토대를 마련합니다.",https://doi.org/10.1109/ISMAR52148.2021.00044,Display & Optics; Collaboration & Social,Other,Qualitative Analysis,System / Framework
524,2021,Excite-O-Meter: Software Framework to Integrate Heart Activity in Virtual Reality,Excite-O-Meter: 가상 현실에 심장 활동을 통합하는 소프트웨어 프레임워크,"Bodily signals can complement subjective and behavioral measures to analyze human factors, such as user engagement or stress, when interacting with virtual reality (VR) environments. Enabling widespread use of (also the real-time analysis) of bodily signals in VR applications could be a powerful method to design more user-centric, personalized VR experiences. However, technical and scientific challenges (e.g., cost of research-grade sensing devices, required coding skills, expert knowledge needed to interpret the data) complicate the integration of bodily data in existing interactive applications. This paper presents the design, development, and evaluation of an open-source software framework named Excite-O-Meter. It allows existing VR applications to integrate, record, analyze, and visualize bodily signals from wearable sensors, with the example of cardiac activity (heart rate and its variability) from the chest strap Polar H10. Survey responses from 58 potential users determined the design requirements for the framework. Two tests evaluated the framework and setup in terms of data acquisition/analysis and data quality. Finally, we present an example experiment that shows how our tool can be an easy-to-use and scientifically validated tool for researchers, hobbyists, or game designers to integrate bodily signals in VR applications.","신체 신호는 가상 현실(VR) 환경과 상호 작용할 때 사용자 참여 또는 스트레스와 같은 인적 요인을 분석하기 위해 주관적 및 행동적 측정을 보완할 수 있습니다. VR 애플리케이션에서 신체 신호의 광범위한 사용(실시간 분석도 가능)을 가능하게 하는 것은 보다 사용자 중심적이고 개인화된 VR 경험을 디자인하는 강력한 방법이 될 수 있습니다. 그러나 기술 및 과학적 문제(예: 연구 등급 감지 장치의 비용, 필요한 코딩 기술, 데이터 해석에 필요한 전문 지식)로 인해 기존 대화형 응용 프로그램에 신체 데이터를 통합하는 것이 복잡해졌습니다. 이 문서에서는 Excite-O-Meter라는 오픈 소스 소프트웨어 프레임워크의 설계, 개발 및 평가를 제시합니다. 이를 통해 기존 VR 애플리케이션은 가슴 스트랩 Polar H10의 심장 활동(심박수 및 변동성)을 예로 들어 웨어러블 센서의 신체 신호를 통합, 기록, 분석 및 시각화할 수 있습니다. 58명의 잠재적 사용자의 설문 조사 응답을 통해 프레임워크의 설계 요구 사항이 결정되었습니다. 두 가지 테스트에서는 데이터 수집/분석 및 데이터 품질 측면에서 프레임워크와 설정을 평가했습니다. 마지막으로, 연구원, 취미생활자 또는 게임 디자이너가 VR 애플리케이션에 신체 신호를 통합하기 위해 우리 도구가 어떻게 사용하기 쉽고 과학적으로 검증된 도구가 될 수 있는지 보여주는 예제 실험을 제시합니다.",https://doi.org/10.1109/ISMAR52148.2021.00052,Interaction & Input,Optical / Display Technology,Questionnaire / Survey,Hardware / Device
525,2021,Exploring Head-based Mode-Switching in Virtual Reality,가상 현실에서 머리 기반 모드 전환 탐색,"Mode-switching supports multilevel operations using a limited number of input methods. In Virtual Reality (VR) head-mounted displays (HMD), common approaches for mode-switching use buttons, controllers, and users’ hands. However, they are inefficient and challenging to do with tasks that require both hands (e.g., when users need to use two hands during drawing operations). Using head gestures for mode-switching can be an efficient and cost-effective way, allowing for a more continuous and smooth transition between modes. In this paper, we explore the use of head gestures for mode-switching especially in scenarios when both users’ hands are performing tasks. We present a first user study that evaluated eight head gestures that could be suitable for VR HMD with a dual-hand line-drawing task. Results show that move forward, move backward, roll left, and roll right led to better performance and are preferred by participants. A second study integrating these four gestures in Tilt Brush, an open-source painting VR application, is conducted to further explore the applicability of these gestures and derive insights. Results show that Tilt Brush with head gestures allowed users to change modes with ease and led to improved interaction and user experience. The paper ends with a discussion on some design recommendations for using head-based mode-switching in VR HMD.","모드 전환은 제한된 수의 입력 방법을 사용하여 다단계 작업을 지원합니다. 가상 현실(VR) 헤드 장착 디스플레이(HMD)에서 모드 전환을 위한 일반적인 접근 방식은 버튼, 컨트롤러 및 사용자 손을 사용합니다. 그러나 양손이 필요한 작업(예: 그리기 작업 중에 사용자가 두 손을 사용해야 하는 경우)에는 비효율적이고 어렵습니다. 모드 전환을 위해 머리 제스처를 사용하는 것은 효율적이고 비용 효율적인 방법이 될 수 있으며, 모드 간 보다 지속적이고 원활한 전환이 가능합니다. 본 논문에서는 특히 두 사용자의 손이 작업을 수행하는 시나리오에서 모드 전환을 위해 머리 제스처를 사용하는 방법을 살펴봅니다. 우리는 양손 선 그리기 작업을 통해 VR HMD에 적합할 수 있는 8가지 머리 제스처를 평가한 첫 번째 사용자 연구를 제시합니다. 결과는 앞으로 이동, 뒤로 이동, 왼쪽으로 굴림, 오른쪽으로 굴림이 더 나은 성과로 이어졌고 참가자들이 선호하는 것으로 나타났습니다. 이러한 네 가지 제스처를 오픈소스 페인팅 VR 애플리케이션인 Tilt Brush에 통합하는 두 번째 연구는 이러한 제스처의 적용 가능성을 더 탐색하고 통찰력을 도출하기 위해 수행됩니다. 결과에 따르면 머리 제스처가 포함된 Tilt Brush를 사용하면 사용자가 모드를 쉽게 변경할 수 있었고 상호 작용 및 사용자 경험이 향상되었습니다. 이 논문은 VR HMD에서 헤드 기반 모드 전환을 사용하기 위한 몇 가지 설계 권장 사항에 대한 논의로 끝납니다.",https://doi.org/10.1109/ISMAR52148.2021.00026,Interaction & Input; Display & Optics,Optical / Display Technology,User Study,Algorithm / Method; Design Guidelines
526,2021,FLASH: Video-Embeddable AR Anchors for Live Events,FLASH: 라이브 이벤트용 비디오 삽입 가능 AR 앵커,"Public spaces like concert stadiums and sporting arenas are ideal venues for AR content delivery to crowds of mobile phone users. Unfortunately, these environments tend to be some of the most challenging in terms of lighting and dynamic staging for vision-based relocalization. In this paper, we introduce FLASH1, a system for delivering AR content within challenging lighting environments that uses active tags (i.e., blinking) with detectable features from passive tags (quads) for marking regions of interest and determining pose. This combination allows the tags to be detectable from long distances with significantly less computational overhead per frame, making it possible to embed tags in existing video displays like large jumbotrons. To aid in pose acquisition, we implement a gravity-assisted pose solver that removes the ambiguous solutions that are often encountered when trying to localize using standard passive tags. We show that our technique outperforms similarly sized passive tags in terms of range by 20-30% and is fast enough to run at 30 FPS even within a mobile web browser on a smartphone.","콘서트 경기장, 스포츠 경기장과 같은 공공 장소는 수많은 휴대폰 사용자에게 AR 콘텐츠를 전달하기에 이상적인 장소입니다. 불행하게도 이러한 환경은 비전 기반 재위치화를 위한 조명 및 동적 스테이징 측면에서 가장 어려운 경향이 있습니다. 본 논문에서는 관심 영역을 표시하고 포즈를 결정하기 위해 수동 태그(쿼드)에서 감지 가능한 기능과 함께 활성 태그(예: 깜박임)를 사용하는 까다로운 조명 환경에서 AR 콘텐츠를 제공하기 위한 시스템인 FLASH1을 소개합니다. 이 조합을 사용하면 프레임당 훨씬 적은 계산 오버헤드로 장거리에서 태그를 감지할 수 있으므로 대형 점보트론과 같은 기존 비디오 디스플레이에 태그를 내장할 수 있습니다. 포즈 획득을 돕기 위해 표준 수동 태그를 사용하여 위치를 파악하려고 할 때 자주 발생하는 모호한 솔루션을 제거하는 중력 보조 포즈 솔버를 구현합니다. 우리는 우리의 기술이 비슷한 크기의 수동 태그보다 범위 측면에서 20~30% 더 뛰어나고 스마트폰의 모바일 웹 브라우저 내에서도 30FPS로 실행될 만큼 빠르다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR52148.2021.00066,Tracking & Localization,Other,Quantitative Experiment,System / Framework
527,2021,Fine Virtual Manipulation with Hands of Different Sizes,다양한 크기의 손을 이용한 미세한 가상 조작,"Natural interaction with virtual objects relies on two major technology components: hand tracking and hand-object physics simulation. There are functional solutions for these two components, but their hand representations may differ in size and skeletal morphology, hence making the connection non-trivial. In this paper, we introduce a pose retargeting strategy to connect the tracked and simulated hand representations, and we have formulated and solved this hand retargeting as an optimization problem. We have also carried out a user study that demonstrates the effectiveness of our approach to enable fine manipulations that are slow and awkward with naïve approaches.","가상 개체와의 자연스러운 상호 작용은 손 추적과 손 개체 물리 시뮬레이션이라는 두 가지 주요 기술 구성 요소에 의존합니다. 이 두 구성 요소에 대한 기능적 솔루션이 있지만 손 표현은 크기와 골격 형태가 다를 수 있으므로 연결이 중요하지 않습니다. 본 논문에서는 추적된 손 표현과 시뮬레이션된 손 표현을 연결하기 위한 포즈 리타겟팅 전략을 소개하고, 이 손 리타겟팅을 최적화 문제로 공식화하고 해결했습니다. 우리는 또한 순진한 접근 방식으로는 느리고 어색한 미세한 조작을 가능하게 하는 우리 접근 방식의 효율성을 입증하는 사용자 연구를 수행했습니다.",https://doi.org/10.1109/ISMAR52148.2021.00046,Interaction & Input,Sensor Fusion,Simulation,Algorithm / Method
528,2021,Gaze Comes in Handy: Predicting and Preventing Erroneous Hand Actions in AR-Supported Manual Tasks,시선이 유용합니다: AR 지원 수동 작업에서 잘못된 손 동작 예측 및 방지,"Emerging Augmented Reality headsets incorporate gaze and hand tracking and can, thus, observe the user’s behavior without interfering with ongoing activities. In this paper, we analyze hand-eye coordination in real-time to predict hand actions during target selection and warn users of potential errors before they occur. In our first user study, we recorded 10 participants playing a memory card game, which involves frequent hand-eye coordination with little task-relevant information. We found that participants’ gaze locked onto target cards 350ms before the hands touched them in 73.3% of all cases, which coincided with the peak velocity of the hand moving to the target. Based on our findings, we then introduce a closed-loop support system that monitors the user’s fingertip position to detect the first card turn and analyzes gaze, hand velocity and trajectory to predict the second card before it is turned by the user. In a second study with 12 participants, our support system correctly displayed color-coded visual alerts in a timely manner with an accuracy of 85.9%. The results indicate the high value of eye and hand tracking features for behavior prediction and provide a first step towards predictive real-time user support.","새롭게 떠오르는 증강 현실 헤드셋에는 시선과 손 추적 기능이 통합되어 있어 진행 중인 활동을 방해하지 않고 사용자의 행동을 관찰할 수 있습니다. 본 논문에서는 손과 눈의 협응을 실시간으로 분석하여 대상 선택 시 손 동작을 예측하고 잠재적인 오류가 발생하기 전에 사용자에게 경고합니다. 첫 번째 사용자 연구에서 우리는 10명의 참가자가 작업 관련 정보가 거의 없이 빈번한 손과 눈의 협응이 필요한 메모리 카드 게임을 하는 것을 기록했습니다. 우리는 모든 경우의 73.3%에서 손이 카드를 터치하기 350ms 전에 참가자의 시선이 대상 카드에 고정되어 있음을 발견했습니다. 이는 대상을 향해 이동하는 손의 최고 속도와 일치합니다. 연구 결과를 바탕으로 사용자의 손가락 끝 위치를 모니터링하여 첫 번째 카드 회전을 감지하고 시선, 손 속도 및 궤적을 분석하여 사용자가 카드를 돌리기 전에 두 번째 카드를 예측하는 폐쇄 루프 지원 시스템을 소개합니다. 12명의 참가자를 대상으로 한 두 번째 연구에서 당사의 지원 시스템은 85.9%의 정확도로 적시에 색상으로 구분된 시각적 경고를 올바르게 표시했습니다. 결과는 행동 예측을 위한 눈과 손 추적 기능의 높은 가치를 나타내며 예측 가능한 실시간 사용자 지원을 향한 첫 번째 단계를 제공합니다.",https://doi.org/10.1109/ISMAR52148.2021.00031,Interaction & Input,Hand / Gesture Recognition,User Study,User Study / Empirical Findings
529,2021,Investigating Textual Visual Sound Effects in a Virtual Environment and their impacts on Object Perception and Sound Perception,가상 환경에서 텍스트 시각적 음향 효과와 객체 인식 및 소리 인식에 미치는 영향 조사,"In comics, Textual Sound Effects (TE) can describe sounds, but also actions, events, etc. TE could be used in Virtual Environment to efficiently create an easily recognizable scene and add more information to objects at a relatively low design cost. We investigate the impact of TE in a Virtual Environment on objects’ material perception (on category and properties) and on sound perception (on volume [dB] and spatial position). Participants (N=13, repeated measures) categorized metallic and wooden spheres and significantly changed their reaction time depending on the TE congruence with the spheres’ material/sound. They then rated a sphere’s properties (i.e., wetness, warmness, softness, smoothness, and dullness) and significantly changed their rating depending on the TE. When comparing 2 sound volumes, they perceived a sound associated with a shrinking TE as less loud and a sound associated with a growing TE as louder. When locating an audio source location, they located it significantly closer to a TE.","만화에서 텍스트 음향 효과(TE)는 소리뿐만 아니라 동작, 이벤트 등도 설명할 수 있습니다. TE는 가상 환경에서 쉽게 인식할 수 있는 장면을 효율적으로 생성하고 상대적으로 낮은 설계 비용으로 개체에 더 많은 정보를 추가하는 데 사용할 수 있습니다. 우리는 가상 환경에서 TE가 물체의 물질 인식(범주 및 속성)과 소리 인식(볼륨[dB] 및 공간 위치)에 미치는 영향을 조사합니다. 참가자(N=13, 반복 측정)는 금속 구체와 나무 구체를 분류하고 구체의 재료/소리와 TE 일치에 따라 반응 시간을 크게 변경했습니다. 그런 다음 구의 속성(예: 촉촉함, 따뜻함, 부드러움, 매끄러움, 칙칙함)을 평가하고 TE에 따라 등급을 크게 변경했습니다. 두 가지 음량을 비교할 때, 그들은 수축하는 TE와 관련된 소리를 덜 크게 인식하고 증가하는 TE와 관련된 소리를 더 크게 인식했습니다. 오디오 소스 위치를 찾을 때 TE에 훨씬 더 가깝게 위치했습니다.",https://doi.org/10.1109/ISMAR52148.2021.00048,Perception & Cognition; Audio & Sound,Other,User Study,User Study / Empirical Findings
530,2021,Investigation of Size Variations in Optical See-through Tangible Augmented Reality,광학투시형 증강현실의 크기 변화 조사,"Optical see-through AR headsets are becoming increasingly attractive for many applications. Interaction with the virtual content is usually achieved via hand gestures or with controllers. A more seamless interaction between the real and virtual world can be achieved by using tangible objects to manipulate the virtual content. Instead of interacting with detailed physical replicas, working with abstractions allows a single physical object to represent a variety of virtual objects. These abstractions would differ from their virtual representations in shape, size, texture and material. This paper investigates for the first time in optical see-through AR whether size variations are possible without major losses in performance, usability and immersion. The conducted study shows that size can be varied within a limited range without significantly affecting task completion times as well as feelings of disturbance and presence. Stronger size deviations are possible for physical objects smaller than the virtual object than for larger physical objects.","광학 투명 AR 헤드셋은 많은 응용 분야에서 점점 더 매력적으로 여겨지고 있습니다. 가상 콘텐츠와의 상호 작용은 일반적으로 손 제스처나 컨트롤러를 통해 이루어집니다. 가상 콘텐츠를 조작하기 위해 유형 개체를 사용하면 현실 세계와 가상 세계 간의 보다 원활한 상호 작용을 달성할 수 있습니다. 상세한 물리적 복제본과 상호 작용하는 대신 추상화 작업을 통해 단일 물리적 개체가 다양한 가상 개체를 나타낼 수 있습니다. 이러한 추상화는 모양, 크기, 질감 및 재질 면에서 가상 표현과 다릅니다. 이 논문에서는 성능, 유용성 및 몰입도에 큰 손실 없이 크기 변형이 가능한지 여부를 광학 투명 AR에서 처음으로 조사합니다. 수행된 연구에 따르면 작업 완료 시간은 물론 방해 및 존재감에 큰 영향을 주지 않으면서 크기가 제한된 범위 내에서 달라질 수 있음이 나타났습니다. 더 큰 물리적 객체보다 가상 객체보다 작은 물리적 객체의 경우 더 큰 크기 편차가 가능합니다.",https://doi.org/10.1109/ISMAR52148.2021.00029,Interaction & Input; Perception & Cognition,Hand / Gesture Recognition,Quantitative Experiment,Hardware / Device
531,2021,Measuring the Perceived Three-Dimensional Location of Virtual Objects in Optical See-Through Augmented Reality,광학 투명 증강 현실에서 가상 객체의 인지된 3차원 위치 측정,"For optical see-through augmented reality (AR), a new method for measuring the perceived three-dimensional location of virtual objects is presented, where participants verbally report a virtual object’s location relative to both a vertical and horizontal grid. The method is tested with a small (1.95 × 1.95 × 1.95 cm) virtual object at distances of 50 to 80 cm, viewed through a Microsoft HoloLens 1st generation AR display. Two experiments examine two different virtual object designs, whether turning in a circle between reported object locations disrupts HoloLens tracking, and whether accuracy errors, including a rightward bias and underestimated depth, might be due to systematic errors that are restricted to a particular display. Turning in a circle did not disrupt HoloLens tracking, and testing with a second display did not suggest systematic errors restricted to a particular display. Instead, the experiments are consistent with the hypothesis that, when looking downwards at a horizontal plane, HoloLens 1st generation displays exhibit a systematic rightward perceptual bias. Precision analysis suggests that the method could measure the perceived location of a virtual object within an accuracy of less than 1 mm.","광학적 투명 증강 현실(AR)의 경우, 참가자가 수직 및 수평 그리드를 기준으로 가상 객체의 위치를 ​​구두로 보고하는 가상 객체의 인지된 3차원 위치를 측정하는 새로운 방법이 제시됩니다. 이 방법은 Microsoft HoloLens 1세대 AR 디스플레이를 통해 50~80cm 거리에 있는 작은(1.95 × 1.95 × 1.95cm) 가상 개체를 사용하여 테스트되었습니다. 두 가지 실험에서는 보고된 개체 위치 사이에서 원을 돌리면 HoloLens 추적이 중단되는지 여부와 오른쪽 편향 및 과소평가된 깊이를 포함한 정확도 오류가 특정 디스플레이로 제한되는 체계적 오류로 인해 발생할 수 있는지 여부 등 두 가지 서로 다른 가상 개체 디자인을 조사합니다. 원을 돌려도 HoloLens 추적이 중단되지 않았으며 두 번째 디스플레이를 사용한 테스트에서는 특정 디스플레이로 제한된 시스템 오류가 나타나지 않았습니다. 대신 실험은 수평면에서 아래쪽을 바라볼 때 HoloLens 1세대 디스플레이가 체계적인 오른쪽 지각 편향을 보인다는 가설과 일치합니다. 정밀 분석에 따르면 이 방법은 1mm 미만의 정확도 내에서 가상 객체의 인지된 위치를 측정할 수 있는 것으로 나타났습니다.",https://doi.org/10.1109/ISMAR52148.2021.00025,Display & Optics,Other,Quantitative Experiment,Hardware / Device
532,2021,"Mirror, Mirror on My Phone: Investigating Dimensions of Self-Face Perception Induced by Augmented Reality Filters","거울, 내 휴대폰의 거울: 증강현실 필터에 의해 유도된 자기 얼굴 인식의 차원 조사","The main use of Augmented Reality (AR) today for the general public is in applications for smartphones. In particular, social network applications allow the use of many AR filters, modifying users’ environments but also their own image. These AR filters are increasingly and frequently being used and can distort in many ways users’ facial traits. Yet, we still do not know clearly how users perceive their faces augmented by these filters. In this paper, we present a study that aims to evaluate the impact of different filters, modifying several facial features such as the size or position of the eyes, the shape of the face or the orientation of the eyebrows, or adding virtual content such as virtual glasses. These filters are evaluated via a self-evaluation questionnaire, asking the participants about the personality, emotion, appeal and intelligence traits that their distorted face conveys. Our results show relative effects between the different filters in line with previous results regarding the perception of others. However, they also reveal specific effects on self-perception, showing, inter alia, that facial deformation decreases participants’ credence towards their image. The findings of this study covering multiple factors allow us to highlight the impact of face deformation on user perception but also the specificity related to this use in AR, paving the way for new works focusing on the psychological impact of such filters.","오늘날 일반 대중을 위한 증강 현실(AR)의 주요 용도는 스마트폰 애플리케이션입니다. 특히 소셜 네트워크 애플리케이션에서는 다양한 AR 필터를 사용하여 사용자의 환경은 물론 사용자 자신의 이미지도 수정할 수 있습니다. 이러한 AR 필터는 점점 더 자주 사용되고 있으며 다양한 방식으로 사용자의 얼굴 특징을 왜곡할 수 있습니다. 그러나 우리는 사용자가 이러한 필터로 강화된 자신의 얼굴을 어떻게 인식하는지 아직 명확하게 알지 못합니다. 본 논문에서는 눈의 크기나 위치, 얼굴의 모양이나 눈썹의 방향과 같은 여러 얼굴 특징을 수정하거나 가상 안경과 같은 가상 콘텐츠를 추가하여 다양한 필터의 영향을 평가하는 것을 목표로 하는 연구를 제시합니다. 이러한 필터는 참가자에게 왜곡된 얼굴이 전달하는 성격, 감정, 매력 및 지능 특성에 대해 묻는 자체 평가 설문지를 통해 평가됩니다. 우리의 결과는 다른 사람의 인식에 관한 이전 결과와 일치하여 다양한 필터 간의 상대적 효과를 보여줍니다. 그러나 그들은 또한 자기 인식에 대한 구체적인 효과를 보여 주었으며, 특히 얼굴 변형이 자신의 이미지에 대한 참가자의 신뢰도를 감소시킨다는 것을 보여줍니다. 여러 요소를 다루는 이 연구 결과를 통해 우리는 얼굴 변형이 사용자 인식에 미치는 영향뿐만 아니라 AR에서의 이러한 사용과 관련된 특수성을 강조할 수 있으며, 이러한 필터의 심리적 영향에 초점을 맞춘 새로운 작업의 길을 열었습니다.",https://doi.org/10.1109/ISMAR52148.2021.00064,Perception & Cognition,Other,User Study,User Study / Empirical Findings
533,2021,Neural Cameras: Learning Camera Characteristics for Coherent Mixed Reality Rendering,신경 카메라: 일관된 혼합 현실 렌더링을 위한 카메라 특성 학습,"Coherent rendering is important for generating plausible Mixed Reality presentations of virtual objects within a user’s real-world environment. Besides photo-realistic rendering and correct lighting, visual coherence requires simulating the imaging system that is used to capture the real environment. While existing approaches either focus on a specific camera or a specific component of the imaging system, we introduce Neural Cameras, the first approach that jointly simulates all major components of an arbitrary modern camera using neural networks. Our system allows for adding new cameras to the framework by learning the visual properties from a database of images that has been captured using the physical camera. We present qualitative and quantitative results and discuss future direction for research that emerge from using Neural Cameras.","일관된 렌더링은 사용자의 실제 환경 내에서 가상 개체의 그럴듯한 혼합 현실 프레젠테이션을 생성하는 데 중요합니다. 사실적인 렌더링과 올바른 조명 외에도 시각적 일관성을 위해서는 실제 환경을 캡처하는 데 사용되는 이미징 시스템을 시뮬레이션해야 합니다. 기존 접근 방식은 특정 카메라 또는 이미징 시스템의 특정 구성 요소에 초점을 맞추는 반면, 신경망을 사용하여 임의의 최신 카메라의 모든 주요 구성 요소를 공동으로 시뮬레이션하는 첫 번째 접근 방식인 신경 카메라를 소개합니다. 우리 시스템을 사용하면 실제 카메라를 사용하여 캡처한 이미지 데이터베이스에서 시각적 속성을 학습하여 프레임워크에 새 카메라를 추가할 수 있습니다. 정성적, 정량적 결과를 제시하고, Neural Camera를 활용한 연구의 향후 방향을 논의합니다.",https://doi.org/10.1109/ISMAR52148.2021.00068,Rendering & Visualization,Deep Learning / Neural Networks,Quantitative Experiment,System / Framework; Algorithm / Method
534,2021,Now I'm Not Afraid: Reducing Fear of Missing Out in 360° Videos on a Head-Mounted Display using a Panoramic Thumbnail,이제 두렵지 않습니다: 파노라마 썸네일을 사용하여 헤드 마운트 디스플레이에서 360° 비디오를 놓칠 염려를 줄이기,"Cinematic virtual reality, or 360° video, provides viewers with an immersive experience, allowing them to enjoy a video while moving their head to watch in any direction. However, there is an inevitable problem of feeling fear of missing out (FOMO) when viewing a 360° video, as only a part of the video is visible to the viewer at any given time. To solve this problem, we developed a technique to present a panoramic thumbnail of a full 360° video to users through a head-mounted display. With this technique, the user can grasp the overall view of the video as needed. We conducted an experiment to evaluate the FOMO, presence, and quality of viewing experience while using this technique compared to normal viewing without it. The results of the experiment show that the proposed technique relieved FOMO, the quality of viewing experience was improved, and there was no difference in presence. We also investigated how users interacted with this new interface based on eye tracking and head tracking data during viewing, which suggested that users used the panoramic thumbnail to actively explore outside their field of view.","시네마틱 가상 현실, 즉 360° 비디오는 시청자에게 몰입형 경험을 제공하여 머리를 움직여 어느 방향으로든 비디오를 감상할 수 있습니다. 그러나 360° 영상을 시청할 때 영상의 일부만 시청자에게 보이기 때문에 FOMO(Fear of Missing Out)를 느끼는 필연적인 문제가 있습니다. 이 문제를 해결하기 위해 우리는 헤드 마운트 디스플레이를 통해 사용자에게 풀 360° 비디오의 파노라마 썸네일을 제공하는 기술을 개발했습니다. 이 기법을 이용하면 사용자는 필요에 따라 영상의 전체적인 모습을 파악할 수 있다. 우리는 이 기술을 사용하지 않은 일반 시청과 비교하여 이 기술을 사용하는 동안 FOMO, 존재감 및 시청 경험의 품질을 평가하기 위한 실험을 수행했습니다. 실험 결과, 제안한 기법이 FOMO를 완화하고, 시청 경험의 질이 향상되었으며, 존재감에도 차이가 없는 것으로 나타났다. 또한 보는 동안 시선 추적 및 머리 추적 데이터를 기반으로 사용자가 이 새로운 인터페이스와 상호 작용하는 방식을 조사했습니다. 이는 사용자가 시야 외부를 적극적으로 탐색하기 위해 파노라마 썸네일을 사용했음을 시사합니다.",https://doi.org/10.1109/ISMAR52148.2021.00032,Display & Optics,Eye / Gaze Tracking,Other,Algorithm / Method
535,2021,"OpenRDW: A Redirected Walking Library and Benchmark with Multi-User, Learning-based Functionalities and State-of-the-art Algorithms","OpenRDW: 다중 사용자, 학습 기반 기능 및 최첨단 알고리즘을 갖춘 리디렉션된 워킹 라이브러리 및 벤치마크","Redirected walking (RDW) is a locomotion technique that guides users on virtual paths, which might vary from the paths they physically walk in the real world. Thereby, RDW enables users to explore a virtual space that is larger than the physical counterpart with near-natural walking experiences. Several approaches have been proposed and developed; each using individual platforms and evaluated on a custom dataset, making it challenging to compare between methods. However, there are seldom public toolkits and recognized benchmarks in this field. In this paper, we introduce OpenRDW, an open-source library and benchmark for developing, deploying and evaluating a variety of methods for walking path redirection. The OpenRDW library provides application program interfaces to access the attributes of scenes, to customize the RDW controllers, to simulate and visualize the navigation process, to export multiple formats of the results, and to evaluate RDW techniques. It also supports the deployment of multi-user real walking, as well as reinforcement learning-based models exported from TensorFlow or PyTorch. The OpenRDW benchmark includes multiple testing conditions, such as walking in size varied tracking spaces or shape varied tracking spaces with obstacles, multiple user walking, etc. On the other hand, procedurally generated paths and walking paths collected from user experiments are provided for a comprehensive evaluation. It also contains several classic and state-of-the-art RDW techniques, which include the above mentioned functionalities.","RDW(Redirected Walking)는 실제 세계에서 물리적으로 걷는 경로와 다를 수 있는 가상 경로로 사용자를 안내하는 이동 기술입니다. 따라서 RDW를 통해 사용자는 실제 공간보다 더 큰 가상 공간을 탐색하고 자연스러운 걷기 경험을 누릴 수 있습니다. 여러 가지 접근법이 제안되고 개발되었습니다. 각각은 개별 플랫폼을 사용하고 사용자 정의 데이터 세트에서 평가되므로 방법 간 비교가 어렵습니다. 그러나 이 분야에는 공개 툴킷과 인정된 벤치마크가 거의 없습니다. 본 논문에서는 도보 경로 리디렉션을 위한 다양한 방법을 개발, 배포 및 평가하기 위한 오픈 소스 라이브러리이자 벤치마크인 OpenRDW를 소개합니다. OpenRDW 라이브러리는 장면의 속성에 액세스하고, RDW 컨트롤러를 사용자 정의하고, 탐색 프로세스를 시뮬레이션 및 시각화하고, 결과의 여러 형식을 내보내고, RDW 기술을 평가할 수 있는 애플리케이션 프로그램 인터페이스를 제공합니다. 또한 다중 사용자 실제 걷기 배포는 물론 TensorFlow 또는 PyTorch에서 내보낸 강화 학습 기반 모델도 지원합니다. OpenRDW 벤치마크에는 크기가 다양한 추적 공간에서의 걷기 또는 장애물이 있는 모양이 다양한 추적 공간에서의 걷기, 다중 사용자 걷기 등과 같은 여러 테스트 조건이 포함됩니다. 반면, 절차적으로 생성된 경로와 사용자 실험에서 수집된 걷기 경로는 종합적인 평가를 위해 제공됩니다. 또한 위에서 언급한 기능을 포함하는 여러 가지 클래식 및 최첨단 RDW 기술도 포함되어 있습니다.",https://doi.org/10.1109/ISMAR52148.2021.00016,Interaction & Input; Content Authoring,Redirected Walking / Locomotion,Technical Evaluation,Algorithm / Method; System / Framework
536,2021,PAVAL: Position-Aware Virtual Agent Locomotion for Assisted Virtual Reality Navigation,PAVAL: 보조 가상 현실 탐색을 위한 위치 인식 가상 에이전트 이동,"Virtual agents are typical assistance tools for navigation and interaction in Virtual Reality (VR) tour, training, education, etc. It has been demonstrated that the gaits, gestures, gazes, and positions of virtual agents are major factors that affect the user’s perception and experience for seated and standing VR. In this paper, we present a novel position-aware virtual agent locomotion method, called PAVAL, that can perform virtual agent positioning (position+orientation) in real time for room-scale VR navigation assistance. We first analyze design guidelines for virtual agent locomotion and model the problem using the positions of the user and the surrounding virtual objects. Then we conduct a one-off preliminary study to collect subjective data and present a model for virtual agent positioning prediction with fixed user position. Based on the model, we propose an algorithm to optimize the object of interest, virtual agent position, and virtual agent orientation in sequence for virtual agent locomotion. As a result, during user navigation in a virtual scene, the virtual agent automatically moves in real time and introduces virtual object information to the user. We evaluate PAVAL and two alternative methods via a user study with humanoid virtual agents in various scenes, including virtual museum, factory, and school gym. The results reveal that our method is superior to the baseline condition.","가상 에이전트는 가상 현실(VR) 투어, 훈련, 교육 등에서 탐색 및 상호 작용을 위한 일반적인 지원 도구입니다. 가상 에이전트의 보행, 제스처, 시선 및 위치는 앉거나 서있는 VR에 대한 사용자의 인식과 경험에 영향을 미치는 주요 요소임이 입증되었습니다. 본 논문에서는 룸 스케일 VR 탐색 지원을 위해 실시간으로 가상 에이전트 포지셔닝(위치+방향)을 수행할 수 있는 PAVAL이라는 새로운 위치 인식 가상 에이전트 이동 방법을 제시합니다. 먼저 가상 에이전트 이동에 대한 설계 지침을 분석하고 사용자의 위치와 주변 가상 객체를 사용하여 문제를 모델링합니다. 그런 다음 주관적인 데이터를 수집하고 고정된 사용자 위치를 가진 가상 에이전트 위치 예측 모델을 제시하기 위해 일회성 예비 연구를 수행합니다. 본 모델을 기반으로 가상 에이전트 이동을 위해 관심 객체, 가상 에이전트 위치, 가상 에이전트 방향을 순차적으로 최적화하는 알고리즘을 제안한다. 결과적으로, 사용자가 가상 ​​장면에서 탐색하는 동안 가상 에이전트는 자동으로 실시간으로 이동하며 사용자에게 가상 객체 정보를 소개합니다. 가상 박물관, 공장, 학교 체육관을 포함한 다양한 장면에서 휴머노이드 가상 에이전트를 사용한 사용자 연구를 통해 PAVAL과 두 가지 대체 방법을 평가합니다. 결과는 우리의 방법이 기본 조건보다 우수하다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR52148.2021.00039,Interaction & Input; Education & Training,Optical / Display Technology,Technical Evaluation,Algorithm / Method
537,2021,Parametric Model Estimation for 3D Clothed Humans from Point Clouds,포인트 클라우드로부터 3D 옷을 입은 인간에 대한 파라메트릭 모델 추정,"This paper presents a novel framework to estimate parametric model- s for 3D clothed humans from partial point clouds. It is a challenging problem due to factors such as arbitrary human shape and pose, large variations in clothing details, and significant missing data. Existing methods mainly focus on estimating the parametric model of undressed bodies or reconstructing the non-parametric 3D shapes from point clouds. In this paper, we propose a hierarchical regression framework to learn the parametric model of detailed human shapes from partial point clouds of a single depth frame. Benefiting from the favorable ability of deep neural networks to model nonlinearity, the proposed framework cascades several successive regression networks to estimate the parameters of detailed 3D human body models in a coarse-to-fine manner. Specifically, the first global regression network extracts global deep features of point clouds to obtain an initial estimation of the undressed human model. Based on the initial estimation, the local regression network then refines the undressed human model by using the local features of neighborhood points of human joints. Finally, the clothing details are inferred as an additive displacement on the refined undressed model using the vertex-level regression network. The experimental results demonstrate that the proposed hierarchical regression approach can accurately predict detailed human shapes from partial point clouds and outperform prior works in the recovery accuracy of 3D human models.","이 논문은 부분 포인트 클라우드로부터 3D 옷을 입은 인간에 대한 파라메트릭 모델을 추정하기 위한 새로운 프레임워크를 제시합니다. 이는 임의적인 인간 모양 및 포즈, 의류 세부 사항의 큰 변화, 중요한 데이터 누락 등의 요인으로 인해 어려운 문제입니다. 기존 방법은 주로 옷을 입지 않은 신체의 파라메트릭 모델을 추정하거나 포인트 클라우드에서 비파라메트릭 3D 형상을 재구성하는 데 중점을 둡니다. 본 논문에서는 단일 깊이 프레임의 부분 포인트 클라우드로부터 상세한 인간 형상의 파라메트릭 모델을 학습하기 위한 계층적 회귀 프레임워크를 제안합니다. 비선형성을 모델링하는 심층 신경망의 유리한 기능을 활용하여 제안된 프레임워크는 여러 개의 연속적인 회귀 네트워크를 계단식으로 배열하여 상세한 3D 인체 모델의 매개변수를 대략적인 방식으로 추정합니다. 구체적으로, 첫 번째 글로벌 회귀 네트워크는 옷을 입지 않은 인간 모델의 초기 추정치를 얻기 위해 포인트 클라우드의 글로벌 심층 특징을 추출합니다. 초기 추정을 기반으로 로컬 회귀 네트워크는 인간 관절의 이웃 지점의 로컬 특징을 사용하여 옷을 입지 않은 인간 모델을 개선합니다. 마지막으로, 정점 수준 회귀 네트워크를 사용하여 정제된 옷을 입지 않은 모델에 대한 추가 변위로 의류 세부 사항을 추론합니다. 실험 결과는 제안된 계층적 회귀 접근 방식이 부분 포인트 클라우드에서 상세한 인간 형태를 정확하게 예측할 수 있으며 3D 인간 모델의 복구 정확도에서 이전 연구보다 성능이 우수하다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR52148.2021.00030,Rendering & Visualization,Deep Learning / Neural Networks,Quantitative Experiment,Algorithm / Method
538,2021,Perception-Driven Hybrid Foveated Depth of Field Rendering for Head-Mounted Displays,헤드 마운트 디스플레이를 위한 인식 기반 하이브리드 포비티드 심도 렌더링,"In this paper, we present a novel perception-driven hybrid rendering method leveraging the limitation of the human visual system (HVS). Features accounted in our model include: foveation from the visual acuity eccentricity (VAE), depth of field (DOF) from vergence & accommodation, and longitudinal chromatic aberration (LCA) from color vision. To allocate computational workload efficiently, first we apply a gaze-contingent geometry simplification. Then we convert the coordinates from screen space to polar space with a scaling strategy coherent with VAE. Upon that, we apply a stochastic sampling based on DOF. Finally, we post-process the Bokeh for DOF, which can at the same time achieve LCA and anti-aliasing. A virtual reality (VR) experiment on 6 Unity scenes with a head-mounted display (HMD) HTC VIVE Pro Eye yields frame rates range from 25.2 to 48.7 fps. Objective evaluation with FovVideoVDP - a perceptual based visible difference metric - suggests that the proposed method gives satisfactory just-objectionable-difference (JOD) scores across 6 scenes from 7.61 to 8.69 (in a 10 unit scheme). Our method achieves better performance compared with the existing methods while having the same or better level of quality scores.","본 논문에서는 인간 시각 시스템(HVS)의 한계를 활용한 새로운 인식 기반 하이브리드 렌더링 방법을 제시합니다. 우리 모델에서 설명하는 기능에는 VAE(시력 이심률)의 초점, Vergence & Accommodation의 DOF(심도), 색각의 LCA(세로 색수차)가 포함됩니다. 계산 작업 부하를 효율적으로 할당하기 위해 먼저 시선 조건에 따른 기하학 단순화를 적용합니다. 그런 다음 VAE와 일관된 스케일링 전략을 사용하여 좌표를 화면 공간에서 극 공간으로 변환합니다. 그런 다음 DOF를 기반으로 확률론적 샘플링을 적용합니다. 마지막으로 DOF에 대한 Bokeh를 후처리하여 LCA와 앤티앨리어싱을 동시에 달성할 수 있습니다. HMD(헤드 마운트 디스플레이) HTC VIVE Pro Eye를 사용하여 6개의 Unity 장면에 대한 가상 현실(VR) 실험에서는 25.2~48.7fps의 프레임 속도 범위를 생성했습니다. 지각 기반 가시적 차이 측정법인 FovVideoVDP를 사용한 객관적인 평가는 제안된 방법이 7.61에서 8.69(10 단위 체계)까지 6개 장면에서 만족스러운 JOD(Just-Objectionable-Difference) 점수를 제공한다는 것을 나타냅니다. 우리의 방법은 기존 방법과 비교하여 동일하거나 더 나은 수준의 품질 점수를 가지면서 더 나은 성능을 달성합니다.",https://doi.org/10.1109/ISMAR52148.2021.00014,Display & Optics; Interaction & Input,Eye / Gaze Tracking,Quantitative Experiment,Algorithm / Method
539,2021,Personal Identifiability and Obfuscation of User Tracking Data From VR Training Sessions,VR 교육 세션의 사용자 추적 데이터에 대한 개인 식별 및 난독화,"Recent research indicates that user tracking data from virtual reality (VR) experiences can be used to personally identify users with degrees of accuracy as high as 95%. However, these results indicating that VR tracking data should be understood as personally identifying data were based on observing 360° videos. In this paper, we present results based on sessions of user tracking data from an ecologically valid VR training application, which indicate that the prior claims may not be as applicable for identifying users beyond the context of observing 360° videos. Our results indicate that the degree of identification accuracy notably decreases between VR sessions. Furthermore, we present results indicating that user tracking data can be obfuscated by encoding positional data as velocity data, which has been successfully used to predict other user experience outcomes like simulator sickness and knowledge acquisition. These results, which show identification accuracies were reduced by more than half, indicate that velocity-based encoding can be used to reduce identifiability and help protect personal identifying data.","최근 연구에 따르면 가상 현실(VR) 경험에서 얻은 사용자 추적 데이터를 사용하면 최대 95%의 정확도로 사용자를 개인적으로 식별할 수 있습니다. 그러나 VR 트래킹 데이터가 개인식별 데이터로 이해되어야 함을 시사하는 이러한 결과는 360° 영상을 관찰한 결과이다. 본 논문에서는 생태학적으로 유효한 VR 교육 애플리케이션의 사용자 추적 데이터 세션을 기반으로 한 결과를 제시합니다. 이는 이전 주장이 360° 비디오를 관찰하는 맥락을 넘어서 사용자를 식별하는 데 적용 가능하지 않을 수 있음을 나타냅니다. 우리의 결과는 VR 세션 사이에 식별 정확도의 정도가 눈에 띄게 감소한다는 것을 나타냅니다. 또한 위치 데이터를 속도 데이터로 인코딩하여 사용자 추적 데이터를 난독화할 수 있음을 나타내는 결과를 제시하며, 이는 시뮬레이터 멀미 및 지식 획득과 같은 다른 사용자 경험 결과를 예측하는 데 성공적으로 사용되었습니다. 식별 정확도가 절반 이상 감소한 것으로 나타난 이러한 결과는 속도 기반 인코딩을 사용하여 식별 가능성을 줄이고 개인 식별 데이터를 보호하는 데 도움이 될 수 있음을 나타냅니다.",https://doi.org/10.1109/ISMAR52148.2021.00037,Tracking & Localization,Sensor Fusion,Quantitative Experiment,Other
540,2021,RNIN-VIO: Robust Neural Inertial Navigation Aided Visual-Inertial Odometry in Challenging Scenes,RNIN-VIO: 까다로운 장면에서 시각적 관성 주행 거리 측정을 지원하는 강력한 신경 관성 탐색,"In this work, we propose a tightly-coupled EKF framework for visual-inertial odometry with NIN (Neural Inertial Navigation) aided. Traditional VIO systems are fragile in challenging scenes with weak or confusing visual information, such as weak/repeated texture, dynamic environment, fast camera motion with serious motion blur, etc. It is extremely difficult for a vision-based algorithm to handle these problems. So we firstly design a robust deep learning based inertial network (called RNIN), using only IMU measurements as input. RNIN is significantly more robust in challenging scenes than traditional VIO systems. In order to take full advantage of vision-based algorithms in AR/VR areas, we further develop a multi-sensor fusion system RNIN-VIO, which tightly couples the visual, IMU and NIN measurements. Our system performs robustly in extremely challenging conditions, with high precision both in trajectories and AR effects. The experimental results of evaluation on dataset evaluation and online AR demo demonstrate the superiority of the proposed system in robustness and accuracy.","이 작업에서 우리는 NIN(Neural Inertial Navigation)의 도움을 받아 시각적 관성 주행 거리 측정을 위한 긴밀하게 결합된 EKF 프레임워크를 제안합니다. 기존 VIO 시스템은 약하거나 반복되는 텍스처, 동적 환경, 심각한 모션 블러가 있는 빠른 카메라 모션 등과 같이 약하거나 혼란스러운 시각적 정보가 있는 까다로운 장면에서 취약합니다. 비전 기반 알고리즘이 이러한 문제를 처리하는 것은 극히 어렵습니다. 따라서 먼저 IMU 측정값만 입력으로 사용하여 강력한 딥러닝 기반 관성 네트워크(RNIN이라고 함)를 설계합니다. RNIN은 기존 VIO 시스템보다 까다로운 장면에서 훨씬 더 강력합니다. AR/VR 영역에서 비전 기반 알고리즘을 최대한 활용하기 위해 우리는 시각적, IMU 및 NIN 측정을 긴밀하게 결합하는 다중 센서 융합 시스템 RNIN-VIO를 추가로 개발합니다. 우리 시스템은 궤적과 AR 효과 모두에서 높은 정밀도로 매우 까다로운 조건에서도 강력하게 작동합니다. 데이터 세트 평가 및 온라인 AR 데모에 대한 평가 실험 결과는 제안된 시스템의 견고성과 정확성이 우수함을 보여줍니다.",https://doi.org/10.1109/ISMAR52148.2021.00043,Interaction & Input,Sensor Fusion,Quantitative Experiment,System / Framework; Algorithm / Method
541,2021,Redirected Walking using Noisy Galvanic Vestibular Stimulation,시끄러운 갈바닉 전정 자극을 이용한 방향 전환 걷기,"In this study, considering the characteristics of multisensory integration, we examined a method for improving redirected walking (RDW) by adding noise to the vestibular system to reduce the effects of vestibular inputs on self-motion perception. In RDW, the contradiction between vestibular inputs and visual sensations may make users notice the RDW manipulation, resulting in discomfort throughout the experience. Because humans integrate multisensory information by considering the reliability of each modality, by reducing the effects of vestibular inputs on self-motion perception, it is possible to suppress awareness of and discomfort during RDW manipulation and improve the effectiveness of the manipulation. Therefore, we hypothesized that adding noise to the vestibular inputs would reduce the reliability of the vestibular sensations and enhances the effectiveness of RDW by improving the relative reliability of vision. In this study, we used noisy galvanic vestibular stimulation (GVS) to reduce the reliability of vestibular inputs. GVS is a method of stimulating vestibular organs and nerves by applying small electrical currents to the bilateral mastoid. To reduce the reliability of vestibular inputs, we employed noisy GVS whose current pattern is white noise. We experimented with comparing the threshold of curvature gains between noisy GVS conditions and a control condition.",본 연구에서는 다감각 통합의 특성을 고려하여 전정계 입력이 자기 운동 인식에 미치는 영향을 줄이기 위해 전정계에 잡음을 추가하여 방향 전환(RDW)을 개선하는 방법을 검토했습니다. RDW에서는 전정 입력과 시각적 감각 간의 모순으로 인해 사용자가 RDW 조작을 알아차리게 되어 경험 전반에 걸쳐 불편함을 겪을 수 있습니다. 인간은 각 양식의 신뢰성을 고려하여 다감각 정보를 통합하기 때문에 전정 입력이 자기 동작 인식에 미치는 영향을 줄임으로써 RDW 조작 중 인식과 불편함을 억제하고 조작의 효율성을 향상시킬 수 있습니다. 따라서 우리는 전정 입력에 소음을 추가하면 전정 감각의 신뢰도가 감소하고 시력의 상대적 신뢰도가 향상되어 RDW의 효율성이 향상된다는 가설을 세웠습니다. 이 연구에서는 전정 입력의 신뢰성을 줄이기 위해 시끄러운 갈바닉 전정 자극(GVS)을 사용했습니다. GVS는 양측 유양돌기에 작은 전류를 흘려 전정기관과 신경을 자극하는 방법이다. 전정 입력의 신뢰성을 줄이기 위해 현재 패턴이 백색 잡음인 시끄러운 GVS를 사용했습니다. 우리는 잡음이 있는 GVS 조건과 제어 조건 사이의 곡률 이득 임계값을 비교하는 실험을 했습니다.,https://doi.org/10.1109/ISMAR52148.2021.00067,Interaction & Input; Perception & Cognition,Deep Learning / Neural Networks,Other,System / Framework
542,2021,Rotation-constrained optical see-through headset calibration with bare-hand alignment,맨손 정렬로 회전이 제한된 광학 투명 헤드셋 교정,"The inaccessibility of user-perceived reality remains an open issue in pursuing the accurate calibration of optical see-through (OST) head-mounted displays (HMDs). Manual user alignment is usually required to collect a set of virtual-to-real correspondences, so that a default or an offline display calibration can be updated to account for the user’s eye position(s). Current alignment-based calibration procedures usually require point-wise alignments between rendered image point(s) and associated physical landmark(s) of a target calibration tool. As each alignment can only provide one or a few correspondences, repeated alignments are required to ensure calibration quality. This work presents an accurate and tool-less online OST calibration method to update an offline-calibrated eye-display model. The user’s bare hand is markerlessly tracked by a commercial RGBD camera anchored to the OST headset to generate a user-specific cursor for correspondence collection. The required alignment is object-wise, and can provide thousands of unordered corresponding points in tracked space. The collected correspondences are registered by a proposed rotation-constrained iterative closest point (rcICP) method to optimise the viewpoint-related calibration parameters. We implemented such a method for the Microsoft HoloLens 1. The resiliency of the proposed procedure to noisy data was evaluated through simulated tests and real experiments performed with an eye-replacement camera. According to the simulation test, the rcICP registration is robust against possible user-induced rotational misalignment. With a single alignment, our method achieves 8.81 arcmin (1.37 mm) positional error and 1. 76° rotational error by camera-based tests in the arm-reach distance, and 10.79 arcmin (7.71 pixels) reprojection error by user tests.","사용자가 인식하는 현실에 접근하기 어렵다는 점은 광학 투명(OST) 헤드 장착 디스플레이(HMD)의 정확한 보정을 추구하는 데 있어 아직 해결되지 않은 문제로 남아 있습니다. 사용자의 눈 위치를 고려하여 기본 또는 오프라인 디스플레이 보정을 업데이트할 수 있도록 가상-실제 대응 집합을 수집하려면 일반적으로 수동 사용자 정렬이 필요합니다. 현재 정렬 기반 교정 절차에는 일반적으로 렌더링된 이미지 지점과 대상 교정 도구의 관련 물리적 랜드마크 간의 지점별 정렬이 필요합니다. 각 정렬은 하나 또는 몇 개의 대응만 제공할 수 있으므로 교정 품질을 보장하려면 반복적인 정렬이 필요합니다. 이 작업은 오프라인으로 보정된 아이 디스플레이 모델을 업데이트하기 위한 정확하고 도구가 필요 없는 온라인 OST 보정 방법을 제시합니다. 사용자의 맨손은 OST 헤드셋에 고정된 상용 RGBD 카메라에 의해 마커 없이 추적되어 서신 수집을 위한 사용자별 커서를 생성합니다. 필요한 정렬은 객체별로 이루어지며 추적된 공간에서 수천 개의 정렬되지 않은 해당 지점을 제공할 수 있습니다. 수집된 대응점은 시점 관련 교정 매개변수를 최적화하기 위해 제안된 회전 제한 반복 최근접점(rcICP) 방법에 의해 등록됩니다. 우리는 Microsoft HoloLens 1에 이러한 방법을 구현했습니다. 제안된 절차의 시끄러운 데이터에 대한 복원력은 시뮬레이션 테스트와 눈 대체 카메라를 사용한 실제 실험을 통해 평가되었습니다. 시뮬레이션 테스트에 따르면 rcICP 등록은 사용자가 유발한 회전 정렬 불량에 대해 강력합니다. 단일 정렬을 통해 우리의 방법은 팔 도달 거리에서 카메라 기반 테스트로 8.81 arcmin(1.37mm) 위치 오류와 1.76° 회전 오류, 사용자 테스트로 10.79 arcmin(7.71 픽셀) 재투영 오류를 달성했습니다.",https://doi.org/10.1109/ISMAR52148.2021.00041,Display & Optics; Tracking & Localization,Sensor Fusion,Simulation,Hardware / Device; Algorithm / Method
543,2021,SAR: Spatial-Aware Regression for 3D Hand Pose and Mesh Reconstruction from a Monocular RGB Image,SAR: 단안 RGB 이미지에서 3D 손 포즈 및 메시 재구성을 위한 공간 인식 회귀,"3D hand reconstruction is a popular research topic in recent years, which has great potential for VR/AR applications. However, due to the limited computational resource of VR/AR equipment, the reconstruction algorithm must balance accuracy and efficiency to make the users have a good experience. Nevertheless, current methods are not doing well in balancing accuracy and efficiency. Therefore, this paper proposes a novel framework that can achieve a fast and accurate 3D hand reconstruction. Our framework relies on three essential modules, including spatial-aware initial graph building (SAIGB), graph convolutional network (GCN) based belief maps regression (GBBMR), and pose-guided refinement (PGR). At first, given image feature maps extracted by convolutional neural networks, SAIGB builds a spatial-aware and compact initial feature graph. Each node in this graph represents a vertex of the mesh and has vertex-specific spatial information that is helpful for accurate and efficient regression. After that, GBBMR first utilizes adaptive-GCN to introduce interactions between vertices to capture short-range and long-range dependencies between vertices efficiently and flexibly. Then, it maps vertices’ features to belief maps that can model the uncertainty of predictions for more accurate predictions. Finally, we apply PGR to compress the redundant vertices’ belief maps to compact-joints’ belief maps with the pose guidance and use these joints’ belief maps to refine previous predictions better to obtain more accurate and robust reconstruction results. Our method achieves state-of-the-art performance on four public benchmarks, FreiHAND, HO-3D, RHD, and STB. Moreover, our method can run at a speed of two to three times that of previous state-of-the-art methods. Our code is available at https://github.com/zxz267/SAR.","3D 손 재구성은 최근 몇 년 동안 인기 있는 연구 주제로, VR/AR 응용 분야에 큰 잠재력을 가지고 있습니다. 그러나 VR/AR 장비의 제한된 계산 리소스로 인해 재구성 알고리즘은 사용자에게 좋은 경험을 제공하기 위해 정확성과 효율성의 균형을 유지해야 합니다. 그럼에도 불구하고 현재의 방법은 정확성과 효율성의 균형을 제대로 맞추지 못하고 있습니다. 따라서 본 논문에서는 빠르고 정확한 3차원 손 재구성을 달성할 수 있는 새로운 프레임워크를 제안합니다. 우리의 프레임워크는 공간 인식 초기 그래프 구축(SAIGB), 그래프 컨벌루션 네트워크(GCN) 기반 신념 맵 회귀(GBBMR) 및 포즈 유도 개선(PGR)을 포함한 세 가지 필수 모듈에 의존합니다. 처음에 컨볼루셔널 신경망에서 추출한 이미지 특징 맵이 주어지면 SAIGB는 공간을 인식하고 컴팩트한 초기 특징 그래프를 구축합니다. 이 그래프의 각 노드는 메쉬의 정점을 나타내며 정확하고 효율적인 회귀에 도움이 되는 정점별 공간 정보를 가지고 있습니다. 그 후 GBBMR은 먼저 적응형 GCN을 활용하여 정점 간의 상호 작용을 도입하여 정점 간의 단거리 및 장거리 종속성을 효율적이고 유연하게 포착합니다. 그런 다음 정점의 특징을 보다 정확한 예측을 위해 예측의 불확실성을 모델링할 수 있는 신념 맵에 매핑합니다. 마지막으로 PGR을 적용하여 포즈 안내를 통해 중복 정점의 신뢰도 맵을 컴팩트 조인트의 신뢰도 맵으로 압축하고 이러한 관절의 신뢰도 맵을 사용하여 이전 예측을 더 잘 다듬어 보다 정확하고 강력한 재구성 결과를 얻습니다. 우리의 방법은 FreiHAND, HO-3D, RHD 및 STB의 4가지 공개 벤치마크에서 최첨단 성능을 달성했습니다. 또한, 우리의 방법은 기존의 최첨단 방법보다 2~3배 빠른 속도로 실행될 수 있습니다. 우리 코드는 https://github.com/zxz267/SAR에서 확인할 수 있습니다.",https://doi.org/10.1109/ISMAR52148.2021.00024,Interaction & Input,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method
544,2021,STGAE: Spatial-Temporal Graph Auto-Encoder for Hand Motion Denoising,STGAE: 손 동작 노이즈 제거를 위한 시공간 그래프 자동 인코더,"Hand object interaction in mixed reality (MR) relies on the accurate tracking and estimation of human hands, which provide users with a sense of immersion. However, raw captured hand motion data always contains errors such as joints occlusion, dislocation, high-frequency noise, and involuntary jitter. Denoising and obtaining the hand motion data consistent with the user’s intention are of the utmost importance to enhance the interactive experience in MR. To this end, we propose an end-to-end method for hand motion denoising using the spatial-temporal graph auto-encoder (STGAE). The spatial and temporal patterns are recognized simultaneously by constructing the consecutive hand joint sequence as a spatial-temporal graph. Considering the complexity of the articulated hand structure, a simple yet effective partition strategy is proposed to model the physic-connected and symmetry-connected relationships. Graph convolution is applied to extract structural constraints of the hand, and a self-attention mechanism is to adjust the graph topology dynamically. Combining graph convolution and temporal convolution, a fundamental graph encoder or decoder block is proposed. We finally establish the hourglass residual auto-encoder to learn a manifold projection operation and a corresponding inverse projection through stacking these blocks. In this work, the proposed framework has been successfully used in hand motion data denoising with preserving structural constraints between joints. Extensive quantitative and qualitative experiments show that the proposed method has achieved better performance than the state-of-the-art approaches.","혼합 현실(MR)의 손 개체 상호 작용은 사용자에게 몰입감을 제공하는 인간 손의 정확한 추적 및 추정에 의존합니다. 그러나 캡처된 원본 손 동작 데이터에는 항상 관절 폐색, 탈구, 고주파 소음 및 비자발적 지터와 같은 오류가 포함되어 있습니다. 사용자의 의도와 일치하는 손 동작 데이터를 제거하고 획득하는 것은 MR의 대화형 경험을 향상시키는 데 가장 중요합니다. 이를 위해 우리는 STGAE(spatial-temporal graph auto-encoder)를 이용한 손동작 잡음 제거를 위한 end-to-end 방법을 제안한다. 연속적인 손 관절 시퀀스를 시공간 그래프로 구성하여 공간적, 시간적 패턴을 동시에 인식합니다. 관절로 연결된 손 구조의 복잡성을 고려하여 물리적 연결 및 대칭 연결 관계를 모델링하기 위한 간단하면서도 효과적인 분할 전략이 제안되었습니다. 그래프 컨볼루션을 적용하여 손의 구조적 제약을 추출하고, Self-Attention 메커니즘을 통해 그래프 토폴로지를 동적으로 조정합니다. 그래프 콘볼루션과 시간적 콘볼루션을 결합한 기본 그래프 인코더 또는 디코더 블록이 제안됩니다. 우리는 마침내 모래시계 잔차 자동 인코더를 구축하여 이러한 블록을 쌓아서 다양체 투영 작업과 해당 역 투영을 학습합니다. 본 연구에서 제안된 프레임워크는 관절 간의 구조적 제약을 보존하면서 손 동작 데이터 노이즈 제거에 성공적으로 사용되었습니다. 광범위한 정량적, 정성적 실험을 통해 제안된 방법이 최신 접근 방식보다 더 나은 성능을 달성했음을 보여줍니다.",https://doi.org/10.1109/ISMAR52148.2021.00018,Perception & Cognition,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method
545,2021,"Safety, Power Imbalances, Ethics and Proxy Sex: Surveying In-The-Wild Interactions Between VR Users and Bystanders","안전, 권력 불균형, 윤리 및 프록시 섹스: VR 사용자와 방관자 간의 실제 상호 작용 조사","VR users and bystanders must sometimes interact, but our understanding of these interactions - their purpose, how they are accomplished, attitudes toward them, and where they break down - is limited. This current gap inhibits research into managing or supporting these interactions, and preventing unwanted or abusive activity. We present the results of the first survey (N=100) that investigates stories of actual emergent in-the-wild interactions between VR users and bystanders. Our analysis indicates VR user and bystander interactions can be categorised into one of three categories: coexisting, demoing, and interrupting. We highlight common interaction patterns and impediments encountered during these interactions. Bystanders play an important role in moderating the VR user’s experience, for example intervening to save the VR user from potential harm. However, our stories also suggest that the occlusive nature of VR introduces the potential for bystanders to exploit the vulnerable state of the VR user; and for the VR user to exploit the bystander for enhanced immersion, introducing significant ethical concerns.","VR 사용자와 구경꾼은 때때로 상호 작용해야 하지만 이러한 상호 작용(목적, 달성 방법, 태도, 붕괴 위치)에 대한 우리의 이해는 제한적입니다. 이러한 현재의 격차는 이러한 상호 작용을 관리 또는 지원하고 원치 않거나 악의적인 활동을 방지하는 연구를 방해합니다. 우리는 VR 사용자와 구경꾼 사이에 실제로 발생하는 야생 내 상호작용에 대한 이야기를 조사한 첫 번째 설문조사(N=100)의 결과를 제시합니다. 우리의 분석에 따르면 VR 사용자와 구경꾼의 상호 작용은 공존, 데모, 방해의 세 가지 범주 중 하나로 분류될 수 있습니다. 우리는 이러한 상호 작용 중에 발생하는 일반적인 상호 작용 패턴과 장애물을 강조합니다. 방관자는 VR 사용자의 경험을 조정하는 데 중요한 역할을 합니다. 예를 들어 VR 사용자를 잠재적인 위험으로부터 구하기 위해 개입하는 등의 역할을 합니다. 그러나 우리의 이야기는 또한 VR의 폐쇄적 특성으로 인해 방관자가 VR 사용자의 취약한 상태를 악용할 가능성이 있음을 시사합니다. VR 사용자가 몰입도를 높이기 위해 방관자를 이용하여 심각한 윤리적 문제를 야기합니다.",https://doi.org/10.1109/ISMAR52148.2021.00036,Interaction & Input,Other,Questionnaire / Survey,User Study / Empirical Findings
546,2021,Scan&Paint: Image-based Projection Painting,스캔&페인트: 이미지 기반 프로젝션 페인팅,"We present a pop-up projection painting system that projects onto an unknown three-dimensional surface, while the user creates the projection content on the fly. The digital paint is projected immediately and follows the object if it is moved. If unexplored surface areas are thereby exposed, an automated trigger system issues new depth recordings that expand and refine the surface estimate. By intertwining scanning and projection painting we scan the exposed surface at the appropriate time and only if needed. Like image-based rendering, multiple automatically recorded depth maps are fused in screen space to synthesize novel views of the object, making projection poses independent from the scan positions. Since the user’s digital paint is also stored in images, we eliminate the need to reconstruct and parametrize a single full mesh, which makes geometry and color updates simple and fast.",우리는 사용자가 프로젝션 콘텐츠를 즉석에서 생성하는 동안 알려지지 않은 3차원 표면에 프로젝션하는 팝업 프로젝션 페인팅 시스템을 제시합니다. 디지털 페인트는 즉시 투영되며 개체가 이동하면 개체를 따라갑니다. 탐험되지 않은 표면 영역이 노출되면 자동화된 트리거 시스템이 표면 추정치를 확장하고 개선하는 새로운 깊이 기록을 발행합니다. 스캐닝과 프로젝션 페인팅을 결합하여 적절한 시간에 필요한 경우에만 노출된 표면을 스캔합니다. 이미지 기반 렌더링과 마찬가지로 자동으로 기록된 여러 깊이 맵이 화면 공간에 융합되어 개체에 대한 새로운 보기를 합성하고 투영 포즈를 스캔 위치와 독립적으로 만듭니다. 사용자의 디지털 페인트도 이미지에 저장되므로 단일 전체 메시를 재구성하고 매개변수화할 필요가 없으므로 형상 및 색상 업데이트가 간단하고 빠르게 이루어집니다.,https://doi.org/10.1109/ISMAR52148.2021.00069,Rendering & Visualization,Other,Other,System / Framework
547,2021,SceneAR: Scene-based Micro Narratives for Sharing and Remixing in Augmented Reality,SceneAR: 증강 현실에서 공유 및 리믹싱을 위한 장면 기반 마이크로 내러티브,"Short-form digital storytelling has become a popular medium for millions of people to express themselves. Traditionally, this medium uses primarily 2D media such as text (e.g., memes), images (e.g., Instagram), GIFs (e.g., Giphy), and videos (e.g., TikTok, Snapchat). To expand the modalities from 2D to 3D media, we present SceneAR, a smartphone application for creating sequential scene-based micro narratives in augmented reality (AR). What sets SceneAR apart from prior work is its ability to share the scene-based stories as AR content. No longer limited to sharing images or videos, users can now experience narratives in their own physical environments. Additionally, SceneAR affords users the ability to remix AR content, empowering them to collectively build on others’ creations. We asked 18 people to use SceneAR in a three-day study, and based on user interviews, analyses of screen recordings, and the stories they created, we extracted three themes. From these themes and the study overall, we derived six strategies for designers interested in supporting short-form AR narratives.","짧은 형식의 디지털 스토리텔링은 수백만 명의 사람들이 자신을 표현하는 인기 있는 매체가 되었습니다. 전통적으로 이 매체는 텍스트(예: 밈), 이미지(예: Instagram), GIF(예: Giphy) 및 비디오(예: TikTok, Snapchat)와 같은 2D 미디어를 주로 사용합니다. 2D에서 3D 미디어로 양식을 확장하기 위해 증강 현실(AR)에서 순차적 장면 기반 마이크로 내러티브를 생성하기 위한 스마트폰 애플리케이션인 SceneAR을 소개합니다. SceneAR이 전작과 차별화되는 점은 장면 기반 스토리를 AR 콘텐츠로 공유할 수 있다는 점이다. 더 이상 이미지나 비디오 공유에만 국한되지 않고 사용자는 이제 자신의 물리적 환경에서 내러티브를 경험할 수 있습니다. 또한 SceneAR은 사용자에게 AR 콘텐츠를 리믹스할 수 있는 기능을 제공하여 다른 사람의 창작물을 종합적으로 구축할 수 있도록 지원합니다. 우리는 3일간의 연구에서 18명에게 SceneAR을 사용하도록 요청했고, 사용자 인터뷰, 화면 녹화 분석, 그들이 만든 스토리를 기반으로 세 가지 주제를 추출했습니다. 이러한 주제와 전반적인 연구를 통해 우리는 짧은 형식의 AR 내러티브 지원에 관심이 있는 디자이너를 위한 6가지 전략을 도출했습니다.",https://doi.org/10.1109/ISMAR52148.2021.00045,Interaction & Input,Other,Qualitative Analysis,Other
548,2021,Selective Foveated Ray Tracing for Head-Mounted Displays,헤드 마운트 디스플레이를 위한 선택적 포비티드 레이 트레이싱,"Although ray tracing produces significantly more realistic images than traditional rasterization techniques, it is still considered computationally burdensome when implemented on a head-mounted display (HMD) system that demands both wide field of view and high rendering rate. A further challenge is that to present high-quality images on an HMD screen, a sufficient number of ray samples should be taken per pixel for effective antialiasing to reduce visually annoying artifacts. In this paper, we present a novel foveated real-time rendering framework that realizes classic Whitted-style ray tracing on an HMD system. In particular, our method proposes combining the selective supersampling technique by Jin et al. [8] with the foveated rendering scheme, resulting in perceptually highly efficient pixel sampling suitable for HMD ray tracing. We show that further enhanced by foveated temporal antialiasing, our ray tracer renders nontrivial 3D scenes in real time on commodity GPUs at high sampling rates as effective as up to 36 samples per pixel (spp) in the foveal area, gradually reducing to at least 1 spp in the periphery.","레이 트레이싱은 기존 래스터화 기술보다 훨씬 더 사실적인 이미지를 생성하지만, 넓은 시야와 높은 렌더링 속도를 모두 요구하는 헤드 마운트 디스플레이(HMD) 시스템에서 구현하는 경우 여전히 계산적으로 부담스러운 것으로 간주됩니다. 또 다른 과제는 HMD 화면에 고품질 이미지를 표시하려면 효과적인 안티앨리어싱을 위해 픽셀당 충분한 수의 광선 샘플을 가져와 시각적으로 짜증나는 아티팩트를 줄여야 한다는 것입니다. 본 논문에서는 HMD 시스템에서 고전적인 Whitted 스타일 광선 추적을 실현하는 새로운 포비티드 실시간 렌더링 프레임워크를 제시합니다. 특히, 우리의 방법은 Jin et al.의 선택적 슈퍼샘플링 기술을 결합하는 것을 제안합니다. [8] 포비티드 렌더링 방식을 사용하면 HMD 광선 추적에 적합한 지각적으로 매우 효율적인 픽셀 샘플링이 가능해집니다. 우리는 Foveated Temporal Antialiasing을 통해 더욱 향상된 Ray Tracer가 Foveal 영역에서 픽셀당 최대 36샘플(spp)만큼 효과적인 높은 샘플링 속도로 상용 GPU에서 중요하지 않은 3D 장면을 실시간으로 렌더링하고 점차적으로 주변에서 최소 1spp로 감소한다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR52148.2021.00058,Rendering & Visualization; Display & Optics,Eye / Gaze Tracking,Other,System / Framework; Algorithm / Method
549,2021,"Separation, Composition, or Hybrid? - Comparing Collaborative 3D Object Manipulation Techniques for Handheld Augmented Reality","분리, 구성 또는 하이브리드? - 휴대용 증강현실을 위한 협업적 3D 객체 조작 기술 비교","Augmented Reality (AR) supported collaboration is a popular topic in HCI research. Previous work has shown the benefits of collaborative 3D object manipulation and identified two possibilities: Either separate or compose users’ inputs. However, their experimental comparison using handheld AR displays is still missing. We, therefore, conducted an experiment in which we tasked 24 dyads with collaboratively positioning virtual objects in handheld AR using three manipulation techniques: 1) Separation – performing only different manipulation tasks (i. e., translation or rotation) simultaneously, 2) Composition – performing only the same manipulation tasks simultaneously and combining individual inputs using a merge policy, and 3) Hybrid – performing any manipulation tasks simultaneously, enabling dynamic transitions between Separation and Composition. While all techniques were similarly effective, Composition was least efficient, with higher subjective workload and worse user experience. Preferences were polarized between clear work division (Separation) and freedom of action (Hybrid). Based on our findings, we offer research and design implications.","증강 현실(AR) 지원 협업은 HCI 연구에서 인기 있는 주제입니다. 이전 작업에서는 공동 3D 개체 조작의 이점을 보여 주었고 사용자 입력을 분리하거나 구성하는 두 가지 가능성을 식별했습니다. 그러나 휴대용 AR 디스플레이를 사용한 실험적 비교는 아직 누락되었습니다. 따라서 우리는 세 가지 조작 기술을 사용하여 24개의 쌍방에 공동으로 가상 객체를 배치하는 작업을 수행하는 실험을 수행했습니다. 1) 분리 - 서로 다른 조작 작업(예: 변환 또는 회전)만 동시에 수행, 2) 구성 - 동일한 조작 작업만 동시에 수행하고 병합 정책을 사용하여 개별 입력을 결합, 3) 하이브리드 - 모든 조작 작업을 동시에 수행하여 분리와 구성 간의 동적 전환을 가능하게 합니다. 모든 기술은 비슷하게 효과적이었지만 컴포지션은 효율성이 가장 낮았으며 주관적인 작업량이 많고 사용자 경험이 더 나빴습니다. 명확한 업무분할(분리)과 행동의 자유(하이브리드) 사이에 선호도가 양극화되었습니다. 우리는 연구 결과를 바탕으로 연구 및 디자인 관련 시사점을 제시합니다.",https://doi.org/10.1109/ISMAR52148.2021.00057,Interaction & Input; Collaboration & Social,Sensor Fusion,Technical Evaluation,Algorithm / Method
550,2021,Simulating Realistic Human Motion Trajectories of Mid-Air Gesture Typing,공중 제스처 타이핑의 사실적인 인간 모션 궤적 시뮬레이션,"The eventual success of many AR and VR intelligent interactive systems relies on the ability to collect user motion data at large scale. Realistic simulation of human motion trajectories is a potential solution to this problem. Simulated user motion data can facilitate prototyping and speed up the design process. There are also potential benefits in augmenting training data for deep learning-based AR/VR applications to improve performance. However, the generation of realistic motion data is nontrivial. In this paper, we examine the specific challenge of simulating index finger movement data to inform mid-air gesture keyboard design. The mid-air gesture keyboard is deployed on an optical see-through display that allows the user to enter text by articulating word gesture patterns with their physical index finger in the vicinity of a visualized keyboard layout. We propose and compare four different approaches to simulating this type of motion data, including a Jerk-Minimization model, a Recurrent Neural Network (RNN)-based generative model, and a Generative Adversarial Network (GAN)-based model with two modes: style transfer and data alteration. We also introduce a procedure for validating the quality of the generated trajectories in terms of realism and diversity. The GAN-based model shows significant potential for generating synthetic motion trajectories to facilitate design and deep learning for advanced gesture keyboards deployed in AR and VR.","많은 AR 및 VR 지능형 대화형 시스템의 궁극적인 성공은 사용자 모션 데이터를 대규모로 수집하는 능력에 달려 있습니다. 인간의 움직임 궤적을 사실적으로 시뮬레이션하는 것이 이 문제에 대한 잠재적인 해결책입니다. 시뮬레이션된 사용자 모션 데이터는 프로토타입 제작을 촉진하고 설계 프로세스 속도를 높일 수 있습니다. 성능을 향상시키기 위해 딥 러닝 기반 AR/VR 애플리케이션에 대한 훈련 데이터를 보강하면 잠재적인 이점도 있습니다. 그러나 사실적인 모션 데이터를 생성하는 것은 쉽지 않습니다. 본 논문에서는 공중 제스처 키보드 디자인을 알리기 위해 검지 손가락 움직임 데이터를 시뮬레이션하는 구체적인 과제를 조사합니다. 공중 제스처 키보드는 사용자가 시각화된 키보드 레이아웃 근처에 있는 물리적 검지로 단어 제스처 패턴을 표현하여 텍스트를 입력할 수 있는 광학 투명 디스플레이에 배치됩니다. 우리는 Jerk-Minimization 모델, RNN(Recurrent Neural Network) 기반 생성 모델, 스타일 전송과 데이터 변경이라는 두 가지 모드를 갖춘 GAN(Generative Adversarial Network) 기반 모델을 포함하여 이러한 유형의 모션 데이터를 시뮬레이션하는 네 가지 서로 다른 접근 방식을 제안하고 비교합니다. 또한 현실성과 다양성 측면에서 생성된 궤적의 품질을 검증하는 절차를 소개합니다. GAN 기반 모델은 AR 및 VR에 배포된 고급 제스처 키보드의 설계 및 딥 러닝을 촉진하기 위해 합성 모션 궤적을 생성할 수 있는 상당한 잠재력을 보여줍니다.",https://doi.org/10.1109/ISMAR52148.2021.00056,Education & Training; Content Authoring,Deep Learning / Neural Networks,Simulation,Algorithm / Method
551,2021,Supporting Iterative Virtual Reality Analytics Design and Evaluation by Systematic Generation of Surrogate Clustered Datasets,대리 클러스터 데이터 세트의 체계적 생성을 통한 반복적 가상 현실 분석 설계 및 평가 지원,"Virtual Reality (VR) is a promising technology platform for immersive visual analytics. However, the design space of VR analytics interface design is vast and difficult to explore using traditional A/B comparisons in formal or informal controlled experiments— a fundamental part of an iterative design process. A key factor that complicates such comparisons is the dataset. Exposing participants to the same dataset in all conditions introduces an unavoidable learning effect. On the other hand, using different datasets for all experimental conditions introduces the dataset itself as an uncontrolled variable, which reduces internal validity to an unacceptable degree. In this paper, we propose to rectify this problem by introducing a generative process for synthesizing clustered datasets for VR analytics experiments. This process generates datasets that are distinct while simultaneously allowing systematic comparisons in experiments. A key advantage is that these datasets can then be used in iterative design processes. In a two-part experiment, we show the validity of the generative process and demonstrate how new insights in VR-based visual analytics can be gained using synthetic datasets.","가상 현실(VR)은 몰입형 시각적 분석을 위한 유망한 기술 플랫폼입니다. 그러나 VR 분석 인터페이스 디자인의 디자인 공간은 방대하고 반복적 디자인 프로세스의 기본 부분인 공식 또는 비공식 제어 실험에서 전통적인 A/B 비교를 사용하여 탐색하기 어렵습니다. 이러한 비교를 복잡하게 만드는 핵심 요소는 데이터 세트입니다. 모든 조건에서 참가자를 동일한 데이터 세트에 노출하면 피할 수 없는 학습 효과가 발생합니다. 반면, 모든 실험 조건에 대해 서로 다른 데이터 세트를 사용하면 데이터 세트 자체가 제어되지 않은 변수로 도입되어 내부 유효성이 허용할 수 없을 정도로 감소합니다. 본 논문에서는 VR 분석 실험을 위해 클러스터링된 데이터 세트를 합성하는 생성 프로세스를 도입하여 이 문제를 해결할 것을 제안합니다. 이 프로세스는 실험에서 체계적인 비교를 허용하는 동시에 고유한 데이터 세트를 생성합니다. 주요 이점은 이러한 데이터세트를 반복적인 설계 프로세스에 사용할 수 있다는 것입니다. 두 부분으로 구성된 실험에서 생성 프로세스의 타당성을 보여주고 합성 데이터 세트를 사용하여 VR 기반 시각적 분석에서 새로운 통찰력을 얻을 수 있는 방법을 보여줍니다.",https://doi.org/10.1109/ISMAR52148.2021.00054,Education & Training,Sensor Fusion,User Study; Technical Evaluation,System / Framework; Design Guidelines
552,2021,"TEyeD: Over 20 Million Real-World Eye Images with Pupil, Eyelid, and Iris 2D and 3D Segmentations, 2D and 3D Landmarks, 3D Eyeball, Gaze Vector, and Eye Movement Types","TeyeD: 동공, 눈꺼풀, 홍채 2D 및 3D 분할, 2D 및 3D 랜드마크, 3D 안구, 시선 벡터 및 눈 움직임 유형을 포함하는 2천만 개 이상의 실제 눈 이미지","We present TEyeD, the world’s largest unified public data set of eye images taken with head-mounted devices. TEyeD was acquired with seven different head-mounted eye trackers. Among them, two eye trackers were integrated into virtual reality (VR) or augmented reality (AR) devices. The images in TEyeD were obtained from various tasks, including car rides, simulator rides, outdoor sports activities, and daily indoor activities. The data set includes 2D&3D landmarks, semantic segmentation, 3D eyeball annotation and the gaze vector and eye movement types for all images. Landmarks and semantic segmentation are provided for the pupil, iris and eyelids. Video lengths vary from a few minutes to several hours. With more than 20 million carefully annotated images, TEyeD provides a unique, coherent resource and a valuable foundation for advancing research in the field of computer vision, eye tracking and gaze estimation in modern VR and AR applications. Data and code at DOWNLOAD LINK.","우리는 머리 장착 장치로 촬영한 눈 이미지에 대한 세계 최대의 통합 공개 데이터 세트인 TEYD를 소개합니다. TeyeD는 7개의 서로 다른 머리 장착 안구 추적기로 획득되었습니다. 이 중 아이트래커 2개는 가상현실(VR)이나 증강현실(AR) 기기에 통합됐다. TeyeD의 이미지는 자동차 타기, 시뮬레이터 타기, 야외 스포츠 활동, 일상적인 실내 활동 등 다양한 작업에서 획득되었습니다. 데이터 세트에는 모든 이미지에 대한 2D&3D 랜드마크, 의미론적 분할, 3D 안구 주석, 시선 벡터 및 안구 움직임 유형이 포함됩니다. 동공, 홍채, 눈꺼풀에 대한 랜드마크와 의미론적 분할이 제공됩니다. 비디오 길이는 몇 분에서 몇 시간까지 다양합니다. 주의 깊게 주석이 달린 2천만 개 이상의 이미지를 갖춘 TEyeD는 최신 VR 및 AR 애플리케이션의 컴퓨터 비전, 시선 추적 및 시선 추정 분야의 연구 발전을 위한 독특하고 일관된 리소스와 귀중한 기반을 제공합니다. 다운로드 링크의 데이터 및 코드.",https://doi.org/10.1109/ISMAR52148.2021.00053,Interaction & Input,Computer Vision; Eye / Gaze Tracking,Other,Algorithm / Method
553,2021,The Effects of Virtual Avatar Visibility on Pointing Interpretation by Observers in 3D Environments,3D 환경에서 관찰자의 포인팅 해석에 대한 가상 아바타 가시성의 효과,"Avatars are often used to provide representations of users in 3D environments, such as desktop games or VR applications. While full-body avatars are often sought to be used in applications, low visibility avatars (i.e., head and hands) are often used in a variety of contexts, either as intentional design choices, for simplicity in contexts where full-body avatars are not needed, or due to external limitations. Avatar style can also vary from more simplistic and abstract to highly realistic depending on application context and user choices. We present the results of two desktop experiments that examine avatar visibility, style, and observer view on accuracy in a pointing interpretation task. Significant effects of visibility were found, with effects varying between horizontal and vertical components of error, and error amounts not always worsening as a result of lowering visibility. Error due to avatar visibility was much smaller than error resulting from avatar style or observer view. Our findings suggest that humans are reasonably able to understand pointing gestures with a limited observable body.","아바타는 데스크톱 게임이나 VR 애플리케이션과 같은 3D 환경에서 사용자를 표현하는 데 자주 사용됩니다. 전신 아바타는 애플리케이션에 사용되는 경우가 많지만 가시성이 낮은 아바타(즉, 머리와 손)는 전신 아바타가 필요하지 않은 상황에서 단순성을 위해 의도적인 디자인 선택으로 또는 외부 제한으로 인해 다양한 상황에서 자주 사용됩니다. 아바타 스타일은 애플리케이션 컨텍스트와 사용자 선택에 따라 더욱 단순하고 추상적인 것부터 매우 현실적인 것까지 다양할 수 있습니다. 포인팅 해석 작업의 정확도에 대한 아바타 가시성, 스타일 및 관찰자 보기를 검사하는 두 가지 데스크톱 실험 결과를 제시합니다. 가시성에 대한 상당한 영향이 발견되었으며, 오류의 수평 및 수직 구성 요소에 따라 효과가 다양하고 가시성이 저하된다고 해서 오류 양이 항상 악화되는 것은 아닙니다. 아바타 가시성으로 인한 오차는 아바타 스타일이나 관찰자 시점으로 인한 오차보다 훨씬 작았습니다. 우리의 발견은 인간이 제한된 관찰 가능한 신체로 가리키는 제스처를 합리적으로 이해할 수 있음을 시사합니다.",https://doi.org/10.1109/ISMAR52148.2021.00019,Interaction & Input,Other,Quantitative Experiment,User Study / Empirical Findings
554,2021,The Object at Hand: Automated Editing for Mixed Reality Video Guidance from Hand-Object Interactions,손에 있는 물체: 손-물체 상호 작용을 통한 혼합 현실 비디오 안내를 위한 자동 편집,"In this paper, we concern with the problem of how to automatically extract the steps that compose real-life hand activities. This is a key competence towards processing, monitoring and providing video guidance in Mixed Reality systems. We use egocentric vision to observe hand-object interactions in real-world tasks and automatically decompose a video into its constituent steps. Our approach combines hand-object interaction (HOI) detection, object similarity measurement and a finite state machine (FSM) representation to automatically edit videos into steps. We use a combination of Convolutional Neural Networks (CNNs) and the FSM to discover, edit cuts and merge segments while observing real hand activities. We evaluate quantitatively and qualitatively our algorithm on two datasets: the GTEA [19], and a new dataset we introduce for Chinese Tea making. Results show our method is able to segment hand-object interaction videos into key step segments with high levels of precision.","본 논문에서는 실제 손 활동을 구성하는 단계를 자동으로 추출하는 방법에 대한 문제에 관심을 둡니다. 이는 혼합 현실 시스템에서 비디오 안내를 처리, 모니터링 및 제공하는 데 있어 핵심 역량입니다. 우리는 자기 중심적 시각을 사용하여 실제 작업에서 손과 물체의 상호 작용을 관찰하고 자동으로 비디오를 구성 단계로 분해합니다. 우리의 접근 방식은 HOI(손-객체 상호 작용) 감지, 객체 유사성 측정 및 FSM(유한 상태 머신) 표현을 결합하여 비디오를 단계별로 자동 편집합니다. 우리는 CNN(Convolutional Neural Networks)과 FSM의 조합을 사용하여 실제 활동을 관찰하면서 세그먼트를 발견, 편집 및 병합합니다. 우리는 GTEA[19]와 중국 차 제조를 위해 소개하는 새로운 데이터 세트라는 두 가지 데이터 세트에 대한 알고리즘을 정량적, 질적으로 평가합니다. 결과는 우리의 방법이 손 개체 상호 작용 비디오를 높은 수준의 정밀도로 주요 단계 세그먼트로 분할할 수 있음을 보여줍니다.",https://doi.org/10.1109/ISMAR52148.2021.00023,Interaction & Input,Deep Learning / Neural Networks,Quantitative Experiment,Algorithm / Method
555,2021,The Passenger Experience of Mixed Reality Virtual Display Layouts in Airplane Environments,비행기 환경에서 혼합 현실 가상 디스플레이 레이아웃의 승객 경험,"Augmented / Mixed Reality headsets will in-time see adoption and use in a variety of mobility and transit contexts, allowing users to view and interact with virtual content and displays for productivity and entertainment. However, little is known regarding how multi-display virtual workspaces should be presented in a transit context, nor to what extent the unique affordances of transit environments (e.g. the social presence of others) might influence passenger perception of virtual display layouts. Using a simulated VR passenger airplane environment, we evaluated three different AR-driven virtual display configurations (Horizontal, Vertical, and Focus main display with smaller secondary windows) at two different depths, exploring their usability, user preferences, and the underlying factors that influenced those preferences. We found that the perception of invading other’s personal space significantly influenced preferred layouts in transit contexts. Based on our findings, we reflect on the unique challenges posed by passenger contexts, provide recommendations regarding virtual display layout in the confined airplane environment, and expand on the significant benefits that AR offers over physical displays in said environments.","증강/혼합 현실 헤드셋은 다양한 이동성 및 이동 상황에서 적시에 채택 및 사용되어 사용자가 생산성 및 엔터테인먼트를 위해 가상 콘텐츠 및 디스플레이를 보고 상호 작용할 수 있게 해줍니다. 그러나 다중 디스플레이 가상 작업 공간이 대중교통 맥락에서 어떻게 제시되어야 하는지, 대중교통 환경의 고유한 어포던스(예: 다른 사람의 사회적 존재)가 가상 디스플레이 레이아웃에 대한 승객의 인식에 어느 정도 영향을 미칠 수 있는지에 대해서는 알려진 바가 거의 없습니다. 시뮬레이션된 VR 여객기 환경을 사용하여 세 가지 AR 기반 가상 디스플레이 구성(수평, 수직 및 더 작은 보조 창을 갖춘 초점 기본 디스플레이)을 두 가지 깊이에서 평가하여 유용성, 사용자 선호도 및 이러한 선호도에 영향을 미치는 기본 요소를 탐색했습니다. 우리는 다른 사람의 개인 공간을 침해한다는 인식이 대중교통 맥락에서 선호하는 레이아웃에 큰 영향을 미친다는 것을 발견했습니다. 연구 결과를 바탕으로 우리는 승객 상황에 따른 고유한 과제를 반영하고, 제한된 비행기 환경에서 가상 디스플레이 레이아웃에 관한 권장 사항을 제공하며, 해당 환경에서 실제 디스플레이에 비해 AR이 제공하는 중요한 이점을 확장합니다.",https://doi.org/10.1109/ISMAR52148.2021.00042,Perception & Cognition; Collaboration & Social,Sensor Fusion,Simulation,User Study / Empirical Findings
556,2021,TransforMR: Pose-Aware Object Substitution for Composing Alternate Mixed Realities,TransforMR: 대체 혼합 현실 구성을 위한 포즈 인식 개체 대체,"Despite the advances in machine perception, semantic scene understanding is still a limiting factor in mixed reality scene composition. In this paper, we present TransforMR, a video see-through mixed reality system for mobile devices that performs 3D-pose-aware object substitution to create meaningful mixed reality scenes. In real-time and for previously unseen and unprepared real-world environments, TransforMR composes mixed reality scenes so that virtual objects assume behavioral and environment-contextual properties of replaced real-world objects. This yields meaningful, coherent, and humaninterpretable scenes, not yet demonstrated by today’s augmentation techniques. TransforMR creates these experiences through our novel pose-aware object substitution method building on different 3D object pose estimators, instance segmentation, video inpainting, and pose-aware object rendering. TransforMR is designed for use in the real-world, supporting the substitution of humans and vehicles in everyday scenes, and runs on mobile devices using just their monocular RGB camera feed as input. We evaluated TransforMR with eight participants in an uncontrolled city environment employing different transformation themes. Applications of TransforMR include real-time character animation analogous to motion capturing in professional film making, however without the need for preparation of either the scene or the actor, as well as narrative-driven experiences that allow users to explore fictional parallel universes in mixed reality. We make all of our source code and assets available1.1TransforMR code release: https://github.com/MohamedKari/transformr","기계 인식의 발전에도 불구하고 의미론적 장면 이해는 혼합 현실 장면 구성에서 여전히 제한 요소입니다. 본 논문에서는 의미 있는 혼합 현실 장면을 만들기 위해 3D 포즈 인식 개체 대체를 수행하는 모바일 장치용 비디오 시스루 혼합 현실 시스템인 TransforMR을 제시합니다. 실시간으로 이전에 볼 수 없었고 준비되지 않은 실제 환경을 위해 TransformMR은 혼합 현실 장면을 구성하여 가상 개체가 대체된 실제 개체의 동작 및 환경 맥락 속성을 가정하도록 합니다. 이는 오늘날의 증강 기술로는 아직 입증되지 않은 의미 있고 일관되며 인간이 해석할 수 있는 장면을 생성합니다. TransforMR은 다양한 3D 객체 포즈 추정기, 인스턴스 분할, 비디오 인페인팅 및 포즈 인식 객체 렌더링을 기반으로 하는 새로운 포즈 인식 객체 대체 방법을 통해 이러한 경험을 생성합니다. TransforMR은 실제 사용을 위해 설계되어 일상 장면에서 인간과 차량의 대체를 지원하며 단안 RGB 카메라 피드만 입력으로 사용하여 모바일 장치에서 실행됩니다. 우리는 다양한 변형 테마를 사용하는 통제되지 않은 도시 환경에서 8명의 참가자를 대상으로 TransformMR을 평가했습니다. TransforMR의 애플리케이션에는 전문 영화 제작의 모션 캡처와 유사하지만 장면이나 배우를 준비할 필요가 없는 실시간 캐릭터 애니메이션뿐만 아니라 사용자가 혼합 현실에서 가상의 평행 우주를 탐색할 수 있는 내러티브 중심 경험이 포함됩니다. 우리는 모든 소스 코드와 자산을 사용할 수 있게 만듭니다. 1.1TransforMR 코드 릴리스: https://github.com/MohamedKari/transformr",https://doi.org/10.1109/ISMAR52148.2021.00021,Interaction & Input; Diminished Reality,Computer Vision,User Study,Algorithm / Method
557,2021,Two-hand Pose Estimation from the non-cropped RGB Image with Self-Attention Based Network,Self-Attention Based Network를 사용하여 자르지 않은 RGB 이미지에서 양손 자세 추정,"Estimating the pose of two hands is a crucial problem for many human-computer interaction applications. Since most of the existing works utilize cropped images to predict the hand pose, they require a hand detection stage before pose estimation or input cropped images directly. In this paper, we propose the first real-time one-stage method for pose estimation from a single RGB image without hand tracking. Combining the self-attention mechanism with convolutional layers, the network we proposed is able to predict the 2.5D hand joints coordinate while locating the two hands regions. And to reduce the extra memory and computational consumption caused by self-attention, we proposed a linear attention structure with a spatial reduction attention block called SRAN block. We demonstrate the effectiveness of each component in our network through the ablation study. And experiments on public datasets showed the competitive result with the state-of-the-art method.",두 손의 자세를 추정하는 것은 많은 인간-컴퓨터 상호 작용 응용 프로그램에서 중요한 문제입니다. 기존 작업의 대부분은 손 포즈를 예측하기 위해 크롭 이미지를 활용하기 때문에 포즈 추정 전 손 검출 단계가 필요하거나 크롭 이미지를 직접 입력해야 합니다. 본 논문에서는 핸드 트래킹 없이 단일 RGB 이미지로부터 포즈 추정을 위한 최초의 실시간 1단계 방법을 제안합니다. self-attention 메커니즘과 convolutional 레이어를 결합한 우리가 제안한 네트워크는 두 손 영역을 찾으면서 2.5D 손 관절 좌표를 예측할 수 있습니다. 그리고 self-attention으로 인한 추가 메모리와 계산 소모를 줄이기 위해 SRAN 블록이라는 공간 감소 Attention 블록을 갖춘 선형 Attention 구조를 제안했습니다. 우리는 절제 연구를 통해 네트워크의 각 구성 요소의 효율성을 입증합니다. 그리고 공개 데이터 세트에 대한 실험에서는 최첨단 방법으로 경쟁력 있는 결과를 보여주었습니다.,https://doi.org/10.1109/ISMAR52148.2021.00040,Interaction & Input; Tracking & Localization,Deep Learning / Neural Networks; Hand / Gesture Recognition,Technical Evaluation,Algorithm / Method
558,2021,Understanding the Two-Step Nonvisual Omnidirectional Guidance for Target Acquisition in 3D Spaces,3D 공간에서 표적 획득을 위한 2단계 비시각적 전방향 안내 이해,"Providing directional guidance is important especially for exploring unfamiliar environments. However, most studies are limited to two-dimensional guidance when many interactions happen in 3D spaces. Moreover, visual feedback that is often used to communicate the 3D position of a particular object may not be available in situations when the target is occluded by other objects or located outside of one’s field of view, or due to visual overload or light conditions. Inspired by a prior finding that showed users’ tendency of scanning a 3D space in one direction at a time, we propose two-step nonvisual omnidirectional guidance feedback designs varying the searching order where the guidance for the vertical location of the target (the altitude) is offered to the users first, followed by the horizontal direction of the target (the azimuth angle) and visa versa. To investigate its effect, we conducted the user study with 12 blind-folded sighted participants. Findings suggest that our proposed two-step guidance outperforms the default condition with no order in terms of task completion time and travel distance, particularly when the guidance in the horizontal direction is presented first. We plan to extend this work to assist with finding a target in 3D spaces in a real-world environment.","특히 익숙하지 않은 환경을 탐색할 때는 방향 안내를 제공하는 것이 중요합니다. 그러나 대부분의 연구는 3D 공간에서 많은 상호 작용이 발생할 때 2차원 안내로 제한됩니다. 더욱이, 특정 물체의 3D 위치를 전달하는 데 자주 사용되는 시각적 피드백은 표적이 다른 물체에 의해 가려지거나 시야 밖에 위치하는 상황이나 시각적 과부하 또는 조명 조건으로 인해 사용하지 못할 수도 있습니다. 사용자가 한 번에 한 방향으로 3D 공간을 스캔하는 경향을 보여주는 이전 연구 결과에서 영감을 받아 대상의 수직 위치(고도)에 대한 안내가 사용자에게 먼저 제공되고 그 다음 대상의 수평 방향(방위각) 및 그 반대로 제공되는 검색 순서를 변경하는 2단계 비시각적 전방향 안내 피드백 디자인을 제안합니다. 그 효과를 조사하기 위해 눈을 가린 정안 참가자 12명을 대상으로 사용자 연구를 실시했습니다. 연구 결과에 따르면 우리가 제안한 2단계 지침은 작업 완료 시간 및 이동 거리 측면에서 특히 수평 방향 지침이 먼저 제시될 때 순서가 없는 기본 조건을 능가하는 것으로 나타났습니다. 우리는 실제 환경의 3D 공간에서 목표를 찾는 데 도움이 되도록 이 작업을 확장할 계획입니다.",https://doi.org/10.1109/ISMAR52148.2021.00050,Interaction & Input,Sensor Fusion,User Study,User Study / Empirical Findings
559,2021,Using Trajectory Compression Rate to Predict Changes in Cybersickness in Virtual Reality Games,가상 현실 게임에서 궤적 압축률을 사용하여 사이버 멀미 변화 예측,"Identifying cybersickness in virtual reality (VR) applications such as games in a fast, precise, non-intrusive, and non-disruptive way remains challenging. Several factors can cause cybersickness, and their identification will help find its origins and prevent or minimize it. One such factor is virtual movement. Movement, whether physical or virtual, can be represented in different forms. One way to represent and store it is with a temporally annotated point sequence. Because a sequence is memory-consuming, it is often preferable to save it in a compressed form. Compression allows redundant data to be eliminated while still preserving changes in speed and direction. Since changes in direction and velocity in VR can be associated with cybersickness, changes in compression rate can likely indicate changes in cybersickness levels. In this research, we explore whether quantifying changes in virtual movement can be used to estimate variation in cybersickness levels of VR users. We investigate the correlation between changes in the compression rate of movement data in two VR games with changes in players’ cybersickness levels captured during gameplay. Our results show (1) a clear correlation between changes in compression rate and cybersickness, and (2) that a machine learning approach can be used to identify these changes. Finally, results from a second experiment show that our approach is feasible for cybersickness inference in games and other VR applications that involve movement.","게임과 같은 가상 현실(VR) 애플리케이션에서 빠르고 정확하며 방해가 되지 않고 중단되지 않는 방식으로 사이버 멀미를 식별하는 것은 여전히 ​​어려운 일입니다. 여러 가지 요인이 사이버 멀미를 유발할 수 있으며 이를 식별하면 원인을 찾고 이를 예방하거나 최소화하는 데 도움이 됩니다. 그러한 요소 중 하나는 가상 움직임입니다. 물리적이든 가상이든 움직임은 다양한 형태로 표현될 수 있습니다. 이를 표현하고 저장하는 한 가지 방법은 일시적으로 주석이 달린 포인트 시퀀스를 사용하는 것입니다. 시퀀스는 메모리를 많이 소모하므로 압축된 형식으로 저장하는 것이 더 나은 경우가 많습니다. 압축을 사용하면 속도와 방향의 변화를 그대로 유지하면서 중복된 데이터를 제거할 수 있습니다. VR의 방향과 속도 변화는 사이버 멀미와 연관될 수 있으므로 압축률의 변화는 사이버 멀미 수준의 변화를 나타낼 수 있습니다. 본 연구에서는 가상 움직임의 변화를 정량화하여 VR 사용자의 사이버 멀미 수준 변화를 추정하는 데 사용할 수 있는지 여부를 탐구합니다. 우리는 두 VR 게임의 움직임 데이터 압축률 변화와 게임 플레이 중에 캡처된 플레이어의 사이버 멀미 수준 변화 간의 상관 관계를 조사합니다. 우리의 결과는 (1) 압축률의 변화와 사이버 멀미 사이의 명확한 상관관계와 (2) 기계 학습 접근 방식을 사용하여 이러한 변화를 식별할 수 있음을 보여줍니다. 마지막으로, 두 번째 실험의 결과는 우리의 접근 방식이 게임 및 움직임과 관련된 기타 VR 애플리케이션에서 사이버 멀미 추론에 실현 가능하다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR52148.2021.00028,Perception & Cognition,Other,Technical Evaluation,Algorithm / Method
560,2021,VR Collaborative Object Manipulation Based on Viewpoint Quality,시점 품질에 따른 VR 협업 객체 조작,,,https://doi.org/10.1109/ISMAR52148.2021.00020,Interaction & Input,Other,Other,Other
561,2021,Varying user agency and interaction opportunities in a home mobile augmented virtuality story,홈 모바일 증강 가상 스토리에서 다양한 사용자 대행사 및 상호 작용 기회,"New opportunities for immersive storytelling experiences have arrived through the technology in mobile phones, including the ability to overlay or register digital content on a user’s real world surroundings, to greater immerse the user in the world of the story. This raises questions around the methods and freedom to interact with the digital elements, that will lead to a more immersive and engaging experience. To investigate these areas the Augmented Virtuality (AV) mobile phone application Home Story was developed for iOS devices. It allows a user to move and interact with objects in a virtual environment displayed on their phone, by physically moving in the real world, completing particular actions to progress a story. A mixed methods study with Home Story either guided participants to the next interaction, or offered them increased agency to choose what object to interact with next. Virtual objects could also be interacted with in one of three ways; imagining the interaction, an embodied interaction using the user’s free hand, or a virtual interaction performed on the phone’s touchscreen. Similar levels of immersion were recorded across both study conditions suggesting both can be effective, though highlighting different issues in each case. The embodied free hand interactions proved particularly memorable, though further work is required to improve their implementation, arising from their novelty and lack of familiarity.","사용자의 실제 환경에 디지털 콘텐츠를 오버레이하거나 등록하여 사용자를 스토리 세계에 더욱 몰입시키는 기능을 포함하여 휴대폰 기술을 통해 몰입형 스토리텔링 경험을 위한 새로운 기회가 도래했습니다. 이는 디지털 요소와 상호 작용하는 방법과 자유에 대한 질문을 제기하여 보다 몰입적이고 매력적인 경험으로 이어질 것입니다. 이러한 영역을 조사하기 위해 증강 가상(AV) 휴대폰 애플리케이션인 Home Story가 iOS 장치용으로 개발되었습니다. 이를 통해 사용자는 현실 세계에서 물리적으로 움직이고 스토리를 진행하기 위한 특정 작업을 완료함으로써 휴대폰에 표시된 가상 환경에서 개체를 움직이고 상호 작용할 수 있습니다. 홈 스토리를 사용한 혼합 방법 연구는 참가자들을 다음 상호 작용으로 안내하거나 다음에 상호 작용할 개체를 선택할 수 있는 더 많은 선택권을 제공했습니다. 가상 객체는 세 가지 방법 중 하나로 상호 작용할 수도 있습니다. 상호 작용, 사용자의 자유로운 손을 사용하여 구현된 상호 작용 또는 휴대폰의 터치스크린에서 수행되는 가상 상호 작용을 상상해 보세요. 두 가지 연구 조건 모두에서 유사한 수준의 몰입도가 기록되었으며, 이는 두 가지 모두 효과적일 수 있음을 시사하지만 각 사례마다 다른 문제를 강조합니다. 구현된 프리핸드 상호작용은 특히 기억에 남는 것으로 나타났지만, 참신함과 익숙함이 부족하여 구현을 개선하려면 추가 작업이 필요합니다.",https://doi.org/10.1109/ISMAR52148.2021.00051,Interaction & Input,Other,User Study,Algorithm / Method
562,2020,3D Hand Pose Estimation with a Single Infrared Camera via Domain Transfer Learning,도메인 전송 학습을 통한 단일 적외선 카메라를 사용한 3D 손 자세 추정,"Previous methods successfully estimated 3D hand poses from unblurred depth images with slow and smooth hand motions. However, the performance drops when the depth images are contaminated by motion blur due to fast hand motion. In this paper, we exploit an infrared (IR) image input, which is weakly blurred under fast hand motion. The proposed method is based on domain transfer learning from depth to infrared images. Note we do not have IR images with hand skeletons, thus proposing self-supervision rather than direct supervision using the skeleton labels. We train a Hand Image Generator (HIG) and two Hand Pose Estimators (HPEs) on paired depth and infrared images via self-supervision using a consistency loss, guided by an existing HPE trained on paired depth and hand skeleton entries. The IR-based HPE is then refined on the weakly blurred infrared images. The qualitative and quantitative experiments demonstrate that the proposed method accurately estimates 3D hand poses under motion blur by fast hand motion, while existing depth-based methods fail. Our solution therefore supports fast 3D manipulation of virtual objects for augmented reality applications. Our model and dataset are publicly available for future research.11https://github.com/baeckgoo/ir-hand","이전 방법은 느리고 부드러운 손 동작으로 흐릿하지 않은 깊이 이미지로부터 3D 손 포즈를 성공적으로 추정했습니다. 그러나 빠른 손 움직임으로 인해 깊이 영상이 모션 블러로 오염되면 성능이 저하됩니다. 본 논문에서는 빠른 손 움직임으로 인해 약하게 흐려지는 적외선(IR) 이미지 입력을 활용합니다. 제안된 방법은 깊이에서 적외선 이미지로의 도메인 전이 학습을 기반으로 합니다. 손 뼈대가 있는 IR 이미지가 없으므로 뼈대 라벨을 사용하여 직접 감독하기보다는 자체 감독을 제안합니다. 우리는 쌍을 이루는 깊이와 손 뼈대 항목에 대해 훈련된 기존 HPE의 안내에 따라 일관성 손실을 사용하여 자체 감독을 통해 쌍을 이루는 깊이 및 적외선 이미지에 대해 손 이미지 생성기(HIG)와 두 개의 손 포즈 추정기(HPE)를 훈련합니다. 그런 다음 IR 기반 HPE는 약하게 흐린 적외선 이미지를 정제합니다. 정성적, 정량적 실험을 통해 제안된 방법은 빠른 손 움직임으로 모션 블러 하에서 3D 손 포즈를 정확하게 추정하는 반면, 기존 깊이 기반 방법은 실패함을 보여줍니다. 따라서 당사의 솔루션은 증강 현실 애플리케이션을 위한 가상 객체의 빠른 3D 조작을 지원합니다. 우리의 모델과 데이터 세트는 향후 연구를 위해 공개적으로 제공됩니다.11https://github.com/baeckgoo/ir-hand",https://doi.org/10.1109/ISMAR50242.2020.00086,Interaction & Input,Hand / Gesture Recognition,Technical Evaluation,Algorithm / Method
563,2020,A Comparative Study of Orientation Support Tools in Virtual Reality Environments with Virtual Teleportation,가상 순간이동을 이용한 가상현실 환경의 방향지원 도구 비교 연구,"Movement-compensating interactions like teleportation are commonly deployed techniques in virtual reality environments. Although practical, they tend to cause disorientation while navigating. Previous studies show the effectiveness of orientation-supporting tools, such as trails, in reducing such disorientation and reveal different strengths and weaknesses of individual tools. However, to date, there is a lack of a systematic comparison of those tools when teleportation is used as a movement-compensating technique, in particular under consideration of different tasks. In this paper, we compare the effects of three orientation-supporting tools, namely minimap, trail, and heatmap. We conducted a quantitative user study with 48 participants to investigate the accuracy and efficiency when executing four exploration and search tasks. As dependent variables, task performance, completion time, space coverage, amount of revisiting, retracing time, and memorability were measured. Overall, our results indicate that orientation-supporting tools improve task completion times and revisiting behavior. The trail and heatmap tools were particularly useful for speed-focused tasks, minimal revisiting, and space coverage. The minimap increased memorability and especially supported retracing tasks. These results suggest that virtual reality systems should provide orientation aid tailored to the specific tasks of the users.","순간 이동과 같은 움직임 보상 상호 작용은 가상 현실 환경에서 일반적으로 배포되는 기술입니다. 실용적이긴 하지만 탐색하는 동안 방향 감각을 잃는 경향이 있습니다. 이전 연구에서는 트레일과 같은 방향 지원 도구가 이러한 방향 감각 상실을 줄이는 효과를 보여주고 개별 도구의 다양한 강점과 약점을 드러냈습니다. 그러나 현재까지 특히 다양한 작업을 고려하여 순간 이동을 움직임 보상 기술로 사용할 때 이러한 도구를 체계적으로 비교하는 방법이 부족합니다. 본 논문에서는 세 가지 방향 지원 도구인 미니맵, 트레일, 히트맵의 효과를 비교합니다. 우리는 4가지 탐색 및 검색 작업을 수행할 때 정확성과 효율성을 조사하기 위해 48명의 참가자를 대상으로 정량적 사용자 연구를 수행했습니다. 종속변수로는 작업수행도, 완료시간, 공간범위, 재방문량, 재탐색시간, 기억력을 측정하였다. 전반적으로 우리의 결과는 오리엔테이션 지원 도구가 작업 완료 시간과 재방문 행동을 향상시키는 것으로 나타났습니다. 트레일 및 히트맵 도구는 속도 중심 작업, 최소한의 재방문 및 공간 적용에 특히 유용했습니다. 미니맵은 기억력을 높이고 특히 추적 작업을 지원했습니다. 이러한 결과는 가상 현실 시스템이 사용자의 특정 작업에 맞는 방향 지원을 제공해야 함을 시사합니다.",https://doi.org/10.1109/ISMAR50242.2020.00046,Interaction & Input,Redirected Walking / Locomotion,Quantitative Experiment; User Study,User Study / Empirical Findings
564,2020,A Neurophysiological Approach for Measuring Presence in Immersive Virtual Environments,몰입형 가상 환경에서 존재감을 측정하기 위한 신경생리학적 접근 방식,"Presence, the feeling of being there, is an important factor that affects the overall experience of Virtual Reality (VR). Higher presence commonly provides a better experience in VR than lower presence. However, presence is commonly measured subjectively through postexperience questionnaires, which can suffer from participant biases, dishonest answers, and fatigue. It can also be difficult for subjects to accurately remember their feelings of presence after they have left the VR experience. In this paper, we measured the effects of different levels of presence (high and low) in VR using physiological and neurological signals. The experiment involved 24 participants in a between-subjects design. Results indicated a significant effect of presence on both physiological and neurological signals. We noticed that higher presence results in higher heart rate, less visual stress, higher theta and beta activities in the frontal region, and higher alpha activities in the parietal region. These findings and insights could lead to an alternative objective measure of presence.","존재감, 즉 그곳에 있는 듯한 느낌은 가상현실(VR)의 전반적인 경험에 영향을 미치는 중요한 요소입니다. 존재감이 높을수록 일반적으로 존재감이 낮을 때보다 VR에서 더 나은 경험을 제공합니다. 그러나 존재감은 일반적으로 경험 후 설문지를 통해 주관적으로 측정되며, 이는 참가자 편견, 부정직한 답변 및 피로로 인해 어려움을 겪을 수 있습니다. 피험자가 VR 경험을 떠난 후 자신의 존재감을 정확하게 기억하는 것도 어려울 수 있습니다. 본 논문에서는 생리학적, 신경학적 신호를 사용하여 VR에서 다양한 수준의 존재감(높음 및 낮음)이 미치는 영향을 측정했습니다. 실험에는 피험자 간 설계에 24명의 참가자가 참여했습니다. 결과는 생리학적 및 신경학적 신호 모두에 대한 존재의 중요한 영향을 나타냅니다. 존재감이 높을수록 심박수가 높아지고 시각적 스트레스가 줄어들며 전두엽 영역에서 세타 및 베타 활동이 높아지고 두정엽 영역에서 알파 활동이 높아지는 것으로 나타났습니다. 이러한 발견과 통찰은 존재에 대한 대안적이고 객관적인 측정으로 이어질 수 있습니다.",https://doi.org/10.1109/ISMAR50242.2020.00072,Perception & Cognition,Other,User Study,User Study / Empirical Findings
565,2020,AR Interfaces for Mid-Air 6-DoF Alignment: Ergonomics-Aware Design and Evaluation,공중 6-DoF 정렬을 위한 AR 인터페이스: 인체공학적 설계 및 평가,"Aligning hand-held objects into mid-air positions and orientations is important for many applications. Task performance depends on speed and accuracy, and also on minimizing the user’s physical exertion. Augmented reality head-mounted displays (AR HMDs) can guide users during mid-air alignments by tracking an object’s pose and delivering visual instruction directly into the user’s field of view (FoV). However, it is unclear which AR HMD interfaces are most effective for mid-air alignment guidance, and how the form factor of current AR HMD hardware (such as heaviness and low FoV) affects how users put themselves into tiring body poses during mid-air alignment. We defined a set of design requirements for mid-air alignment interfaces that target reduction of high-exertion body poses during alignment. We then designed, implemented, and tested several interfaces in a user study in which novice participants performed a sequence of mid-air alignments using each interface.Results show that interfaces that rely on visual guidance located near the hand-held object reduce acquisition times and translation errors, while interfaces that involve aiming at a faraway virtual object reduce rotation errors. Users tend to avoid focus shifts and to position the head and arms to maximize how much AR visualization is contained within a single FoV without moving the head. We found that changing the size of visual elements affected how far out the user extends the arm, which affects torque forces. We also found that dynamically adjusting where visual guidance is placed relative to the mid-air pose can help keep the head level during alignment, which is important for distributing the weight of the AR HMD.","손에 들고 있는 물체를 공중 위치와 방향으로 정렬하는 것은 많은 응용 분야에서 중요합니다. 작업 성과는 속도와 정확성, 그리고 사용자의 육체적 노력을 최소화하는 것에 따라 달라집니다. 증강 현실 헤드 마운트 디스플레이(AR HMD)는 물체의 자세를 추적하고 사용자의 시야(FoV)에 직접 시각적 지침을 제공하여 공중 정렬 중에 사용자를 안내할 수 있습니다. 그러나 어떤 AR HMD 인터페이스가 공중 정렬 안내에 가장 효과적인지, 현재 AR HMD 하드웨어의 폼 팩터(예: 무거움 및 낮은 FoV)가 공중 정렬 중에 사용자가 피곤한 신체 자세를 취하는 방식에 어떤 영향을 미치는지는 확실하지 않습니다. 우리는 정렬 중 고단력 신체 자세를 줄이는 것을 목표로 하는 공중 정렬 인터페이스에 대한 일련의 설계 요구 사항을 정의했습니다. 그런 다음 초보 참가자가 각 인터페이스를 사용하여 일련의 공중 정렬을 수행하는 사용자 연구에서 여러 인터페이스를 설계, 구현 및 테스트했습니다. 결과에 따르면 손에 들고 있는 개체 근처에 있는 시각적 안내에 의존하는 인터페이스는 획득 시간과 변환 오류를 줄이는 반면, 멀리 있는 가상 개체를 겨냥하는 인터페이스는 회전 오류를 줄이는 것으로 나타났습니다. 사용자는 초점 이동을 피하고 머리와 팔의 위치를 ​​지정하여 머리를 움직이지 않고도 단일 FoV 내에 AR 시각화가 얼마나 많이 포함되는지를 최대화하는 경향이 있습니다. 우리는 시각적 요소의 크기를 변경하면 사용자가 팔을 얼마나 멀리 뻗는지에 영향을 미치고 이는 토크 힘에 영향을 미친다는 것을 발견했습니다. 또한 공중 포즈를 기준으로 시각적 안내가 배치되는 위치를 동적으로 조정하면 정렬 중에 머리 높이를 유지하는 데 도움이 될 수 있으며 이는 AR HMD의 무게를 분산하는 데 중요합니다.",https://doi.org/10.1109/ISMAR50242.2020.00055,Display & Optics; Tracking & Localization,Other,User Study,User Study / Empirical Findings
566,2020,ARPads: Mid-air Indirect Input for Augmented Reality,ARPad: 증강 현실을 위한 공중 간접 입력,"Interacting efficiently and comfortably with Augmented Reality (AR) headsets remains a major issue. We investigate the concept of mid-air pads as an alternative to gaze or direct hand input to control a cursor in windows anchored in the environment. ARPads allow users to control the cursor displayed in the headset screen through movements on a mid-air plane, which is not spatially aligned with the headset screen. We investigate a design space for ARPads, which takes into account the position of the pad relative to the user’s body, and the orientation of the pad relative to that of the headset screen. Our study suggests that 1) indirect input can achieve the same performance as direct input while causing less fatigue than hand raycast, 2) an ARPad should be attached to the wrist or waist rather than to the thigh, and 3) the ARPad and the screen should have the same orientation.","증강 현실(AR) 헤드셋을 사용하여 효율적이고 편안하게 상호 작용하는 것은 여전히 ​​중요한 문제로 남아 있습니다. 우리는 환경에 고정된 창의 커서를 제어하기 위해 시선이나 직접적인 손 입력 대신 공중 패드의 개념을 조사합니다. ARPad를 사용하면 헤드셋 화면과 공간적으로 정렬되지 않은 공중 평면의 움직임을 통해 헤드셋 화면에 표시된 커서를 사용자가 제어할 수 있습니다. 우리는 사용자 신체에 대한 패드의 위치와 헤드셋 화면에 대한 패드의 방향을 고려하는 ARPad의 디자인 공간을 조사합니다. 우리의 연구에서는 1) 간접 입력이 직접 입력과 동일한 성능을 얻을 수 있으면서도 손으로 레이캐스트하는 것보다 피로도가 낮고, 2) ARPad를 허벅지가 아닌 손목이나 허리에 부착해야 하며, 3) ARPad와 화면의 방향이 동일해야 한다고 제안합니다.",https://doi.org/10.1109/ISMAR50242.2020.00060,Interaction & Input,Other,Other,Design Guidelines
567,2020,An Efficient Planar Bundle Adjustment Algorithm,효율적인 평면 번들 조정 알고리즘,"This paper presents an efficient algorithm for the least-squares problem using the point-to-plane cost, which aims to jointly optimize depth sensor poses and plane parameters for 3D reconstruction. We call this least-squares problem Planar Bundle Adjustment (PBA), due to the similarity between this problem and the original Bundle Adjustment (BA) in visual reconstruction. As planes ubiquitously exist in the man-made environment, they are generally used as landmarks in SLAM algorithms for various depth sensors. PBA is important to reduce drift and improve the quality of the map. However, directly adopting the well-established BA framework in visual reconstruction will result in a very inefficient solution for PBA. This is because a 3D point only has one observation at a camera pose. In contrast, a depth sensor can record hundreds of points in a plane at a time, which results in a very large nonlinear least-squares problem even for a small-scale space. The main contribution of this paper is an efficient solution for the PBA problem using the point-to-plane cost. We introduce a reduced Jacobian matrix and a reduced residual vector, and prove that they can replace the original Jacobian matrix and residual vector in the generally adopted Levenberg-Marquardt (LM) algorithm. This significantly reduces the computational cost. Besides, when planes are combined with other features for 3D reconstruction, the reduced Jacobian matrix and residual vector can also replace the corresponding parts derived from planes. Our experimental results show that our algorithm can significantly reduce the computational time compared to the solution using the traditional BA framework. In addition, our algorithm is faster, more accurate, and more robust to initialization errors compared to the start-of-the-art solution using the plane-to-plane cost [3].","본 논문에서는 3차원 재구성을 위한 깊이 센서 자세와 평면 매개변수를 공동으로 최적화하는 것을 목표로 하는 점-평면 비용을 사용하여 최소 제곱 문제에 대한 효율적인 알고리즘을 제시합니다. 우리는 이 문제와 시각적 재구성의 원래 번들 조정(BA) 사이의 유사성 때문에 이 최소 제곱 문제를 평면 번들 조정(PBA)이라고 부릅니다. 평면은 인공 환경 어디에나 존재하기 때문에 일반적으로 다양한 깊이 센서의 SLAM 알고리즘에서 랜드마크로 사용됩니다. PBA는 드리프트를 줄이고 지도의 품질을 향상시키는 데 중요합니다. 그러나 시각적 재구성에 잘 확립된 BA 프레임워크를 직접 채택하면 PBA에 대한 솔루션이 매우 비효율적입니다. 이는 3D 포인트가 카메라 포즈에서 하나의 관찰만 갖기 때문입니다. 대조적으로, 깊이 센서는 한 번에 평면에서 수백 개의 점을 기록할 수 있으므로 소규모 공간에서도 매우 큰 비선형 최소 제곱 문제가 발생합니다. 이 논문의 주요 기여는 점-평면 비용을 사용하여 PBA 문제에 대한 효율적인 솔루션을 제공한다는 것입니다. 우리는 축소된 야코비안 행렬과 축소된 잔차 벡터를 소개하고, 일반적으로 채택되는 Levenberg-Marquardt(LM) 알고리즘에서 원래의 야코비안 행렬과 잔차 벡터를 대체할 수 있음을 증명합니다. 이로 인해 계산 비용이 크게 절감됩니다. 게다가, 평면이 3차원 재구성을 위해 다른 특징과 결합되면 축소된 야코비안 행렬과 잔차 벡터가 평면에서 파생된 해당 부분을 대체할 수도 있습니다. 우리의 실험 결과는 우리의 알고리즘이 기존 BA 프레임워크를 사용하는 솔루션에 비해 계산 시간을 크게 줄일 수 있음을 보여줍니다. 또한, 우리의 알고리즘은 평면 대 평면 비용을 사용하는 최첨단 솔루션에 비해 초기화 오류에 대해 더 빠르고 정확하며 더 강력합니다[3].",https://doi.org/10.1109/ISMAR50242.2020.00035,Tracking & Localization,SLAM / Spatial Mapping,Qualitative Analysis,System / Framework
568,2020,An In-Depth Exploration of the Effect of 2D/3D Views and Controller Types on First Person Shooter Games in Virtual Reality,가상 현실의 1인칭 슈팅 게임에 대한 2D/3D 뷰 및 컨트롤러 유형의 효과에 대한 심층 탐구,"The amount of interest in Virtual Reality (VR) research has significantly increased over the past few years, both in academia and industry. The release of commercial VR Head-Mounted Displays (HMDs) has been a major contributing factor. However, there is still much to be learned, especially how views and input techniques, as well as their interaction, affect the VR experience. There is little work done on First-Person Shooter (FPS) games in VR, and those few studies have focused on a single aspect of VR FPS. They either focused on the view, e.g., comparing VR to a typical 2D display or on the controller types. To the best of our knowledge, there are no studies investigating variations of 2D/3D views in HMDs, controller types, and their interactions. As such, it is challenging to distinguish findings related to the controller type from those related to the view. If a study does not control for the input method and finds that 2D displays lead to higher performance than VR, we cannot generalize the results because of the confounding variables. To understand their interaction, we propose to analyze in more depth, whether it is the view (2D vs. 3D) or the way it is controlled that gives the platforms their respective advantages. To study the effects of the 2D/3D views, we created a 2D visual technique, PlaneFrame, that was applied inside the VR headset. Our results show that the controller type can have a significant positive impact on performance, immersion, and simulator sickness when associated with a 2D view. They further our understanding of the interactions that controllers and views have and demonstrate that comparisons are highly dependent on how both factors go together. Further, through a series of three experiments, we developed a technique that can lead to a substantial performance, a good level of immersion, and can minimize the level of simulator sickness.","가상 현실(VR) 연구에 대한 관심은 지난 몇 년 동안 학계와 업계 모두에서 크게 증가했습니다. 상용 VR 헤드마운트디스플레이(HMD)의 출시가 주요 기여 요인이었습니다. 그러나 아직 배워야 할 것이 많습니다. 특히 뷰와 입력 기술, 그리고 이들의 상호 작용이 VR 경험에 어떤 영향을 미치는지 말입니다. VR의 1인칭 슈팅 게임(FPS) 게임에 대한 작업은 거의 없으며 이러한 소수의 연구는 VR FPS의 단일 측면에 중점을 두었습니다. 그들은 VR을 일반적인 2D 디스플레이와 비교하거나 컨트롤러 유형과 같은 뷰에 중점을 두었습니다. 우리가 아는 한, HMD의 2D/3D 뷰 변형, 컨트롤러 유형 및 상호 작용을 조사한 연구는 없습니다. 따라서 컨트롤러 유형과 관련된 결과와 뷰와 관련된 결과를 구별하는 것은 어렵습니다. 연구에서 입력 방법을 제어하지 않고 2D 디스플레이가 VR보다 더 높은 성능을 가져온다는 사실이 밝혀지면 교란 변수로 인해 결과를 일반화할 수 없습니다. 이들의 상호 작용을 이해하기 위해 우리는 플랫폼에 각각의 이점을 제공하는 뷰(2D 대 3D)인지 제어 방식인지에 대해 더 깊이 분석할 것을 제안합니다. 2D/3D 뷰의 효과를 연구하기 위해 VR 헤드셋 내부에 적용된 2D 시각적 기술인 PlaneFrame을 만들었습니다. 우리의 결과는 컨트롤러 유형이 2D 뷰와 연관될 때 성능, 몰입도 및 시뮬레이터 멀미에 상당히 긍정적인 영향을 미칠 수 있음을 보여줍니다. 컨트롤러와 뷰의 상호 작용에 대한 이해를 높이고 두 요소가 어떻게 결합되는지에 따라 비교가 크게 좌우된다는 것을 보여줍니다. 또한, 일련의 3번의 실험을 통해 실질적인 성능과 좋은 몰입감을 이끌어 낼 수 있으며, 시뮬레이터 멀미를 최소화할 수 있는 기술을 개발했습니다.",https://doi.org/10.1109/ISMAR50242.2020.00102,Interaction & Input; Display & Optics,Sensor Fusion,Quantitative Experiment,User Study / Empirical Findings; Algorithm / Method
569,2020,An Intelligent Augmented Reality Training Framework for Neonatal Endotracheal Intubation,신생아 기관내 삽관을 위한 지능형 증강 현실 훈련 프레임워크,"Neonatal Endotracheal Intubation (ETI) is a critical resuscitation skill that requires tremendous practice of trainees before clinical exposure. However, current manikin-based training regimen is ineffective in providing satisfactory real-time procedural guidance for accurate assessment due to the lack of see-through visualization within the manikin. The training efficiency is further reduced by the limited availability of expert instructors, which inevitably results in a long learning curve for trainees. To this end, we propose an intelligent Augmented Reality (AR) training framework that provides trainees with a complete visualization of the ETI procedure for real-time guidance and assessment. Specifically, the proposed framework is capable of capturing the motions of the laryngoscope and the manikin and offer 3D see-through visualization rendered to the head-mounted display (HMD). Furthermore, an attention-based Convolutional Neural Network (CNN) model is developed to automatically assess the ETI performance from the captured motions as well as identify regions of motions that significantly contribute to the performance evaluation. Lastly, augmented user-friendly feedback is delivered with interpretable results with the ETI scoring rubric through the color-coded motion trajectory that classifies highlighted regions that need more practice. The classification accuracy of our machine learning model is 84.6%.","신생아 기관내 삽관(ETI)은 임상 노출 전에 훈련생의 엄청난 연습이 필요한 중요한 소생 기술입니다. 그러나 현재의 마네킹 기반 훈련 방식은 마네킹 내에서 투명한 시각화가 부족하여 정확한 평가를 위한 만족스러운 실시간 절차 지침을 제공하는 데 효과적이지 않습니다. 전문 강사의 가용성이 제한되어 훈련 효율성이 더욱 떨어지며, 이로 인해 훈련생의 학습 곡선이 길어지게 됩니다. 이를 위해 우리는 훈련생에게 실시간 안내 및 평가를 위한 ETI 절차의 완전한 시각화를 제공하는 지능형 증강 현실(AR) 훈련 프레임워크를 제안합니다. 특히, 제안된 프레임워크는 후두경과 마네킹의 동작을 캡처하고 HMD(헤드 마운트 디스플레이)에 렌더링된 3D 투명 시각화를 제공할 수 있습니다. 또한 캡처된 모션에서 ETI 성능을 자동으로 평가하고 성능 평가에 크게 기여하는 모션 영역을 식별하기 위해 주의 기반 CNN(Convolutional Neural Network) 모델이 개발되었습니다. 마지막으로, 더 많은 연습이 필요한 강조 영역을 분류하는 색상으로 구분된 모션 궤적을 통해 ETI 채점 기준표를 통해 해석 가능한 결과와 함께 사용자 친화적인 증강 피드백이 제공됩니다. 우리 기계 학습 모델의 분류 정확도는 84.6%입니다.",https://doi.org/10.1109/ISMAR50242.2020.00097,Display & Optics; Education & Training,Deep Learning / Neural Networks,Quantitative Experiment,Algorithm / Method
570,2020,Augmented Mirrors,증강 거울,"A recurrent problem in egocentric Augmented Reality (AR) applications is the misestimation of depth. Providing alternative views from non-egocentric perspectives can convey useful information for applications that require the correct judgment of depth as it is in the case of placement and alignment of virtual and real content, but also for exploration and visualization tasks.In this paper, we introduce Augmented Mirrors. Through the integration of a real mirror, our approach is capable to reflect changes of the real and virtual content of an AR application while users benefit from the perceptual advantages of using mirrors. Our concept, simple yet effective, only requires tracking the user and mirror poses with the accuracy demanded by a specific application. To showcase the potential and flexibility of the Augmented Mirrors, we present and discuss multiple examples ranging from alignment, exploration, spatial understanding, and selective content visualization using different AR-enabled devices and tracking technologies. We envision the Augmented Mirrors as a new and valuable concept that can be used in applications that benefit from additional viewpoints and require the simultaneous visualization of real and virtual content.","자기 중심적인 증강 현실(AR) 애플리케이션에서 반복되는 문제는 깊이를 잘못 평가하는 것입니다. 자기 중심적이지 않은 관점에서 대체 뷰를 제공하면 가상 및 실제 콘텐츠의 배치 및 정렬의 경우처럼 깊이에 대한 올바른 판단이 필요한 애플리케이션뿐만 아니라 탐색 및 시각화 작업에도 유용한 정보를 전달할 수 있습니다. 본 논문에서는 증강 거울을 소개합니다. 실제 거울의 통합을 통해 우리의 접근 방식은 AR 애플리케이션의 실제 콘텐츠와 가상 콘텐츠의 변화를 반영하는 동시에 사용자는 거울 사용의 지각적 이점을 누릴 수 있습니다. 간단하면서도 효과적인 우리의 개념은 특정 응용 프로그램에서 요구하는 정확도로 사용자와 거울 자세를 추적하기만 하면 됩니다. 증강 거울의 잠재력과 유연성을 보여주기 위해 우리는 다양한 AR 지원 장치 및 추적 기술을 사용하여 정렬, 탐색, 공간 이해 및 선택적 콘텐츠 시각화에 이르는 다양한 예를 제시하고 논의합니다. 우리는 증강 거울을 추가적인 관점의 이점을 활용하고 실제 콘텐츠와 가상 콘텐츠를 동시에 시각화해야 하는 애플리케이션에 사용할 수 있는 새롭고 가치 있는 개념으로 생각합니다.",https://doi.org/10.1109/ISMAR50242.2020.00045,Tracking & Localization,Sensor Fusion,Quantitative Experiment,Algorithm / Method
571,2020,Automatic Detection and Prediction of Cybersickness Severity using Deep Neural Networks from user's Physiological Signals,사용자의 생리신호로부터 심층신경망을 활용한 사이버 멀미 심각도 자동 감지 및 예측,"Cybersickness is one of the primary challenges to the usability and acceptability of virtual reality (VR). Cybersickness can cause motion sickness-like discomforts, including disorientation, headache, nausea, and fatigue, both during and after the VR immersion. Prior research suggested a significant correlation between physiological signals and cybersickness severity, as measured by the simulator sickness questionnaire (SSQ). However, SSQ may not be suitable for automatic detection of cybersickness severity during immersion, as it is usually reported before and after the immersion. In this study, we introduced an automated approach for the detection and prediction of cybersickness severity from the user’s physiological signals. We collected heart rate, breathing rate, heart rate variability, and galvanic skin response data from 31 healthy participants while immersed in a VR roller coaster simulation. We found a significant difference in the participants’ physiological signals during their cybersickness state compared to their resting baseline. We compared a support vector machine classifier and three deep neural classifiers for cybersickness severity detection and prediction in two minutes’ future, given the previous two minutes of physiological signals. Our proposed simplified convolutional long short-term memory classifier achieved an accuracy of 97.44% for detecting current cybersickness severity and 87.38% for predicting future cybersickness severity from the physiological signals.","사이버 멀미는 가상 현실(VR)의 유용성과 수용성에 대한 주요 과제 중 하나입니다. 사이버 멀미는 VR에 몰입하는 동안과 몰입한 후에 방향 감각 상실, 두통, 메스꺼움, 피로 등 멀미와 유사한 불편함을 유발할 수 있습니다. 이전 연구에서는 시뮬레이터 질병 설문지(SSQ)로 측정한 생리적 신호와 사이버 질병 심각도 사이의 중요한 상관관계가 제시되었습니다. 그러나 SSQ는 일반적으로 침수 전후에 보고되므로 침수 중 사이버 멀미 심각도를 자동으로 감지하는 데 적합하지 않을 수 있습니다. 본 연구에서는 사용자의 생리적 신호를 통해 사이버 멀미 심각도를 감지하고 예측하는 자동화된 접근 방식을 도입했습니다. 우리는 VR 롤러코스터 시뮬레이션에 몰입한 31명의 건강한 참가자로부터 심박수, 호흡수, 심박 변이도, 피부 전기 반응 데이터를 수집했습니다. 우리는 휴식 기준과 비교하여 사이버 멀미 상태 동안 참가자의 생리적 신호에 상당한 차이가 있음을 발견했습니다. 우리는 이전 2분간의 생리학적 신호를 바탕으로 2분 후의 사이버 멀미 심각도 감지 및 예측을 위해 지원 벡터 머신 분류기와 3개의 심층 신경 분류기를 비교했습니다. 우리가 제안한 단순화된 컨벌루션 장단기 기억 분류기는 현재 사이버 멀미 심각도를 탐지하는 데 97.44%의 정확도를 달성했으며 생리학적 신호로부터 미래의 사이버 멀미 심각도를 예측하는 데 87.38%의 정확도를 달성했습니다.",https://doi.org/10.1109/ISMAR50242.2020.00066,Perception & Cognition,Deep Learning / Neural Networks,Questionnaire / Survey,Algorithm / Method
572,2020,Automatic Generation of Diegetic Guidance in Cinematic Virtual Reality,영화적 가상 현실에서 디에제틱 가이던스의 자동 생성,"One of the advantages of Cinematic Virtual Reality over traditional movies is that viewers can freely explore the virtual space. However, such freedom leads to a problem of missing some key events during watching. Therefore, directional visual guidance in the virtual space is vital for the viewer to follow the storyline and capture key events. Generating moving objects is one of the most commonly used diegetic guidance techniques which can implicitly guide the viewer’s attention in the virtual space. In this paper, we investigate the formulation of key events in Cinematic Virtual Reality with event-of-interest script. Based on the formulation, we analyze the factors influencing diegetic guidance and propose an automatic-generating approach named Dynamic Diegetic Guidance. User study showed Dynamic Diegetic Guidance has a higher degree of immersion and takes less time to redirect viewers to key events compared with fixed guidance techniques, which makes the viewing experience more informative and entertaining.",기존 영화에 비해 시네마틱 가상 현실의 장점 중 하나는 관객이 가상 공간을 자유롭게 탐색할 수 있다는 점입니다. 그러나 이러한 자유로움은 시청 중 일부 주요 이벤트를 놓치는 문제로 이어진다. 따라서 시청자가 스토리라인을 따르고 주요 이벤트를 포착하려면 가상 공간에서의 방향성 시각적 안내가 필수적입니다. 움직이는 객체를 생성하는 것은 가상 공간에서 시청자의 주의를 암묵적으로 안내할 수 있는 가장 일반적으로 사용되는 디제시스 안내 기술 중 하나입니다. 본 논문에서는 관심 이벤트 스크립트를 사용하여 영화 가상 현실의 주요 이벤트 공식화를 조사합니다. 공식화를 바탕으로 식생활 지도에 영향을 미치는 요인을 분석하고 동적 식생활 지도라는 자동 생성 접근 방식을 제안합니다. 사용자 연구에 따르면 동적 디에제틱 가이던스는 고정 가이던스 기술에 비해 몰입도가 더 높고 시청자를 주요 이벤트로 리디렉션하는 데 더 적은 시간이 소요되므로 시청 경험이 더 유익하고 재미있다는 사실이 밝혀졌습니다.,https://doi.org/10.1109/ISMAR50242.2020.00087,Perception & Cognition,Other,User Study,Algorithm / Method
573,2020,Bare-hand Depth Inpainting for 3D Tracking of Hand Interacting with Object,물체와 상호작용하는 손의 3D 추적을 위한 맨손 깊이 인페인팅,"We propose a 3D hand tracking system using bare-hand depth inpainting from an RGB-depth image for a hand interacting with an object. The effectiveness of most existing hand-object tracking methods is impeded by the insufficiency of data, which do not include hand data occluded by the object, and their reliance on the information inferred from assuming the specific object type. We generate a sufficiently accurate bare-hand depth image from a hand interacting with an object using a conditional generative adversarial network, which is trained using the synthesized 2D silhouettes of the object to learn the morphology of the hand. We evaluate the proposed approach using a hierarchical particle filter-based hand tracker and prove that our approach utilizing the bare-hand tracker in the hand-object interaction dataset achieve state-of-the-art performance. The generalization of our work will enable visual-tactile interaction that is more natural in various wearable augmented reality applications.",우리는 물체와 상호작용하는 손에 대한 RGB 깊이 이미지로부터 맨손 깊이 인페인팅을 사용하는 3D 손 추적 시스템을 제안합니다. 대부분의 기존 손 개체 추적 방법의 효율성은 개체에 의해 가려진 손 데이터를 포함하지 않는 데이터의 부족과 특정 개체 유형을 가정하여 추론된 정보에 대한 의존성으로 인해 방해됩니다. 우리는 손의 형태를 학습하기 위해 물체의 합성된 2D 실루엣을 사용하여 훈련된 조건부 생성 적대 신경망을 사용하여 물체와 상호 작용하는 손에서 충분히 정확한 맨손 깊이 이미지를 생성합니다. 우리는 계층적 입자 필터 기반 손 추적기를 사용하여 제안된 접근 방식을 평가하고 손 개체 상호 작용 데이터 세트에서 맨손 추적기를 활용하는 접근 방식이 최첨단 성능을 달성한다는 것을 증명합니다. 우리 작업의 일반화는 다양한 웨어러블 증강 현실 애플리케이션에서 보다 자연스러운 시각적-촉각적 상호 작용을 가능하게 할 것입니다.,https://doi.org/10.1109/ISMAR50242.2020.00048,Interaction & Input,Sensor Fusion,Technical Evaluation,Algorithm / Method
574,2020,Body Weight Perception of Females using Photorealistic Avatars in Virtual and Augmented Reality,가상 및 증강 현실에서 사실적인 아바타를 사용한 여성의 체중 인식,"The appearance of avatars can potentially alter changes in their users’ perception and behavior. Based on this finding, approaches to support the therapy of body perception disturbances in eating or body weight disorders by mixed reality (MR) systems gain in importance. However, the methodological heterogeneity of previous research has made it difficult to assess the suitability of different MR systems for therapeutic use in these areas. The effects of MR system properties and related psychometric factors on body-related perceptions have so far remained unclear. We developed an interactive virtual mirror embodiment application to investigate the differences between an augmented reality see-through head-mounted-display (HMD) and a virtual reality HMD on the before-mentioned factors. Additionally, we considered the influence of the participant’s body-mass-index (BMI) and the BMI difference between participants and their avatars on the estimations. The 54 normal-weight female participants significantly underestimated the weight of their photorealistic, generic avatar in both conditions. Body weight estimations were significantly predicted by the participants’ BMI and the BMI difference. We also observed partially significant differences in presence and tendencies for differences in virtual body ownership between the systems. Our results offer new insights into the relationships of body weight perception in different MR environments and provide new perspectives for the development of therapeutic applications. Index Terms: Human-centered computing—Human computer interaction (HCI)—Empirical studies in HCI; Human-centered computing—Human computer interaction (HCI)—Mixed / augmented reality; Human-centered computing—Human computer interaction (HCI)—Virtual reality",아바타의 모습은 사용자의 인식과 행동의 변화를 잠재적으로 바꿀 수 있습니다. 이 발견을 바탕으로 혼합 현실(MR) 시스템을 통해 섭식 시 신체 인식 장애 또는 체중 장애 치료를 지원하는 접근 방식이 중요해졌습니다. 그러나 이전 연구의 방법론적 이질성으로 인해 이러한 분야에서 치료에 사용하기 위한 다양한 MR 시스템의 적합성을 평가하는 것이 어려워졌습니다. MR 시스템 속성 및 관련 심리 측정 요인이 신체 관련 인식에 미치는 영향은 지금까지 불분명한 상태로 남아 있습니다. 우리는 위에서 언급한 요소에 대한 증강 현실 투시형 헤드 마운트 디스플레이(HMD)와 가상 현실 HMD 간의 차이점을 조사하기 위해 대화형 가상 거울 구현 애플리케이션을 개발했습니다. 또한 참가자의 체질량 지수(BMI)와 참가자와 아바타 간의 BMI 차이가 추정에 미치는 영향을 고려했습니다. 54명의 정상 체중 여성 참가자는 두 조건 모두에서 사실적인 일반 아바타의 무게를 상당히 과소평가했습니다. 체중 추정은 참가자의 BMI와 BMI 차이에 의해 유의하게 예측되었습니다. 우리는 또한 시스템 간 가상 신체 소유권의 차이에 대한 존재 및 경향에서 부분적으로 중요한 차이를 관찰했습니다. 우리의 결과는 다양한 MR 환경에서 체중 인식의 관계에 대한 새로운 통찰력을 제공하고 치료 응용 프로그램 개발에 대한 새로운 관점을 제공합니다. 색인 용어: 인간 중심 컴퓨팅 - 인간 컴퓨터 상호 작용(HCI) - HCI에 대한 실증적 연구; 인간 중심 컴퓨팅 - 인간 컴퓨터 상호 작용(HCI) - 혼합/증강 현실; 인간 중심 컴퓨팅 - 인간 컴퓨터 상호작용(HCI) - 가상 현실,https://doi.org/10.1109/ISMAR50242.2020.00071,Perception & Cognition; Display & Optics,Other,User Study,Algorithm / Method; User Study / Empirical Findings
575,2020,Can Retinal Projection Displays Improve Spatial Perception in Augmented Reality?,망막 프로젝션 디스플레이가 증강 현실에서 공간 인식을 향상시킬 수 있습니까?,"Commonly used Head Mounted Displays (HMDs) in Augmented Reality (AR), namely Optical See-Through (OST) displays, suffer from a main drawback: their focal lenses can only provide a fixed focal distance. Such a limitation is suspected to be one of the main factors for distance misperception in AR. In this paper, we studied the use of an emerging new kind of AR display to tackle such perception issues: Retinal Projection Displays (RPDs). With RPDs, virtual images have no focal distance and the AR content is always in focus. We conducted the first reported experiment evaluating egocentric distance perception of observers using Retinal Projection Displays. We compared the precision and accuracy of the depth estimation between real and virtual targets, displayed by either OST HMDs or RPDs. Interestingly, our results show that RPDs provide depth estimates in AR closer to real ones compared to OST HMDs. Indeed, the use of an OST device was found to lead to an overestimation of the perceived distance by 16%, whereas the distance overestimation bias dropped to 4% with RPDs. Besides, the task was reported with the same level of difficulty and no difference in precision. As such, our results shed the first light on retinal projection displays’ benefits in terms of user’s perception in Augmented Reality, suggesting that RPD is a promising technology for AR applications in which an accurate distance perception is required.","증강 현실(AR)에서 일반적으로 사용되는 헤드 장착 디스플레이(HMD), 즉 광학 투명(OST) 디스플레이에는 초점 렌즈가 고정된 초점 거리만 제공할 수 있다는 주요 단점이 있습니다. 이러한 제한은 AR의 거리 인식 오류의 주요 요인 중 하나로 의심됩니다. 본 논문에서는 이러한 인식 문제를 해결하기 위해 새로운 종류의 AR 디스플레이인 망막 프로젝션 디스플레이(RPD)를 사용하는 방법을 연구했습니다. RPD를 사용하면 가상 이미지에 초점 거리가 없으며 AR 콘텐츠에 항상 초점이 맞춰집니다. 우리는 망막 프로젝션 디스플레이를 사용하여 관찰자의 자기중심적 거리 인식을 평가하는 첫 번째 보고된 실험을 수행했습니다. 우리는 OST HMD 또는 RPD에 의해 표시되는 실제 표적과 가상 표적 간의 깊이 추정의 정밀도와 정확성을 비교했습니다. 흥미롭게도 우리의 결과는 RPD가 OST HMD에 비해 AR에서 실제 깊이에 더 가까운 깊이 추정을 제공한다는 것을 보여줍니다. 실제로 OST 장치를 사용하면 인지된 거리가 16% 과대평가되는 것으로 나타났으나, RPD를 사용하면 거리 과대평가 편향이 4%로 감소했습니다. 게다가 작업의 난이도는 동일하고 정확도에는 차이가 없는 것으로 보고되었습니다. 따라서 우리의 결과는 증강 현실에서 사용자의 인식 측면에서 망막 프로젝션 디스플레이의 이점을 처음으로 밝혀 주었으며, 이는 RPD가 정확한 거리 인식이 요구되는 AR 애플리케이션에 유망한 기술임을 시사합니다.",https://doi.org/10.1109/ISMAR50242.2020.00028,Display & Optics; Perception & Cognition,Computer Vision,Quantitative Experiment,Hardware / Device
576,2020,CatARact: Simulating Cataracts in Augmented Reality,CatARact: 증강 현실에서 백내장 시뮬레이션,"For our society to be more inclusive and accessible, the more than 2.2 billion people worldwide with limited vision should be considered more frequently in design decisions, such as architectural planning. To help architects in evaluating their designs and give medical personnel some insight on how patients experience cataracts, we worked with ophthalmologists to develop the first medically-informed, pilot-studied simulation of cataracts in eye-tracked augmented reality (AR). To test our methodology and simulation, we conducted a pilot study with cataract patients between surgeries of their two cataract-affected eyes. Participants compared the vision of their corrected eye, viewing through simulated cataracts, to that of their still affected eye, viewing an unmodified AR view. In addition, we conducted remote experiments via video call, live adjusting our simulation and comparing it to related work, with participants who had cataract surgery a few months before. We present our findings and insights from these experiments and outline avenues for future work.","우리 사회가 더욱 포용적이고 접근 가능해지기 위해서는 건축 계획과 같은 디자인 결정에서 시력이 제한된 전 세계 22억 명 이상의 사람들을 더 자주 고려해야 합니다. 건축가가 설계를 평가하는 데 도움을 주고 의료진에게 환자가 백내장을 경험하는 방식에 대한 통찰력을 제공하기 위해 우리는 안과의사와 협력하여 안구 추적 증강 현실(AR)에서 최초의 의학 정보를 바탕으로 파일럿 연구를 통해 백내장 시뮬레이션을 개발했습니다. 우리의 방법론과 시뮬레이션을 테스트하기 위해 우리는 백내장에 걸린 두 눈의 수술 사이에 백내장 환자를 대상으로 파일럿 연구를 수행했습니다. 참가자들은 시뮬레이션된 백내장을 통해 보는 교정된 눈의 시력과 수정되지 않은 AR 보기를 보는 여전히 영향을 받는 눈의 시력을 비교했습니다. 또한, 몇 달 전에 백내장 수술을 받은 참가자들을 대상으로 화상 통화를 통해 원격 실험을 진행하고 시뮬레이션을 실시간으로 조정하며 관련 작업과 비교했습니다. 우리는 이러한 실험에서 얻은 결과와 통찰력을 제시하고 향후 작업을 위한 방법을 간략하게 설명합니다.",https://doi.org/10.1109/ISMAR50242.2020.00098,Medical & Healthcare,Sensor Fusion,Simulation,Algorithm / Method
577,2020,CollaboVR: A Reconfigurable Framework for Creative Collaboration in Virtual Reality,CollaboVR: 가상 현실에서 창의적인 협업을 위한 재구성 가능한 프레임워크,"Writing or sketching on whiteboards is an essential part of collaborative discussions in business meetings, reading groups, design sessions, and interviews. However, prior work in collaborative virtual reality (VR) systems has rarely explored the design space of multi-user layouts and interaction modes with virtual whiteboards. In this paper, we present CollaboVR, a reconfigurable framework for both co-located and geographically dispersed multi-user communication in VR. Our system unleashes users’ creativity by sharing freehand drawings, converting 2D sketches into 3D models, and generating procedural animations in real-time. To minimize the computational expense for VR clients, we leverage a cloud architecture in which the computational expensive application (Chalktalk) is hosted directly on the servers, with results being simultaneously streamed to clients. We have explored three custom layouts – integrated, mirrored, and projective – to reduce visual clutter, increase eye contact, or adapt different use cases. To evaluate CollaboVR, we conducted a within-subject user study with 12 participants. Our findings reveal that users appreciate the custom configurations and real-time interactions provided by CollaboVR. We have open sourced CollaboVR at https://github.com/snowymo/CollaboVR to facilitate future research and development of natural user interfaces and real-time collaborative systems in virtual and augmented reality.","화이트보드에 글을 쓰거나 스케치하는 것은 비즈니스 회의, 독서 그룹, 디자인 세션 및 인터뷰에서 공동 토론의 필수적인 부분입니다. 그러나 협업 가상 현실(VR) 시스템의 이전 작업에서는 가상 화이트보드를 사용한 다중 사용자 레이아웃 및 상호 작용 모드의 디자인 공간을 거의 탐색하지 않았습니다. 본 논문에서는 VR에서 같은 위치에 있거나 지리적으로 분산된 다중 사용자 통신을 위한 재구성 가능한 프레임워크인 CollaboVR을 제시합니다. 우리 시스템은 자유형 도면을 공유하고, 2D 스케치를 3D 모델로 변환하고, 절차적 애니메이션을 실시간으로 생성함으로써 사용자의 창의성을 불러일으킵니다. VR 클라이언트의 컴퓨팅 비용을 최소화하기 위해 우리는 컴퓨팅 비용이 많이 드는 애플리케이션(Chalktalk)이 서버에서 직접 호스팅되고 결과가 클라이언트에 동시에 스트리밍되는 클라우드 아키텍처를 활용합니다. 우리는 시각적 혼란을 줄이고, 눈맞춤을 늘리거나, 다양한 사용 사례를 적용하기 위해 통합형, 미러링형, 투영형 등 세 가지 사용자 정의 레이아웃을 탐색했습니다. CollaboVR을 평가하기 위해 우리는 12명의 참가자를 대상으로 피험자 내 사용자 연구를 실시했습니다. 연구 결과에 따르면 사용자는 CollaboVR이 제공하는 맞춤형 구성과 실시간 상호 작용을 높이 평가하는 것으로 나타났습니다. 우리는 가상 및 증강 현실에서 자연스러운 사용자 인터페이스와 실시간 협업 시스템에 대한 향후 연구 및 개발을 촉진하기 위해 https://github.com/snowymo/CollaboVR에서 CollaboVR을 오픈 소스로 제공했습니다.",https://doi.org/10.1109/ISMAR50242.2020.00082,Collaboration & Social; Interaction & Input,Sensor Fusion,User Study,System / Framework; User Study / Empirical Findings
578,2020,Collaborative Augmented Reality on Smartphones via Life-long City-scale Maps,평생 도시 규모의 지도를 통한 스마트폰의 협업 증강 현실,"In this paper we present the first published end-to-end production computer-vision system for powering city-scale shared augmented reality experiences on mobile devices. In doing so we propose a new formulation for an experience-based mapping framework as an effective solution to the key issues of city-scale SLAM scalability, robustness, map updates and all-time all-weather performance required by a production system. Furthermore, we propose an effective way of synchronising SLAM systems to deliver seamless real-time localisation of multiple edge devices at the same time. All this in the presence of network latency and bandwidth limitations. The resulting system is deployed and tested at scale in San Francisco where it delivers AR experiences in a mapped area of several hundred kilometers. To foster further development of this area we offer the data set to the public, constituting the largest of this kind to date","이 논문에서는 모바일 장치에서 도시 규모의 공유 증강 현실 경험을 지원하기 위해 최초로 공개된 엔드투엔드 프로덕션 컴퓨터 비전 시스템을 제시합니다. 이를 통해 우리는 생산 시스템에 필요한 도시 규모의 SLAM 확장성, 견고성, 지도 업데이트 및 전천후 성능과 같은 주요 문제에 대한 효과적인 솔루션으로서 경험 기반 매핑 프레임워크에 대한 새로운 공식을 제안합니다. 또한, 우리는 동시에 여러 에지 장치의 원활한 실시간 위치 파악을 제공하기 위해 SLAM 시스템을 동기화하는 효과적인 방법을 제안합니다. 이 모든 것은 네트워크 대기 시간과 대역폭 제한이 있는 상황에서 발생합니다. 결과 시스템은 샌프란시스코에서 대규모로 배포 및 테스트되어 수백 킬로미터에 달하는 매핑된 영역에서 AR 경험을 제공합니다. 이 분야의 추가 개발을 촉진하기 위해 우리는 현재까지 이러한 종류의 데이터 세트 중 가장 큰 데이터 세트를 대중에게 제공합니다.",https://doi.org/10.1109/ISMAR50242.2020.00081,Collaboration & Social,Cloud / Edge Computing,Quantitative Experiment,System / Framework
579,2020,Color Moiré Reduction and Resolution Improvement for Integral 3D Displays Using Multiple Wobbling Optics,다중 워블링 광학 장치를 사용한 일체형 3D 디스플레이의 색상 모아레 감소 및 해상도 개선,"The integral three-dimensional (3D) display is an ideal visual 3D user interface. It is a display method that fulfills many of the physiological factors of human vision. However, in integral 3D displays for mobile applications that use direct-view flat panels to display elemental images, color moiré is a problem that occurs because of the sampling of subpixels by elemental lenses and the insufficient resolution and depth reproduction of the reconstructed 3D image. In the conventional moiré reduction method, the degree of defocus of elemental lenses has to be set to a large value, which is one of the factors that reduces the performance in terms of depth reproduction. In contrast, only one-step optics can be installed and the installation positions are limited in the conventional wobbling method. This is because, in the method, which uses a birefringent optical element, two-step optics are thicker than the focal length of the lens array. For this reason, it was difficult to achieve ideal moiré reduction and depth reproduction performance improvements. To solve these problems, we propose a method that utilizes multiple optical wobbling spatiotemporal multiplexing using polarization diffractive elements and liquid-crystal polarization controllers. Using the proposed method, the wobbling optics can be designed to be thin, allowing two-step optics to be installed between the display panel and lens array. When the moiré modulation degree without wobbling is normalized as 100%, it decreases to 25% with wobbling. The proposed method not only achieves effective color moiré reduction without deteriorating the 3D image quality, but also can double the resolution of the elemental images to improve the depth reproduction.","일체형 3차원(3D) 디스플레이는 이상적인 시각적 3D 사용자 인터페이스입니다. 이는 인간 시각의 생리학적 요소 중 많은 부분을 충족시키는 디스플레이 방식입니다. 그러나 직시형 평면 패널을 사용하여 요소 이미지를 표시하는 모바일 애플리케이션용 일체형 3D 디스플레이에서는 요소 렌즈에 의한 하위 픽셀 샘플링과 재구성된 3D 이미지의 해상도 및 깊이 재현이 부족하여 컬러 모아레가 발생하는 문제가 있습니다. 기존의 모아레 감소 방식에서는 요소렌즈의 디포커스(defocus) 정도를 크게 설정해야 하는데, 이는 심도 재현 측면에서 성능을 저하시키는 요인 중 하나이다. 이에 반해 기존 워블링 방식은 원스텝 광학장치만 설치할 수 있고 설치 위치도 제한된다. 이는 복굴절 광학소자를 사용하는 방식에서는 2단 광학계가 렌즈 어레이의 초점거리보다 두껍기 때문이다. 이 때문에 이상적인 모아레 감소와 깊이 재현 성능 향상이 어려웠습니다. 이러한 문제를 해결하기 위해 우리는 편광 회절 소자와 액정 편광 제어기를 사용하여 다중 광 워블링 시공간 다중화를 활용하는 방법을 제안합니다. 제안된 방법을 사용하면 워블링 광학계를 얇게 설계할 수 있어 디스플레이 패널과 렌즈 어레이 사이에 2단계 광학계를 설치할 수 있다. 워블링이 없는 모아레 변조도를 100%로 정규화하면 워블링이 있는 경우에는 25%로 감소합니다. 제안된 방법은 3D 영상의 품질 저하 없이 효과적인 색상 모아레 감소를 달성할 수 있을 뿐만 아니라, 요소 영상의 해상도를 두 배로 높여 깊이 재현을 향상시킬 수 있습니다.",https://doi.org/10.1109/ISMAR50242.2020.00031,Interaction & Input; Display & Optics,Optical / Display Technology,Other,Hardware / Device
580,2020,Comparing World and Screen Coordinate Systems in Optical See-Through Head-Mounted Displays for Text Readability while Walking,걷는 동안 텍스트 가독성을 위한 광학 투명 헤드 장착 디스플레이의 세계 및 화면 좌표계 비교,"Augmented reality (AR) optical-see-through (OST) head-mounted displays (HMD) have developed to a point where browsing information on the go is possible. In this paper, we investigate the readability of text on an AR HMD while the user is walking. There are two common methods of displaying text on a HMD: anchoring the text on the screen coordinate system or the world coordinate system. We report on the results of two laboratory experiments comparing text readability when the text is displayed in these two coordinate systems, and while the participants walked on a treadmill. In the first experiment, the participants read letter strings comprising Sloane letters, whereas the second experiment used English words. In addition to evaluating the text readability and workload experienced by participants, we employed IMU sensors to compare the effects of the text display method on the participants’ head movement and gait. In both experiments, the reading speed and head movement were significantly higher and mental workload significantly lower for the world coordinate system than for the screen coordinate system. These results suggest that text readability while walking is better on the world coordinate system, and displaying text with the screen coordinate system results in an unnatural gait owing to the user trying to keep their head still in an effort to stabilize the HMD screen.","증강현실(AR) 광학투시(OST) 헤드마운트디스플레이(HMD)는 이동 중에도 정보 검색이 가능한 수준으로 발전했다. 본 논문에서는 사용자가 걷는 동안 AR HMD에서 텍스트의 가독성을 조사합니다. HMD에 텍스트를 표시하는 일반적인 방법에는 화면 좌표계 또는 월드 좌표계에 텍스트를 고정하는 두 가지 방법이 있습니다. 우리는 텍스트가 이 두 좌표계에 표시될 때와 참가자가 런닝머신을 걷는 동안 텍스트 가독성을 비교하는 두 가지 실험실 실험의 결과를 보고합니다. 첫 번째 실험에서 참가자들은 슬론 문자로 구성된 문자열을 읽었고, 두 번째 실험에서는 영어 단어를 사용했습니다. 참가자가 경험하는 텍스트 가독성과 작업량을 평가하는 것 외에도 IMU 센서를 사용하여 텍스트 표시 방법이 참가자의 머리 움직임과 보행에 미치는 영향을 비교했습니다. 두 실험 모두 화면 좌표계보다 세계 좌표계에서 읽기 속도와 머리 움직임이 상당히 높았고 정신 작업량은 현저히 낮았습니다. 이러한 결과는 걷는 동안 텍스트 가독성이 세계 좌표계에서 더 우수함을 시사하며, 화면 좌표계로 텍스트를 표시하면 사용자가 HMD 화면을 안정화하기 위해 머리를 가만히 유지하려고 하기 때문에 부자연스러운 보행이 발생합니다.",https://doi.org/10.1109/ISMAR50242.2020.00093,Display & Optics,Sensor Fusion,User Study,Hardware / Device
581,2020,Determining Detection Thresholds for Fixed Positional Offsets for Virtual Hand Remapping in Virtual Reality,가상 현실에서 가상 손 재매핑을 위한 고정 위치 오프셋에 대한 감지 임계값 결정,"Virtual reality commonly makes use of tracked hand interactions for user input. Interaction techniques sometimes alter the mapping between the real and virtual coordinate systems to modify interaction possibilities. This paper studies fixed positional offsets applied to the location of the virtual hand. We present a controlled experiment in which users’ hands were subject to fixed positional offsets of varying magnitudes while completing target-touching tasks. The study provides estimations for detection thresholds for positional hand offsets in six directions relative to the real-world location of the hand and provides evidence performance using offset virtual hands can vary based on offset parameters. Significant differences in offset detection were identified based on offset direction, indicating that positional adjustments made to virtual hands should consider directionality when limiting techniques rather than just a constant value. Hand offsets kept within the threshold value resulted in comparable performance to unmodified hand registration, while offsets beyond the threshold resulted in larger completion times.","가상 현실은 일반적으로 사용자 입력을 위해 추적된 손 상호 작용을 사용합니다. 상호 작용 기술은 때때로 상호 작용 가능성을 수정하기 위해 실제 좌표계와 가상 좌표계 간의 매핑을 변경합니다. 본 논문에서는 가상 손의 위치에 적용되는 고정된 위치 오프셋을 연구합니다. 우리는 대상 접촉 작업을 완료하는 동안 사용자의 손이 다양한 크기의 고정 위치 오프셋을 받는 통제된 실험을 제시합니다. 이 연구는 실제 손 위치를 기준으로 6개 방향의 위치 손 오프셋에 대한 감지 임계값 추정치를 제공하고 오프셋 가상 손을 사용하는 성능이 오프셋 매개변수에 따라 달라질 수 있다는 증거를 제공합니다. 오프셋 감지의 중요한 차이는 오프셋 방향을 기반으로 식별되었으며, 이는 가상 손에 대한 위치 조정이 기술을 제한할 때 단지 상수 값이 아닌 방향성을 고려해야 함을 나타냅니다. 임계값 내에서 유지된 손 오프셋은 수정되지 않은 손 등록과 유사한 성능을 제공하는 반면 임계값을 초과하는 오프셋은 완료 시간이 더 길어졌습니다.",https://doi.org/10.1109/ISMAR50242.2020.00050,Interaction & Input,Optical / Display Technology,User Study,Algorithm / Method
582,2020,Digital Full-Face Mask Display with Expression Recognition using Embedded Photo Reflective Sensor Arrays,내장된 광반사 센서 어레이를 사용한 표정 인식 기능을 갖춘 디지털 전면 마스크 디스플레이,"This paper presents a thin digital full-face mask display that can reflect an entire facial expression of a user onto an avatar to support augmented face-to-face communication in real environments. Although camera-based facial expression recognition technology has enabled people to augment their faces with avatars, application was limited to face-to-face communication in virtual environments. To enable digital facial augmentation with an avatar in a real space, we propose a digital face mask display system that integrates a lightweight flexible display with a thin facial expression recognition system. The thin wearable facial expression recognition system was implemented with photo reflective sensor arrays which can measure facial expressions at 40 feature points distributed across an entire face. We investigated a ten-class facial expression identification model based on an SVM training algorithm. The trained model achieved an average accuracy of 79% when identifying the facial expressions of multiple users. User experiments indicated that the proposed thin digital full-face mask display allows the wearer to control the facial expression of the avatar with a fast response rate and create a positive sense of self-agency and self-ownership toward the augmented avatar face.","본 논문은 실제 환경에서 증강 대면 커뮤니케이션을 지원하기 위해 사용자의 얼굴 표정 전체를 아바타에 반영할 수 있는 얇은 디지털 전면 마스크 디스플레이를 제시한다. 카메라 기반 얼굴 표정 인식 기술을 통해 아바타로 얼굴을 확대할 수 있게 되었지만, 가상 환경에서의 대면 커뮤니케이션에만 적용이 제한되었습니다. 실제 공간에서 아바타를 이용한 디지털 얼굴 증강이 가능하도록 경량의 플렉서블 디스플레이와 얇은 얼굴 표정 인식 시스템을 통합한 디지털 얼굴 마스크 디스플레이 시스템을 제안한다. 얇은 웨어러블 얼굴 표정 인식 시스템은 얼굴 전체에 분포된 40개의 특징점에서 얼굴 표정을 측정할 수 있는 광 반사 센서 어레이로 구현되었습니다. 우리는 SVM 훈련 알고리즘을 기반으로 10가지 클래스의 얼굴 표정 식별 모델을 조사했습니다. 훈련된 모델은 여러 사용자의 얼굴 표정을 식별할 때 평균 79%의 정확도를 달성했습니다. 사용자 실험에 따르면 제안된 얇은 디지털 전면 마스크 디스플레이를 사용하면 착용자가 빠른 응답 속도로 아바타의 얼굴 표정을 제어할 수 있고 증강된 아바타 얼굴에 대한 긍정적인 자기 주체성과 자기 소유권을 창출할 수 있는 것으로 나타났습니다.",https://doi.org/10.1109/ISMAR50242.2020.00030,Display & Optics,Optical / Display Technology,Quantitative Experiment,Algorithm / Method; Hardware / Device
583,2020,ElaMorph Projection: Deformation of 3D Shape by Dynamic Projection Mapping,ElaMorph 투영: 동적 투영 매핑을 통한 3차원 형상 변형,"We propose a projector-based method that provides an illusion of geometry change, similar to that caused by a change in physical properties such as elasticity, in response to inertia caused by physical motion. The proposed method is named “ElaMorph projection.” Although several projection mapping methods capable of deforming targets have been previously proposed, these methods require the preparation of animations in advance. Moreover, these methods are unable to deform the shape in real-time according to actual movements of the object. To address these issues, we perform real-time geometry deformation and rendering based on the spatial motion of the object. To render the projection image, we extend the conventional method of deformation for 2D pictures or static 3D objects to include dynamic 3D objects. This study involves projection onto a dynamic 3D object; however, the projection quality decreases if a part of the rendered image extends beyond the projection target. To address this issue, the proposed algorithm ensures that the vertices after deformation always remain within the projection target. In addition, we develop a robust algorithm to generate projection images under dynamic illuminative conditions, through real-time estimation of the environmental lighting required for rendering. Moreover, using an elasticity map that can be easily constructed using a UV map, our method enables users to specify the vertices to be deformed, using an elasticity map. We present projections under several different sets of elasticity maps, environmental lighting, and elasticities. Finally, we evaluate the latency and throughput of our system.","우리는 물리적 움직임으로 인한 관성에 반응하여 탄성과 같은 물리적 특성의 변화로 인한 것과 유사한 기하학적 변화의 환상을 제공하는 프로젝터 기반 방법을 제안합니다. 제안하는 방법의 이름은 'ElaMorph 투영'이다. 이전에도 대상을 변형할 수 있는 여러 가지 프로젝션 매핑 방법이 제안되었지만 이러한 방법을 사용하려면 사전에 애니메이션을 준비해야 합니다. 더욱이 이러한 방법은 물체의 실제 움직임에 따라 실시간으로 형태를 변형시킬 수 없습니다. 이러한 문제를 해결하기 위해 우리는 객체의 공간적 움직임을 기반으로 실시간 기하 변형 및 렌더링을 수행합니다. 투영 이미지를 렌더링하기 위해 2D 그림이나 정적 3D 개체에 대한 기존 변형 방법을 확장하여 동적 3D 개체를 포함합니다. 이 연구에는 동적 3D 개체에 대한 투영이 포함됩니다. 그러나 렌더링된 이미지의 일부가 투영 대상을 넘어 확장되면 투영 품질이 저하됩니다. 이 문제를 해결하기 위해 제안된 알고리즘은 변형 후 정점이 항상 투영 대상 내에 유지되도록 보장합니다. 또한 렌더링에 필요한 환경 조명의 실시간 추정을 통해 동적 조명 조건에서 투영 이미지를 생성하는 강력한 알고리즘을 개발합니다. 또한, UV 맵을 이용하여 쉽게 구성할 수 있는 탄성 맵을 이용하여 사용자가 탄성 맵을 이용하여 변형할 정점을 지정할 수 있다. 우리는 다양한 탄력성 맵, 환경 조명 및 탄력성에 대한 예측을 제시합니다. 마지막으로 시스템의 대기 시간과 처리량을 평가합니다.",https://doi.org/10.1109/ISMAR50242.2020.00038,Rendering & Visualization,Optical / Display Technology,Quantitative Experiment,Algorithm / Method
584,2020,Enhancing First-Person View Task Instruction Videos with Augmented Reality Cues,증강 현실 단서를 통해 1인칭 시점 작업 지침 비디오 향상,"This research investigates enhancing first-person view (FPV) task instruction videos by applying Augmented Reality (AR) visualisation of spatial cues. With personal mobile devices, recording and sharing a video clip has become very easy, and how-to videos are becoming popular on social video sharing services. Instructional videos are actively used not only in formal education and training, but also in everyday life. However, video clips are limited to two-dimensional representation of the task space, making it hard for the viewer to follow and match the objects in the video to those in the real world task space. We propose augmenting task instruction videos with AR visualisation of spatial cues to overcome this problem, focusing on creating and viewing FPV instruction videos. We designed and implemented a prototype system, AR Tips, which allows users to capture and share augmented FPV instruction videos on a wearable AR device. We conducted a user study to evaluate the benefit of our approach, and the results showed that with the help of augmented spatial cues users better understood the instructions, performed the tasks faster with fewer errors, and had lower mental effort.","이 연구에서는 공간 단서의 증강 현실(AR) 시각화를 적용하여 1인칭 시점(FPV) 작업 지침 비디오를 향상시키는 방법을 조사합니다. 개인용 모바일 장치를 사용하면 비디오 클립을 녹화하고 공유하는 것이 매우 쉬워졌으며 소셜 비디오 공유 서비스에서 사용법 비디오가 인기를 얻고 있습니다. 교육용 비디오는 정규 교육 및 훈련뿐만 아니라 일상 생활에서도 적극적으로 활용됩니다. 그러나 비디오 클립은 작업 공간의 2차원 표현으로 제한되어 시청자가 비디오의 개체를 실제 작업 공간의 개체와 일치시키고 따라가기가 어렵습니다. 우리는 이 문제를 극복하기 위해 공간 단서의 AR 시각화를 통해 작업 지침 비디오를 보강하고 FPV 지침 비디오를 만들고 보는 데 중점을 둘 것을 제안합니다. 우리는 사용자가 웨어러블 AR 장치에서 증강 FPV 지침 비디오를 캡처하고 공유할 수 있는 프로토타입 시스템인 AR 팁을 설계하고 구현했습니다. 우리는 접근 방식의 이점을 평가하기 위해 사용자 연구를 실시했으며, 그 결과 증강된 공간 단서의 도움으로 사용자가 지침을 더 잘 이해하고 오류를 줄이며 작업을 더 빠르게 수행하고 정신적 노력을 덜 쏟는 것으로 나타났습니다.",https://doi.org/10.1109/ISMAR50242.2020.00078,Education & Training,Other,User Study,Hardware / Device
585,2020,Enhancing Participation Experience in VR Live Concerts by Improving Motions of Virtual Audience Avatars,가상 관객 아바타의 모션을 개선하여 VR 라이브 콘서트 참여 경험 향상,"While participating in live concerts is a promising application of virtual reality (VR), it falls short of our participation experience in the real world. In particular, to increase the engagement of participants, previous studies emphasized the importance of social experience among audience members, such as the sense of co-presence elicited by sharing physical reactions or body movements synchronized with music. In this respect, a common strategy in existing platforms is to present avatars of remote human participants in a VR venue and make every avatar imitate movements of the corresponding participant. However, this strategy implicitly assumes that a not small number of users connect simultaneously to watch the same content and thus is not applicable when only a few users gather or a user is watching alone. Therefore, with the aim of providing better experience to a user who participates in live concerts as one of the audience, we examine computational approaches to enhancing the sense of co-presence through virtual audience avatars. We propose four methods of presenting avatar movements: copying the user’s own movements, copying other users’ movements, repeating beat-synchronous movements, and synthesizing machine-learning-based movements. We compare their effectiveness in a user experiment and discuss application scenarios and design implications that open up new ways of active media consumption in VR environments.","라이브 콘서트에 참여하는 것은 가상 현실(VR)의 유망한 응용 프로그램이지만 현실 세계에서의 참여 경험에는 미치지 못합니다. 특히, 이전 연구에서는 참가자의 참여도를 높이기 위해 신체 반응이나 음악에 동기화된 신체 움직임을 공유함으로써 유발되는 공존감과 같은 청중 간의 사회적 경험의 중요성을 강조했습니다. 이러한 측면에서 기존 플랫폼의 일반적인 전략은 원격 참가자의 아바타를 VR 장소에 제시하고 모든 아바타가 해당 참가자의 움직임을 모방하도록 만드는 것입니다. 그러나 이 전략은 동일한 콘텐츠를 시청하기 위해 적지 않은 수의 사용자가 동시에 접속한다는 것을 암묵적으로 가정하므로 소수의 사용자만 모이거나 사용자가 혼자 시청하는 경우에는 적용할 수 없습니다. 따라서 라이브 콘서트에 관객의 한 사람으로서 참여하는 사용자에게 더 나은 경험을 제공하기 위해 가상 관객 아바타를 통해 공존감을 높이는 컴퓨팅 접근 방식을 검토합니다. 아바타의 움직임 표현 방법으로는 사용자 자신의 움직임 복사, 다른 사용자의 움직임 복사, 비트 동기 움직임 반복, 머신러닝 기반 움직임 합성 등 4가지 방법을 제안한다. 우리는 사용자 실험에서 그 효과를 비교하고 VR 환경에서 능동적인 미디어 소비의 새로운 방식을 여는 응용 시나리오와 디자인 의미에 대해 논의합니다.",https://doi.org/10.1109/ISMAR50242.2020.00083,Perception & Cognition; Collaboration & Social,Sensor Fusion,User Study,Algorithm / Method
586,2020,Enhancing Visitor Experience or Hindering Docent Roles: Attentional Issues in Augmented Reality Supported Installations,방문자 경험 향상 또는 도슨트 역할 방해: 증강 현실 지원 설치의 주의 문제,"Studies using augmented reality (AR) technology have suggested that users focus excessively on the virtual content in the AR environment at the expense of the physical world around them. This has implications related to the design of installations that aim to incorporate the user’s physical environment as part of the AR experience. To better understand how user attention is managed in an AR environment, we present an observational study of Rewild Our Planet, a multi-modal installation that combined video, audio, a human docent and mobile AR to promote awareness about environmental issues. We found that, while AR was successful in engaging visitors, it drew attention away from other modalities within the installation. This impacts the work of the human docent and affects how visitors absorb information presented in the installation. Based on these observations, we present guidelines to inform the design of future AR-supported installations with the aim of minimizing or taking advantage of the observed attentional issues.","증강 현실(AR) 기술을 사용한 연구에 따르면 사용자는 주변의 물리적 세계를 희생하면서 AR 환경의 가상 콘텐츠에 과도하게 집중하는 것으로 나타났습니다. 이는 사용자의 물리적 환경을 AR 경험의 일부로 통합하려는 설치 디자인과 관련된 의미를 갖습니다. AR 환경에서 사용자 주의가 어떻게 관리되는지 더 잘 이해하기 위해 환경 문제에 대한 인식을 높이기 위해 비디오, 오디오, 인간 도슨트 및 모바일 AR을 결합한 다중 모드 설치물인 Rewild Our Planet에 대한 관찰 연구를 제시합니다. 우리는 AR이 방문객의 참여를 유도하는 데는 성공했지만 설치물 내의 다른 방식에서는 관심을 끌었다는 사실을 발견했습니다. 이는 인간 도슨트의 작업에 영향을 미치며 방문자가 설치물에 제시된 정보를 흡수하는 방식에 영향을 미칩니다. 이러한 관찰을 바탕으로 우리는 관찰된 주의 문제를 최소화하거나 활용하려는 목적으로 향후 AR 지원 설치의 설계를 알리는 지침을 제시합니다.",https://doi.org/10.1109/ISMAR50242.2020.00053,Perception & Cognition,Other,Qualitative Analysis,Other
587,2020,Evaluating Mixed and Augmented Reality: A Systematic Literature Review (2009-2019),혼합 및 증강 현실 평가: 체계적인 문헌 검토(2009-2019),"We present a systematic review of 45S papers that report on evaluations in mixed and augmented reality (MR/AR) published in ISMAR, CHI, IEEE VR, and UIST over a span of 11 years (2009-2019). Our goal is to provide guidance for future evaluations of MR/AR approaches. To this end, we characterize publications by paper type (e.g., technique, design study), research topic (e.g., tracking, rendering), evaluation scenario (e.g., algorithm performance, user performance), cognitive aspects (e.g., perception, emotion), and the context in which evaluations were conducted (e.g., lab vs. in-thewild). We found a strong coupling of types, topics, and scenarios. We observe two groups: (a) technology-centric performance evaluations of algorithms that focus on improving tracking, displays, reconstruction, rendering, and calibration, and (b) human-centric studies that analyze implications of applications and design, human factors on perception, usability, decision making, emotion, and attention. Amongst the 458 papers, we identified 248 user studies that involved 5,761 participants in total, of whom only 1,619 were identified as female. We identified 43 data collection methods used to analyze 10 cognitive aspects. We found nine objective methods, and eight methods that support qualitative analysis. A majority (216/248) of user studies are conducted in a laboratory setting. Often (138/248), such studies involve participants in a static way. However, we also found a fair number (30/248) of in-the-wild studies that involve participants in a mobile fashion. We consider this paper to be relevant to academia and industry alike in presenting the state-of-the-art and guiding the steps to designing, conducting, and analyzing results of evaluations in MR/AR.","우리는 11년(2009-2019)에 걸쳐 ISMAR, CHI, IEEE VR 및 UIST에 출판된 혼합 및 증강 현실(MR/AR) 평가에 대해 보고하는 45S 논문에 대한 체계적인 검토를 제시합니다. 우리의 목표는 MR/AR 접근 방식의 향후 평가에 대한 지침을 제공하는 것입니다. 이를 위해 우리는 논문 유형(예: 기술, 디자인 연구), 연구 주제(예: 추적, 렌더링), 평가 시나리오(예: 알고리즘 성능, 사용자 성능), 인지적 측면(예: 인식, 감정) 및 평가가 수행된 맥락(예: 실험실 대 실제 환경)을 기준으로 출판물을 특성화합니다. 우리는 유형, 주제 및 시나리오가 강력하게 결합되어 있음을 발견했습니다. 우리는 (a) 추적, 디스플레이, 재구성, 렌더링 및 교정 개선에 초점을 맞춘 알고리즘의 기술 중심 성능 평가와 (b) 응용 프로그램 및 디자인의 의미, 인식, 유용성, 의사 결정, 감정 및 주의에 대한 인간 요소를 분석하는 인간 중심 연구라는 두 그룹을 관찰합니다. 458개의 논문 중에서 우리는 총 5,761명의 참가자가 참여한 248개의 사용자 연구를 식별했으며 그 중 1,619명만이 여성으로 확인되었습니다. 우리는 10가지 인지 측면을 분석하는 데 사용되는 43가지 데이터 수집 방법을 확인했습니다. 우리는 9가지 객관적인 방법과 정성적 분석을 지원하는 8가지 방법을 발견했습니다. 사용자 연구의 대부분(216/248)은 실험실 환경에서 수행됩니다. 종종(138/248) 이러한 연구에는 참가자가 정적인 방식으로 참여합니다. 그러나 우리는 또한 모바일 방식으로 참가자를 포함하는 실제 연구의 상당수(30/248)를 발견했습니다. 우리는 이 논문이 최첨단 기술을 제시하고 MR/AR의 평가 결과를 설계, 수행 및 분석하는 단계를 안내하는 데 있어 학계 및 업계 모두와 관련이 있다고 생각합니다.",https://doi.org/10.1109/ISMAR50242.2020.00069,Perception & Cognition,Other,Literature Review / Meta-analysis,Algorithm / Method; Survey / Review
588,2020,Evaluating Remote Virtual Hands Models on Social Presence in Hand-based 3D Remote Collaboration,손 기반 3D 원격 협업의 사회적 존재에 대한 원격 가상 손 모델 평가,"This study investigates the effects of a virtual hand representation on the user experience including social presence during hand-based 3D remote collaboration. Although a remote hand appearance is a critical parts of a hand-based telepresence, it has been rarely studied in comparison to studies on the self-embodiment of virtual hands in a 3D environment. Thus, we conducted a user study comparing the three virtual hands models (Skeleton, Low Polygon and Realistic) while performing a remote collaborative task based on the American Sign Language (ASL) using both Augmented Reality (AR) and Virtual Reality (VR) environments. We found that the realistic type was perceived as the most sense of being together, human-like, and trustable representation. The low polygon model could also convey a clear sign and moderate level of social presence. Although the system was configured asymmetrically in AR and VR, little difference in perception was found except for the participant’s mental load and message understanding. We then discuss the results and suggest design implications for future hand-based 3D telepresence systems.","본 연구에서는 손 기반 3D 원격 협업 중 사회적 존재감을 포함한 사용자 경험에 대한 가상 손 표현의 효과를 조사합니다. 원격 손 모양은 손 기반 텔레프레즌스의 중요한 부분이지만 3D 환경에서 가상 손의 자기 구현에 대한 연구와 비교하여 거의 연구되지 않았습니다. 이에 증강 현실(AR)과 가상 현실(VR) 환경을 모두 사용하여 미국 수화(ASL) 기반의 원격 협업 작업을 수행하면서 세 가지 가상 손 모델(Skeleton, Low Polygon 및 Realistic)을 비교하는 사용자 연구를 수행했습니다. 사실적 유형은 가장 함께 있음을 느끼고, 인간적이며, 신뢰할 수 있는 표현으로 인식되는 것으로 나타났다. 로우 폴리곤 모델은 명확한 표시와 적당한 수준의 사회적 존재감을 전달할 수도 있습니다. AR과 VR에서 시스템이 비대칭으로 구성되었음에도 불구하고, 참여자의 정신부하와 메시지 이해를 제외하면 인식의 차이는 거의 발견되지 않았다. 그런 다음 결과를 논의하고 미래의 수동 기반 3D 텔레프레즌스 시스템에 대한 설계 시사점을 제안합니다.",https://doi.org/10.1109/ISMAR50242.2020.00080,Collaboration & Social; Perception & Cognition,Hand / Gesture Recognition,User Study,User Study / Empirical Findings
589,2020,Exploration of Hands-free Text Entry Techniques For Virtual Reality,가상 현실을 위한 핸즈프리 텍스트 입력 기술 탐색,"Text entry is a common activity in virtual reality (VR) systems. There is a limited number of available hands-free techniques, which allow users to carry out text entry when users’ hands are busy such as holding items or hand-based devices are not available. The most used hands-free text entry technique is DwellType, where a user selects a letter by dwelling over it for a specific period. However, its performance is limited due to the fixed dwell time for each character selection. In this paper, we explore two other hands-free text entry mechanisms in VR: BlinkType and NeckType, which leverage users’ eye blinks and neck’s forward and backward movements to select letters. With a user study, we compare the performance of the two techniques with DwellType. Results show that users can achieve an average text entry rate of 13.47, 11.18 and 11.65 words per minute with BlinkType, NeckType, and DwellType, respectively. Users’ subjective feedback shows BlinkType as the preferred technique for text entry in VR. Index Terms: Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Virtual reality; Human-centered computing—Human computer interaction (HCI)—Interaction techniques—Text input","텍스트 입력은 가상 현실(VR) 시스템에서 일반적인 활동입니다. 물건을 들고 있거나 손에 기반을 둔 장치를 사용할 수 없는 등 사용자의 손이 바쁠 때 사용자가 텍스트 입력을 수행할 수 있도록 하는 사용 가능한 핸즈프리 기술의 수가 제한되어 있습니다. 가장 많이 사용되는 핸즈프리 텍스트 입력 기술은 DwellType으로, 사용자는 특정 기간 동안 문자 위에 체류하여 문자를 선택합니다. 하지만 각 캐릭터 선택마다 고정된 체류 시간으로 인해 성능이 제한됩니다. 본 논문에서는 VR의 두 가지 핸즈프리 텍스트 입력 메커니즘인 BlinkType과 NeckType을 살펴보겠습니다. 이 메커니즘은 사용자의 눈 깜박임과 목의 앞뒤 움직임을 활용하여 문자를 선택합니다. 사용자 연구를 통해 두 기술의 성능을 DwellType과 비교합니다. 결과는 사용자가 BlinkType, NeckType 및 DwellType을 사용하여 각각 분당 13.47, 11.18 및 11.65 단어의 평균 텍스트 입력 속도를 달성할 수 있음을 보여줍니다. 사용자의 주관적인 피드백은 BlinkType이 VR의 텍스트 입력에 선호되는 기술임을 보여줍니다. 색인 용어: 인간 중심 컴퓨팅 - 인간 컴퓨터 상호 작용(HCI) - 상호 작용 패러다임 - 가상 현실; 인간 중심 컴퓨팅 - 인간 컴퓨터 상호 작용(HCI) - 상호 작용 기술 - 텍스트 입력",https://doi.org/10.1109/ISMAR50242.2020.00061,Interaction & Input,Other,User Study,System / Framework
590,2020,Extracting Velocity-Based User-Tracking Features to Predict Learning Gains in a Virtual Reality Training Application,가상 현실 교육 애플리케이션에서 학습 성과를 예측하기 위한 속도 기반 사용자 추적 기능 추출,"Virtual Reality (VR) for training and education of real-world tasks has been researched extensively and has growing use in industry. The data generated by trainees in VR could be leveraged to improve the ability to evaluate learning beyond that which is possible in traditional training scenarios. In this paper, we present a machinelearning approach that is able to classify users into participants with low-learning (LL) and high-learning (HL) gains, based on a knowledge test, using only the linear and angular velocities of the head-mounted display (HMD) and handheld controllers. To collect this data, we conduct a VR training user study. We demonstrate that even with a limited data set, it is possible to train a machinelearning classifier to predict a trainee’s learning performance for a given task with high degrees of accuracy and confidence. We investigate three different sets of velocity-based input features and two feature representations in a machine learning experiment. Our results indicate that all feature combinations resulted in high degrees of accuracy and confidence for predicting learning gains in our testing data. By employing a novel visualization technique, we were able to determine that participants with HL gains moved with greater velocities and fewer changes in direction than those with LL gains. These results indicate that it may be feasible to create VR training applications that can predict a user’s learning gains and dynamically adapt the training to better support the user’s learning, based on commonly available tracking data.",실제 작업의 훈련 및 교육을 위한 가상 현실(VR)은 광범위하게 연구되어 왔으며 산업계에서 사용이 증가하고 있습니다. VR에서 훈련생이 생성한 데이터는 기존 훈련 시나리오에서 가능한 것 이상으로 학습을 평가하는 능력을 향상시키는 데 활용될 수 있습니다. 본 논문에서는 헤드 마운트 디스플레이(HMD)와 휴대용 컨트롤러의 선형 및 각속도만을 사용하여 지식 테스트를 기반으로 사용자를 저학습(LL) 이득과 고학습(HL) 참가자로 분류할 수 있는 머신러닝 접근 방식을 제시합니다. 이 데이터를 수집하기 위해 VR 교육 사용자 연구를 수행합니다. 우리는 제한된 데이터 세트를 사용하더라도 머신러닝 분류기를 훈련하여 주어진 작업에 대한 훈련생의 학습 성과를 높은 정확도와 신뢰도로 예측할 수 있음을 보여줍니다. 우리는 기계 학습 실험에서 속도 기반 입력 기능의 세 가지 세트와 두 가지 기능 표현을 조사합니다. 우리의 결과는 모든 기능 조합이 테스트 데이터에서 학습 이득을 예측하는 데 있어 높은 수준의 정확성과 신뢰도를 가져왔다는 것을 나타냅니다. 새로운 시각화 기술을 사용함으로써 우리는 HL 이득을 얻은 참가자가 LL 이득을 얻은 참가자보다 더 큰 속도로 움직이고 방향 변화가 적다는 것을 확인할 수 있었습니다. 이러한 결과는 일반적으로 사용 가능한 추적 데이터를 기반으로 사용자의 학습 향상을 예측하고 사용자의 학습을 더 잘 지원하기 위해 훈련을 동적으로 조정할 수 있는 VR 훈련 애플리케이션을 만드는 것이 실현 가능하다는 것을 나타냅니다.,https://doi.org/10.1109/ISMAR50242.2020.00099,Display & Optics; Education & Training,Optical / Display Technology,User Study,Algorithm / Method
591,2020,Face Commands - User-Defined Facial Gestures for Smart Glasses,얼굴 명령 - 스마트 안경을 위한 사용자 정의 얼굴 제스처,"We propose the use of face-related gestures involving the movement of the face, eyes, and head for augmented reality (AR). This technique allows us to use computer systems via hands-free, discreet interactions. In this paper, we present an elicitation study to explore the proper use of facial gestures for daily tasks in the context of a smart home. We used Amazon Mechanical Turk to conduct this study (N=37). Based on the proposed gestures, we report usage scenarios and complexity, proposed associations between gestures/tasks, a user-defined gesture set, and insights from the participants. We also conducted a technical feasibility study (N=13) with participants using smart eyewear to consider their uses in daily life. The device has 16 optical sensors and an inertial measurement unit (IMU). We can potentially integrate the system into optical see-through displays or other smart glasses. The results demonstrate that the device can detect eight temporal face-related gestures with a mean F1 score of 0.911 using a convolutional neural network (CNN). We also report the results of user-independent training and a one-hour recording of the experimenter testing two of the gestures.","증강현실(AR)을 위해 얼굴, 눈, 머리의 움직임과 관련된 얼굴 관련 제스처의 활용을 제안합니다. 이 기술을 통해 우리는 핸즈프리의 신중한 상호 작용을 통해 컴퓨터 시스템을 사용할 수 있습니다. 본 논문에서는 스마트 홈의 맥락에서 일상 업무에 대한 얼굴 제스처의 적절한 사용을 탐구하기 위한 유도 연구를 제시합니다. 우리는 Amazon Mechanical Turk를 사용하여 이 연구를 수행했습니다(N=37). 제안된 제스처를 기반으로 사용 시나리오 및 복잡성, 제안된 제스처/작업 간의 연관성, 사용자 정의 제스처 세트 및 참가자의 통찰력을 보고합니다. 또한, 스마트 아이웨어를 사용하는 참가자들을 대상으로 일상생활에서의 활용도를 고려한 기술적 타당성 조사(N=13)도 진행했습니다. 이 장치에는 16개의 광학 센서와 관성 측정 장치(IMU)가 있습니다. 잠재적으로 시스템을 광학 투명 디스플레이나 기타 스마트 안경에 통합할 수 있습니다. 결과는 장치가 CNN(컨볼루션 신경망)을 사용하여 평균 F1 점수 0.911로 8개의 얼굴 관련 제스처를 감지할 수 있음을 보여줍니다. 우리는 또한 사용자 독립적인 훈련 결과와 두 가지 제스처를 테스트하는 실험자의 1시간 기록을 보고합니다.",https://doi.org/10.1109/ISMAR50242.2020.00064,Interaction & Input; Display & Optics,Deep Learning / Neural Networks; Sensor Fusion,User Study,Hardware / Device; Algorithm / Method
592,2020,Flower Factory: A Component-based Approach for Rapid Flower Modeling,Flower Factory: 신속한 꽃 모델링을 위한 구성 요소 기반 접근 방식,"The rapid 3D objects modeling provides an effective way to enrich digital content, which is one of the essential tasks in VR/AR research. Flowers are frequently utilized in real-time applications, such as video games and VR/AR scenes. Technically, a realistic flower generation using the existing 3D modeling software is complicated and time-consuming for designers. Moreover, it is difficult to create imaginary and surreal flowers, which might be more interesting and attractive for the artists and game players. In this paper, we propose a component-based framework for rapid flower modeling, called Flower Factory. The flowers are assembled by different components, e.g., petals, stamens, receptacles and leaves. The shape of these components are created using simple primitives such as points and splines. After the shape of models are determined, the textures are synthesized automatically based on a predefine mask, according to a number of rules from real flowers. The whole modeling process can be controlled by several parameters, which describe the physical attributes of the flowers. Our technique is capable of producing a variety of flowers rapidly. Even novices without any modeling skills are able to control and model the 3D flowers. Furthermore, the developed system will be integrated in a lightweight application of smartphone due to its low computational cost.","신속한 3D 개체 모델링은 VR/AR 연구의 필수 작업 중 하나인 디지털 콘텐츠를 풍부하게 하는 효과적인 방법을 제공합니다. 꽃은 비디오 게임, VR/AR 장면 등 실시간 애플리케이션에 자주 활용됩니다. 기술적으로 기존 3D 모델링 소프트웨어를 사용하여 사실적인 꽃을 생성하는 것은 디자이너에게 복잡하고 시간이 많이 걸립니다. 더욱이, 예술가와 게임 플레이어에게 더 흥미롭고 매력적일 수 있는 상상적이고 초현실적인 꽃을 창조하는 것은 어렵습니다. 본 논문에서는 Flower Factory라는 빠른 꽃 모델링을 위한 컴포넌트 기반 프레임워크를 제안합니다. 꽃은 꽃잎, 수술, 꽃받침, 잎 등 다양한 구성 요소로 구성됩니다. 이러한 구성요소의 모양은 점 및 스플라인과 같은 간단한 기본 요소를 사용하여 생성됩니다. 모델의 모양이 결정되면 실제 꽃의 여러 규칙에 따라 미리 정의된 마스크를 기반으로 텍스처가 자동으로 합성됩니다. 전체 모델링 프로세스는 꽃의 물리적 특성을 설명하는 여러 매개변수로 제어할 수 있습니다. 우리의 기술은 다양한 꽃을 빠르게 생산할 수 있습니다. 모델링 기술이 전혀 없는 초보자도 3D 꽃을 제어하고 모델링할 수 있습니다. 또한, 개발된 시스템은 계산 비용이 낮기 때문에 스마트폰의 경량 애플리케이션에 통합될 것입니다.",https://doi.org/10.1109/ISMAR50242.2020.00019,Content Authoring,Optical / Display Technology,Other,Algorithm / Method; System / Framework
593,2020,Foveated Instant Radiosity,포비티드 인스턴트 라디오시티,"Foveated rendering distributes computational resources based on visual acuity, more in the foveal regions of our eyes and less in the periphery. The traditional rasterization method can be adapted into the foveated rendering framework in a quite straightforward way, but it’s difficult for estimating global illumination. Instant Radiosity is an efficient global illumination method. It generates Virtual Point Lights (VPLs) on the surface of the virtual scenes from light sources and uses these VPLs to simulate light bounces. However, instant radiosity can not be adapted into the foveated rendering pipeline directly, and is too slow for virtual reality experience. What’s more, instant radiosity does not consider temporal coherence, therefore it lacks temporal stability for dynamic scenes. In this paper, we propose a foveated rendering method for instant radiosity with more accurate global illumination effects in the foveal region and less accurate global illumination in the peripheral region. We define a foveated importance for each VPL, and use it to smartly distribute the VPLs to guarantee the rendering precision of the foveal region. Meanwhile, we propose a novel VPL reuse scheme, which updates only a small fraction of VPLs over frames, which ensures temporal coherence and improves time efficiency. Our method supports dynamic scenes and achieves high quality in the foveal regions at interactive frame rates.","포비티드 렌더링은 시력을 기반으로 계산 리소스를 눈의 중심와 영역에 더 많이 분배하고 주변 영역에는 덜 분산합니다. 전통적인 래스터화 방법은 매우 간단한 방법으로 포비티드 렌더링 프레임워크에 적용할 수 있지만 전역 조명을 추정하기는 어렵습니다. 인스턴트 라디오시티는 효율적인 전역 조명 방법입니다. 광원으로부터 가상 장면 표면에 VPL(Virtual Point Lights)을 생성하고 이러한 VPL을 사용하여 빛 바운스를 시뮬레이션합니다. 그러나 즉각적인 라디오시티는 포비티드 렌더링 파이프라인에 직접 적용할 수 없으며 가상 현실 경험에는 너무 느립니다. 게다가 즉각적인 라디오시티는 시간적 일관성을 고려하지 않으므로 동적 장면에 대한 시간적 안정성이 부족합니다. 본 논문에서는 중심와 영역에서는 보다 정확한 전역 조명 효과를 제공하고 주변 영역에서는 덜 정확한 전역 조명 효과를 갖는 순간 라디오시티를 위한 포비티드 렌더링 방법을 제안합니다. 우리는 각 VPL에 대해 중심 중요도를 정의하고 이를 사용하여 중심 영역의 렌더링 정밀도를 보장하기 위해 VPL을 스마트하게 배포합니다. 한편, 우리는 프레임에 걸쳐 VPL의 작은 부분만 업데이트하여 시간적 일관성을 보장하고 시간 효율성을 향상시키는 새로운 VPL 재사용 방식을 제안합니다. 우리의 방법은 동적 장면을 지원하고 대화형 프레임 속도로 중심와 영역에서 높은 품질을 달성합니다.",https://doi.org/10.1109/ISMAR50242.2020.00017,Rendering & Visualization,Sensor Fusion,Quantitative Experiment,System / Framework
594,2020,Gain A New Perspective: Towards Exploring Multi-View Alignment in Mixed Reality,새로운 관점 확보: 혼합 현실에서 다중 뷰 정렬 탐색을 향하여,"Manufacturing, maintenance, assembly, and training tasks represent some of the human activities that have captured special interest for Mixed Reality (MR) applications. For most of these scenarios, accurate object alignment constitutes a requirement to ensure the desired outcome. This task has proved to be especially challenging in egocentric approaches, frequently leading to estimation errors in depth. While traditional MR methods provide virtual guides such as text, arrows, or animations to assist users during alignment, this work explores the feasibility of using additional views generated by virtual cameras and mirrors. Presenting additional views from different perspectives can help to mitigate the estimation errors and show information that is not directly visible to users.To explore the benefits of using additional views for alignment tasks, and to collect reliable data and diminish external factors, we conducted a user study in a controlled virtual environment where participants aligned objects supported by additional views from a top-down camera and virtual mirrors. Data regarding alignment error, time to completion, user’s interaction and attention, distance traveled, average head velocity, usability, and mental effort were collected. Our results show that using additional views reduces the mental effort and distance traveled by users and increases acceptance without negatively affecting the alignment accuracy. Therefore, we believe that users will also benefit from integrating these techniques during alignment tasks in MR environments.","제조, 유지 관리, 조립 및 교육 작업은 혼합 현실(MR) 애플리케이션에 대한 특별한 관심을 사로잡은 인간 활동 중 일부를 나타냅니다. 대부분의 시나리오에서 원하는 결과를 보장하려면 정확한 개체 정렬이 필요합니다. 이 작업은 자기 중심적 접근 방식에서 특히 어려운 것으로 입증되었으며, 종종 깊이 있는 추정 오류로 이어졌습니다. 기존 MR 방법은 정렬 중에 사용자를 돕기 위해 텍스트, 화살표 또는 애니메이션과 같은 가상 가이드를 제공하는 반면, 이 작업에서는 가상 카메라 및 거울에 의해 생성된 추가 뷰를 사용할 수 있는 가능성을 탐구합니다. 다양한 관점에서 추가 뷰를 제공하면 추정 오류를 완화하고 사용자에게 직접 표시되지 않는 정보를 표시하는 데 도움이 될 수 있습니다. 정렬 작업에 추가 뷰를 사용하는 이점을 탐색하고 신뢰할 수 있는 데이터를 수집하고 외부 요인을 줄이기 위해 참가자가 하향식 카메라 및 가상 거울의 추가 뷰를 통해 지원되는 제어된 가상 환경에서 사용자 연구를 수행했습니다. 정렬 오류, 완료 시간, 사용자의 상호 작용 및 주의력, 이동 거리, 평균 머리 속도, 사용성 및 정신적 노력에 관한 데이터가 수집되었습니다. 우리의 결과는 추가 뷰를 사용하면 사용자가 이동하는 정신적 노력과 거리를 줄이고 정렬 정확도에 부정적인 영향을 주지 않으면서 수용도를 높이는 것으로 나타났습니다. 따라서 우리는 사용자가 MR 환경에서 정렬 작업 중에 이러한 기술을 통합함으로써 이점을 얻을 수 있다고 믿습니다.",https://doi.org/10.1109/ISMAR50242.2020.00044,Education & Training; Interaction & Input,3D Reconstruction,User Study,Algorithm / Method; User Study / Empirical Findings
595,2020,Generating Emotive Gaits for Virtual Agents Using Affect-Based Autoregression,영향 기반 자동 회귀를 사용하여 가상 에이전트에 대한 감정적 보행 생성,"We present a novel autoregression network to generate virtual agents that convey various emotions through their walking styles or gaits. Given the 3D pose sequences of a gait, our network extracts pertinent movement features and affective features from the gait. We use these features to synthesize subsequent gaits such that the virtual agents can express and transition between emotions represented as combinations of happy, sad, angry, and neutral. We incorporate multiple regularizations in the training of our network to simultaneously enforce plausible movements and noticeable emotions on the virtual agents. We also integrate our approach with an AR environment using a Microsoft HoloLens and can generate emotive gaits at interactive rates to increase the social presence. We evaluate how human observers perceive both the naturalness and the emotions from the generated gaits of the virtual agents in a web-based study. Our results indicate around 89% of the users found the naturalness of the gaits satisfactory on a five-point Likert scale, and the emotions they perceived from the virtual agents are statistically similar to the intended emotions of the virtual agents. We also use our network to augment existing gait datasets with emotive gaits and will release this augmented dataset for future research in emotion prediction and emotive gait synthesis. Our project website is available at https://gamma.umd.edu/gen-emotive-gaits/.","우리는 걷는 스타일이나 걸음걸이를 통해 다양한 감정을 전달하는 가상 에이전트를 생성하기 위한 새로운 자동 회귀 네트워크를 제시합니다. 보행의 3D 포즈 시퀀스가 ​​주어지면 우리 네트워크는 보행에서 적절한 움직임 특징과 정서적 특징을 추출합니다. 우리는 이러한 기능을 사용하여 가상 에이전트가 행복, 슬픔, 분노, 중립의 조합으로 표현되는 감정을 표현하고 전환할 수 있도록 후속 보행을 합성합니다. 우리는 가상 에이전트에 그럴듯한 움직임과 눈에 띄는 감정을 동시에 적용하기 위해 네트워크 훈련에 여러 정규화를 통합합니다. 또한 우리의 접근 방식을 Microsoft HoloLens를 사용하여 AR 환경과 통합하고 대화형 속도로 감정적인 보행을 생성하여 사회적 존재감을 높일 수 있습니다. 우리는 웹 기반 연구를 통해 인간 관찰자가 가상 ​​에이전트의 생성된 보행에서 자연스러움과 감정을 어떻게 인식하는지 평가합니다. 우리의 결과는 사용자의 약 89%가 5점 리커트 척도에서 보행의 자연스러움이 만족스럽다고 답했으며, 그들이 가상 에이전트로부터 인지한 감정은 가상 에이전트가 의도한 감정과 통계적으로 유사하다는 것을 나타냅니다. 또한 우리는 네트워크를 사용하여 감정적 보행으로 기존 보행 데이터 세트를 보강하고 감정 예측 및 감성 보행 합성에 대한 향후 연구를 위해 이 증강된 데이터 세트를 출시할 예정입니다. 우리 프로젝트 웹사이트는 https://gamma.umd.edu/gen-emotive-gaits/에서 확인할 수 있습니다.",https://doi.org/10.1109/ISMAR50242.2020.00020,Perception & Cognition,Sensor Fusion,Technical Evaluation,Algorithm / Method
596,2020,Guideline and Tool for Designing an Assembly Task Support System Using Augmented Reality,증강현실을 활용한 조립작업 지원 시스템 설계 가이드라인 및 도구,"Augmented reality (AR) systems support complex tasks like assembly by overlaying task-related content onto the real world. In recent years, the effort of designing and developing assembly task support systems in AR decreased with the availability of high potential head-mounted displays and provision of integrated development environments. Nevertheless, problems still arise when companies craft an effective AR task support system, particularly in the difficulty of selecting appropriate techniques and information-presentation methods, and the requirements that vary with each use case. In this study, we formulated a corresponding guideline, developed a selection aid tool that incorporates filtering based on the categorization of subtasks and the degree of freedom of available tracking, and evaluated their effectiveness in two experiments. First, to confirm effects on system design, we asked 18 participants to perform the design action of the AR system with the guideline for two tasks (PC assembly and rope work). Consequently, to verify the quality of the designed AR systems from Experiment 1, we asked another set of 20 participants to perform the same tasks with those systems. The results confirm that using the guideline can considerably lower efforts creating media and alleviate the error for a specific process. We envision our guideline and tool to be accessible as an online web page, assisting AR assembly task support system designer/developers worldwide.","증강 현실(AR) 시스템은 작업 관련 콘텐츠를 현실 세계에 오버레이하여 조립과 같은 복잡한 작업을 지원합니다. 최근 몇 년 동안 잠재력이 높은 헤드마운트 디스플레이의 가용성과 통합 개발 환경의 제공으로 인해 AR에서 조립 작업 지원 시스템을 설계하고 개발하려는 노력이 줄어들었습니다. 그럼에도 불구하고, 기업이 효과적인 AR 작업 지원 시스템을 구축할 때 특히 적절한 기술과 정보 제시 방법을 선택하기가 어렵고 각 사용 사례에 따라 요구 사항이 달라지는 등의 문제가 여전히 발생합니다. 본 연구에서는 해당 지침을 수립하고, 하위 작업 분류 및 추적 가능한 자유도에 따른 필터링을 통합한 선택 보조 도구를 개발하고, 두 번의 실험을 통해 그 효과를 평가했습니다. 먼저, 시스템 설계에 미치는 영향을 확인하기 위해 18명의 참가자에게 두 가지 작업(PC 조립 및 로프 작업)에 대한 지침에 따라 AR 시스템의 설계 작업을 수행하도록 요청했습니다. 결과적으로 실험 1에서 설계된 AR 시스템의 품질을 검증하기 위해 우리는 다른 20명의 참가자에게 해당 시스템으로 동일한 작업을 수행하도록 요청했습니다. 결과는 가이드라인을 사용하면 미디어를 만드는 노력을 상당히 줄이고 특정 프로세스의 오류를 줄일 수 있음을 확인합니다. 우리는 우리의 지침과 도구가 온라인 웹 페이지로 접근 가능하여 전 세계 AR 조립 작업 지원 시스템 설계자/개발자를 지원할 수 있는 것을 구상하고 있습니다.",https://doi.org/10.1109/ISMAR50242.2020.00077,Interaction & Input; Display & Optics,Other,User Study,Algorithm / Method
597,2020,Haptic Handshank - A Handheld Multimodal Haptic Feedback Controller for Virtual Reality,Haptic Handshank - 가상 현실을 위한 휴대용 다중 모드 햅틱 피드백 컨트롤러,"Compared to wearable devices, handheld haptic devices are promising for large scale virtual reality applications because of their portability and capability of supporting large workspace haptic interaction. However, it remains a challenge to render multimodal haptic stimuli in handheld devices due to space confinement. In this paper, we present a modular approach to build a Multimodal Handheld Haptic Controller called “Haptic Handshank” that includes a thumb feedback component, a palm feedback component, and a motion tracking component. In the thumb feedback component, a compact pneumatically-driven silicone airbag is utilized to simulate softness, and a flexible membrane based on the electro-vibration principle which covers the top portion of the airbag for rendering virtual textures. In the palm feedback component, vibrational motors and Peltier devices are embedded into the device’s body for rendering vibrotactile flow and distributing thermal stimuli. In the motion tracking component, an HTC-Vive tracker is mounted on the bottom of the controller’s handle to enable 6-DOF palm motion tracking. The performance of the handheld device is evaluated through quantitative experimental studies, which validate the ability of the device to simulate multimodal haptic sensations in accordance with diverse hand manipulation gestures such as enclosure, static contact, rubbing, squeezing and shaking of a cup of cold drink in 3D virtual space.","웨어러블 장치에 비해 휴대용 햅틱 장치는 휴대성과 대규모 작업 공간 햅틱 상호 작용을 지원하는 기능으로 인해 대규모 가상 현실 애플리케이션에 유망합니다. 그러나 공간 제약으로 인해 휴대용 장치에서 다중 모드 햅틱 자극을 렌더링하는 것은 여전히 ​​어려운 과제입니다. 본 논문에서는 엄지 피드백 구성 요소, 손바닥 피드백 구성 요소 및 동작 추적 구성 요소를 포함하는 ""Haptic Handshank""라는 다중 모드 휴대용 햅틱 컨트롤러를 구축하기 위한 모듈식 접근 방식을 제시합니다. 엄지 피드백 구성 요소에서는 소형 공압 구동 실리콘 에어백을 사용하여 부드러움을 시뮬레이션하고 전기 진동 원리를 기반으로 하는 유연한 멤브레인을 사용하여 가상 질감을 렌더링하기 위해 에어백 상단 부분을 덮습니다. 손바닥 피드백 구성 요소에는 진동 촉각 흐름을 렌더링하고 열 자극을 분산시키기 위해 진동 모터와 펠티에 장치가 장치 본체에 내장되어 있습니다. 모션 추적 구성 요소에서는 HTC-Vive 추적기가 컨트롤러 핸들 하단에 장착되어 6-DOF 손바닥 모션 추적이 가능합니다. 휴대용 장치의 성능은 3D 가상 공간에서 인클로저, 정적 접촉, 문지르기, 짜내기 및 흔들기와 같은 다양한 손 조작 제스처에 따라 다중 모드 햅틱 감각을 시뮬레이션하는 장치의 능력을 검증하는 정량적 실험 연구를 통해 평가됩니다.",https://doi.org/10.1109/ISMAR50242.2020.00047,Interaction & Input; Tracking & Localization,Haptic / Tactile Feedback,Quantitative Experiment,Hardware / Device
598,2020,HydrogenAR: Interactive Data-Driven Presentation of Dispenser Reliability,HydrogenAR: 디스펜서 신뢰성에 대한 대화형 데이터 기반 프레젠테이션,"When delivering presentations to a co-located audience, we typically use slides with text and 2D graphics to complement the spoken narrative. Though presentations have largely been explored on 2D media, augmented reality (AR) allows presentation designers to add data and augmentations to existing physical infrastructure on display. This coupling could provide a more engaging experience to the audience and support comprehension. With HydrogenAR, we present a novel application that leverages the benefits of data-driven storytelling with those of AR to explain the unique challenges of hydrogen dispenser reliability. Utilizing physical props, situated data, and virtual augmentations and animations, HydrogenAR serves as a unique presentation tool, particularly critical for stakeholders, tour groups, and VIPs. HydrogenAR is a product of multiple collaborative design iterations with a local Hydrogen Fuel research team and is evaluated through interviews with team members and a user study with end-users to evaluate the usability and quality of the interactive AR experience. Through this work, we provide design considerations for AR data-driven presentations and discuss how AR could be used for innovative content delivery beyond traditional slide-based presentations.","같은 장소에 있는 청중에게 프레젠테이션을 전달할 때 우리는 일반적으로 텍스트와 2D 그래픽이 포함된 슬라이드를 사용하여 음성 내러티브를 보완합니다. 프레젠테이션은 주로 2D 미디어에서 탐색되었지만 증강 현실(AR)을 통해 프레젠테이션 디자이너는 디스플레이의 기존 물리적 인프라에 데이터와 증강 기능을 추가할 수 있습니다. 이 결합은 청중에게 더욱 매력적인 경험을 제공하고 이해를 지원할 수 있습니다. HydrogenAR을 통해 우리는 AR의 이점과 데이터 기반 스토리텔링의 이점을 활용하여 수소 디스펜서 신뢰성의 고유한 과제를 설명하는 새로운 애플리케이션을 제시합니다. 물리적 소품, 위치 데이터, 가상 확대 및 애니메이션을 활용하는 HydrogenAR은 특히 이해관계자, 투어 그룹 및 VIP에게 중요한 고유한 프레젠테이션 도구 역할을 합니다. HydrogenAR은 현지 수소 연료 연구팀과의 여러 공동 설계 반복의 산물이며, 대화형 AR 경험의 유용성과 품질을 평가하기 위해 팀 구성원과의 인터뷰 및 최종 사용자와의 사용자 연구를 통해 평가됩니다. 이 작업을 통해 AR 데이터 기반 프레젠테이션에 대한 디자인 고려 사항을 제공하고 기존 슬라이드 기반 프레젠테이션을 넘어 혁신적인 콘텐츠 전달에 AR을 사용할 수 있는 방법에 대해 논의합니다.",https://doi.org/10.1109/ISMAR50242.2020.00101,Collaboration & Social,Other,User Study,System / Framework
599,2020,Improved vergence and accommodation via Purkinje Image tracking with multiple cameras for AR glasses,AR 안경용 여러 대의 카메라를 사용한 Purkinje 이미지 추적을 통해 향상된 버전성 및 조절 기능,"We present a personalized, comprehensive eye-tracking solution based on tracking higher-order Purkinje images, suited explicitly for eyeglasses-style AR and VR displays. Existing eye-tracking systems for near-eye applications are typically designed to work for an on-axis configuration and rely on pupil center and corneal reflections (PCCR) to estimate gaze with an accuracy of only about 0.5°to 1°. These are often expensive, bulky in form factor, and fail to estimate monocular accommodation, which is crucial for focus adjustment within the AR glasses.Our system independently measures the binocular vergence and monocular accommodation using higher-order Purkinje reflections from the eye, extending the PCCR based methods. We demonstrate that these reflections are sensitive to both gaze rotation and lens accommodation and model the Purkinje images’ behavior in simulation. We also design and fabricate a user-customized eye tracker using cheap off-the-shelf cameras and LEDs. We use an end-to-end convolutional neural network (CNN) for calibrating the eye tracker for the individual user, allowing for robust and simultaneous estimation of vergence and accommodation. Experimental results show that our solution, specifically catering to individual users, outperforms state-of-the-art methods for vergence and depth estimation, achieving an accuracy of 0.3782° and 1.108cm respectively.",우리는 안경 스타일의 AR 및 VR 디스플레이에 적합한 고차 Purkinje 이미지 추적을 기반으로 하는 개인화되고 포괄적인 시선 추적 솔루션을 제시합니다. 근거리 응용 분야를 위한 기존 안구 추적 시스템은 일반적으로 축상 구성에서 작동하도록 설계되었으며 동공 중심 및 각막 반사(PCCR)를 사용하여 약 0.5°~1°의 정확도로 시선을 추정합니다. 이는 종종 비용이 많이 들고 폼 팩터가 부피가 크며 AR 안경 내 초점 조정에 중요한 단안 조절력을 추정하지 못합니다. 우리 시스템은 눈의 고차 Purkinje 반사를 사용하여 양안 수직성과 단안 조절력을 독립적으로 측정하여 PCCR 기반 방법을 확장합니다. 우리는 이러한 반사가 시선 회전과 렌즈 조절에 민감하다는 것을 입증하고 시뮬레이션에서 Purkinje 이미지의 동작을 모델링합니다. 우리는 또한 저렴한 기성 카메라와 LED를 사용하여 사용자 맞춤형 시선 추적기를 설계하고 제작합니다. 우리는 개별 사용자에 대한 시선 추적기를 보정하기 위해 종단 간 CNN(컨벌루션 신경망)을 사용하여 이향성과 조절을 강력하고 동시에 추정할 수 있습니다. 실험 결과는 특히 개별 사용자를 대상으로 하는 당사의 솔루션이 최신 버전 및 깊이 추정 방법보다 성능이 뛰어나 각각 0.3782° 및 1.108cm의 정확도를 달성한다는 것을 보여줍니다.,https://doi.org/10.1109/ISMAR50242.2020.00058,Display & Optics; Interaction & Input,Deep Learning / Neural Networks,Quantitative Experiment,Algorithm / Method
600,2020,Influence of hand visualization on tool-based motor skills training in an immersive VR simulator,몰입형 VR 시뮬레이터에서 도구 기반 운동 능력 훈련에 대한 손 시각화의 영향,"Immersive VR technologies offer versatile training tools by recreating real-world situations in a virtual and safe environment and allowing users to have a first-person experience. The design of such training systems requires defining the most critical components to simulate, and to what extent they can be simulated successfully. One open research question for designing such systems is how to represent the user in the virtual environment, and which is the added value of this representation for training purposes. In this work, we focus on how the user’s hand representation in an immersive virtual environment can impact the training of tool-based motor skills.To investigate this question, we have designed a VR trainer for a simple tool-based pick and place task. A user experiment was conducted to evaluate how the movements of the users’ real hand representation influence their performance and subjective experience in the virtual environment. For that purpose, the participants performed the task on the VR simulator with two conditions: the presence or absence of their animated virtual hands representation. The results of this study show that, although users prefer to have a visual representation of their hands, they achieved similar and correlated performance in the VR system regardless of the hand representation condition. These results suggest that the presence of the user’s hand representation is not necessary when performing a tool-based motor skill task in a VR trainer. These findings have practical implications for the design of VR simulators for training motor skills tasks since adding users’ hand representation may require cumbersome and expensive additional devices.","몰입형 VR 기술은 안전한 가상 환경에서 실제 상황을 재현하고 사용자가 1인칭 경험을 할 수 있도록 하여 다양한 교육 도구를 제공합니다. 이러한 교육 시스템을 설계하려면 시뮬레이션할 가장 중요한 구성 요소를 정의하고 해당 구성 요소를 어느 정도까지 성공적으로 시뮬레이션할 수 있는지 정의해야 합니다. 이러한 시스템을 설계하기 위한 공개 연구 질문 중 하나는 가상 환경에서 사용자를 어떻게 표현하는지이며, 이는 교육 목적으로 이러한 표현의 부가 가치입니다. 이 작업에서 우리는 몰입형 가상 환경에서 사용자의 손 표현이 도구 기반 운동 기술 훈련에 어떤 영향을 미칠 수 있는지에 중점을 둡니다. 이 질문을 조사하기 위해 우리는 간단한 도구 기반 선택 및 배치 작업을 위한 VR 트레이너를 설계했습니다. 사용자의 실제 손 표현의 움직임이 가상 환경에서의 성능과 주관적 경험에 어떤 영향을 미치는지 평가하기 위해 사용자 실험이 수행되었습니다. 이를 위해 참가자들은 애니메이션 가상 손 표현의 유무라는 두 가지 조건으로 VR 시뮬레이터에서 작업을 수행했습니다. 본 연구의 결과는 사용자가 손의 시각적 표현을 선호하지만 손 표현 조건에 관계없이 VR 시스템에서 유사하고 상관된 성능을 달성했음을 보여줍니다. 이러한 결과는 VR 트레이너에서 도구 기반 운동 기술 작업을 수행할 때 사용자의 손 표현이 필요하지 않음을 시사합니다. 사용자의 손 표현을 추가하려면 번거롭고 값비싼 추가 장치가 필요할 수 있으므로 이러한 발견은 운동 기술 작업 훈련을 위한 VR 시뮬레이터 설계에 실질적인 영향을 미칩니다.",https://doi.org/10.1109/ISMAR50242.2020.00049,Education & Training,Sensor Fusion,Simulation,User Study / Empirical Findings
601,2020,Investigating Remote Tactile Feedback for Mid-Air Text-Entry in Virtual Reality,가상 현실에서 공중 텍스트 입력에 대한 원격 촉각 피드백 조사,"In this paper, we investigate the utility of remote tactile feedback for freehand text-entry on a mid-air Qwerty keyboard in VR. To that end, we use insights from prior work to design a virtual keyboard along with different forms of tactile feedback, both spatial and non-spatial, for fingers and for wrists. We report on a multi-session text-entry study with 24 participants where we investigated four vibrotactile feedback conditions: on-fingers, on-wrist spatialized, on-wrist non-spatialized, and audio-visual only. We use micro-metrics analyses and participant interviews to analyze the mechanisms underpinning the observed performance and user experience. The results show comparable performance across feedback types. However, participants overwhelmingly prefer the tactile feedback conditions and rate on-fingers feedback as significantly lower in mental demand, frustration, and effort. Results also show that spatialization of vibrotactile feedback on the wrist as a way to provide finger-specific feedback is comparable in performance and preference to a single vibration location. The micro-metrics analyses suggest that users compensated for the lack of tactile feedback with higher visual and cognitive attention, which ensured similar performance but higher user effort.","본 논문에서는 VR의 공중 Qwerty 키보드에 자유롭게 텍스트를 입력하기 위한 원격 촉각 피드백의 유용성을 조사합니다. 이를 위해 우리는 이전 작업에서 얻은 통찰력을 사용하여 손가락과 손목에 대한 공간적, 비공간적 촉각 피드백의 다양한 형태와 함께 가상 키보드를 설계했습니다. 우리는 24명의 참가자를 대상으로 4가지 진동 촉각 피드백 조건(손가락 위, 손목 위 공간화, 손목 위 비공간화 및 시청각 전용)을 조사한 다중 세션 텍스트 입력 연구에 대해 보고합니다. 우리는 관찰된 성능과 사용자 경험을 뒷받침하는 메커니즘을 분석하기 위해 미시적 측정 분석과 참가자 인터뷰를 사용합니다. 결과는 피드백 유형 전반에 걸쳐 비슷한 성능을 보여줍니다. 그러나 참가자들은 촉각 피드백 조건을 압도적으로 선호하고 손가락 피드백을 정신적 요구, 좌절 및 노력 측면에서 훨씬 낮은 것으로 평가했습니다. 결과는 또한 손가락별 피드백을 제공하는 방법으로 손목에 대한 진동 촉각 피드백의 공간화가 성능 및 선호도 측면에서 단일 진동 위치와 유사하다는 것을 보여줍니다. 미시적 측정 분석에 따르면 사용자는 더 높은 시각적, 인지적 주의력으로 촉각 피드백 부족을 보완하여 유사한 성능을 보장하지만 사용자 노력은 더 높아졌습니다.",https://doi.org/10.1109/ISMAR50242.2020.00062,Perception & Cognition,Haptic / Tactile Feedback,User Study,Design Guidelines
602,2020,Landmark-based mixed-reality perceptual alignment of medical imaging data and accuracy validation in living subjects,의료 영상 데이터의 랜드마크 기반 혼합 현실 인식 정렬 및 살아있는 피험자의 정확도 검증,"Medical augmented reality (AR) applications where virtual renderings are aligned with the real world allow to visualize internal anatomy of the patient to a medical caregiver wearing an AR headset. Accurate alignment of virtual and real content is important for applications where the virtual rendering is used to guide the medical procedure such as a surgery. Compared to 2D AR applications, where the alignment accuracy can be directly measured on the 2D screen, 3D medical AR applications require alignment measurements using phantoms and external tracking systems. In this paper we present an approach for landmark-based alignment, validation and accuracy measurement of a 3D AR overlay of medical images on the real-world subject. This is done by performing an initial MRI of a subject’s head, an AR alignment task of the virtual rendering of the head MRI data to the subject’s real-world head using virtual fiducials, and a second MRI scan to test the accuracy of the AR alignment task. We have performed these 3D medical AR alignment measurements on seven volunteers using a MagicLeap AR head-mounted display. Across all seven volunteers we measured an alignment accuracy of $4.7 \pm 2.6$ mm. These results suggest that such an AR application can be a valuable tool for guiding non-invasive transcranial magnetic brain stimulation treatment. The presented MRI-based accuracy validation will furthermore be an important versatile tool to establish the safety of medical AR techniques. Index Terms: Mixed / augmented reality; Visualization design and evaluation methods","가상 렌더링이 실제 세계와 정렬되는 의료용 증강 현실(AR) 애플리케이션을 통해 AR 헤드셋을 착용한 의료인에게 환자의 내부 해부학적 구조를 시각화할 수 있습니다. 가상 렌더링을 사용하여 수술과 같은 의료 절차를 안내하는 애플리케이션에서는 가상 콘텐츠와 실제 콘텐츠를 정확하게 정렬하는 것이 중요합니다. 2D 화면에서 정렬 정확도를 직접 측정할 수 있는 2D AR 애플리케이션에 비해 3D 의료용 AR 애플리케이션은 팬텀과 외부 추적 시스템을 활용한 정렬 측정이 필요합니다. 본 논문에서는 실제 대상에 대한 의료 이미지의 3D AR 오버레이에 대한 랜드마크 기반 정렬, 검증 및 정확도 측정에 대한 접근 방식을 제시합니다. 이는 피험자 머리의 초기 MRI, 가상 기준점을 사용하여 피험자의 실제 머리에 머리 MRI 데이터를 가상 렌더링하는 AR 정렬 작업, AR 정렬 작업의 정확성을 테스트하기 위한 두 번째 MRI 스캔을 수행하여 수행됩니다. 우리는 MagicLeap AR 헤드 마운트 디스플레이를 사용하여 7명의 지원자를 대상으로 이러한 3D 의료 AR 정렬 측정을 수행했습니다. 7명의 지원자 모두에서 $4.7 \pm 2.6$ mm의 정렬 정확도를 측정했습니다. 이러한 결과는 이러한 AR 응용 프로그램이 비침습적 경두개 자기 뇌 자극 치료를 안내하는 데 유용한 도구가 될 수 있음을 시사합니다. 제시된 MRI 기반 정확도 검증은 또한 의료 AR 기술의 안전성을 확립하는 중요한 다용도 도구가 될 것입니다. 색인 용어: 혼합/증강 현실; 시각화 설계 및 평가 방법",https://doi.org/10.1109/ISMAR50242.2020.00095,Medical & Healthcare; Tracking & Localization,Sensor Fusion,User Study; Quantitative Experiment,Algorithm / Method
603,2020,Learning Bipartite Graph Matching for Robust Visual Localization,강력한 시각적 현지화를 위한 이분 그래프 매칭 학습,"2D-3D matching is an essential step for visual localization, where the accuracy of the camera pose is mainly determined by the quality of 2D-3D correspondences. The matching is typically achieved by the nearest neighbor search of local features. Many existing works have shown impressive results on both the efficiency and accuracy. Recently emerged learning-based features further improve the robustness compared to the traditional hand-crafted ones. However, it is still hard to establish enough correct matches in challenging scenes with illumination changes or repetitive patterns due to the intrinsic local properties of local features. In this work, we propose a novel method to deal with 2D-3D matching in a very robust way. We first establish as many potential correct matches as possible using the local similarity. Then we construct a bipartite graph and use a deep neural network, referred to as Bipartite Graph Network (BGNet), to extract the global geometric information. The network predicts the likelihood of being an inlier for each edge and outputs the globally optimal one-to-one correspondences with a Hungarian pooling layer. The experiments show that the proposed method can find more correct matches and improves localization on both the robustness and accuracy. The results on multiple visual localization datasets are obviously better than the existing state-of-the-arts, which demonstrates the effectiveness of the proposed method.","2D-3D 매칭은 시각적 위치 파악을 위한 필수 단계로, 카메라 포즈의 정확도는 주로 2D-3D 대응 품질에 따라 결정됩니다. 일치는 일반적으로 지역 특징의 가장 가까운 이웃 검색을 통해 이루어집니다. 기존의 많은 연구에서는 효율성과 정확성 모두에서 인상적인 결과를 보여주었습니다. 최근 등장한 학습 기반 기능은 기존의 수작업 기능에 비해 견고성을 더욱 향상시킵니다. 그러나 로컬 기능의 본질적인 로컬 속성으로 인해 조명 변화나 반복적인 패턴이 있는 까다로운 장면에서 충분히 정확한 일치를 설정하는 것은 여전히 ​​어렵습니다. 본 연구에서는 매우 강력한 방식으로 2D-3D 매칭을 처리하는 새로운 방법을 제안합니다. 먼저 지역적 유사성을 사용하여 가능한 한 많은 잠재적인 정확한 일치 항목을 설정합니다. 그런 다음 이분 그래프를 구성하고 BGNet(Bipartite Graph Network)이라는 심층 신경망을 사용하여 전역 기하학적 정보를 추출합니다. 네트워크는 각 에지에 대해 인라이어가 될 가능성을 예측하고 헝가리 풀링 레이어를 사용하여 전역적으로 최적의 일대일 대응을 출력합니다. 실험은 제안된 방법이 더 정확한 일치 항목을 찾을 수 있고 견고성과 정확성 모두에서 위치 파악을 향상시킬 수 있음을 보여줍니다. 여러 시각적 위치 파악 데이터 세트의 결과는 기존의 최첨단 기술보다 분명히 우수하며 이는 제안된 방법의 효율성을 보여줍니다.",https://doi.org/10.1109/ISMAR50242.2020.00036,Tracking & Localization,Deep Learning / Neural Networks,Technical Evaluation,Algorithm / Method
604,2020,Object Detection in the Context of Mobile Augmented Reality,모바일 증강 현실의 맥락에서 객체 감지,"In the past few years, numerous Deep Neural Network (DNN) models and frameworks have been developed to tackle the problem of real-time object detection from RGB images. Ordinary object detection approaches process information from the images only, and they are oblivious to the camera pose with regard to the environment and the scale of the environment. On the other hand, mobile Augmented Reality (AR) frameworks can continuously track a camera’s pose within the scene and can estimate the correct scale of the environment by using Visual-Inertial Odometry (VIO). In this paper, we propose a novel approach that combines the geometric information from VIO with semantic information from object detectors to improve the performance of object detection on mobile devices. Our approach includes three components: (1) an image orientation correction method, (2) a scale-based filtering approach, and (3) an online semantic map. Each component takes advantage of the different characteristics of the VIO-based AR framework. We implemented the AR-enhanced features using ARCore and the SSD Mobilenet model on Android phones. To validate our approach, we manually labeled objects in image sequences taken from 12 room-scale AR sessions. The results show that our approach can improve on the accuracy of generic object detectors by 12% on our dataset.","지난 몇 년 동안 RGB 이미지에서 실시간 객체 감지 문제를 해결하기 위해 수많은 DNN(Deep Neural Network) 모델과 프레임워크가 개발되었습니다. 일반적인 객체 감지 접근 방식은 이미지의 정보만 처리하며 환경 및 환경 규모와 관련된 카메라 포즈를 인식하지 못합니다. 반면, 모바일 증강 현실(AR) 프레임워크는 장면 내에서 카메라의 자세를 지속적으로 추적할 수 있으며 VIO(Visual-Inertial Odometry)를 사용하여 환경의 정확한 규모를 추정할 수 있습니다. 본 논문에서는 모바일 장치에서 객체 감지 성능을 향상시키기 위해 VIO의 기하학적 정보와 객체 감지기의 의미 정보를 결합하는 새로운 접근 방식을 제안합니다. 우리의 접근 방식에는 (1) 이미지 방향 수정 방법, (2) 스케일 기반 필터링 접근 방식, (3) 온라인 의미 지도의 세 가지 구성 요소가 포함됩니다. 각 구성 요소는 VIO 기반 AR 프레임워크의 다양한 특성을 활용합니다. Android 휴대폰에서 ARCore 및 SSD Mobilenet 모델을 사용하여 AR 강화 기능을 구현했습니다. 우리의 접근 방식을 검증하기 위해 우리는 12개의 실내 규모 AR 세션에서 가져온 이미지 시퀀스의 개체에 수동으로 레이블을 지정했습니다. 결과는 우리의 접근 방식이 데이터 세트에서 일반 객체 감지기의 정확도를 12% 향상시킬 수 있음을 보여줍니다.",https://doi.org/10.1109/ISMAR50242.2020.00037,Content Authoring,Deep Learning / Neural Networks,Quantitative Experiment,Algorithm / Method
605,2020,Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors,공간적으로 희박한 단일 픽셀 감지기를 사용한 광학 시선 추적,"Gaze tracking is an essential component of next generation displays for virtual reality and augmented reality applications. Traditional camera-based gaze trackers used in next generation displays are known to be lacking in one or multiple of the following metrics: power consumption, cost, computational complexity, estimation accuracy, latency, and form-factor. We propose the use of discrete photodiodes and light-emitting diodes (LEDs) as an alternative to traditional camera-based gaze tracking approaches while taking all of these metrics into consideration. We begin by developing a rendering-based simulation framework for understanding the relationship between light sources and a virtual model eyeball. Findings from this framework are used for the placement of LEDs and photodiodes. Our first prototype uses a neural network to obtain an average error rate of $2. 67^{\circ}$ at 400 Hz while demanding only 16 mW. By simplifying the implementation to using only LEDs, duplexed as light transceivers, and more minimal machine learning model, namely a light-weight supervised Gaussian process regression algorithm, we show that our second prototype is capable of an average error rate of $1. 57^{\circ}$ at 250 Hz using 800 mW.","시선 추적은 가상 현실 및 증강 현실 애플리케이션을 위한 차세대 디스플레이의 필수 구성 요소입니다. 차세대 디스플레이에 사용되는 기존의 카메라 기반 시선 추적기는 전력 소비, 비용, 계산 복잡성, 추정 정확도, 대기 시간 및 폼 팩터와 같은 지표 중 하나 이상이 부족한 것으로 알려져 있습니다. 우리는 이러한 모든 지표를 고려하면서 기존의 카메라 기반 시선 추적 접근 방식의 대안으로 개별 포토다이오드 및 발광 다이오드(LED)의 사용을 제안합니다. 우리는 광원과 가상 모델 안구 사이의 관계를 이해하기 위한 렌더링 기반 시뮬레이션 프레임워크를 개발하는 것부터 시작합니다. 이 프레임워크의 결과는 LED 및 포토다이오드 배치에 사용됩니다. 첫 번째 프로토타입은 신경망을 사용하여 $2의 평균 오류율을 얻습니다. 400Hz에서 67^{\circ}$ 16mW만 요구합니다. LED만 사용하고 광 트랜시버로 이중화하고 보다 최소한의 기계 학습 모델, 즉 경량 지도 가우스 프로세스 회귀 알고리즘을 사용하여 구현을 단순화함으로써 두 번째 프로토타입의 평균 오류율이 1달러라는 것을 보여줍니다. 800mW를 사용하여 250Hz에서 57^{\circ}$.",https://doi.org/10.1109/ISMAR50242.2020.00033,Education & Training,Deep Learning / Neural Networks,Quantitative Experiment,Algorithm / Method; User Study / Empirical Findings
606,2020,Optical distortions in VR bias the perceived slant of moving surfaces,VR의 광학 왜곡은 움직이는 표면의 인지된 경사를 편향시킵니다.,"The magnifying optics of virtual reality (VR) head-mounted displays (HMD) often cause undesirable pincushion distortion in the displayed imagery. Eccentrically increasing magnification radially displaces image-points away from the optical axis, causing straight lines to curve outwards. This, in turn, should affect the 3D perception of surface shape by warping binocular and monocular depth cues. Previous research has shown that distortion-induced biases in perceived slant do occur in static images. However, most use cases in VR involve moving images. Here we evaluate the impact of motion on biases in perceived slant. An HMD was used to present flat, textured surfaces that varied in slant and were either stationary, or translated laterally by the observer. In separate studies we varied the degree of distortion and evaluated the impact on perceived slant at several locations along the surface. We found that, irrespective of whether the surface was moving or stationary, distortion introduced significant bias into local slant estimates. The pattern of results is consistent with the surface appearing to be concave (as if viewing the inside surface of a bowl), as predicted from the warping of binocular and monocular cues. Importantly, the intermediate distortion level produced the same, but weaker, pattern of biases seen in the fully-distorted condition. When an appropriate level of pre-warping was applied, slant perception was veridical. Overall, our results highlight the importance of sufficiently correcting for optical distortions in VR HMDs to enable veridical perception of surface attitude.",가상 현실(VR) 헤드 장착 디스플레이(HMD)의 확대 광학 장치는 표시되는 이미지에 바람직하지 않은 핀쿠션 왜곡을 일으키는 경우가 많습니다. 편심적으로 증가하는 배율은 이미지 포인트를 광축에서 멀어지게 방사형으로 변위시켜 직선이 바깥쪽으로 구부러지게 만듭니다. 이는 결과적으로 양안 및 단안 깊이 신호를 왜곡하여 표면 모양의 3D 인식에 영향을 미칩니다. 이전 연구에서는 인지된 경사의 왜곡으로 인한 편향이 정적 이미지에서 실제로 발생하는 것으로 나타났습니다. 그러나 VR의 대부분의 사용 사례에는 움직이는 이미지가 포함됩니다. 여기에서는 인지된 경사의 편향에 대한 모션의 영향을 평가합니다. HMD는 경사가 다양하고 고정되어 있거나 관찰자에 의해 측면으로 이동되는 편평하고 질감이 있는 표면을 제시하는 데 사용되었습니다. 별도의 연구에서 우리는 왜곡 정도를 다양하게 하고 표면을 따라 여러 위치에서 인지된 경사에 대한 영향을 평가했습니다. 우리는 표면이 움직이는지 고정되어 있는지에 관계없이 왜곡으로 인해 국지적 경사 추정에 상당한 편향이 발생한다는 것을 발견했습니다. 결과의 패턴은 양안 및 단안 단서의 뒤틀림에서 예측된 것처럼 표면이 오목하게 나타나는 것과 일치합니다(마치 그릇의 내부 표면을 보는 것처럼). 중요한 것은 중간 왜곡 수준이 완전히 왜곡된 조건에서 볼 수 있는 편향 패턴과 동일하지만 더 약한 편향 패턴을 생성한다는 것입니다. 적절한 수준의 사전 뒤틀림이 적용되면 경사 인식이 실제로 가능해졌습니다. 전반적으로 우리의 결과는 VR HMD의 광학 왜곡을 충분히 보정하여 표면 자세에 대한 실제 인식을 가능하게 하는 중요성을 강조합니다.,https://doi.org/10.1109/ISMAR50242.2020.00027,Display & Optics,Other,Case Study / Application Demo,User Study / Empirical Findings
607,2020,Pen-based Interaction with Spreadsheets in Mobile Virtual Reality,모바일 가상 현실에서 스프레드시트와 펜 기반 상호 작용,"Virtual Reality (VR) can enhance the display and interaction of mobile knowledge work and in particular, spreadsheet applications. While spreadsheets are widely used yet are challenging to interact with, especially on mobile devices, using them in VR has not been explored in depth. A special uniqueness of the domain is the contrast between the immersive and large display space afforded by VR, contrasted by the very limited interaction space that may be afforded for the information worker on the go, such as an airplane seat or a small work-space. To close this gap, we present a tool-set for enhancing spreadsheet interaction on tablets using immersive VR headsets and pen-based input. This combination opens up many possibilities for enhancing the productivity for spreadsheet interaction. We propose to use the space around and in front of the tablet for enhanced visualization of spreadsheet data and meta-data. For example, extending sheet display beyond the bounds of the physical screen, or easier debugging by uncovering hidden dependencies between sheet’s cells. Combining the precise on-screen input of a pen with spatial sensing around the tablet, we propose tools for the efficient creation and editing of spreadsheets functions such as off-the-screen layered menus, visualization of sheets dependencies, and gaze-and-touch-based switching between spreadsheet tabs. We study the feasibility of the proposed tool-set using a video-based online survey and an expert-based assessment of indicative human performance potential.","가상 현실(VR)은 모바일 지식 작업, 특히 스프레드시트 애플리케이션의 표시 및 상호 작용을 향상시킬 수 있습니다. 스프레드시트는 널리 사용되지만 특히 모바일 장치에서 상호작용하기 어려운 반면, VR에서 스프레드시트를 사용하는 방법은 심도 있게 탐구되지 않았습니다. 이 영역의 특별한 고유성은 VR이 제공하는 몰입형 공간과 넓은 디스플레이 공간 사이의 대비이며, 이는 비행기 좌석이나 작은 작업 공간과 같이 이동 중인 정보 작업자에게 제공될 수 있는 매우 제한된 상호 작용 공간과 대조됩니다. 이러한 격차를 줄이기 위해 우리는 몰입형 VR 헤드셋과 펜 기반 입력을 사용하여 태블릿에서 스프레드시트 상호 작용을 향상시키는 도구 세트를 제시합니다. 이 조합은 스프레드시트 상호 작용의 생산성을 향상시킬 수 있는 많은 가능성을 열어줍니다. 우리는 스프레드시트 데이터와 메타데이터의 시각화를 강화하기 위해 태블릿 주변과 앞 공간을 활용할 것을 제안합니다. 예를 들어 실제 화면의 경계를 넘어 시트 표시를 확장하거나 시트 셀 사이의 숨겨진 종속성을 찾아 디버깅을 더 쉽게 할 수 있습니다. 펜의 정확한 화면 입력과 태블릿 주변의 공간 감지를 결합하여 화면 밖 레이어 메뉴, 시트 종속성 시각화, 스프레드시트 탭 간 응시 및 터치 기반 전환과 같은 스프레드시트 기능을 효율적으로 생성 및 편집할 수 있는 도구를 제안합니다. 우리는 비디오 기반 온라인 설문 조사와 인간 성과 잠재력에 대한 전문가 기반 평가를 사용하여 제안된 도구 세트의 타당성을 연구합니다.",https://doi.org/10.1109/ISMAR50242.2020.00063,Interaction & Input,Optical / Display Technology,Questionnaire / Survey,Hardware / Device
608,2020,Perception of Multisensory Wind Representation in Virtual Reality,가상 현실에서의 다감각적 바람 표현의 인식,"The set of physical and sensitive phenomena that interacts with the urban morphology acts on the resulting perception from the users of a place. Its study and representation provides elements beyond the aesthetics aspects that can allow a better understanding of the space and future urban projects. We aim to analyze the effects of three different wind representations in terms of perception and sense of presence in virtual reality (VR). We focus on the following conditions: (R) reference scene with the audiovisual representation of the mechanical effects of the wind on the elements of the context, (V) reference scene plus the visualization of the wind flow, present (among others) in the architecture field, (T) reference scene plus tactile restitution of wind and eventually (V+T) assembling all previous conditions. For the experiment, we present to the participants (R), then (V) followed by (T), and finish with (V+T). 37 participants evaluated 12 different stop points (divided into four routes in the same simulated street), where they had to determine the perceived wind force and direction concerning the four different conditions (each one corresponding to one route). At the end of each route, participants evaluated their sense of presence in the VR scene. Our analysis showed significant effects of tactile restitution over the visual effects used in the study, both for understanding wind properties and for increasing the sense of presence in the VR scene. In terms of wind direction, (T) reduced the estimation error by 27% compared to (V). Concerning wind force, the reduction was 9.8%. As far as presence was concerned, (T) increased the sense of presence by 12.2% compared to (V). Index Terms: Human-centered computing—Human computer interaction (HCI)—HCI design and evaluation methods—User studies.","도시 형태와 상호작용하는 일련의 물리적이고 민감한 현상은 장소 사용자의 결과적인 인식에 영향을 미칩니다. 그것의 연구와 표현은 공간과 미래의 도시 프로젝트에 대한 더 나은 이해를 가능하게 하는 미학적 측면을 넘어서는 요소를 제공합니다. 우리는 가상 현실(VR)에서 지각과 존재감 측면에서 세 가지 바람 표현의 효과를 분석하는 것을 목표로 합니다. 우리는 다음 조건에 중점을 둡니다: (R) 컨텍스트 요소에 대한 바람의 기계적 효과를 시청각적으로 표현한 참조 장면, (V) 참조 장면과 건축 분야에 존재하는 바람 흐름의 시각화, (T) 참조 장면과 바람의 촉각적 복원 및 최종적으로 (V+T) 모든 이전 조건을 조합합니다. 실험을 위해 참가자들에게 (R)을 제시한 다음 (V), (T)를 제시하고 (V+T)로 마무리합니다. 37명의 참가자는 12개의 서로 다른 정지 지점(동일한 시뮬레이션 거리에서 4개의 경로로 나누어짐)을 평가했으며, 여기서 그들은 4개의 서로 다른 조건(각각 하나의 경로에 해당)과 관련하여 인지된 풍력 및 방향을 결정해야 했습니다. 각 경로가 끝날 때 참가자들은 VR 장면에서 자신의 존재감을 평가했습니다. 우리의 분석은 바람의 특성을 이해하고 VR 장면에서 현장감을 높이는 데 있어 연구에 사용된 시각적 효과에 비해 촉각 복원이 중요한 효과를 보인다는 것을 보여주었습니다. 풍향 측면에서 (T)는 (V)에 비해 추정오차를 27% 감소시켰다. 풍력의 경우 감소율은 9.8%였습니다. 존재감에서는 (T)가 (V)에 비해 존재감을 12.2% 증가시켰습니다. 색인 용어: 인간 중심 컴퓨팅 - 인간 컴퓨터 상호 작용(HCI) - HCI 설계 및 평가 방법 - 사용자 연구.",https://doi.org/10.1109/ISMAR50242.2020.00024,Perception & Cognition,Sensor Fusion,User Study,Algorithm / Method; User Study / Empirical Findings
609,2020,Perspective Matters: Design Implications for Motion Guidance in Mixed Reality,관점의 문제: 혼합 현실의 모션 안내에 대한 설계 의미,"We investigate how Mixed Reality (MR) can be used to guide human body motions, such as in physiotherapy, dancing, or workout applications. While first MR prototypes have shown promising results, many dimensions of the design space behind such applications remain largely unexplored. To better understand this design space, we approach the topic from different angles by contributing three user studies. In particular, we take a closer look at the influence of the perspective, the characteristics of motions, and visual guidance on different user performance measures. Our results indicate that a first-person perspective performs best for all visible motions, whereas the type of visual instruction plays a minor role. From our results we compile a set of considerations that can guide future work on the design of instructions, evaluations, and the technical setup of MR motion guidance systems.","우리는 혼합 현실(MR)을 사용하여 물리 치료, 춤 또는 운동 응용 프로그램과 같은 인체 동작을 안내하는 방법을 조사합니다. 첫 번째 MR 프로토타입이 유망한 결과를 보여줬지만, 이러한 애플리케이션 뒤에 있는 설계 공간의 많은 측면은 아직 대부분 탐구되지 않은 상태로 남아 있습니다. 이 디자인 공간을 더 잘 이해하기 위해 우리는 세 가지 사용자 연구를 제공하여 다양한 각도에서 주제에 접근합니다. 특히 다양한 사용자 성능 측정에 대한 관점의 영향, 동작의 특성, 시각적 안내를 자세히 살펴봅니다. 우리의 결과는 1인칭 관점이 모든 가시적 움직임에 대해 가장 좋은 성능을 발휘하는 반면, 시각적 지시 유형은 작은 역할을 한다는 것을 나타냅니다. 결과를 바탕으로 지침 설계, 평가 및 MR 모션 안내 시스템의 기술 설정에 대한 향후 작업을 안내할 수 있는 일련의 고려 사항을 정리합니다.",https://doi.org/10.1109/ISMAR50242.2020.00085,Interaction & Input,Other,Case Study / Application Demo,Design Guidelines
610,2020,RGB-D-E: Event Camera Calibration for Fast 6-DOF object Tracking,RGB-D-E: 빠른 6-DOF 객체 추적을 위한 이벤트 카메라 보정,"Augmented reality devices require multiple sensors to perform various tasks such as localization and tracking. Currently, popular cameras are mostly frame-based (e.g. RGB and Depth) which impose a high data bandwidth and power usage. With the necessity for low power and more responsive augmented reality systems, using solely frame-based sensors imposes limits to the various algorithms that needs high frequency data from the environement. As such, event-based sensors have become increasingly popular due to their low power, bandwidth and latency, as well as their very high frequency data acquisition capabilities. In this paper, we propose, for the first time, to use an event-based camera to increase the speed of 3D object tracking in 6 degrees of freedom. This application requires handling very high object speed to convey compelling AR experiences. To this end, we propose a new system which combines a recent RGB-D sensor (Kinect Azure) with an event camera (DAVIS346). We develop a deep learning approach, which combines an existing RGB-D network along with a novel event-based network in a cascade fashion, and demonstrate that our approach significantly improves the robustness of a state-of-the-art frame-based 6-DOF object tracker using our RGB-D-E pipeline. Our code and our RGB-D-E evaluation dataset are available at https://github.com/lvsn/rgbde-tracking.","증강 현실 장치는 위치 파악 및 추적과 같은 다양한 작업을 수행하기 위해 여러 센서가 필요합니다. 현재 널리 사용되는 카메라는 대부분 높은 데이터 대역폭과 전력 사용량을 요구하는 프레임 기반(예: RGB 및 심도)입니다. 저전력 및 보다 반응성이 뛰어난 증강 현실 시스템의 필요성으로 인해 프레임 기반 센서만 사용하면 환경의 고주파 데이터가 필요한 다양한 알고리즘에 제한이 발생합니다. 따라서 이벤트 기반 센서는 저전력, 대역폭, 대기 시간은 물론 매우 높은 빈도의 데이터 수집 기능으로 인해 점점 인기를 얻고 있습니다. 본 논문에서는 처음으로 이벤트 기반 카메라를 사용하여 6자유도에서 3D 객체 추적 속도를 높이는 방법을 제안합니다. 이 애플리케이션은 매력적인 AR 경험을 전달하기 위해 매우 빠른 개체 속도를 처리해야 합니다. 이를 위해 최신 RGB-D 센서(Kinect Azure)와 이벤트 카메라(DAVIS346)를 결합한 새로운 시스템을 제안한다. 우리는 기존 RGB-D 네트워크와 새로운 이벤트 기반 네트워크를 계단식 방식으로 결합하는 딥 러닝 접근 방식을 개발하고, 우리의 접근 방식이 RGB-D-E 파이프라인을 사용하여 최첨단 프레임 기반 6-DOF 객체 추적기의 견고성을 크게 향상시킨다는 것을 보여줍니다. 우리의 코드와 RGB-D-E 평가 데이터 세트는 https://github.com/lvsn/rgbde-tracking에서 확인할 수 있습니다.",https://doi.org/10.1109/ISMAR50242.2020.00034,Tracking & Localization,Cloud / Edge Computing,Technical Evaluation,Algorithm / Method; System / Framework
611,2020,Real-Time Adaptive Color Correction in Dynamic Projection Mapping,동적 프로젝션 매핑의 실시간 적응형 색상 교정,"Projection mapping augments a real-world object’s appearance by projecting digital content on its surface. However, a remaining obstacle to immersive projection mapping is the limitation to white Lambertian surfaces and uniform neutral environment light, if any. Violating one of these assumptions results in a discernible difference between the source material and the appearance of the projected content. For example, some colors may not be visible due to intense environment lighting or pronounced surface colors. We present a system that actively subdues many of those real-world influences, especially environment lighting. Our system supports dynamic (i.e., movable) target objects as well as changing lighting conditions while requiring no prior color calibration of the projector nor any precomputed environment probing. We automatically and continuously estimate these influences during runtime in a real-time feedback-loop and adjust the projected colors accordingly.","프로젝션 매핑은 디지털 콘텐츠를 표면에 투사하여 실제 물체의 모양을 강화합니다. 그러나 몰입형 프로젝션 매핑에 남아 있는 장애물은 흰색 Lambertian 표면과 균일한 중립 환경 조명(있는 경우)에 대한 제한입니다. 이러한 가정 중 하나를 위반하면 원본 자료와 투사된 콘텐츠의 모양이 눈에 띄게 달라집니다. 예를 들어, 일부 색상은 강렬한 환경 조명이나 뚜렷한 표면 색상으로 인해 보이지 않을 수 있습니다. 우리는 실제 세계의 많은 영향, 특히 환경 조명을 적극적으로 억제하는 시스템을 제시합니다. 우리 시스템은 프로젝터의 사전 색상 보정이나 미리 계산된 환경 프로빙이 필요하지 않으면서 동적(즉, 이동 가능한) 대상 개체와 변화하는 조명 조건을 지원합니다. 실시간 피드백 루프를 통해 런타임 중에 이러한 영향을 자동으로 지속적으로 추정하고 그에 따라 투영된 색상을 조정합니다.",https://doi.org/10.1109/ISMAR50242.2020.00039,Rendering & Visualization,Optical / Display Technology,Other,System / Framework
612,2020,Rock or Roll - Locomotion Techniques with a Handheld Spherical Device in Virtual Reality,Rock or Roll - 가상 현실에서 휴대용 구형 장치를 사용한 이동 기술,"We investigate the use of a handheld spherical object as a controller for locomotion in VR. Rotating the object controls avatar movement in two different ways: As a zero order controller, it is continuously rotated to the target position as if rolling a ball on the floor. As a first order controller, it is tilted like a joystick to determine the direction and speed of movement. We describe how our prototype was built from low-cost commercially available hardware and discuss our design decisions. Then we evaluate both locomotion techniques in a user study (N=20) and compare them to established methods using handheld VR controllers. Our prototype matched and in some cases outperformed these methods regarding task time and accuracy. All results were obtained without any usage instructions, indicating easy learnability. Some of our insights may transfer to interaction with other naturally shaped objects in VR experiences.",우리는 VR에서 이동을 위한 컨트롤러로 휴대용 구형 개체의 사용을 조사합니다. 객체 회전은 두 가지 방법으로 아바타의 움직임을 제어합니다. 영차 컨트롤러로서 바닥에 공을 굴리는 것처럼 대상 위치로 계속 회전합니다. 1차 컨트롤러로서 조이스틱처럼 기울여 이동 방향과 속도를 결정합니다. 우리는 저렴한 상용 하드웨어로 프로토타입을 제작한 방법을 설명하고 설계 결정에 대해 논의합니다. 그런 다음 사용자 연구(N=20)에서 두 가지 이동 기술을 평가하고 이를 휴대용 VR 컨트롤러를 사용하여 확립된 방법과 비교합니다. 우리의 프로토타입은 작업 시간과 정확성 측면에서 이러한 방법과 일치했고 어떤 경우에는 더 나은 성능을 보였습니다. 모든 결과는 사용 지침 없이 얻은 것이므로 쉽게 배울 수 있음을 나타냅니다. 우리의 통찰력 중 일부는 VR 경험에서 자연적으로 형성된 다른 물체와의 상호 작용으로 전환될 수 있습니다.,https://doi.org/10.1109/ISMAR50242.2020.00089,Interaction & Input,Redirected Walking / Locomotion,User Study,Algorithm / Method; Hardware / Device
613,2020,Scale-aware Insertion of Virtual Objects in Monocular Videos,단안 비디오에 가상 개체의 규모 인식 삽입,"In this paper, we propose a scale-aware method for inserting virtual objects with proper sizes into monocular videos. To tackle the scale ambiguity problem of geometry recovery from monocular videos, we estimate the global scale objects in a video with a Bayesian approach incorporating the size priors of objects, where the scene objects sizes should strictly conform to the same global scale and the possibilities of global scales are maximized according to the size distribution of object categories. To do so, we propose a dataset of sizes of object categories: Metric-Tree, a hierarchical representation of sizes of more than 900 object categories with the corresponding images. To handle the incompleteness of objects recovered from videos, we propose a novel scale estimation method that extracts plausible dimensions of objects for scale optimization. Experiments have shown that our method for scale estimation performs better than the state-of-the-art methods, and has considerable validity and robustness for different video scenes. Metric-Tree has been made available at: https://metric-tree.github.io",본 논문에서는 단안 영상에 적절한 크기의 가상 객체를 삽입하기 위한 규모 인식 방법을 제안합니다. 단안 비디오에서 형상 복구의 규모 모호성 문제를 해결하기 위해 객체의 크기 사전을 통합하는 베이지안 접근 방식을 사용하여 비디오의 전역 규모 객체를 추정합니다. 여기서 장면 객체 크기는 동일한 전역 규모를 엄격히 준수해야 하며 전역 규모의 가능성은 객체 범주의 크기 분포에 따라 최대화됩니다. 이를 위해 우리는 객체 카테고리 크기의 데이터 세트인 Metric-Tree를 제안합니다. 이는 해당 이미지와 함께 900개 이상의 객체 카테고리 크기를 계층적으로 표현한 것입니다. 영상에서 복구된 객체의 불완전성을 처리하기 위해 스케일 최적화를 위해 객체의 그럴듯한 크기를 추출하는 새로운 스케일 추정 방법을 제안합니다. 실험에 따르면 규모 추정을 위한 우리의 방법은 최첨단 방법보다 성능이 뛰어나고 다양한 비디오 장면에 대해 상당한 타당성과 견고성을 가지고 있는 것으로 나타났습니다. Metric-Tree는 https://metric-tree.github.io에서 사용할 수 있습니다.,https://doi.org/10.1109/ISMAR50242.2020.00022,Interaction & Input,Other,Technical Evaluation,Algorithm / Method
614,2020,"Seamless, Bi-directional Transitions along the Reality-Virtuality Continuum: A Conceptualization and Prototype Exploration",현실-가상 연속체를 따른 원활한 양방향 전환: 개념화 및 프로토타입 탐색,"With head mounted displays, consumers are able to transition from the real world to virtual realities. However, this requires frequent transitions between the two realities to maintain their physical integrity and awareness of the real world while in the virtual space. We completed two consecutive studies to investigate the dimensions of a system that supports seamless transition between realities without requiring the user to remove the headset. Our results are twofold: First, based on the the analysis of structured interviews (n=20), we present a conceptualization of existing solutions (n=37) and novel ideas (n=9) in the form of a design space. Second, we present the results of a user study (n=36) in which we tested two exemplary prototypes that evolved from the design space, called “Sky Portal” and “Virtual Phone.” Our exploration shows that our “Virtual Phone” metaphor has the potential to support HMD users in completing bidirectional transitions along Milgram’s reality-virtuality continuum. Users are also enabled to complete micro-interactions across the realities, even without performance loss.","헤드 마운트 디스플레이를 통해 소비자는 현실 세계에서 가상 현실로 전환할 수 있습니다. 그러나 이를 위해서는 가상 공간에 있는 동안 물리적 무결성과 현실 세계에 대한 인식을 유지하기 위해 두 현실 사이를 자주 전환해야 합니다. 우리는 사용자가 헤드셋을 벗지 않고도 현실 사이의 원활한 전환을 지원하는 시스템의 차원을 조사하기 위해 두 가지 연속 연구를 완료했습니다. 우리의 결과는 두 가지입니다. 첫째, 구조화된 인터뷰 분석(n=20)을 기반으로 기존 솔루션(n=37)과 새로운 아이디어(n=9)의 개념을 디자인 공간 형태로 제시합니다. 둘째, 디자인 공간에서 진화한 두 가지 예시적인 프로토타입인 ""스카이 포털""과 ""가상 전화기""를 테스트한 사용자 연구(n=36)의 결과를 제시합니다. 우리의 탐구는 ""가상 전화"" 비유가 HMD 사용자가 Milgram의 현실-가상 연속체를 따라 양방향 전환을 완료하도록 지원할 수 있는 잠재력을 가지고 있음을 보여줍니다. 또한 사용자는 성능 손실 없이도 현실 전반에 걸쳐 마이크로 상호 작용을 완료할 수 있습니다.",https://doi.org/10.1109/ISMAR50242.2020.00067,Interaction & Input; Display & Optics,Optical / Display Technology,User Study,System / Framework
615,2020,Stimulating the Human Visual System Beyond Real World Performance in Future Augmented Reality Displays,미래 증강 현실 디스플레이에서 실제 성능을 뛰어넘는 인간 시각 시스템 자극,"New augmented-reality near-eye displays provide capabilities for enriching real-world visual experiences with digital content. Most current research focuses on improving both hardware and software to provide digital content that seamlessly blends with the real world. This is believed to not only contribute to the visual experience but also increase human task performance. In this work, we take a step further and ask the question of whether the capabilities of current and future display designs combined with efficient perception-inspired content optimizations can be used to improve human task performance beyond the human capabilities in the natural world. Based on an in-depth analysis of previous literature, we hypothesize here that such enhancements can be achieved when the human visual system is provided with content that optimizes the oculomotor responses. To further investigate possible gains, we present a series of perceptual experiments that built upon this idea. More specifically, we focus on speeding up accommodation response, which significantly contributes to the eye-adaptation when a new stimulus is shown. Through our experiments, we demonstrate that such speedups canbe achieved, and more importantly, they can lead to significant improvements in human task performance. While not all of our results give definite answers, we believe that they reveal plentiful opportunities for further enhancing the human experience and task performance when using new augmented-reality displays.","새로운 증강 현실 근거리 디스플레이는 디지털 콘텐츠로 실제 시각적 경험을 풍부하게 하는 기능을 제공합니다. 대부분의 최근 연구는 현실 세계와 원활하게 조화를 이루는 디지털 콘텐츠를 제공하기 위해 하드웨어와 소프트웨어를 모두 개선하는 데 중점을 두고 있습니다. 이는 시각적 경험에 기여할 뿐만 아니라 인간의 작업 수행 능력도 향상시키는 것으로 여겨집니다. 이 작업에서 우리는 한 걸음 더 나아가 효율적인 인식 기반 콘텐츠 최적화와 결합된 현재 및 미래 디스플레이 디자인의 기능을 사용하여 자연 세계에서 인간의 능력을 넘어 인간 작업 성능을 향상시킬 수 있는지에 대한 질문을 던집니다. 이전 문헌에 대한 심층 분석을 바탕으로 우리는 인간 시각 시스템에 안구 운동 반응을 최적화하는 콘텐츠가 제공될 때 이러한 향상이 달성될 수 있다는 가설을 세웠습니다. 가능한 이득을 추가로 조사하기 위해 이 아이디어를 기반으로 한 일련의 지각 실험을 제시합니다. 보다 구체적으로, 우리는 새로운 자극이 나타날 때 눈의 적응에 크게 기여하는 조절 반응의 속도를 높이는 데 중점을 둡니다. 실험을 통해 우리는 이러한 속도 향상이 달성될 수 있으며 더 중요하게는 인간 작업 성능이 크게 향상될 수 있음을 보여줍니다. 모든 결과가 명확한 답변을 제공하는 것은 아니지만 새로운 증강 현실 디스플레이를 사용할 때 인간 경험과 작업 성능을 더욱 향상시킬 수 있는 풍부한 기회를 제시한다고 믿습니다.",https://doi.org/10.1109/ISMAR50242.2020.00029,Display & Optics,Sensor Fusion,Other,System / Framework
616,2020,Studying the Inter-Relation Between Locomotion Techniques and Embodiment in Virtual Reality,가상 현실에서의 이동 기술과 구현 간의 상호 관계 연구,"Virtual embodiment and navigation are two topics widely studied in the virtual reality community. Despite the potential inter-relation between embodiment and locomotion, studies on virtual navigation rarely supply users with a virtual representation, while studies on virtual embodiment rarely allow users to virtually navigate. This paper therefore explores this potential inter-relation by focusing on the two following questions: Does the locomotion technique have an impact on the user’s sense of embodiment? Does embodying an avatar have an impact on the user’s preference and performance depending on the locomotion technique?To address these questions, we conducted a user study (N=60) exploring the relationship between the locomotion technique and the user’s sense of embodiment over a virtual avatar seen from a first-person perspective. Three widely used locomotion techniques were evaluated: real walking, walking-in-place and virtual steering. All participants performed four different tasks involving a different awareness of their virtual avatar. Participants also performed the same tasks without being embodied in an avatar. The results show that participants had a comparable sense of embodiment with all techniques when embodied in an avatar, and that the presence or absence of the virtual avatar did not alter their performance while navigating, independently of the technique. Taken together, our results represent a first attempt to qualify the inter-relation between virtual navigation and virtual embodiment, and suggest that the 3D locomotion technique used has little influence on the user’s sense of embodiment in VR.","가상 구현과 탐색은 가상 현실 커뮤니티에서 널리 연구되는 두 가지 주제입니다. 구현과 이동 사이의 잠재적인 상호 관계에도 불구하고 가상 탐색에 대한 연구는 사용자에게 가상 표현을 거의 제공하지 않는 반면, 가상 구현에 대한 연구에서는 사용자가 가상 ​​탐색을 거의 허용하지 않습니다. 따라서 본 논문에서는 다음 두 가지 질문에 초점을 맞춰 이러한 잠재적인 상호 관계를 탐구합니다. 운동 기술이 사용자의 체현 감각에 영향을 미칩니까? 아바타 구현은 이동 기술에 따라 사용자의 선호도와 성능에 영향을 미치는가? 이러한 질문을 해결하기 위해 우리는 1인칭 시점에서 본 가상 아바타에 대한 이동 기술과 사용자의 구현 감각 사이의 관계를 탐색하는 사용자 연구(N=60)를 수행했습니다. 널리 사용되는 세 가지 이동 기술인 실제 걷기, 제자리 걷기 및 가상 조향이 평가되었습니다. 모든 참가자는 가상 아바타에 대한 다양한 인식과 관련된 네 가지 작업을 수행했습니다. 참가자들도 아바타로 구현되지 않고 동일한 작업을 수행했습니다. 결과는 참가자들이 아바타에 구현되었을 때 모든 기술에 대해 비슷한 구체화 감각을 가지고 있었고, 가상 아바타의 존재 여부가 기술과 관계없이 탐색하는 동안 성능을 변경하지 않았음을 보여줍니다. 종합해보면, 우리의 결과는 가상 내비게이션과 가상 구현 사이의 상호 관계를 검증하려는 첫 번째 시도를 나타내며 사용된 3D 이동 기술이 VR에서 사용자의 구현 감각에 거의 영향을 미치지 않는다는 것을 나타냅니다.",https://doi.org/10.1109/ISMAR50242.2020.00070,Perception & Cognition; Interaction & Input,Redirected Walking / Locomotion,User Study,User Study / Empirical Findings
617,2020,Super Wide-view Optical See-through Head Mounted Displays with Per-pixel Occlusion Capability,픽셀당 폐색 기능을 갖춘 초광각 광학 투명 머리 장착형 디스플레이,"Augmented reality (AR) has been widely used that combines human with the digital world tightly at an unprecedented level, and various types of optical see-through head-mounted displays (OSTHMDs) have been actively developed to meet the requirement of everyday AR use. Correct mutual occlusion between real and virtual objects is often necessary for displaying realistic virtual images for users in the AR application scenarios. Some optical designs have been proposed to realize mutual occlusion by means of an OSTHMD in recent years. However, all of them support a limited field of view (FOV) that is much narrower than that of a natural human. The main limiting factor of the FOV of general OSTHMDs is the limited numerical aperture (NA) of the lenses. To address the problem, we propose an OSTHMD based on the double ellipsoidal mirror structure to avoid a stack of lenses and to achieve a wide FOV close to that of the naked eye with small distortion. A pair of imaging lenses are carefully arranged, then assembled between the two ellipsoidal mirrors with a pinhole mask to improve image quality.An experiment using a monocular prototype shows that a sharp see-through view is achieved which remains clear regardless of the focus of the eye-simulating camera. An image undistortion algorithm is also developed to obtain a full-view display, enabling a virtual image to be displayed spanning a super wide FOV of $H160^{\circ}\times V74^{\circ}$. Finally, per-pixel mutual occlusion of a wide FOV of $H122^{\circ}\times V74^{\circ}$ is realized by placing a spatial light modulator (SLM) in front of the entrance pupil.","인간과 디지털 세계를 전례 없는 수준으로 긴밀하게 결합하는 증강 현실(AR)이 널리 사용되고 있으며, 일상적인 AR 사용 요구 사항을 충족하기 위해 다양한 유형의 광학 투명 헤드 마운트 디스플레이(OSTHMD)가 활발히 개발되었습니다. AR 애플리케이션 시나리오에서 사용자에게 사실적인 가상 이미지를 표시하려면 실제 개체와 가상 개체 간의 올바른 상호 폐색이 필요한 경우가 많습니다. 최근 몇 년 동안 OSTHMD를 통해 상호 폐색을 구현하기 위한 일부 광학 설계가 제안되었습니다. 그러나 이들 모두는 자연인보다 훨씬 좁은 제한된 시야(FOV)를 지원합니다. 일반 OSTHMD의 FOV에 대한 주요 제한 요소는 렌즈의 제한된 NA(개구수)입니다. 이 문제를 해결하기 위해 우리는 렌즈의 스택을 피하고 작은 왜곡으로 육안과 가까운 넓은 FOV를 달성하기 위해 이중 타원형 거울 구조를 기반으로 한 OSTHMD를 제안합니다. 한 쌍의 이미징 렌즈를 조심스럽게 배열한 다음 핀홀 마스크가 있는 두 개의 타원 거울 사이에 조립하여 이미지 품질을 향상시킵니다. 단안 프로토타입을 사용한 실험에서는 눈 시뮬레이션 카메라의 초점에 관계없이 선명하게 유지되는 선명한 투시 뷰가 달성되는 것을 보여줍니다. 풀뷰 디스플레이를 얻기 위한 이미지 왜곡 방지 알고리즘도 개발되어 $H160^{\circ}\times V74^{\circ}$의 초광각 FOV에 걸쳐 가상 이미지를 표시할 수 있습니다. 마지막으로, 입구 동공 앞에 공간 광 변조기(SLM)를 배치하여 $H122^{\circ}\times V74^{\circ}$의 넓은 FOV의 픽셀별 상호 폐색을 구현합니다.",https://doi.org/10.1109/ISMAR50242.2020.00056,Display & Optics; Diminished Reality,Optical / Display Technology,Case Study / Application Demo,Hardware / Device
618,2020,Supporting Medical Auxiliary Work: The Central Sterile Services Department as a Challenging Environment for Augmented Reality Applications,의료 보조 작업 지원: 증강 현실 애플리케이션을 위한 도전적인 환경인 중앙 멸균 서비스 부서,"This paper reports on the central sterile services department (CSSD) as a potential new design space for future research in Human-Computer Interaction (HCI) and Augmented Reality (AR). Within the last 2 years, we explored processes, tools and user needs in this field to identify use cases with the capability of enhancing everyday work using AR head-mounted displays (HMD). The conducted research was focused on the potential of applying AR technology as a proof of concept in which 8 problem-driven use cases were identified. These use cases enable interesting aspects for future investigation and utilization of new technologies. In addition to that, this paper describes the insights into user groups, their tasks, challenges, and needs for work support in this specific domain. Furthermore, a sample application is introduced which demonstrates the possibilities of HMD-based AR in the CSSD.","이 문서에서는 인간-컴퓨터 상호작용(HCI) 및 증강 현실(AR)에 대한 향후 연구를 위한 잠재적인 새로운 설계 공간인 중앙 멸균 서비스 부서(CSSD)에 대해 보고합니다. 지난 2년 동안 우리는 AR 헤드마운트디스플레이(HMD)를 사용하여 일상 업무를 향상시키는 기능이 포함된 사용 사례를 식별하기 위해 이 분야의 프로세스, 도구 및 사용자 요구 사항을 조사했습니다. 수행된 연구는 8가지 문제 중심 사용 사례가 확인된 개념 증명으로 AR 기술을 적용할 수 있는 잠재력에 중점을 두었습니다. 이러한 사용 사례는 새로운 기술의 향후 조사 및 활용에 대한 흥미로운 측면을 가능하게 합니다. 그 외에도 이 문서에서는 이 특정 도메인에서 사용자 그룹, 작업, 과제 및 작업 지원 요구 사항에 대한 통찰력을 설명합니다. 또한 CSSD에서 HMD 기반 AR의 가능성을 보여주는 샘플 애플리케이션이 소개됩니다.",https://doi.org/10.1109/ISMAR50242.2020.00096,Display & Optics,Other,Case Study / Application Demo,Design Guidelines
619,2020,Tangi: Tangible Proxies For Embodied Object Exploration And Manipulation In Virtual Reality,Tangi: 가상 현실에서 구현된 객체 탐색 및 조작을 위한 유형의 프록시,"Exploring and manipulating complex virtual objects is challenging due to limitations of conventional controllers and free-hand interaction techniques. We present the TanGi toolkit which enables novices to rapidly build physical proxy objects using Composable Shape Primitives. TanGi also provides Manipulators allowing users to build objects including movable parts, making them suitable for rich object exploration and manipulation in VR. With a set of different use cases and applications we show the capabilities of the TanGi toolkit and evaluate its use. In a study with 16 participants, we demonstrate that novices can quickly build physical proxy objects using the Composable Shape Primitives and explore how different levels of object embodiment affect virtual object exploration. In a second study with 12 participants we evaluate TanGi’s Manipulators and investigate the effectiveness of embodied interaction. Findings from this study show that TanGi’s proxies outperform traditional controllers and were generally favored by participants.",복잡한 가상 개체를 탐색하고 조작하는 것은 기존 컨트롤러와 자유 상호 작용 기술의 한계로 인해 어렵습니다. 초보자가 Composable Shape Primitives를 사용하여 물리적 프록시 객체를 빠르게 구축할 수 있는 TanGi 툴킷을 소개합니다. TanGi는 또한 사용자가 움직이는 부품을 포함한 객체를 만들 수 있는 매니퓰레이터를 제공하므로 VR에서 풍부한 객체 탐색 및 조작에 적합합니다. 다양한 사용 사례와 응용 프로그램을 통해 TanGi 툴킷의 기능을 보여주고 그 사용을 평가합니다. 16명의 참가자가 참여한 연구에서 우리는 초보자가 Composable Shape Primitives를 사용하여 물리적 프록시 객체를 신속하게 구축하고 다양한 수준의 객체 구현이 가상 객체 탐색에 어떤 영향을 미치는지 탐색할 수 있음을 보여줍니다. 12명의 참가자를 대상으로 한 두 번째 연구에서 우리는 TanGi의 조작기를 평가하고 구체화된 상호 작용의 효과를 조사했습니다. 이 연구 결과에 따르면 TanGi의 프록시는 기존 컨트롤러보다 성능이 뛰어나며 일반적으로 참가자들이 선호하는 것으로 나타났습니다.,https://doi.org/10.1109/ISMAR50242.2020.00042,Interaction & Input,Other,User Study,User Study / Empirical Findings
620,2020,The Cognitive Load and Usability of Three Walking Metaphors for Consumer Virtual Reality,소비자 가상 현실에 대한 세 가지 걷기 은유의 인지 부하 및 유용성,"Walking metaphors have been extensively researched for travel in virtual reality (VR) applications. However, only a few walking metaphors are feasible for most consumer VR systems. In this paper, we present a study that compares three of these suitable metaphors: Scaled Walking, Human Joystick, and Walking-In-Place. Our study empirically assesses the cognitive loads and travel performances of these three walking metaphors by employing a novel dual-task methodology. We also evaluated their effects on simulator sickness, presence, and perceived usability. The results of our study indicate that Scaled Walking afforded significantly better travel performances and perceived usability than Human Joystick and Walking-In-Place. Our results also indicate that Walking-In-Place required the worst cognitive loads and that Human Joystick induced the worst simulator sickness. Given these results, we discuss the implications of using a high-fidelity, full-gait walking metaphor.","가상 현실(VR) 애플리케이션에서 여행에 대한 걷기 은유는 광범위하게 연구되었습니다. 그러나 대부분의 소비자 VR 시스템에서는 몇 가지 걷기 비유만 사용할 수 있습니다. 본 논문에서는 Scaled Walking, Human Joystick 및 Walking-In-Place라는 세 가지 적합한 은유를 비교하는 연구를 제시합니다. 우리의 연구는 새로운 이중 작업 방법론을 사용하여 이 세 가지 걷기 은유의 인지 부하와 여행 성능을 경험적으로 평가합니다. 또한 시뮬레이터 멀미, 존재감 및 인지된 유용성에 대한 효과도 평가했습니다. 우리 연구 결과에 따르면 Scaled Walking은 Human Joystick 및 Walking-In-Place보다 훨씬 더 나은 여행 성능과 인지된 유용성을 제공하는 것으로 나타났습니다. 우리의 결과는 또한 Walking-In-Place가 최악의 인지 부하를 요구했으며 인간 조이스틱이 최악의 시뮬레이터 멀미를 유발했음을 나타냅니다. 이러한 결과를 바탕으로 우리는 충실도가 높은 완전 보행 비유 사용의 의미를 논의합니다.",https://doi.org/10.1109/ISMAR50242.2020.00091,Perception & Cognition,Sensor Fusion,Other,System / Framework
621,2020,The Effects of Body Tracking Fidelity on Embodiment of an Inverse-Kinematic Avatar for Male Participants,남성 참가자의 역운동학적 아바타 구현에 대한 신체 추적 충실도의 효과,"Many research studies have investigated avatar embodiment and its effects on self-location, agency, and body ownership. Researchers have also investigated the effects of various external stimuli and avatar appearances during embodiment. However, the effects of body tracking fidelity while embodying an inverse-kinematic avatar are relatively unexplored. In this paper, we present two studies using a set of six trackers that investigate four levels of body tracking fidelity during avatar embodiment for male participants only: Complete (head, hands, feet, and pelvis trackers), Head-and-Extremities (head, hands, and feet trackers), Head-and-Hands (head and hands trackers), and No-Avatar (head and hands trackers; only controllers visible). Our results indicate that tracking the head, hands, and feet significantly increases the sense of embodiment and the sense of spatial presence when embodying an inverse-kinematic avatar for male participants.","많은 연구에서 아바타 구현과 그것이 자기 위치, 주체 및 신체 소유권에 미치는 영향을 조사했습니다. 연구자들은 또한 구현 중 다양한 외부 자극과 아바타 출현의 영향을 조사했습니다. 그러나 역운동학적 아바타를 구현하는 동안 신체 추적 충실도의 효과는 상대적으로 탐구되지 않았습니다. 본 논문에서는 남성 참가자만을 위한 아바타 구현 중 신체 추적 충실도의 4가지 수준을 조사하는 6개의 추적기 세트를 사용하는 두 가지 연구를 제시합니다. 전체(머리, 손, 발 및 골반 추적기), 머리 및 사지(머리, 손 및 발 추적기), 머리 및 손(머리 및 손 추적기) 및 아바타 없음(머리 및 손 추적기, 컨트롤러만 표시됨). 우리의 결과는 남성 참가자의 역운동학적 아바타를 구현할 때 머리, 손, 발을 추적하면 구체화 감각과 공간적 존재감이 크게 증가한다는 것을 나타냅니다.",https://doi.org/10.1109/ISMAR50242.2020.00025,Perception & Cognition,Sensor Fusion,User Study,User Study / Empirical Findings
622,2020,"The Effects of Object Shape, Fidelity, Color, and Luminance on Depth Perception in Handheld Mobile Augmented Reality","휴대용 모바일 증강현실에서 사물의 모양, 충실도, 색상, 휘도가 깊이지각에 미치는 영향","Depth perception of objects can greatly affect a user’s experience of an augmented reality (AR) application. Many AR applications require depth matching of real and virtual objects and have the possibility to be influenced by depth cues. Color and luminance are depth cues that have been traditionally studied in two-dimensional (2D) objects. However, there is little research investigating how the properties of three-dimensional (3D) virtual objects interact with color and luminance to affect depth perception, despite the substantial use of 3D objects in visual applications. In this paper, we present the results of a paired comparison experiment that investigates the effects of object shape, fidelity, color, and luminance on depth perception of 3D objects in handheld mobile AR. The results of our study indicate that bright colors are perceived as nearer than dark colors for a high-fidelity, simple 3D object, regardless of hue. Additionally, bright red is perceived as nearer than any other color. These effects were not observed for a low-fidelity version of the simple object or for a more-complex 3D object. High-fidelity objects had more perceptual differences than low-fidelity objects, indicating that fidelity interacts with color and luminance to affect depth perception. These findings reveal how the properties of 3D models influence the effects of color and luminance on depth perception in handheld mobile AR and can help developers select colors for their applications.","객체에 대한 깊이 인식은 사용자의 증강 현실(AR) 애플리케이션 경험에 큰 영향을 미칠 수 있습니다. 많은 AR 애플리케이션에는 실제 객체와 가상 객체의 깊이 일치가 필요하며 깊이 단서의 영향을 받을 가능성이 있습니다. 색상과 휘도는 전통적으로 2차원(2D) 객체에서 연구되어 온 깊이 단서입니다. 그러나 시각적 응용 분야에서 3D 개체를 많이 사용함에도 불구하고 3차원(3D) 가상 개체의 속성이 색상 및 휘도와 어떻게 상호 작용하여 깊이 인식에 영향을 미치는지 조사한 연구는 거의 없습니다. 본 논문에서는 핸드헬드 모바일 AR에서 객체의 모양, 충실도, 색상, 휘도가 3D 객체의 깊이 인식에 미치는 영향을 조사한 쌍 비교 실험 결과를 제시합니다. 우리의 연구 결과에 따르면 색상에 관계없이 충실도가 높고 단순한 3D 개체의 경우 밝은 색상이 어두운 색상보다 더 가깝게 인식되는 것으로 나타났습니다. 또한, 밝은 빨간색은 다른 어떤 색상보다 더 가까운 것으로 인식됩니다. 이러한 효과는 단순한 개체의 충실도가 낮은 버전이나 더 복잡한 3D 개체에서는 관찰되지 않았습니다. 충실도가 높은 개체는 충실도가 낮은 개체보다 지각 차이가 더 많았으며, 이는 충실도가 색상 및 휘도와 상호 작용하여 깊이 인식에 영향을 미친다는 것을 나타냅니다. 이러한 연구 결과는 3D 모델의 속성이 휴대용 모바일 AR의 깊이 인식에 대한 색상 및 휘도의 영향에 어떻게 영향을 미치고 개발자가 애플리케이션에 맞는 색상을 선택하는 데 도움이 될 수 있는지를 보여줍니다.",https://doi.org/10.1109/ISMAR50242.2020.00026,Perception & Cognition,Other,Technical Evaluation,User Study / Empirical Findings
623,2020,Towards Eyeglass-style Holographic Near-eye Displays with Statically,정적으로 안경형 홀로그램 근안 디스플레이를 향하여,"Holography is perhaps the only method demonstrated so far that can achieve a wide field of view (FOV) and a compact eyeglass-style form factor for augmented reality (AR) near-eye displays (NEDs). Unfortunately, the eyebox of such NEDs is impractically small ($\sim \lt$ 1 mm). In this paper, we introduce and demonstrate a design for holographic NEDs with a practical, wide eyebox of $\sim$ 10 mm and without any moving parts, based on holographic lenslets. In our design, a holographic optical element (HOE) based on a lenslet array was fabricated as the image combiner with expanded eyebox. A phase spatial light modulator (SLM) alters the phase of the incident laser light projected onto the HOE combiner such that the virtual image can be perceived at different focus distances, which can reduce the vergence-accommodation conflict (VAC). We have successfully implemented a bench-top prototype following the proposed design. The experimental results show effective eyebox expansion to a size of $\sim$ 10 mm. With further work, we hope that these design concepts can be incorporated into eyeglass-size NEDs.",홀로그래피는 아마도 증강 현실(AR) 근안 디스플레이(NED)를 위한 넓은 시야(FOV)와 소형 안경 스타일 폼 팩터를 달성할 수 있는 지금까지 입증된 유일한 방법일 것입니다. 불행하게도 이러한 NED의 아이박스는 비현실적으로 작습니다($\sim \lt$ 1mm). 본 논문에서는 홀로그램 렌즈렛을 기반으로 움직이는 부품이 없고 $\sim$ 10mm의 실용적이고 넓은 아이박스를 갖춘 홀로그램 NED용 설계를 소개하고 시연합니다. 우리의 디자인에서는 렌즈릿 어레이를 기반으로 한 홀로그램 광학 요소(HOE)가 확장된 아이박스를 갖춘 이미지 결합기로 제작되었습니다. 위상 공간 광 변조기(SLM)는 가상 이미지가 다양한 초점 거리에서 인식될 수 있도록 HOE 결합기에 투사된 입사 레이저 광의 위상을 변경하여 VAC(수렴-조절 충돌)를 줄일 수 있습니다. 우리는 제안된 설계에 따라 벤치탑 프로토타입을 성공적으로 구현했습니다. 실험 결과는 $\sim$ 10 mm 크기까지 효과적인 eyebox 확장을 보여줍니다. 추가 작업을 통해 이러한 설계 개념이 안경 크기 NED에 통합될 수 있기를 바랍니다.,https://doi.org/10.1109/ISMAR50242.2020.00057,Display & Optics,Optical / Display Technology,Case Study / Application Demo,Hardware / Device
624,2020,Towards Real-Time Recognition of Users Mental Workload Using Integrated Physiological Sensors Into a VR HMD,VR HMD에 통합된 생리학적 센서를 활용한 사용자의 정신적 부하 실시간 인식을 위해,"This paper describes an “all-in-one” solution for the real-time recognition of users’ mental workloads in virtual reality through the customization of a commercial HMD with physiological sensors. First, we describe the hardware and software solution employed to build the system. Second, we detail the machine learning methods used for the automatic recognition of the users’ mental workload, which are based on the well-known Random Forest algorithm. In order to gather data to train the system, we conducted an extensive user study with 75 participants using a VR flight simulator to induce different levels of mental workload. In contrast to previous works which label the data based on a standardized task (e.g., n-back task) or on a pre-defined task-difficulty, participants were asked about their perceived mental workload level along the experiment. With the data collected, we were able to train the system in order to classify four different levels of mental workload with an accuracy up to 65%. In addition, we discuss the role of the signal normalization procedures, the contribution of the different physiological signals on the recognition accuracy and compare the results obtained with the sensors embedded in the HMD with commercial grade systems. Preliminary results show our pipeline is able to recognize mental workload in real-time. Taken together, our results suggest that such all-in-one approach, with physiological sensors directly embedded in the HMD, is a promising path for VR applications in which the real-time or off-line estimation of Mental Workload assessment is beneficial.","본 논문에서는 생리학적 센서를 갖춘 상업용 HMD의 맞춤화를 통해 가상 현실에서 사용자의 정신적 작업량을 실시간으로 인식하는 ""올인원"" 솔루션에 대해 설명합니다. 먼저 시스템 구축에 사용된 하드웨어 및 소프트웨어 솔루션을 설명합니다. 둘째, 잘 알려진 Random Forest 알고리즘을 기반으로 사용자의 정신적 작업량을 자동으로 인식하는 데 사용되는 기계 학습 방법을 자세히 설명합니다. 시스템 교육을 위한 데이터를 수집하기 위해 우리는 VR 비행 시뮬레이터를 사용하여 다양한 수준의 정신적 작업량을 유도하는 75명의 참가자를 대상으로 광범위한 사용자 연구를 수행했습니다. 표준화된 작업(예: n-back 작업) 또는 사전 정의된 작업 난이도를 기반으로 데이터에 레이블을 지정하는 이전 작업과 달리 참가자는 실험에 따라 인지된 정신적 작업량 수준에 대해 질문을 받았습니다. 수집된 데이터를 통해 우리는 최대 65%의 정확도로 네 가지 수준의 정신 작업 부하를 분류하기 위해 시스템을 훈련할 수 있었습니다. 또한 신호 정규화 절차의 역할, 다양한 생리학적 신호가 인식 정확도에 미치는 영향에 대해 논의하고 HMD에 내장된 센서로 얻은 결과를 상용 등급 시스템과 비교합니다. 예비 결과는 우리 파이프라인이 정신적인 작업량을 실시간으로 인식할 수 있음을 보여줍니다. 종합적으로 말하자면, 우리의 결과는 HMD에 생리학적 센서가 직접 내장된 올인원 접근 방식이 정신 작업량 평가의 실시간 또는 오프라인 추정이 유익한 VR 애플리케이션을 위한 유망한 경로임을 시사합니다.",https://doi.org/10.1109/ISMAR50242.2020.00068,Interaction & Input,Sensor Fusion,User Study,Algorithm / Method; System / Framework
625,2020,Transitioning360: Content-aware NFoV Virtual Camera Paths for 360° Video Playback,Transitioning360: 360° 비디오 재생을 위한 콘텐츠 인식 NFOV 가상 카메라 경로,"Despite the increasing number of head-mounted displays, many 360° VR videos are still being viewed by users on existing 2D displays. To this end, a subset of the 360° video content is often shown inside a manually or semi-automatically selected normal-field-of-view (NFoV) window. However, during the playback, simply watching an NFoV video can easily miss concurrent off-screen content. We present Transitioning360, a tool for 360° video navigation and playback on 2D displays by transitioning between multiple NFoV views that track potentially interesting targets or events. Our method computes virtual NFoV camera paths considering content awareness and diversity in an offline preprocess. During playback, the user can watch any NFoV view corresponding to a precomputed camera path. Moreover, our interface shows other candidate views, providing a sense of concurrent events. At any time, the user can transition to other candidate views for fast navigation and exploration. Experimental results including a user study demonstrate that the viewing experience using our method is more enjoyable and convenient than previous methods.","헤드 마운트 디스플레이의 수가 증가함에도 불구하고 사용자는 여전히 기존 2D 디스플레이를 통해 많은 360° VR 비디오를 시청하고 있습니다. 이를 위해 360° 비디오 콘텐츠의 하위 집합이 수동 또는 반자동으로 선택된 일반 시야(NFoV) 창 내부에 표시되는 경우가 많습니다. 그러나 재생 중에 단순히 NFOV 비디오를 시청하는 것만으로도 동시 오프스크린 콘텐츠를 쉽게 놓칠 수 있습니다. 잠재적으로 흥미로운 대상이나 이벤트를 추적하는 여러 NFOV 뷰 간을 전환하여 360° 비디오 탐색 및 2D 디스플레이 재생을 위한 도구인 Transitioning360을 소개합니다. 우리의 방법은 오프라인 전처리에서 콘텐츠 인식과 다양성을 고려하여 가상 NFOV 카메라 경로를 계산합니다. 재생 중에 사용자는 미리 계산된 카메라 경로에 해당하는 모든 NFOV 보기를 볼 수 있습니다. 또한, 우리의 인터페이스는 동시 이벤트에 대한 감각을 제공하는 다른 후보 뷰를 표시합니다. 언제든지 사용자는 빠른 탐색 및 탐색을 위해 다른 후보 보기로 전환할 수 있습니다. 사용자 연구를 포함한 실험 결과는 우리의 방법을 사용한 시청 경험이 이전 방법보다 더 즐겁고 편리하다는 것을 보여줍니다.",https://doi.org/10.1109/ISMAR50242.2020.00040,Display & Optics,Other,User Study,Algorithm / Method
626,2020,View Splicing for Effective VR Collaboration,효과적인 VR 협업을 위한 뷰 스플라이싱,"In a co-located multi-user collaborative virtual reality (VR) application a collaborator should be able to indicate a workspace location to the user, such that they can refer to it simultaneously as they work together. Due to their different viewpoints, the collaborator sees some parts of the virtual environment (VE) that the user does not, and communication breaks down when the collaborator’s reference is not visible to the user. The conventional solutions of asking the user to move around to gain line of sight to the collaborator’s reference, or of asking the user to toggle back and forth between their view and that of the collaborator can be inefficient and ineffective. This paper proposes a method for improving collaboration in VR by alleviating the disparity between the user and the collaborator views of the VE. The user is shown a multiperspective visualization of the VE that transitions smoothly from the user to the collaborator’s perspective. The multiperpsective visualization is based on the switch camera, a novel camera model with curved rays that splice together the user and collaborator views. The multiperspective visualization is computed first by warping the VE geometry, through projection with the switch camera followed by unprojection with a conventional camera, and then by rendering the warped VE conventionally, for each user eye. A controlled user study with three tasks shows that VR collaboration using the switch camera multiperpsective visualization is faster, more reliable, and less taxing on the user than the conventional approaches of viewpoint translation or view toggling.","함께 배치된 다중 사용자 협업 가상 현실(VR) 애플리케이션에서 공동 작업자는 사용자가 함께 작업하면서 동시에 참조할 수 있도록 작업 공간 위치를 사용자에게 표시할 수 있어야 합니다. 서로 다른 관점으로 인해 협력자는 사용자가 보지 못하는 가상 환경(VE)의 일부 부분을 보게 되며, 협력자의 참조가 사용자에게 표시되지 않으면 의사소통이 중단됩니다. 사용자에게 공동작업자의 참조를 보기 위해 이동하도록 요청하거나 사용자에게 자신의 보기와 공동작업자의 보기 사이를 앞뒤로 전환하도록 요청하는 기존 솔루션은 비효율적이고 비효율적일 수 있습니다. 본 논문에서는 VE의 사용자와 협력자 관점의 차이를 완화하여 VR에서 협업을 향상시키는 방법을 제안합니다. 사용자에게는 사용자 관점에서 공동작업자의 관점으로 원활하게 전환되는 VE의 다관점 시각화가 표시됩니다. 다중 관점 시각화는 사용자와 공동 작업자 보기를 결합하는 곡선 광선이 있는 새로운 카메라 모델인 스위치 카메라를 기반으로 합니다. 다중 관점 시각화는 먼저 스위치 카메라를 사용한 투영과 기존 카메라를 사용한 비투영을 통해 VE 형상을 워핑한 다음 각 사용자 눈에 대해 워핑된 VE를 전통적으로 렌더링하여 계산됩니다. 세 가지 작업을 수행하는 통제된 사용자 연구에서는 스위치 카메라 다중 관점 시각화를 사용한 VR 협업이 기존의 관점 변환 또는 뷰 전환 접근 방식보다 더 빠르고 안정적이며 사용자에게 부담을 덜 주는 것으로 나타났습니다.",https://doi.org/10.1109/ISMAR50242.2020.00079,Collaboration & Social; Rendering & Visualization,Sensor Fusion,User Study,Algorithm / Method
627,2020,Virtual Reality Racket Sports: Virtual Drills for Exercise and Training,가상 현실 라켓 스포츠: 운동 및 훈련을 위한 가상 훈련,"We have developed a modular virtual reality gaming application that can be used to synthesize exercise drills for racket sports. By defining cost terms that are related to the gameplay and the mechanics of the game, as well as by allowing a user to control the parameters of the cost terms, users can easily adjust the objectives and the intensity levels of the exercise drills. Based on the user-defined exercise objectives, a Markov chain Monte Carlo optimization method called “simulated annealing” was used to optimize the exercise drill. The effectiveness of the developed virtual reality gaming application was measured in two studies by using virtual reality table tennis as the evaluation tool. The first study investigated the potential usefulness of the developed virtual reality gaming application as an exercise tool by comparing its workout effectiveness at three intensity levels (low, medium, and high) through the collection of heart rate readings. The second study explored the potential utility of the virtual reality gaming application as a training tool by exploring whether there was any improvement in participants’ performance across the three conditions (no training, virtual reality training, and real-world training). The results indicate that a virtual reality gaming application, such as the examined virtual reality table tennis exergame, could indeed be used effectively as both an exercise and a training tool. Limitations and future research directions are discussed further below.","We have developed a modular virtual reality gaming application that can be used to synthesize exercise drills for racket sports. 게임플레이 및 게임 메커니즘과 관련된 비용 조건을 정의하고 사용자가 비용 조건의 매개변수를 제어할 수 있도록 함으로써 사용자는 운동 훈련의 목표와 강도 수준을 쉽게 조정할 수 있습니다. 사용자가 정의한 운동 목표를 기반으로 ""simulated annealing""이라는 Markov chain Monte Carlo 최적화 방법을 사용하여 훈련을 최적화했습니다. 개발된 가상 현실 게임 애플리케이션의 효과는 가상 현실 탁구를 평가 도구로 사용하여 두 가지 연구에서 측정되었습니다. 첫 번째 연구에서는 심박수 측정값 수집을 통해 세 가지 강도 수준(낮음, 중간, 높음)에서 운동 효과를 비교함으로써 개발된 가상 현실 게임 애플리케이션이 운동 도구로서 잠재적인 유용성을 조사했습니다. 두 번째 연구에서는 세 가지 조건(훈련 없음, 가상 현실 훈련, 실제 훈련)에서 참가자의 성과가 향상되었는지 여부를 조사하여 훈련 도구로서 가상 현실 게임 애플리케이션의 잠재적 유용성을 조사했습니다. 결과는 검토된 가상현실 탁구 엑서게임과 같은 가상현실 게임 애플리케이션이 실제로 운동 및 훈련 도구로서 효과적으로 사용될 수 있음을 나타냅니다. 제한 사항과 향후 연구 방향은 아래에서 자세히 설명합니다.",https://doi.org/10.1109/ISMAR50242.2020.00084,Education & Training,Sensor Fusion,Simulation,Algorithm / Method
628,2020,Virtual reality sickness detection: an approach based on physiological signals and machine learning,가상 현실 질병 감지: 생리적 신호와 기계 학습을 기반으로 한 접근 방식,"Virtual Reality (VR) is spreading to the general public but still has a major issue: VR sickness. To take it into consideration and minimize its occurrence, evaluation methods are required. The current methods are mainly based on subjective measurements and therefore have several drawbacks (e.g., non-continuous, intrusive). Physiological signals combined with Machine Learning (ML) methods seem an interesting approach to go beyond these limits. In this paper, we present a large-scale experimentation (103 participants) where physiological data (cardiac and electrodermal activities) and subjective data (perceived VR sickness) were gathered during 30-minute VR video game sessions. Using ML methods, models were trained to predict VR sickness level (based on the physiological data labeled with the subjective data). Results showed an explained variance up to 75% (in a regression approach) and an accuracy up to 91% (in a classification approach). Despite generalization issues, this method seems promising and valuable for a real time, automatic and continuous evaluation of VR sickness, based on physiological signals and ML models.","VR(가상현실)이 일반 대중에게 확산되고 있지만 여전히 VR 멀미라는 큰 문제가 남아 있습니다. 이를 고려하고 발생을 최소화하기 위해서는 평가방법이 필요하다. 현재 방법은 주로 주관적인 측정을 기반으로 하므로 몇 가지 단점(예: 비연속적, 방해적)이 있습니다. 머신러닝(ML) 방법과 결합된 생리학적 신호는 이러한 한계를 뛰어넘는 흥미로운 접근 방식으로 보입니다. 본 논문에서는 30분 VR 비디오 게임 세션 동안 생리학적 데이터(심장 및 피부 전기 활동)와 주관적 데이터(인지된 VR 질병)를 수집한 대규모 실험(참가자 103명)을 제시합니다. ML 방법을 사용하여 VR 질병 수준을 예측하도록 모델을 훈련했습니다(주관적 데이터로 라벨이 지정된 생리학적 데이터를 기반으로 함). 결과는 최대 75%(회귀 접근 방식)의 설명된 분산과 최대 91%(분류 접근 방식)의 정확도를 보여주었습니다. 일반화 문제에도 불구하고 이 방법은 생리학적 신호 및 ML 모델을 기반으로 VR 질병을 실시간, 자동 및 지속적으로 평가하는 데 유망하고 가치 있는 것으로 보입니다.",https://doi.org/10.1109/ISMAR50242.2020.00065,Interaction & Input,Optical / Display Technology,Quantitative Experiment,Algorithm / Method
629,2020,Visual-Auditory Redirection: Multimodal Integration of Incongruent Visual and Auditory Cues for Redirected Walking,시각-청각 방향 전환: 방향 전환을 위한 부적합한 시각 및 청각 단서의 다중 모드 통합,"In this paper, we present a study of redirected walking (RDW) that shifts the positional relationship between visual and auditory cues during curvature manipulation. It has been shown that, when presented with incongruent visual and auditory spatial cues during a localization task, human observers integrate that information based on each cue’s relative reliability, which determines their final perception of the target object’s location. This multi-modal integration model is known as maximum likelihood estimation (MLE). By altering the visual location of objects that users perceive in virtual reality (VR) through auditory cues during redirection manipulation, we expect fewer users to notice the manipulation, which helps increase the usable curvature gain. Most existing studies on MLE in multi-modal integration have used random-dot stereograms as visual cues under stable motion states. In the present study, we first investigated whether this model holds while walking in VR environment. Our results indicate that in a walking state, users’ perceptions of the target object’s location shift toward auditory cue as the reliability of vision decreases, in keeping with the trend shown in previous studies on MLE. Based on this result, we then investigated the detection threshold of curvature gains during redirection manipulation under a condition with congruent visual-auditory cues as well as a condition in which users’ location perceptions of the target object are considered to be affected by the incongruent auditory cue. We found that the detection threshold of curvature gains was higher with incongruent visual-auditory cues than with congruent cues. These results show that incongruent multimodal cues in VR may have a promising application in the area of redirected walking.","본 논문에서는 곡률 조작 중에 시각 신호와 청각 신호 사이의 위치 관계를 이동시키는 방향 전환(RDW)에 대한 연구를 제시합니다. 현지화 작업 중에 불일치한 시각적 및 청각적 공간 단서가 제시되면 인간 관찰자는 각 단서의 상대적 신뢰성을 기반으로 해당 정보를 통합하여 대상 개체의 위치에 대한 최종 인식을 결정하는 것으로 나타났습니다. 이 다중 모드 통합 모델은 최대 우도 추정(MLE)으로 알려져 있습니다. 리디렉션 조작 중 청각 신호를 통해 사용자가 가상 ​​현실(VR)에서 인식하는 객체의 시각적 위치를 변경함으로써 조작을 알아차리는 사용자가 줄어들 것으로 예상되며, 이는 사용 가능한 곡률 이득을 높이는 데 도움이 됩니다. 다중 모드 통합에서 MLE에 대한 대부분의 기존 연구는 안정적인 동작 상태에서 시각적 신호로 무작위 점 입체도를 사용했습니다. 본 연구에서 우리는 먼저 이 모델이 VR 환경에서 걷는 동안 유지되는지 여부를 조사했습니다. 우리의 결과는 걷는 상태에서 MLE에 대한 이전 연구에서 나타난 추세와 일치하여 시력의 신뢰성이 감소함에 따라 대상 물체의 위치에 대한 사용자의 인식이 청각 신호 쪽으로 이동한다는 것을 나타냅니다. 이 결과를 바탕으로 우리는 일치하는 시-청각 단서가 있는 조건과 사용자의 대상 객체에 대한 위치 인식이 부조화한 청각 단서의 영향을 받는 것으로 간주되는 조건에서 방향 전환 조작 중 곡률 이득의 감지 임계값을 조사했습니다. 우리는 곡률 이득의 감지 임계값이 합동 단서보다 부적합한 시각-청각 단서에서 더 높다는 것을 발견했습니다. 이러한 결과는 VR의 부적합한 다중 모드 큐가 방향 전환된 걷기 영역에서 유망한 응용 프로그램을 가질 수 있음을 보여줍니다.",https://doi.org/10.1109/ISMAR50242.2020.00092,Perception & Cognition,Redirected Walking / Locomotion,Other,Algorithm / Method
630,2020,Walking and Teleportation in Wide-area Virtual Reality Experiences,광역 가상현실 체험에서의 걷기와 순간이동,"Location-based or Out-of-Home Entertainment refers to experiences such as theme and amusement parks, laser tag and paintball arenas, roller and ice skating rinks, zoos and aquariums, or science centers and museums among many other family entertainment and cultural venues. More recently, location-based VR has emerged as a new category of out-of-home entertainment. These VR experiences can be likened to social entertainment options such as laser tag, where physical movement is an inherent part of the experience versus at-home VR experiences where physical movement often needs to be replaced by artificial locomotion techniques due to tracking space constraints. In this work, we present the first VR study to understand the impact of natural walking in a large physical space on presence and user preference. We compare it with teleportation in the same large space, since teleportation is the most commonly used locomotion technique for consumer, at-home VR. Our results show that walking was overwhelmingly preferred by the participants and teleportation leads to significantly higher self-reported simulator sickness. The data also shows a trend towards higher self-reported presence for natural walking.","위치 기반 또는 옥외 엔터테인먼트는 다양한 가족 엔터테인먼트 및 문화 장소 중에서 테마 공원, 놀이 공원, 레이저 태그 및 페인트볼 경기장, 롤러 및 아이스 스케이트장, 동물원 및 수족관, 과학 센터 및 박물관과 같은 경험을 의미합니다. 최근에는 위치 기반 VR이 옥외 엔터테인먼트의 새로운 카테고리로 떠오르고 있습니다. 이러한 VR 경험은 물리적 움직임이 경험의 본질적인 부분인 레이저 태그와 같은 소셜 엔터테인먼트 옵션에 비유될 수 있으며, 추적 공간 제약으로 인해 물리적 움직임이 종종 인공 이동 기술로 대체되어야 하는 가정 내 VR 경험과 비교할 수 있습니다. 이 연구에서는 넓은 물리적 공간에서 자연스러운 걷기가 존재감과 사용자 선호도에 미치는 영향을 이해하기 위한 최초의 VR 연구를 제시합니다. 텔레포트는 소비자, 재택 VR에 가장 일반적으로 사용되는 이동 기술이기 때문에 동일한 넓은 공간에서의 순간 이동과 비교합니다. 우리의 결과는 걷기가 참가자들에 의해 압도적으로 선호되었으며 순간 이동이 자가 보고된 시뮬레이터 멀미로 이어진다는 것을 보여줍니다. 또한 데이터는 자연스러운 걷기에 대해 자체 보고된 존재감이 높아지는 추세를 보여줍니다.",https://doi.org/10.1109/ISMAR50242.2020.00088,Perception & Cognition,Redirected Walking / Locomotion,Questionnaire / Survey,Algorithm / Method
